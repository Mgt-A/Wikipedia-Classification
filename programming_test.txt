a lisp is a speech impediment historically also known as sigmatism stereotypically people with a lisp are unable to pronounce sibilants and replace them with interdentals though there are actually several kinds of lisps the result is that the speech is unclear the cause of a lisp can vary in some instances the cause is physiological and the patient has some sort of deformity or medical condition which causes a lisp for example a child with swollen adenoids may tend to lisp as will people who have recurring stuffy noses more commonly a lisp appears to be psychological in origin and lisps often emerge as a reaction to stress children may start lisping for example to gain attention or someone may develop a lisp after a traumatic incident treating lisps in children usually involves short term speech therapy and is generally successful speech therapy sessions include a wide variety of activities and speech drills though what specifically happens in any given session will depend upon many variables the length of the therapy session the location of the therapy session the age of the child involved whether the therapy session is private or involves a group and the type of lisp that is being treated will all affect the content of these sessions during these sessions the child will be taught the isolated sound that he or she is having trouble with when this sound is mastered the child will then learn to say the sound in syllables then words then phrases and then sentences when a child is able to speak a whole sentence without lisping attention is then focused on making correct sounds throughout natural conversation towards then end of the course of therapy the child will be taught how to monitor his or her own speech and how to correct as necessary 
in computer software an application binary interface describes the low level interface between an application program and the operating system or another application abis cover details such as data type size and alignment the calling convention which controls how functions arguments are passed and return values retrieved the system call numbers and how an application should make system calls to the operating system and in the case of a complete operating system abi the binary format of object files program libraries and so on a complete abi such as the intel binary compatibility standard allows a program from one operating system supporting that abi to run without modifications on any other such system provided that necessary shared libraries are present and similar prerequisites are fulfilled other abis standardize details such as the c name decoration exception propagation and calling convention between compilers on the same platform but do not require cross platform compatibility an abi should not be confused with an application programming interface which defines a library of routines to call data structures to manipulate and or object classes to use in the construction of an application using that particular api an embedded application binary interface specifies standard conventions for file formats data types register usage stack frame organization and function parameter passing of an embedded software program compilers that support the eabi create object code that is compatible with code generated by other such compilers thus allowing developers to link libraries generated with one compiler with object code generated with a different compiler developers writing their own assembly language code may also use the eabi to interface with assembly generated by a compliant compiler the main differences of an eabi with respect to an abi for general purpose operating systems are that privileged instructions are allowed in application code dynamic linking is not required and a more compact stack frame organization to save memory widely used eabis include powerpc and arm 
in computer science a is a best first graph search algorithm that finds the least cost path from a given initial node to one goal node it uses a distance plus cost heuristic function to determine the order in which the search visits nodes in the tree the distance plus cost heuristic is a sum of two functions the path cost function which may or may not be a heuristic and an admissible heuristic estimate of the distance to the goal the path cost function g is the cost from the starting node to the current node since the h part of the f function must be an admissible heuristic it must not overestimate the distance to the goal thus for an application like routing h might represent the straight line distance to the goal since that is physically the smallest possible distance between any two points the algorithm was first described in 1968 by peter hart nils nilsson and bertram raphael in their paper it was called algorithm a since using this algorithm yields optimal behavior for a given heuristic it has been called a this algorithm has been generalized into a bidirectional heuristic search algorithm see bidirectional search like all informed search algorithms it first searches the routes that appear to be most likely to lead towards the goal what sets a apart from a greedy best first search is that it also takes the distance already traveled into account part of the heuristic is the cost from the start and not simply the local cost from the previously expanded node the algorithm traverses various paths from start to goal for each node x traversed it maintains 3 values starting with the initial node it maintains a priority queue of nodes to be traversed known as the open set the lower f for a given node x the higher its priority at each step of the algorithm the node with the lowest f value is removed from the queue the f and h values of its neighbors are updated accordingly and these neighbors are added to the queue the algorithm continues until a goal node has a lower f value than any node in the queue the f value of the goal is then the length of the shortest path since h at the goal is zero in an admissible heuristic if the actual shortest path is desired the algorithm may also update each neighbor with its immediate predecessor in the best path found so far this information can then be used to reconstruct the path by working backwards from the goal node additionally if the heuristic is monotonic a closed set of nodes already traversed may be used to make the search more efficient the closed set can be omitted if a solution is guaranteed to exist or if the algorithm is adapted so that new nodes are added to the open set only if they have a lower f value than at any previous iteration an example of a star algorithm in action is the straight line distance to target point green start blue target orange visitedlike breadth first search a is complete in the sense that it will always find a solution if there is one if the heuristic function h is admissible meaning that it never overestimates the actual minimal cost of reaching the goal then a is itself admissible if we do not use a closed set if a closed set is used then h must also be monotonic for a to be optimal this means that for any pair of adjacent nodes x and y where d denotes the length of the edge between them we must have this ensures that for any path x from the initial node to x where denotes the length of a path and y is the path x extended to include y in other words it is impossible to decrease by extending a path to include a neighboring node monotonicity implies admissibility when the heuristic estimate at any goal node itself is zero since a is also optimally efficient for any heuristic h meaning that no algorithm employing the same heuristic will expand fewer nodes than a except when there are multiple partial solutions where h exactly predicts the cost of the optimal path even in this case for each graph there exists some order of breaking ties in the priority queue such that a examines the fewest possible nodes generally speaking depth first search and breadth first search are two special cases of a algorithm dijkstra s algorithm as another example of a best first search algorithm is the special case of a where h 0 for all x for depth first search we may consider that there is a global counter c initialized with a very big value every time we process a node we assign c to all of its newly discovered neighbors after each single assignment we decrease the counter c by one thus the earlier a node is discovered the higher its h value there are a number of simple optimizations or implementation details that can significantly affect the performance of an a implementation the first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations if ties are broken so the queue behaves in a lifo manner a will behave like depth first search among equal cost paths if ties are broken so the queue behaves in a fifo manner a will behave like breadth first search among equal cost paths when a path is required at the end of the search it is common to keep with each node a reference to that node s parent at the end of the search these references can be used to recover the optimal path if these references are being kept then it can be important that the same node doesn t appear in the priority queue more than once a standard approach here is to check if a node about to be added already appears in the priority queue if it does then the priority and parent pointers are changed to correspond to the lower cost path when finding a node in a queue to perform this check many standard implementations of a min heap require o time augmenting the heap with a hash table can reduce this to constant time a is both admissible and considers fewer nodes than any other admissible search algorithm with the same heuristic because a works from an optimistic estimate of the cost of a path through every node that it considers optimistic in that the true cost of a path through that node to the goal will be at least as great as the estimate but critically as far as a knows that optimistic estimate might be achievable when a terminates its search it has by definition found a path whose actual cost is lower than the estimated cost of any path through any open node but since those estimates are optimistic a can safely ignore those nodes in other words a will never overlook the possibility of a lower cost path and so is admissible suppose now that some other search algorithm b terminates its search with a path whose actual cost is not less than the estimated cost of a path through some open node algorithm b cannot rule out the possibility based on the heuristic information it has that a path through that node might have a lower cost so while b might consider fewer nodes than a it cannot be admissible accordingly a considers the fewest nodes of any admissible search algorithm that uses a no more accurate heuristic estimate this is only true when a uses a consistent heuristic otherwise a is not guaranteed to expand fewer nodes than another search algorithm with the same heuristic see the time complexity of a depends on the heuristic in the worst case the number of nodes expanded is exponential in the length of the solution but it is polynomial when the search space is a tree there is a single goal state and the heuristic function h meets the following condition where h is the optimal heuristic i e the exact cost to get from x to the goal in other words the error of h should not grow faster than the logarithm of the perfect heuristic h that returns the true distance from x to the goal and also russell and norvig 2003 p 101 more problematic than its time complexity is a s memory usage in the worst case it must also remember an exponential number of nodes several variants of a have been developed to cope with this including iterative deepening a memory bounded a and simplified memory bounded a and recursive best first search 
a user agent is the client application used with a particular network protocol the phrase is most commonly used in reference to those which access the world wide web other systems such as session initiation protocol use the term user agent to refer to both end points of a phone call server and client web user agents range from web browsers and e mail clients to search engine crawlers as well as mobile phones screen readers and braille browsers used by people with disabilities when internet users visit a web site a text string is generally sent to identify the user agent to the server this forms part of the http request prefixed with user agent and typically includes information such as the application name version host operating system and language bots such as web crawlers often also include a url and or e mail address so that the webmaster can contact the operator of the bot the user agent string is one of the criteria by which web crawlers can be excluded from certain pages or parts of a website using the robots exclusion standard this allows webmasters who feel that certain parts of their website should not be included in the data gathered by a particular crawler or that a particular crawler is using up too much bandwidth to request that crawler not to visit those pages at various points in its history use of the web has been dominated by one browser to the extent that many websites are designed to work only with that particular browser rather than according to standards from bodies such as the w3c and ietf such sites often include browser sniffing code which alters the information sent out depending on the user agent string received this can mean that less popular browsers are not sent complex content even though they might be able to deal with it correctly or in extreme cases refused all content thus various browsers cloak or spoof this string in order to identify themselves as something else to such detection code often the browser s real identity is then included later in the string the earliest example of this is internet explorer s use of a user agent string beginning mozilla version then msie and finally the actual browser such as opera opera also offers a full masking as internet explorer or firefox which hides opera completely beside browsers other programs using the http protocol like most download managers and offline browsers also had the ability to change the user agent string sent to servers to user s liking this is presumably done in an effort to maintain compatibility with certain servers some web developers have started a viewable with any browser campaign which encourages developers to design webpages that work regardless of the browser used one result of user agent spoofing is that the usage share of internet explorer the user agent browsers typically spoof is probably overestimated and the usage share of other browsers may be underestimated user agent spoofing can also provide a security issue by spoofing search engine bots and bypassing key parts in a website the term user agent sniffing refers to the practice of websites showing different content when viewed with a certain user agent on the internet this will result in a different site being shown when browsing the page with a specific browser an infamous example of this is microsoft exchange server 2003 s outlook web access feature when viewed with ie much more functionality is displayed compared to the same page in any other browser user agent sniffing is mostly considered poor practice for web 2 0 web sites since it encourages browser specific design many webmasters are recommended to create an html markup that is as standardised as possible to allow correct rendering in as many browsers as possible websites specifically targeted towards mobile phones like ntt docomo s i mode or vodafone s vodafone live portals often rely heavily on user agent sniffing since mobile browsers often differ greatly from each other many developments in mobile browsing have been made in the last few years while many older phones that do not possess these new technologies are still heavily used therefore mobile webportals will often generate completely different markup code depending on the mobile phone used to browse them these differences can be small or quite extensive there are a number of ways to perform user agent sniffing within web applications including using public domain scripts and commercial products web browsers created in the united states such as netscape navigator internet explorer and some others use one of these three letters to specify the browser s encryption strength since the us government formerly would not allow encryption higher than 40 bit to be exported from the country different versions were released with different encryption strengths u stands for usa  i stands for international  n stands for none originally the u version of such browsers was only available for download to those in the usa the us government has since loosened its policy and exporting high encryption is now permitted to most countries now netscape and mozilla distribute their browsers only in a u version supporting up to 256 bit encryption since an international version is no longer required this information can be seen in mozilla firefox browser by typing about config and searching for the string general useragent security 
ws security is a communications protocol providing a means for applying security to web services on april 19 2004 the ws security 1 0 standard was released by oasis open on february 17 2006 they released version 1 1 originally developed by ibm microsoft and verisign the protocol is now officially called wss and developed via committee in oasis open the protocol contains specifications on how integrity and confidentiality can be enforced on web services messaging the wss protocol includes details on the use of saml and kerberos and certificate formats such as x 509 ws security describes how to attach signatures and encryption headers to soap messages in addition it describes how to attach security tokens including binary security tokens such as x 509 certificates and kerberos tickets to messages ws security incorporates security features in the header of a soap message working in the application layer thus it ensures end to end security the following draft specifications are associated with ws security in point to point situations confidentiality and data integrity can also be enforced on web services through the use of transport layer security for example by sending messages over https ws security however addresses the wider problem of maintaining integrity and confidentiality of messages until after a message was sent from the originating node providing so called end to end security applying tls can significantly reduce the overhead involved by removing the need to encode keys and message signatures into ascii before sending a challenge in using tls would be if messages needed to go through a proxy server as it would need to be able to see the request for routing in such an example the server would see the request coming from the proxy not the client this could be worked around by having the proxy have a copy of the client s key and certificate or by having a signing certificate trusted by the server with which it could generate a key certificate pair matching those of the client however as the proxy is operating on the message it does not ensure end to end security but only ensures point to point security 
the java remote method invocation api or java rmi a java application programming interface performs the object oriented equivalent of remote procedure calls two common implementations of the api exist usage of the term rmi may denote solely the programming interface or may signify both the api and jrmp whereas the term rmi iiop denotes the rmi interface delegating most of the functionality to the supporting corba implementation the programmers of the original rmi api generalized the code somewhat to support different implementations such as an http transport additionally work was done to corba adding a pass by value capability to support the rmi interface still the rmi iiop and jrmp implementations do not have fully identical interfaces rmi functionality comes in the package java rmi while most of sun s implementation is located in the sun rmi package note that with java versions before java 5 0 developers had to compile rmi stubs in a separate compilation step using rmic version 5 0 of java and beyond no longer require this step jini offers a more advanced version of rmi in java it functions similarly but provides more advanced searching capabilities and mechanisms for distributed object applications 
java business integration is a specification developed under the java community process for an approach to implementing a service oriented architecture the jcp reference is jsr 208 for jbi 1 0 and jsr 312 for jbi 2 0 jbi is built on a web services model and provides a pluggable architecture for a container that hosts service producer and consumer components services connect to the container via binding components or can be hosted inside the container as part of a service engine the services model used is web services description language 2 0 the central message delivery mechanism the normalized message router delivers normalized messages via one of four message exchange patterns taken from wsdl 2 0 to handle functionality that deals with installation deployment monitoring and lifecycle concerns amongst bcs and ses java management extensions is used jbi defines standardized packaging for bcs and ses allowing components to be portable to any jbi implementation without modification jbi defines standard packaging for composite applications applications that are composed of service consumers and providers individual service units are deployable to components groups of service units are gathered together into a service assembly the service assembly includes metadata for wiring the service units together as well as wiring service units to external services this provides a simple mechanism for performing composite application assembly using services the following are open source software jbi based esb implementations available open source jbi implementations certified by the tck are sun open esb and ow2 petals in addition the project glassfish open source java ee application server comes with the jbi runtime from the open esb project java ee sdk also includes the jbi runtime and a bpel orchestration engine oracle claims its fusion middleware comes with jbi implementation tibco activematrix service grid is a service virtualization product that provides a service container framework based on the jsr 208 and sca specifications allowing service containers to be added as needed as composite applications on a common foundation the open jbi components project on java net is an incubator project started to foster community based development of jbi components that conform to the java business integration specification 
in computing virtualization is a broad term that refers to the abstraction of computer resources virtualization can also refer to physicalization is the converse approach to virtualized data centers where instead of allocating workloads across subdivided single machine virtualized resources the hardware is organized as replicated sets of simple atomic compute resources that are not virtualized external control software dispatches workloads to these individual atomic compute nodes in much the same way as a vmm manages workload placement in a virtualized system 
the bresenham line algorithm is an algorithm which determines which points in an n dimensional raster should be plotted in order to form a close approximation to a straight line between two given points it is commonly used to draw lines on a computer screen as it uses only integer addition subtraction and bit shifting all of which are very cheap operations in standard computer architectures it is one of the earliest algorithms developed in the field of computer graphics through a minor expansion the original algorithm for lines can also be used to draw circles also this can be done with simple arithmetic operations quadratic or trigonometric expressions can be avoided or recursively dissolved into simpler steps the mentioned properties make it still an important algorithm and it is used among others in plotters in graphics chips of modern graphics cards and in many graphics libraries because the algorithm is very simple it is often implemented in both the firmware and the hardware of graphics cards the label bresenham is today often used for a whole family of algorithms which have been developed by others later yet in succession of bresenham and with a similar basic approach see deeper references below the common conventions that pixel coordinates increase in the down and right directions and that pixel centers have integer coordinates will be used the endpoints of the line are the pixels at and where the first coordinate of the pair is the column and the second is the row the algorithm will be initially presented only for the octant in which the segment goes down and to the right and its horizontal projection x1 x0 is longer than the vertical projection y1 y0 in this octant for each column x between x0 and x1 there is exactly one row y containing a pixel of the line while each row between y0 and y1 may contain multiple rasterized pixels bresenham s algorithm chooses the integer y corresponding to the pixel center that is closest to the ideal y for the same x on successive columns y can remain the same or increase by 1 the general equation of the line through the endpoints is given by since we know the column x the pixel s row y is given by rounding this quantity to the nearest integer the slope  depends on the endpoint coordinates only and can be precomputed and the ideal y for successive integer values of x can be computed starting from y0 and repeatedly adding the slope in practice the algorithm can track instead of possibly large y values a small error value between 0 5 and 0 5 the vertical distance between the rounded and the exact y values for the current x each time x is increased the error is increased by the slope if it exceeds 0 5 the rasterization y is increased by 1 and the error is decremented by 1 0 in the following pseudocode sample plot plots a point and abs returns absolute value the version above only handles lines that descend to the right we would of course like to be able to draw all lines the first case is allowing us to draw lines that still slope downwards but head in the opposite direction this is a simple matter of swapping the initial points if x0 x1 trickier is determining how to draw lines that go up to do this we check if y0 y1 if so we step y by 1 instead of 1 lastly we still need to generalize the algorithm to drawing lines in all directions up until now we have only been able to draw lines with a slope less than one to be able to draw lines with a steeper slope we take advantage of the fact that a steep line can be reflected across the line y x to obtain a line with a small slope the effect is to switch the x and y variables throughout including switching the parameters to plot the code looks like this the function now handles all lines and implements the complete bresenham s algorithm the problem with this approach is that computers operate relatively slowly on fractional numbers like error and deltaerr moreover errors can accumulate over many floating point additions working with integers will be both faster and more accurate the trick we use is to multiply all the fractional numbers above by deltax which enables us to express them as integers the only problem remaining is the constant 0 5 to deal with this we change the initialization of the variable error and invert it for an additional small optimization the new program looks like this the following is a c implementation of the above with an additional variable xstep used in place of the swap such that the line is always drawn from the starting point to the ending point definitions of swap are provided for efficiency here is another way to implement the algorithm was developed by jack e bresenham in 1962 at ibm in 2001 bresenham wrote bresenham s algorithm was later modified to produce circles the resulting algorithm being sometimes known as either bresenham s circle algorithm or midpoint circle algorithm the bresenham algorithm can be interpreted as slightly modified dda the principle of using an incremental error in place of division operations has other applications in graphics it is possible to use this technique to calculate the u v co ordinates during raster scan of texture mapped polygons the voxel heightmap software rendering engines seen in some pc games also used this principle bresenham also published a run slice computational algorithm 
password cracking is the process of recovering passwords from data that has been stored in or transmitted by a computer system a common approach is to repeatedly try guesses for the password the purpose of password cracking might be to help a user recover a forgotten password to gain unauthorized access to a system or as a preventive measure by system administrators to check for easily crackable passwords on a file by file basis password cracking is utilized to gain access to digital evidence for which a judge has allowed access but the particular file s access is restricted passwords to access computer systems are usually stored in a database so the system can perform password verification when a user attempts to login or access a restricted resource to preserve confidentiality of system passwords the password verification data is typically not stored in cleartext form but instead a one way function is applied to the password possibly in combination with other data and the resulting value is stored when a user later attempts to authenticate by entering the password the same function is applied to the entered value and the result is compared with the stored value if they match there is an extremely high likelihood the entered password was correct for simplicity in this discussion we will refer to the one way function employed as a hash and its output as a hashed password even though functions that create hashed passwords may be cryptographically secure possession of the hashed password provides a quick way to test guesses for the password by applying the one way function to each guess and comparing the result to the verification data the most commonly used hash functions can be computed rapidly and the attacker can test guesses repeatedly with different guesses until one succeeds meaning the plaintext password has been recovered the term password cracking generally refers to recovery of one or more plaintext passwords from hashed passwords but there are also many other ways of obtaining passwords illicitly without the hashed version of a password the attacker can still attempt access to the computer system in question with guessed passwords however well designed systems limit the number of failed access attempts and can alert administrators to trace the source of the attack if that quota is exceeded with the hashed password the attacker can work undetected and if the attacker has obtained several hashed passwords the chances in practice for cracking at least one is quite high other ways to obtain passwords include social engineering wiretapping keystroke logging login spoofing dumpster diving phishing shoulder surfing timing attack acoustic cryptanalysis using a trojan horse or virus identity management system attacks and compromising host security common methods for verifying users over a computer network often expose the hashed password for example use of a hash based challenge response authentication method for password verification may provide a hashed password to a network eavesdropper who can then crack the password a number of stronger cryptographic protocols exist that do not expose hashed passwords during verification over a network either by protecting them in transmission using a high grade key or by using a zero knowledge password proof if a system uses a poorly designed password hashing scheme to protect stored passwords an attacker can exploit any weaknesses to recover even well chosen passwords one example is the lm hash that microsoft windows uses by default to store user passwords of less than 15 characters in length lm hash converts the password into all uppercase letters then breaks the password into two 7 character fields which are hashed separately which allows each half to be attacked separately password encryption schemes that use stronger hash functions like md5 sha 512 sha 1 and ripemd 160 can still be vulnerable to brute force and precomputation attacks such attacks do not depend on reversing the hash function instead they work by hashing a large number of words or random permutations and comparing the result of each guess to a user s stored password hash modern schemes such as md5 crypt and bcrypt use purposefully slow algorithms so that the number of guesses that an attacker can make in a given period of time is relatively low salting described below prevents precomputation attackspasswords can sometimes be guessed by humans with knowledge of the user s personal information examples of guessable passwords include personal data about individuals are now available from various sources many on line and can often be obtained by someone using social engineering techniques such as posing as an opinion surveyor or a security control checker attackers who know the user may have information as well for example if a user chooses the password yalelaw78 because he graduated from yale law school in 1978 a disgruntled business partner might be able to guess the password users often choose weak passwords examples of insecure choices include the above list plus single words found in dictionaries given and family names and any password of 6 characters or less repeated research over some 40 years has demonstrated that around 40 of user chosen passwords are readily guessable by sophisticated cracking programs armed with dictionaries and perhaps the user s personal information in one survey of myspace passwords which had been phished 3 8 percent of passwords were a single word found in a dictionary and another 12 percent were a word plus a final digit two thirds of the time that digit was 1 some users neglect to change the default password that came with their account on the computer system and some administrators neglect to change default account passwords provided by the operating system vendor or hardware supplier a famous example is the use of fieldservice as a user name with guest as the password if not changed at system configuration time anyone familiar with such systems will have cracked an important password such service accounts often have higher access privileges than a normal user account lists of default passwords are available on the internet gary mckinnon accused by the united states of perpetrating the biggest military computer hack of all time has claimed that he was able to get into the military s networks simply by using a perl script that searched for blank passwords in other words his report suggests that there were computers on these networks with the default passwords active cracking programs exist which accept personal information about the user being attacked and generate common variations for passwords suggested by that information a last resort is to try every possible password known as a brute force attack in theory a brute force attack will always be successful since the rules for acceptable passwords must be publicly known but as the length of the password increases so does the number of possible passwords this method is unlikely to be practical unless the password is relatively small but how small is too small this depends heavily on whether the prospective attacker has access to the hash of the password in which case the attack is called an offline attack or not in which case it is called an online attack offline attack is generally a lot easier because testing a password is reduced to a quickly calculated mathematical computation i e calculating the hash of the password to be tried and comparing it to the hash of the real password in an online attack the attacker has to actually try to authenticate himself with all the possible passwords where arbitrary rules and delays can be imposed by the system and the attempts can be logged a common password length recommendation is 8 or more randomly chosen characters combining letters numbers and special characters this recommendation make sense for systems using stronger password hashing mechanisms such as md5 crypt and the blowfish based bcrypt but is inappropriate for many microsoft windows systems because they store a legacy lan manager hash which splits the password into two seven character halves on these systems an eight character password is converted into a seven character password and a one character password for better security lan manager password storage should be disabled if it will not break supported legacy systems systems which limit passwords to numeric characters only or upper case only or generally which exclude possible password character choices also make brute force attacks easier using longer passwords in these cases can compensate for the limited allowable character set of course even with an adequate range of character choice users who ignore that range make brute force attacks against their accounts much easier generic brute force search techniques are often successful but smart brute force techniques which exploit knowledge about how people tend to choose passwords pose an even greater threat nist sp 800 63 provides further discussion of password quality and suggests for example that an 8 character user chosen password may provide somewhere between 18 and 30 bits of entropy depending on how it is chosen this amount of entropy is far less than what is generally considered safe for an encryption key how small is too small for offline attacks thus depends partly on an attacker s ingenuity and resources the latter of which will increase as computers get faster most commonly used hashes can be implemented using specialized hardware allowing faster attacks large numbers of computers can be harnessed in parallel each trying a separate portion of the search space unused overnight and weekend time on office computers can also be used for this purpose the distinction between guessing dictionary and brute force attacks is not strict they are similar in that an attacker goes through a list of candidate passwords one by one the list may be explicitly enumerated or implicitly defined can incorporate knowledge about the victim and can be linguistically derived each of the three approaches particularly dictionary attack is frequently used as an umbrella term to denote all the three attacks and the spectrum of attacks encompassed by them in its most basic form precomputation involves hashing each word in the dictionary and storing the word and its computed hash in a way that enables lookup on the list of computed hashes this way when a new encrypted password is obtained password recovery is instantaneous precomputation can be very useful for a dictionary attack if salt is not used properly and the dramatic decrease in the cost of mass storage has made it practical for fairly large dictionaries advanced precomputation methods exist that are even more effective by applying a time memory tradeoff a middle ground can be reached a search space of size n can be turned into an encrypted database of size o in which searching for an encrypted password takes time o the theory has recently been refined into a practical technique another example cracks alphanumeric windows lan manager passwords in a few seconds this is much faster than brute force attacks on the obsolete lan manager which uses a particularly weak method of hashing the password windows systems prior to windows vista server 2008 compute and store a lan manager hash by default for backwards compatibility a technique similar to precomputation known generically as memoization can be used to crack multiple passwords at the cost of cracking just one since encrypting a word takes much longer than comparing it with a stored word a lot of effort is saved by encrypting each word only once and comparing it with each of the encrypted passwords using an efficient list search algorithm the two approaches may of course be combined the time space tradeoff attack can be modified to crack multiple passwords simultaneously in a shorter time than cracking them one after the other the benefits of precomputation and memorization can be nullified by randomizing the hashing process this is known as salting when the user sets a password a short random string called the salt is suffixed to the password before encrypting it the salt is stored along with the encrypted password so that it can be used during verification since the salt is usually different for each user the attacker can no longer construct tables with a single encrypted version of each candidate password early unix systems used a 12 bit salt attackers could still build tables with common passwords encrypted with all 4096 possible 12 bit salts however if the salt is long enough there are too many possibilities and the attacker must repeat the encryption of every guess for each user modern methods such as md5 crypt and bcrypt use salts of 48 and 128 bits respectively early unix implementations limited passwords to 8 characters and used a 12 bit salt which allowed for 4096 possible salt values while 12 bits was good enough for most purposes in the 1970s by 2005 disk storage had become cheap enough that an attacker can precompute the hashes of millions of common passwords including all 4096 possible salt variations for each password and store the precomputed values on a single portable hard drive an attacker with a larger budget can build a disk farm with all 6 character passwords and the most common 7 and 8 character passwords stored in encrypted form for all 4096 possible salts and when several thousand passwords are being cracked at once memoization still offers some benefit since there is little downside to using a longer salt and because they render any precomputation or memoization hopeless modern implementations choose to do so the best method of preventing password cracking is to ensure that attackers cannot get access even to the encrypted password for example on the unix operating system encrypted passwords were originally stored in a publicly accessible file etc passwd on modern unix systems on the other hand they are stored in the file etc shadow which is accessible only to programs running with enhanced privileges this makes it harder for a malicious user to obtain the encrypted passwords in the first instance unfortunately many common network protocols transmit passwords in cleartext or use weak challenge response schemes modern unix systems have replaced traditional des based password hashing with stronger methods based on md5 and blowfish other systems have also begun to adopt these methods for instance the cisco ios originally used a reversible vigenere cipher to encrypt passwords but now uses md5 crypt with a 24 bit salt when the enable secret command is used these newer methods use large salt values which prevent attackers from efficiently mounting offline attacks against multiple user accounts simultaneously the algorithms are also much slower to execute which drastically increases the time required to mount a successful offline attack there are many password cracking software tools but the most popular are cain and abel john the ripper and hydra many litigation support software packages also include password cracking functionality most of these packages employ a mixture of cracking strategies with brute force and dictionary attacks proving to be the most productive 
in computer science computability theory is the branch of the theory of computation that studies which problems are computationally solvable using different models of computation computability theory differs from the related discipline of computational complexity theory which deals with the question of how efficiently a problem can be solved rather than whether it is solvable at all a central question of computer science is to address the limits of computing devices one approach to addressing this question is understanding the problems we can use computers to solve modern computing devices often seem to possess infinite capacity for calculation and it s easy to imagine that given enough time we might use computers to solve any problem however it is possible to show clear limits to the ability of computers even given arbitrarily vast computational resources to solve even seemingly simple problems problems are formally expressed as a decision problem which is to construct a mathematical function that for each input returns either 0 or 1 if the value of the function on the input is 0 then the answer is no and otherwise the answer is yes to explore this area computer scientists invented automata theory which addresses problems such as the following given a formal language and a string is the string a member of that language this is a somewhat esoteric way of asking this question so an example is illuminating we might define our language as the set of all strings of digits which represent a prime number to ask whether an input string is a member of this language is equivalent to asking whether the number represented by that input string is prime similarly we define a language as the set of all palindromes or the set of all strings consisting only of the letter a in these examples it is easy to see that constructing a computer to solve one problem is easier in some cases than in others but in what real sense is this observation true can we define a formal sense in which we can understand how hard a particular problem is to solve on a computer it is the goal of computability theory of automata to answer just this question in order to begin to answer the central question of automata theory it is necessary to define in a formal way what an automaton is there are a number of useful models of automata some widely known models are with these computational models in hand we can determine what their limits are that is what classes of languages can they accept computer scientists call any language that can be accepted by a finite state machine a regular language because of the restriction that the number of possible states in a finite state machine is finite we can see that to find a language that is not regular we must construct a language that would require an infinite number of states an example of such a language is the set of all strings consisting of the letters a and b which contain an equal number of the letter a and b to see why this language cannot be correctly recognized by a finite state machine assume first that such a machine m exists m must have some number of states n now consider the string x consisting of a s followed by b s as m reads in x there must be some state in the machine that is repeated as it reads in the first series of a s since there are a s and only n states by the pigeonhole principle call this state s and further let d be the number of a s that our machine read in order to get from the first occurrence of s to some subsequent occurrence during the a sequence we know then that at that second occurrence of s we can add in an additional d a s and we will be again at state s this means that we know that a string of a s must end up in the same state as the string of a s this implies that if our machine accepts x it must also accept the string of a s followed by b s which is not in the language of strings containing an equal number of a s and b s in other words m cannot correctly distinguish between a string of equal number of a s and b s and a string with a s and n 1 b s we know therefore that this language cannot be accepted correctly by any finite state machine and is thus not a regular language a more general form of this result is called the pumping lemma for regular languages which can be used to show that broad classes of languages cannot be recognized by a finite state machine computer scientists define a language that can be accepted by a pushdown automaton as a context free language which can be specified as a context free grammar the language consisting of strings with equal numbers of a s and b s which we showed was not a regular language can be decided by a push down automaton also in general a push down automaton can behave just like a finite state machine so it can decide any language which is regular this model of computation is thus strictly more powerful than finite state machines however it turns out there are languages that cannot be decided by push down automaton either the result is similar to that for regular expressions and won t be detailed here there exists a pumping lemma for context free languages an example of such a language is the set of prime numbers turing machines can decide any context free language in addition to languages not decidable by a push down automaton such as the language consisting of prime numbers it is therefore a strictly more powerful model of computation because turing machines have the ability to back up in their input tape it is possible for a turing machine to run for a long time in a way that is not possible with the other computation models previously described it is possible to construct a turing machine that will never finish running on some inputs we say that a turing machine can decide a language if it eventually will halt on all inputs and give an answer a language that can be so decided is called a recursive language we can further describe turing machines that will eventually halt and give an answer for any input in a language but which may run forever for input strings which are not in the language such turing machines could tell us that a given string is in the language but we may never be sure based on its behavior that a given string is not in a language since it may run forever in such a case a language which is accepted by such a turing machine is called a recursively enumerable language the turing machine it turns out is an exceedingly powerful model of automata attempts to amend the definition of a turing machine to produce a more powerful machine are surprisingly met with failure for example adding an extra tape to the turing machine giving it a 2 dimensional infinite surface to work with can all be simulated by a turing machine with the basic 1 dimensional tape these models are thus not more powerful in fact a consequence of the church turing thesis is that there is no reasonable model of computation which can decide languages that cannot be decided by a turing machine the question to ask then is do there exist languages which are recursively enumerable but not recursive and furthermore are there languages which are not even recursively enumerable the halting problem is one of the most famous problems in computer science because it has profound implications on the theory of computability and on how we use computers in everyday practice the problem can be phrased here we are asking not a simple question about a prime number or a palindrome but we are instead turning the tables and asking a turing machine to answer a question about another turing machine it can be shown that it is not possible to construct a turing machine that can answer this question in all cases that is the only general way to know for sure if a given program will halt on a particular input in all cases is simply to run it and see if it halts if it does halt then you know it halts if it doesn t halt however you may never know if it will eventually halt the language consisting of all turing machine descriptions paired with all possible input streams on which those turing machines will eventually halt is not recursive the halting problem is therefore called non computable or undecidable an extension of the halting problem is called rice s theorem which states that it is undecidable whether a given language possesses any specific nontrivial property the halting problem is easy to solve however if we allow that the turing machine that decides it may run forever when given input which is a representation of a turing machine that does not itself halt the halting language is therefore recursively enumerable it is possible to construct languages which are not even recursively enumerable however a simple example of such a language is the complement of the halting language that is the language consisting of all turing machines paired with input strings where the turing machines do not halt on their input to see that this language is not recursively enumerable imagine that we construct a turing machine m which is able to give a definite answer for all such turing machines but that it may run forever on any turing machine that does eventually halt we can then construct another turing machine m that simulates the operation of this machine along with simulating directly the execution of the machine given in the input as well by interleaving the execution of the two programs since the direct simulation will eventually halt if the program it is simulating halts and since by assumption the simulation of m will eventually halt if the input program would never halt we know that m will eventually have one of its parallel versions halt m is thus a decider for the halting problem we have previously shown however that the halting problem is undecidable we have a contradiction and we have thus shown that our assumption that m exists is incorrect the complement of the halting language is therefore not recursively enumerable a number of computational models based on concurrency have been developed including the parallel random access machine and the petri net these models of concurrent computation still do not implement any mathematical functions that cannot be implemented by turing machines the church turing thesis conjectures that there is no reasonable model of computing that can compute more mathematical functions than a turing machine in this section we will explore some of the unreasonable ideas for computational models which violate this conjecture computer scientists have imagined many varieties of hypercomputers imagine a machine where each step of the computation requires half the time of the previous step if we normalize to 1 time unit the amount of time required for the first step the execution would requiretime to run this infinite series converges to 2 time units which means that this turing machine can run an infinite execution in 2 time units this machine is capable of deciding the halting problem by directly simulating the execution of the machine in question by extension any convergent series would work assuming that the series converges to a value n the turing machine would complete an infinite execution in n time units so called oracle machines have access to various oracles which provide the solution to specific undecidable problems for example the turing machine may have a halting oracle which answers immediately whether a given turing machine will ever halt on a given input these machines are a central topic of study in recursion theory even these machines which seemingly represent the limit of automata that we could imagine run into their own limitations while each of them can solve the halting problem for a turing machine they cannot solve their own version of the halting problem for example an oracle machine cannot answer the question of whether a given oracle machine will ever halt the lambda calculus an important precursor to formal computability theory was developed by alonzo church and stephen cole kleene alan turing is most often considered the father of modern computer science and laid many of the important foundations of computability and complexity theory including the first description of the turing machine 1936 as well as many of the important early results 
algol known colloquially as the demon star is a bright star in the constellation perseus it is one of the best known eclipsing binaries the first such star to be discovered and also one of the first variable stars to be discovered algol is actually a three star system in which the large and bright primary beta persei a is regularly eclipsed by the dimmer beta persei b thus algol s magnitude is usually near constant at 2 1 but regularly dips to 3 4 every 2days 20hours and 49minutes during the roughly 10 hour long partial eclipses there is also a secondary eclipse when the brighter star occults the fainter secondary this secondary eclipse can only be detected photoelectrically the variability of algol was first recorded in 1667 by geminiano montanari but it is probable that this property was noticed long before this time the first person to propose a mechanism for the variability of this star was the british amateur astronomer john goodricke in may 1783 he presented his findings to the royal society suggesting that the periodic variability was caused by a dark body passing in front of the star for his report he was awarded the copley medal in 1881 the harvard astronomer edward pickering presented evidence that algol was actually an eclipsing binary this was confirmed a few years later in 1889 when the potsdam astronomer hermann carl vogel found periodic doppler shifts in the spectrum of algol inferring variations in the radial velocity of this binary system thus algol became one of the first known spectroscopic binaries algol a and algol b are an eclipsing binary because their orbital plane coincidentally contains the earth s line of sight to be more precise however algol is a triple star system the eclipsing binary pair is separated by only 0 062 au while the third star in the sytstem is at an average distance of 2 69 au from the pair and the mutual orbital period is 681days the total mass of the system is about 5 8 solar masses and the mass ratios of a b and c are about 4 5 1 2 studies of algol led to the algol paradox in the theory of stellar evolution although components of a binary star form at the same time and massive stars evolve much faster than the less massive ones it was observed that the more massive component algol a is still in the main sequence while the less massive algol b is a subgiant star at a later evolutionary stage the paradox can be solved by mass transfer when the more massive star became a subgiant it filled its roche lobe and most of the mass was transferred to the other star which is still in the main sequence in some binaries similar to algol a gas flow can be seen this system also exhibits variable activity in the form of x ray and radio flares the former is thought to be caused by the magnetic fields of the ab components interacting with the mass transfer the radio emissions may be created by magnetic cycles similar to sunspots but as the magnetic fields around these stars are up to ten times stronger than that of the sun these radio flares are more powerful and longer lasting algol is 92 8 light years from earth however about 7 3million years ago it passed within 9 8 light years and its apparent magnitude was approximately 2 5 considerably brighter than sirius is today because the total mass of the system is 5 8 solar masses and despite the fairly large distance at closest approach this may have been enough to perturb the solar system s oort cloud slightly and to increase the number of comets entering the inner solar system however the actual increase in net cratering rate is believed to have been quite small the name algol derives from arabic   ra s al ghl head of the ogre which was given from its position in the constellation perseus representing the head of gorgon medusa the english names of demon star and blinking demon are direct translations in hebrew folklore it was known as rsh ha sn satan s head via edmund chilmead who called it divels head or rosch hassatan a latin term from the 16th century was caput larvae spectre s head it was also linked with lilith hipparchus and pliny made this a separate though connected constellation it is known as  in chinese astronomy and also bore the grim name tseih she meaning piled up corpses astrologically algol is considered the most unfortunate star in the sky in the middle ages it was one of the 15 behenian stars associated with the diamond and hellebore and marked with the kabbalistic sign one of the earliest films about alien invasion was a 1920 german silent film titled algol renowned at the time for its sets it featured emil jannings as mephisto an alien from algol all prints of the film were believed to have been lost but an intact copy has been recovered coordinates 03h 08m 10 1315s 40 57 20 332 
a physics engine is a computer program that simulates newtonian physics models using variables such as mass velocity friction and wind resistance it can simulate and predict effects under different conditions that would approximate what happens in real life or in a fantasy world its main uses are in scientific simulation and in video games there are generally two classes of physics engines real time and high precision high precision physics engines require more processing power to calculate very precise physics and are usually used by scientists and computer animated movies in video games or other forms of interactive computing the physics engine simplifies its calculations and lowers its accuracy so that they can be performed in time for the game to respond at an appropriate rate for gameplay this is referred to as real time physics computer games use physics engines to improve realism physics engines have two core components a collision detection system and the dynamics simulation component responsible for solving the forces affecting the simulated objects modern physics engines may also contain fluid simulations animation control systems and asset integration tools there are three major paradigms for the physical simulation of solids finally hybrid methods are possible that combine aspects of the above paradigms one of the first general purpose computers eniac was used as a very simple type of physics engine it was used to design ballistics tables to help the united states military estimate where artillery shells of various mass would land when fired at varying angles and gunpowder charges also accounting for drift caused by wind the results were calculated a single time only and were tabulated into printed tables handed out to the artillery commanders physics engines have been commonly used on supercomputers since the 1980s to simulate the flowing of atmospheric air and water in order to predict weather patterns this is known as computational fluid dynamics modeling where particles of air are assigned a force vector and these combined forces are calculated across vast regions of space to show how the overall weather patterns will circulate due to the requirements of speed and high precision special computer processors known as vector processors were developed to accelerate the calculations generally weather prediction is still an inaccurate science because the resolution of the simulated atmosphere is not detailed enough to match real world conditions and small fluctuations not modeled in the simulation can drastically change the predicted results after several days similar fluid dynamic modeling is also commonly used for designing new types of aircraft and watercraft and can provide engineers the information that used to be obtained solely from wind tunnel testing tire manufacturers use physics simulations to examine how new tire tread types will perform under wet and dry conditions using new tire materials of varying flexibility and under different levels of weight loading electronics manufacturers use fluid dynamic modeling to examine how cooling air will flow through the computer case to locate thermal hotspots that may need additional cooling in most computer games speed of simulation is more important than accuracy of simulation typically most 3d objects in a game are represented by two separate meshes or shapes one of these meshes is a highly complex and detailed shape which the player sees in the game for example a vase with elegant curved and looping handles however for purposes of speed a second highly simplified invisible mesh is used to represent the object to the physics engine to the physics engine the object may be processed as nothing more than a simple tall cylinder it is therefore impossible to insert a rod or fire a projectile through the handle holes on the vase because the physics engine does not know the handles exist and only processes the rough cylindrical shape the simplified mesh used for physics processing is often referred to as the collision geometry this may be a bounding box sphere or convex hull engines that use bounding boxes or bounding spheres as the final shape for collision detection are considered extremely simple generally a bounding box is used for broad phase collision detection to narrow down the number of possible collisions before costly mesh on mesh collision detection is done in the narrow phase of collision detection in the real world physics is always active there is a constant brownian motion jitter to all particles in our universe as the forces push back and forth against each other for a game physics engine such constant active precision is unnecessary and a waste of the limited cpu power in the 3d virtual world second life if an object is resting on the floor and the object does not move beyond a certain minimal distance in about two seconds then the physics calculations are disabled for the object and it becomes frozen in place it remains frozen until a collision occurs with some other actively physical object and that reactivates physics processing for the object this freezing of stable non moving objects allows the physics engine to conserve processing power and increase the framerate of other objects currently in motion but can lead to unusual problems such as a huge slow pendulum freezing in place on the upswing as it slows down and starts to reverse direction the primary limit of physics engine realism is the precision of the numbers representing the position of an object and the forces acting on that object when the precision is too low errors can creep into the calculations due to rounding causing an object to overshoot or undershoot the correct position these errors are compounded in situations where two free moving objects are fitted together with a precision that is greater than what the physics engine can calculate this can lead to an unnatural buildup energy in the object due to the rounding errors that begins to violently shake and eventually blow the objects apart any type of free moving compound physics object can demonstrate this problem but it is especially prone to affecting chain links under high tension and wheeled objects with actively physical bearing surfaces higher precision reduces the positional force errors but at the cost of greater cpu power needed for the calculations another unusual aspect of physics precision involves the framerate or the number of moments in time per second when physics is calculated each frame is treated as separate from all other frames and the space between frames is not calculated a low framerate and a small fast moving object leads to a situation where the object does not move smoothly through space but in fact seems to teleport from one point in space to the next point in space as each frame is calculated at sufficiently high speeds a projectile will miss a target if the target is small enough to fit in the gap between the calculated frames of the fast moving projectile in second life this problem is resolved by making all projectiles as if they were arrows a long invisible shaft trails behind the bullet so that as the bullet teleports forward the shaft is long enough to cover the gap between successive teleports of the bullet and collide with any object that might fit between the calculated frames physics based character animation in the past only used rigid body dynamics because they are faster and easier to calculate but modern games and movies are starting to use soft body physics now that it is possible soft body physics are also used for particle effects liquids and cloth some form of limited fluid dynamics simulation is sometimes provided to simulate water and other liquids as well as the flow of fire and explosions through the air february 2006 saw the release of the first dedicated physics processing unit from ageia called physx which functions in a similar manner to the graphic processing unit in a graphics card off loading the majority of the physics processing weight off the cpu and into a dedicated processor the unit was most effective in accelerating particle systems only a small performance improvement was measured for rigid body physics the ageia ppu is documented in depth in their us patent application 20050075849 an early academic ppu research project named sparta was carried out at penn state and university of georgia this was a simple fpga based ppu that was limited to two dimensions this project was extended into a considerably more advanced asic based system named hellas example sparta animationsparta printed circuit boardhellas die photogpgpu is another promising approach for realtime physics engines including rigid body dynamics ati and nvidia provide rigid body dynamics on their latest graphics cards ati claims that their x1900 xt should deliver 9 times the performance of an nvidia physx card nvidia s geforce 8 series supports a new gpu based newtonian physics acceleration technology named quantum effects technology which will compete directly with the physx ppu hardware nvidia provides an sdk toolkit for what they call cuda technology that offers both a low and high level api to the gpu few technical details are available about the physics side of it and it is not yet clear whether this is part of havok fx sdk and or nvidia physx sdk or a completely separate engine ati amd offer a similar sdk for their ati based gpus and that sdk and technology is called ctm which provides a thin hardware interface amd has also announced the amd stream processor product line which combines a cpu and gpu technology on one chip 
auto_ptr is a class template available in the c standard library that provides some basic raii features for c raw pointers the auto_ptr template class describes an object that stores a pointer to an allocated object of type type that ensures that the object to which it points gets destroyed automatically when control leaves a scope the shared_ptr template class proposed in technical report 1 and available in the boost library can be used as an alternative to auto_ptr for collections with ownership semantics the auto_ptr class is declared in iso iec 14882 section 20 4 5 as the auto_ptr has semantics of strict ownership meaning that the auto_ptr instance is the sole entity responsible for the object s lifetime if an auto_ptr is copied the source loses the reference for example this code will print a null address for the first auto_ptr object and some non null address for the second showing that the source object lost the reference during the assignment the raw pointer i in the example should not be deleted as it will be deleted by the auto_ptr that owns the reference in fact new int could be passed directly into x eliminating the need for i notice that the object pointed by an auto_ptr is destructed using operator delete this means that you should only use auto_ptr for pointers obtained with operator new this excludes pointers returned by malloc calloc realloc and operator new 
in computing regular expressions provide a concise and flexible means for identifying strings of text of interest such as particular characters words or patterns of characters regular expressions are written in a formal language that can be interpreted by a regular expression processor a program that either serves as a parser generator or examines text and identifies parts that match the provided specification the following examples illustrate a few specifications that could be expressed in a regular expression regular expressions can be much more complex than these examples regular expressions are used by many text editors utilities and programming languages to search and manipulate text based on patterns for example perl ruby and tcl have a powerful regular expression engine built directly into their syntax several utilities provided by unix distributions including the editor ed and the filter grep were the first to popularize the concept of regular expressions as an example of the syntax the regular expression bex can be used to search for all instances of the string ex that occur after word boundaries thus in the string texts for experts bex matches the ex in experts but not in texts many modern computing systems provide wildcard characters in matching filenames from a file system this is a core capability of many command line shells and is also known as globbing wildcards differ from regular expressions in that they generally only express very limited forms of alternatives a regular expression often called a pattern is an expression that describes a set of strings they are usually used to give a concise description of a set without having to list all elements for example the set containing the three strings handel hndel and haendel can be described by the pattern hndel in most formalisms if there is any regex that matches a particular set then there is an infinite number of such expressions most formalisms provide the following operations to construct regular expressions these constructions can be combined to form arbitrarily complex expressions much like one can construct arithmetical expressions from numbers and the operations  and for example hndel and hndel are both valid patterns which match the same strings as the earlier example hndel the precise syntax for regular expressions varies among tools and with context more detail is given in the syntax section the origins of regular expressions lie in automata theory and formal language theory both of which are part of theoretical computer science these fields study models of computation and ways to describe and classify formal languages in the 1950s mathematician stephen cole kleene described these models using his mathematical notation called regular sets the snobol language was an early implementation of pattern matching but not identical to regular expressions ken thompson built kleene s notation into the editor qed as a means to match patterns in text files he later added this capability to the unix editor ed which eventually led to the popular search tool grep s use of regular expressions since that time many variations of thompson s original adaptation of regular expressions have been widely used in unix and unix like utilities including expr awk emacs vi and lex perl and tcl regular expressions were derived from a regex library written by henry spencer though perl later expanded on spencer s library to add many new features philip hazel developed pcre which attempts to closely mimic perl s regular expression functionality and is used by many modern tools including php and apache http server part of the effort in the design of perl 6 is to improve perl s regular expression integration and to increase their scope and capabilities to allow the definition of parsing expression grammars the result is a mini language called perl 6 rules which are used to define perl 6 grammar as well as provide a tool to programmers in the language these rules maintain existing features of perl 5 x regular expressions but also allow bnf style definition of a recursive descent parser via sub rules the use of regular expressions in structured information standards for document and database modeling started in the 1960s and expanded in the 1980s when industry standards like iso sgml consolidated the kernel of the structure specification language standards are regular expressions simple use is evident in the dtd element group syntax see also pattern matching history regular expressions can be expressed in terms of formal language theory regular expressions consist of constants and operators that denote sets of strings and operations over these sets respectively given a finite alphabet  the following constants are defined the following operations are defined to avoid parentheses it is assumed that the kleene star has the highest priority then concatenation and then set union if there is no ambiguity then parentheses may be omitted for example c can be written as abc and a can be written as a bc examples the formal definition of regular expressions is purposely parsimonious and avoids defining the redundant quantifiers and which can be expressed as follows a aa and a sometimes the complement operator is added r denotes the set of all strings over  that are not in r the complement operator is redundant as it can always be expressed by using the other operators regular expressions in this sense can express the regular languages exactly the class of languages accepted by finite state automata there is however a significant difference in compactness some classes of regular languages can only be described by automata that grow exponentially in size while the length of the required regular expressions only grow linearly regular expressions correspond to the type 3 grammars of the chomsky hierarchy on the other hand there is a simple mapping from regular expressions to nondeterministic finite automata that does not lead to such a blowup in size for this reason nfas are often used as alternative representations of regular expressions we can also study expressive power within the formalism as the examples show different regular expressions can express the same language the formalism is redundant it is possible to write an algorithm which for two given regular expressions decides whether the described languages are essentially equal reduces each expression to a minimal deterministic finite state machine and determines whether they are isomorphic to what extent can this redundancy be eliminated can we find an interesting subset of regular expressions that is still fully expressive kleene star and set union are obviously required but perhaps we can restrict their use this turns out to be a surprisingly difficult problem as simple as the regular expressions are it turns out there is no method to systematically rewrite them to some normal form the lack of axiomatization in the past led to the star height problem recently cornell university professor dexter kozen axiomatized regular expressions with kleene algebra it is worth noting that many real world regular expression engines implement features that cannot be expressed in the regular expression algebra see below for more on this traditional unix regular expression syntax followed common conventions but often differed from tool to tool the ieee posix basic regular expressions standard was designed mostly for backward compatibility with the traditional syntax but provided a common standard which has since been adopted as the default syntax of many unix regular expression tools though there is often some variation or additional features many such tools also provide support for ere syntax with command line arguments in the bre syntax most characters are treated as literals they match only themselves the exceptions listed below are called metacharacters or metasequences the character is treated as a literal character if it is the last or the first character within the brackets or if it is escaped with a backslash or examples the meaning of metacharacters escaped with a backslash is reversed for some characters in the posix extended regular expression syntax with this syntax a backslash causes the metacharacter to be treated as a literal character so for example is now and  is now  additionally support is removed for n backreferences and the following metacharacters are added examples posix extended regular expressions can often be used with modern unix utilities by including the command line flag e since many ranges of characters depend on the chosen locale setting the posix standard defines some classes or categories of characters as shown in the following table posix character classes can only be used within bracket expressions for example ab matches the uppercase letters and lowercase a and b in perl regular expressions matches union an additional non posix class understood by some tools is which is usually defined as plus underscore this reflects the fact that in many programming languages these are the characters that may be used in identifiers the editor vim further distinguishes word and word head classes since in many programming languages the characters that can begin an identifier are not the same as those that can occur in other positions note that what the posix regular expression standards call character classes are commonly referred to as posix character classes in other regular expression flavors which support them with most other regular expression flavors the term character class is used to describe what posix calls bracket expressions perl has a more consistent and richer syntax than the posix basic and extended regular expression standards an example of its consistency is that always escapes a non alphanumeric character another example of functionality possible with perl but not posix compliant regular expressions is the concept of lazy quantification due largely to its expressive power many other utilities and programming languages have adopted syntax similar to perl s for example java javascript pcre python ruby microsoft s net framework and the w3c s xml schema all use regular expression syntax similar to perl s some languages and tools such as php support multiple regular expression flavors perl derivative regular expression implementations are not identical and many implement only a subset of perl s features with perl 5 10 this process has come full circle with perl incorporating syntax extensions originally from python pcre the net framework and java simple regular expressions is a syntax that may be used by historical versions of application programs and may be supported within some applications for the purpose of providing backward compatibility but these forms of regular expression syntax are considered deprecated and should not be used the standard quantifiers in regular expressions are greedy meaning they match as much as they can only giving back as necessary to match the remainder of the regex for example someone new to regexes wishing to find the first instance of an item between and symbols in this example would likely come up with the pattern  or similar however this pattern will actually return january 26 2004 instead of the january 26 which might be expected because the quantifier is greedy it will consume as many characters as possible from the input and january 26 2004 has more characters than january 26 though this problem can be avoided in a number of ways modern regular expression tools allow a quantifier to be specified as lazy by putting a question mark after the quantifier or by using a modifier which reverses the greediness of quantifiers by using a lazy quantifier the expression tries the minimal match first though in the previous example lazy matching is used to select one of many matching results in some cases it can also be used to improve performance when greedy matching would require more backtracking many features found in modern regular expression libraries provide an expressive power that far exceeds the regular languages for example the ability to group subexpressions with parentheses and recall the value they match in the same expression means that a pattern can match strings of repeated words like papa or wikiwiki called squares in formal language theory the pattern for these strings is 1 however the language of squares is not regular nor is it context free pattern matching with an unbounded number of back references as supported by numerous modern tools is np hard however many tools libraries and engines that provide such constructions still use the term regular expression for their patterns this has led to a nomenclature where the term regular expression has different meanings in formal language theory and pattern matching for this reason some people have taken to using the term regex or simply pattern to describe the latter larry wall writes in apocalypse 5 there are at least three different algorithms that decide if and how a given regular expression matches a string the oldest and fastest two rely on a result in formal language theory that allows every nondeterministic finite state machine to be transformed into a deterministic finite state machine the dfa can be constructed explicitly and then run on the resulting input string one symbol at a time constructing the dfa for a regular expression of size m has the time and memory cost of o but it can be run on a string of size n in time o an alternative approach is to simulate the nfa directly essentially building each dfa state on demand and then discarding it at the next step possibly with caching this keeps the dfa implicit and avoids the exponential construction cost but running cost rises to o the explicit approach is called the dfa algorithm and the implicit approach the nfa algorithm as both can be seen as different ways of executing the same dfa they are also often called the dfa algorithm without making a distinction these algorithms are fast but using them for recalling grouped subexpressions lazy quantification and similar features is tricky the third algorithm is to match the pattern against the input string by backtracking this algorithm is commonly called nfa but this terminology can be confusing its running time can be exponential which simple implementations exhibit when matching against expressions like b that contain both alternation and unbounded quantification and force the algorithm to consider an exponentially increasing number of sub cases more complex implementations will often identify and speed up or abort common cases where they would otherwise run slowly although backtracking implementations only give an exponential guarantee in the worst case they provide much greater flexibility and expressive power for example any implementation which allows the use of backreferences or implements the various extensions introduced by perl must use a backtracking implementation some implementations try to provide the best of both algorithms by first running a fast dfa match to see if the string matches the regular expression at all and only in that case perform a potentially slower backtracking match regular expressions were originally used with ascii characters many regular expression engines can now handle unicode in most respects it makes no difference what the character set is but some issues do arise when extending regular expressions to support unicode regular expressions are useful in the production of syntax highlighting systems data validation and many other tasks while regular expressions would be useful on search engines such as google or live search processing them across the entire database could consume excessive computer resources depending on the complexity and design of the regex although in many cases system administrators can run regex based queries internally most search engines do not offer regex support to the public a notable exception is google code search 
ai3 links here for american idol click hereautoit is a freeware automation language for microsoft windows in its earliest release the software was primarily intended to create automation scripts for microsoft windows programs such scripts proved useful for hands free completion of highly repetitive tasks such as rolling out a large number of pcs with identical installation instructions with later releases autoit grew to include enhancements in both programming language design and overall functionality with the release of autoit version 3 the syntax of the programming language was restructured to be more like the basic family of languages a script can be compiled into a compressed stand alone executable which can then be run on computers that do not have the autoit interpreter installed a wide range of function libraries are included as standard or available from the website to add specialized functionality autoit like other scripting languages is a third generation programming language using a classical data model utilizing a variant data type that can store several types of data including arrays it has a basic like syntax and is compatible with windows 95 98 me nt4 2000 xp 2003 vista however support for operating systems older than windows 2000 has been dropped as of autoit v3 3 0 autoit is distributed with an ide based on the free scite editor the compiler and help text are fully integrated and provide a de facto standard environment for developers using autoit a popular use of autoit is the production of utility software for microsoft windows common tasks such as website monitoring network monitoring disk defragging and backup can be automated and combined to provide a customised utility another use of autoit is for botting in online games botting allows the user to run a script which automates some repetitive action in the game usually so the script user can advance in the game more quickly use of autoit in this way is frowned upon by some in the autoit community as it can generate bad publicity however a few botting scripts can still be found in the official autoit forums because botting is considered to be cheating by most autoit users you should not expect much help in the use of autoit for this purpose many forum members will refuse to help those that are trying to create bots this is not to say that support will not be provided however you may have to wait long periods of time for answers regarding bots it is also used to simulate application users whereby an application is driven by an autoit script in place of manual application control during software testing it has also been used to create malware note at some point in the life of autoit the developers released the source code under the gpl license in hopes to accelerate development soon however a few individuals took the code and released their own competing products using a non gpl license while giving no credit to the original autoit developers the autoit developers decided to prevent such pilfering for future versions of autoit and changed the license and limited access to the source code the release of source code was stopped beginning with version 3 2 0 in august 2006 
the universal software radio peripheral is a high speed usb based board for making software radios the usrp is intended to be a comparatively inexpensive hardware device facilitating the building of a software radio the usrp has an open design with freely available schematics and drivers and free software to integrate with gnu radio it is also designed to be flexible allowing developers to make their own daughterboards for specific needs with regard to connectors different frequency bands etc the usrp is developed by a team led by matt ettus the usrp consists of when the usrp is powered up it puts the ad9862 is a low power state and blinks the led 3 times per second the host detects a usb device 04b4 8613 and therefore knows it is an unconfigured fx2 device the driver will now load a firmware into the fx2 and when it boots up the host will now detect a different usb device of fffe 0002 using a capability of the newly loaded fx2 firmware the driver will now initialize the fpga once that is done boot is complete and the device is ready for tuning the next version usrp2 was made available in early september 2008 it was made clear that the usrp2 is not intended to replace the original usrp which will continue to be manufactured and sold in parallel to the usrp2 usrp2 contains xilinx s free programming tool ise webpack is not compatible with the fpga used for usrp none of xilinx s tools are open source and the compatible tool is not free of cost so it is harder to compile the usrp2 firmware daughterboards serve as the rf frontend they allow the output signal to be modulated to a higher frequency and an input signal to be stripped of its carrier several classes of boards exist receivers transmitters and transceivers receivers only support rx and consume only one rx port transmitters only support tx and consume one tx port transceivers are both tx and rx and consume 2 ports together daughterboards allow the usrp to use the entire spectrum however when several daughterboards are used together in the same usrp case some shielding may be required in order to reduce cross talk between the daugherboards when a full duplex application requires a high degree of receiver sensitivity please be aware that transmitting on radio frequencies without a special permit is illegal in most of the world there are certain bands which allow unlicensed usage but in some countries you need a licensed device in order to use these bands usrp is sold as a test equipment and is not a licensed transmitter therefore you may not connect it to an antenna unless you have a specific permit to do so amateur radio licenses are one type of licenses that allow you to use usrp as a transmitter on some parts of the spectrum in addition receiving transmissions not intended to you are illegal in many countries under anti eavesdropping laws in particular imsi catching and military bands are off limits in the us some hams may be licensed to use military bands on some conditions the usrp has been used as 
a programmable logic controller or programmable controller is a digital computer used for automation of electromechanical processes such as control of machinery on factory assembly lines control of amusement rides or control of lighting fixtures plcs are used in many different industries and machines such as packaging and semiconductor machines unlike general purpose computers the plc is designed for multiple inputs and output arrangements extended temperature ranges immunity to electrical noise and resistance to vibration and impact programs to control machine operation are typically stored in battery backed or non volatile memory a plc is an example of a real time system since output results must be produced in response to input conditions within a bounded time otherwise unintended operation will result the main difference from other computers is that plcs are armored for severe conditions and have the facility for extensive input output arrangements these connect the plc to sensors and actuators plcs read limit switches analog process variables and the positions of complex positioning systems some even use machine vision on the actuator side plcs operate electric motors pneumatic or hydraulic cylinders magnetic relays or solenoids or analog outputs the input output arrangements may be built into a simple plc or the plc may have external i o modules attached to a computer network that plugs into the plc a small plc will have a fixed number of connections built in for inputs and outputs typically expansions are available if the base model does not have enough i o modular plcs have a chassis into which are placed modules with different functions the processor and selection of i o modules is customised for the particular application several racks can be administered by a single processor and may have thousands of inputs and outputs a special high speed serial i o link is used so that racks can be distributed away from the processor reducing the wiring costs for large plants plcs may need to interact with people for the purpose of configuration alarm reporting or everyday control a human machine interface is employed for this purpose hmis are also referred to as mmis and gui a simple system may use buttons and lights to interact with the user text displays are available as well as graphical touch screens more complex systems use a programming and monitoring software installed on a computer with the plc connected via a communication interface plcs have built in communications ports usually 9 pin rs232 and optionally for rs485 and ethernet modbus or df1 is usually included as one of the communications protocols others options include various fieldbuses such as devicenet or profibus other communications protocols that may be used are listed in the list of automation protocols most modern plcs can communicate over a network to some other system such as a computer running a scada system or web browser plcs used in larger i o systems may have peer to peer communication between processors this allows separate parts of a complex process to have individual control while allowing the subsystems to co ordinate over the communication link these communication links are also often used for hmi devices such as keypads or pc type workstations some of today s plcs can communicate over a wide range of media including rs 485 coaxial and even ethernet for i o control at network speeds up to 100 mbit s plcs are well adapted to a range of automation tasks these are typically industrial processes in manufacturing where the cost of developing and maintaining the automation system is high relative to the total cost of the automation and where changes to the system would be expected during its operational life plcs contain input and output devices compatible with industrial pilot devices and controls little electrical design is required and the design problem centers on expressing the desired sequence of operations in ladder logic notation plc applications are typically highly customized systems so the cost of a packaged plc is low compared to the cost of a specific custom built controller design on the other hand in the case of mass produced goods customized control systems are economic due to the lower cost of the components which can be optimally chosen instead of a generic solution and where the non recurring engineering charges are spread over thousands or millions of units for high volume or very simple fixed automation tasks different techniques are used for example a consumer dishwasher would be controlled by an electromechanical cam timer costing only a few dollars in production quantities a microcontroller based design would be appropriate where hundreds or thousands of units will be produced and so the development cost can be spread over many sales and where the end user would not need to alter the control automotive applications are an example millions of units are built each year and very few end users alter the programming of these controllers however some specialty vehicles such as transit busses economically use plcs instead of custom designed controls because the volumes are low and the development cost would be uneconomic very complex process control such as used in the chemical industry may require algorithms and performance beyond the capability of even high performance plcs very high speed or precision controls may also require customized solutions for example aircraft flight controls programmable controllers are widely used in motion control positioning control and torque control some manufacturers produce motion control units to be integrated with plc so that g code can be used to instruct machine movements plcs may include logic for single variable feedback analog control loop a proportional integral derivative or pid controller a pid loop could be used to control the temperature of a manufacturing process for example historically plcs were usually configured with only a few analog control loops where processes required hundreds or thousands of loops a distributed control system would instead be used however as plcs have become more powerful the boundary between dcs and plc applications has become less clear cut plcs have similar functionality as remote terminal units an rtu however usually does not support control algorithms or control loops as hardware rapidly becomes more powerful and cheaper rtus plcs and dcss are increasingly beginning to overlap in responsibilities and many vendors sell rtus with plc like features and vice versa the industry has standardized on the iec 61131 3 functional block language for creating programs to run on rtus and plcs although nearly all vendors also offer proprietary alternatives and associated development environments digital or discrete signals behave as binary switches yielding simply an on or off signal push buttons limit switches and photoelectric sensors are examples of devices providing a discrete signal discrete signals are sent using either voltage or current where a specific range is designated as on and another as off for example a plc might use 24 v dc i o with values above 22 v dc representing on values below 2vdc representing off and intermediate values undefined initially plcs had only discrete i o analog signals are like volume controls with a range of values between zero and full scale these are typically interpreted as integer values by the plc with various ranges of accuracy depending on the device and the number of bits available to store the data as plcs typically use 16 bit signed binary processors the integer values are limited between 32 768 and 32 767 pressure temperature flow and weight are often represented by analog signals analog signals can use voltage or current with a magnitude proportional to the value of the process signal for example an analog 4 20 ma or 0 10v input would be converted into an integer value of 0 32767 current inputs are less sensitive to electrical noise than voltage inputs as an example say a facility needs to store water in a tank the water is drawn from the tank by another system as needed and our example system must manage the water level in the tank using only digital signals the plc has two digital inputs from float switches when the water level is above the switch it closes a contact and passes a signal to an input the plc uses a digital output to open and close the inlet valve into the tank when the water level drops enough so that the low level float switch is off the plc will open the valve to let more water in once the water level rises enough so that the high level switch is on the plc will shut the inlet to stop the water from overflowing this rung is an example of seal in logic the output is sealed in until some condition breaks the circuit an analog system might use a water pressure sensor or a load cell and an adjustable dripping out of the tank the valve adjusts to slowly drip water back into the tank in this system to avoid flutter adjustments that can wear out the valve many plcs incorporate hysteresis which essentially creates a deadband of activity a technician adjusts this deadband so the valve moves only for a significant change in rate this will in turn minimize the motion of the valve and reduce its wear a real system might combine both approaches using float switches and simple valves to prevent spills and a rate sensor and rate valve to optimize refill rates and prevent water hammer backup and maintenance methods can make a real system very complicated plc programs are typically written in a special application on a personal computer then downloaded by a direct connection cable or over a network to the plc the program is stored in the plc either in battery backed up ram or some other non volatile flash memory often a single plc can be programmed to replace thousands of relays under the iec 61131 3 standard plcs can be programmed using standards based programming languages a graphical programming notation called sequential function charts is available on certain programmable controllers recently the international standard iec 61131 3 has become popular iec 61131 3 currently defines five programming languages for programmable control systems fbd ld st il and sfc these techniques emphasize logical organization of operations while the fundamental concepts of plc programming are common to all manufacturers differences in i o addressing memory organization and instruction sets mean that plc programs are never perfectly interchangeable between different makers even within the same product line of a single manufacturer different models may not be directly compatible the plc was invented in response to the needs of the american automotive manufacturing industry programmable controllers were initially adopted by the automotive industry where software revision replaced the re wiring of hard wired control panels when production models changed before the plc control sequencing and safety interlock logic for manufacturing automobiles was accomplished using hundreds or thousands of relays cam timers and drum sequencers and dedicated closed loop controllers the process for updating such facilities for the yearly model change over was very time consuming and expensive as the relay systems needed to be rewired by skilled electricians in 1968 gm hydramatic issued a request for proposal for an electronic replacement for hard wired relay systems the winning proposal came from bedford associates of bedford massachusetts the first plc designated the 084 because it was bedford associates eighty fourth project was the result bedford associates started a new company dedicated to developing manufacturing selling and servicing this new product modicon which stood for modular digital controller one of the people who worked on that project was dick morley who is considered to be the father of the plc the modicon brand was sold in 1977 to gould electronics and later acquired by german company aeg and then by french schneider electric the current owner one of the very first 084 models built is now on display at modicon s headquarters in north andover massachusetts it was presented to modicon by gm when the unit was retired after nearly twenty years of uninterrupted service modicon used the 84 moniker at the end of its product range until the 984 made its appearance the automotive industry is still one of the largest users of plcs early plcs were designed to replace relay logic systems these plcs were programmed in ladder logic which strongly resembles a schematic diagram of relay logic modern plcs can be programmed in a variety of ways from ladder logic to more traditional programming languages such as basic and c another method is state logic a very high level programming language designed to program plcs based on state transition diagrams many of the earliest plcs expressed all decision making logic in simple ladder logic which appeared similar to electrical schematic diagrams this program notation was chosen to reduce training demands for the existing technicians other early plcs used a form of instruction list programming based on a stack based logic solver early plcs up to the mid 1980s were programmed using proprietary programming panels or special purpose programming terminals which often had dedicated function keys representing the various logical elements of plc programs programs were stored on cassette tape cartridges facilities for printing and documentation were very minimal due to lack of memory capacity the very oldest plcs used non volatile magnetic core memory the functionality of the plc has evolved over the years to include sequential relay control motion control process control distributed control systems and networking the data handling storage processing power and communication capabilities of some modern plcs are approximately equivalent to desktop computers plc like programming combined with remote i o hardware allow a general purpose desktop computer to overlap some plcs in certain applications well known plc brands include siemens allen bradley idec abb mitsubishi omron honeywell schneider electric rockwell automation and general electric 
pl sql is oracle corporation s proprietary procedural extension to the sql database language used in the oracle database some other sql database management systems offer similar extensions to the sql language pl sql s syntax strongly resembles that of ada and just like ada compilers of the 1980s the pl sql runtime system uses diana as intermediate representation the key strength of pl sql is its tight integration with the oracle database pl sql is one of three languages embedded in the oracle database the other two being sql and java pl sql supports variables conditions arrays and exceptions implementations from version 8 of oracle database onwards have included features associated with object orientation and some constructs such as loops pl sql however as a turing complete procedural language that fills in these gaps allows oracle database developers to interface with the underlying relational database in an imperative manner sql statements can make explicit in line calls to pl sql functions or can cause pl sql triggers to fire upon pre defined data manipulation language events pl sql stored procedures performing dml will get compiled into an oracle database to this extent their sql code can undergo syntax checking programmers working in an oracle database environment can construct pl sql blocks of functionality to serve as procedures functions or they can write in line segments of pl sql within sql plus scripts while programmers can readily incorporate sql dml statements into pl sql data definition language statements such as create table drop index etc require the use of dynamic sql earlier versions of oracle database required the use of a complex built in dbms_sql package for dynamic sql where the system needed to explicitly parse and execute an sql statement later versions have included an execute immediate syntax called native dynamic sql which considerably simplifies matters any use of ddl in an oracle database will result in an implicit commit programmers can also use dynamic sql to execute dml where they do not know the exact content of the statement in advance pl sql offers several pre defined packages for specific purposes such pl sql packages include oracle corporation customarily adds more packages and or extends package functionality with each successive release of oracle database anonymous blocks are the basis of standalone pl sql scripts and have the following structure the label and the declare and exception sections are optional exceptions errors which arise during the execution of the code have one of two types user defined exceptions are always raised explicitly by the programmers using the raise or raise_application_error commands in any situation where they have determined that it is impossible for normal execution to continue raise command has the syntax oracle corporation has pre defined several exceptions like no_data_found too_many_rows etc each exception has a sql error number and sql error message associated with it programmers can access these by using the sqlcode and sqlerrm functions the declare section defines and initialises variables if not initialised specifically they default to null for example the symbol functions as an assignment operator to store a value in a variable the major datatypes in pl sql include number integer char varchar2 date timestamp text etc functions in pl sql are a collection of sql and pl sql statements that perform a task and should return a value to the calling environment procedures are the same as functions in that they are also used to perform some task with the difference being that procedures cannot be used in a sql statement and although they can have multiple out parameters they do not return a value anonymous pl sql blocks can be embedded in an oracle precompiler or oci program at run time the program lacking a local pl sql engine sends these blocks to the oracle server where they are compiled and executed likewise interactive tools such as sql plus and enterprise manager lacking a local pl sql engine must send anonymous blocks to oracle packages are the combination of functions procedures variable constants cursors etc which is used to group the related things make for reusable purpose packages usually have two parts a specification and a body although sometimes the body is unnecessary the specification is the interface to your applications it declares the types variables constants exceptions cursors and subprograms available for use the body fully defines cursors and subprograms and so implements the spec to define a numeric variable the programmer appends the variable type number to the name definition to specify the precision and the scale one can further append these in round brackets separated by a comma a selection of other datatypes for numeric variables would include binary_float binary_double dec decimal double precision float integer int numeric real smallint binary_integervariable_name varchar2 text to define a character variable the programmer normally appends the variable type varchar2 to the name definition there follows in brackets the maximum number of characters which the variable can store other datatypes for character variables include variable_name date  01 jan 2005 oracle provides a number of data types that can store dates however date is most commonly used programmers define date variables by appending the datatype code date to a variable name the to_date function can be used to convert strings to date values the function converts the first quoted string into a date using as a definition the second quoted string for example orto convert the dates to strings one uses the function to_char pl sql also supports the use of ansi date and interval literals the following clause gives an 18 month range variable_name table_name column_name type this syntax defines a variable of the type of the referenced column on the referenced tables programmers specify user defined datatypes with the syntax for example this sample program defines its own datatype called t_address which contains the fields name street street_number and postcode so according the example we are able to copy the data from database to the fields in program using this datatype the programmer has defined a variable called v_address and loaded it with data from the address table programmers can address individual attributes in such a structure by means of the dot notation thus v_address street high street the following code segment shows the if then elsif construct the elsif and else parts are optional so it is possible to create simpler if then or if then else constructs the case statement simplifies some large if then else structures case statement can be used with predefined selector pl sql refers to arrays as collections the language offers three types of collections programmers must specify an upper limit for varrays but need not for index by tables or for nested tables the language includes several collection methods used to manipulate collection elements for example first last next prior extend trim delete etc index by tables can be used to simulate associative arrays as in this example of a memo function for ackermann s function in pl sql as a procedural language by definition pl sql provides several iteration constructs including basic loop statements while loops for loops and cursor for loops syntax loops can be terminated by using the exit keyword or by raising an exception cursor for loops automatically open a cursor read in their data and close the cursor againas an alternative the pl sql programmer can pre define the cursor s select statement in advance in order to allow re use or to make the code more understandable the concept of the person_code within the for loop gets expressed with dot notation output pl sql functions analogously to the embedded procedural languages associated with other relational databases sybase ase and microsoft sql server have transact sql postgresql has pl pgsql and ibm db2 includes sql procedural language which conforms to the iso sql s sql psm standard the designers of pl sql modelled its syntax on that of ada both ada and pl sql have pascal as a common ancestor and so pl sql also resembles pascal in numerous aspects the structure of a pl sql package closely resembles the basic pascal s program structure or a borland delphi unit programmers can define global data types constants and static variables public and private in a pl sql package pl sql also allows for the definition of classes and instantiating these as objects in pl sql code this resembles usages in object oriented programming languages like object pascal c and java pl sql refers to a class as an advanced data type or user defined type and defines it as an oracle sql data type as opposed to a pl sql user defined type allowing its use in both the oracle sql engine and the oracle pl sql engine the constructor and methods of an advanced data type are written in pl sql the resulting advanced data type can operate as an object class in pl sql such objects can also persist as column values in oracle database tables pl sql does not resemble transact sql despite superficial similarities porting code from one to the other usually involves non trivial work not only due to the differences in the feature sets of the two languages but also due to the very significant differences in the way oracle and sql server deal with concurrency and locking the fyracle project aims to enable the execution of pl sql code in the open source firebird database 
systems development life cycle or software development life cycle in systems engineering and software engineering refers to the process of creating or altering systems and the models and methodologies that people use to develop these systems the concept generally refers to computer or information systems in software engineering the sdlc concept underpins many kinds of software development methodologies these methodologies form the framework for planning and controlling the creation of an information system the software development process systems development life cycle is any logical process used by a systems analyst to develop an information system including requirements validation training and user ownership an sdlc should result in a high quality system that meets or exceeds customer expectations reaches completion within time and cost estimates works effectively and efficiently in the current and planned information technology infrastructure and is inexpensive to maintain and cost effective to enhance computer systems have become more complex and often link multiple traditional systems potentially supplied by different software vendors to manage this level of complexity a number of system development life cycle models have been created waterfall fountain spiral build and fix rapid prototyping incremental and synchronize and stabilize although the term sdlc can refer to various models it typically denotes a waterfall methodology in project management a project has both a life cycle and a systems development life cycle during which a number of typical activities occur the project life cycle encompasses all the activities of the project while the systems development life cycle focuses on realizing the product requirements systems development life cycle is the oldest formalized methodology for building information systems intended to develop information systems in a very deliberate structured and methodical way reiterating each stage of the life cycle the traditional systems development life cycle originated in the 1960s to develop large scale functional business systems in an age of large scale business conglomerates information systems activities resolved around heavy data processing and number crunching routines in the 1980s the structured systems analysis and design method was based in sdlc ssadm is a systems approach to the analysis and design of information systems produced for the office of government commerce a uk government office concerned with the use of technology in government since the 1980s the traditional life cycle approaches to systems development has been increasingly replaced with alternative approaches and frameworks which attempted to overcome some of the inherent deficiencies of the traditional sdlc systems development life cycle adheres to important phases that are essential for developers such as planning analysis design and implementation and are explained in the section below there are several systems development life cycle models in existence the oldest model that was originally regarded as the systems development life cycle is the waterfall model a sequence of stages in which the output of each stage becomes the input for the next these stages generally follow the same basic steps but many different waterfall methodologies give the steps different names and the number of steps seems to vary between 4 and 7 there is no definitively correct systems development life cycle model but the steps can be characterized and divided in several steps to generate a high level view of the intended project and determine the goals of the project the feasibility study is sometimes used to present the project to upper management in an attempt to gain funding projects are typically evaluated in three areas of feasibility economical operational and technical furthermore it is also used as a reference to keep the project on track and to evaluate the progress of the mis team the mis is also a complement of those phases this phase is also called the analysis phase the goal of systems analysis is to determine where the problem is in attempt to fix the system this step involves breaking down the system in different pieces and drawing diagrams to analyze the situation analysts project goals breaking down functions that need to be created and attempt to engage users so that definite requirements can be defined in systems design functions and operations are described in detail including screen layouts business rules process diagrams and other documentation the output of this stage will describe the new system as a collection of modules or subsystems modular and subsystem programming code will be accomplished during this stage unit testing and module testing are done in this stage by the developers this stage is intermingled with the next in that individual modules will need testing before integration to the main project planning in software life cycle involves setting goals defining targets establishing schedules and estimating budgets for an entire software project the code is tested at various levels in software testing unit system and user acceptance testing are often performed this is a grey area as many different opinions exist as to what the stages of testing are and how much if any iteration occurs iteration is not generally part of the waterfall model but usually some occurs at this stage types of testing the deployment of the system includes changes and enhancements before the decommissioning or sunset of the system maintaining the system is an important aspect of sdlc as key personnel change positions in the organization new changes will be implemented which will require system updates the systems development life cycle phases serve as a programmatic guide to project activity and provide a flexible but consistent way to conduct projects to a depth matching the scope of the project each sdlc phase objectives are described in this section with key deliverables a description of recommended tasks and a summary of related control objectives for effective management it is critical for the project manager to establish and monitor control objectives during each sdlc phase while executing projects control objectives help to provide a clear statement of the desired result or purpose and should be used throughout the entire sdlc process control objectives can be grouped into major categories and relate to the sdlc phases as shown in the figure to manage and control an sdlc initiative each project will be required to establish some degree of a work breakdown structure wbs to capture and schedule the work necessary to complete the project the wbs and all programmatic material should be kept in the project description section of the project notebook the wbs format is mostly left to the project manager to establish in a way that best describes the project work there are some key areas that must be defined in the wbs as part of the sdlc policy the following diagram describes three key areas that will be addressed in the wbs in a manner established by the project manager the upper section of the work breakdown structure should identify the major phases and milestones of the project in a summary fashion in addition the upper section should provide an overview of the full scope and timeline of the project and will be part of the initial project description effort leading to project approval the middle section of the wbs is based on the seven systems development life cycle phases as a guide for wbs task development the wbs elements should consist of milestones and tasks as opposed to activities and have a definitive period each task must have a measurable output a wbs task may rely on one or more activities and may require close coordination with other tasks either internal or external to the project any part of the project needing support from contractors should have a statement of work written to include the appropriate tasks from the sdlc phases the development of a sow does not occur during a specific phase of sdlc but is developed to include the work from the sdlc process that may be conducted by external resources such as contractors baselines are an important part of the systems development life cycle these baselines are established after four of the five phases of the sdlc and are critical to the iterative nature of the model each baseline is considered as a milestone in the sdlc complementary software development methods to systems development life cycle are few people in the modern computing world would use a strict waterfall model for their systems development life cycle as many modern methodologies have superseded this thinking some will argue that the sdlc no longer applies to models like agile computing but it is still a term widely in use in technology circles a comparison of the strength and weaknesses of sdlc an alternative to the sdlc is rapid application development which combines prototyping joint application development and implementation of case tools the advantages of rad are speed reduced development cost and active user involvement in the development process it should not be assumed that just because the waterfall model is the oldest original sdlc model that it is the most efficient system at one time the model was beneficial mostly to the world of automating activities that were assigned to clerks and accountants however the world of technological evolution is demanding that systems have a greater functionality that would assist help desk technicians administrators or information technology specialists analysts 
37signals is a privately held web application company based in chicago illinois united states the firm was co founded in 1999 by ceo jason fried carlos segura and ernest kim as a web design company with a self described focus on usability simplicity and clarity in design and writing 37signals also produces a blog signal vs noise segura left in 2000 and kim left in 2003 leaving fried as the only remaining founder since 2003 37signals has been primarily a developer and provider of business and personal productivity web applications its first application was basecamp this was followed by ta da list backpack writeboard campfire and highrise 37signals was responsible for launching the open source web application framework software ruby on rails which it uses in its own applications the products have gained popularity using what has come to be known as a freemium business model the company is named for the 37 radio telescope signals identified by astronomer paul horowitz as potential messages from extraterrestrial intelligence 37signals designed meetup com and redesigned sites for customers such as panera bread and shopping com in 2000 they created the enormicom website a satire of the dot com era in 2003 37signals launched a web design service called 37express where for a set fee they would redesign one page on a website in one week also in 2003 37signals began work on a web application for project management named basecamp originally intended for internal use which took the company in a new direction basecamp has since been followed by five other web applications see the products section below for more details by 2005 the company had moved away from consulting work to focus exclusively on its web applications each application had a free limited feature version and most had monthly subscription levels with more features by april 2009 they had stopped offering free versions of most applications the ruby on rails web application framework was extracted from the work on basecamp and released as open source on july 20 2006 the company announced that jeff bezos had acquired a minority stake through his personal investment company bezos expeditions 37signals promotes an internally developed agile software development methodology and philosophy called getting real getting real eschews formal programming methodology and focuses on creating useful alpha software with small teams then iterating to a simple useful application based in part on real world customer feedback the company initially expanded without venture capital and advocated the self funded startup approach although it has since taken investment from jeff bezos 37signals has held seminars about their methods in chicago and other u s cities the philosophy recommends online advertising rather than standard methods the company uses apple computers exclusively and has said it would never hire someone who doesn t use a mac ruby on rails is a free web application framework created by david heinemeier hansson one of the 37signals programmers it was originally used to make 37signals first product basecamp and was since extracted and released as open source in 2004 as well as being the framework that 37signals use to make their web applications often shortened to rails or ror it is programmed in the ruby programming language the official ruby on rails website states that it is sponsored by 37signals the development of ruby on rails is now managed by the rails core team with hansson still contributing queen bee is 37signals custom internal statistics billing and administration application based around centralization the company s blog signal vs noise was launched in 1999 and is self described as featuring entrepreneurship design experience simplicity constraints pop culture our products products we like and more it is regularly updated by the company s employees and allows commenting by readers the company often uses the blog to communicate with users regarding new products features and their design philosophy getting real content since february 2005 is archived and the blog is powered by a custom built blogging tool although it formerly used movable type the blog is written in a casual style and sometimes contains profanity revenue from the blog is gained through three methods of advertising small image ads via the deck a small advertising network and a 37signals job board the company also maintains another blog the 37signals product blog it launched on may 15 2007 and is self described as news talk opinions tips and tricks about our entire product line it replaced the individual blogs for basecamp and backpack the company s website also has a section called developerland self described as apis examples wrappers forums and everything else you need to integrate with 37signals products as of march 2007 37signals has created four commercial web applications and two free web applications all of the 37signals web applications use a page based design as opposed to the window based interface of web desktops such as youos all of the 37signals applications have feature limited free versions and 30 day free trials of the full versions after which the user must either pay to continue or stop using the application basecamp is 37signals first product a web based project management tool that launched in 2004 the ruby on rails framework was extracted from the basecamp project the primary features of basecamp include to do lists milestone management forum like messaging file sharing and time tracking 37signals have since created an api for basecamp allowing interaction with other web applications as well as desktop applications one example of the api in use is a mac os x dashboard widget ta da list 37signals first free web application is a free to do list application launched in january 2005 to use the application users must create an account which will manage all of their to do lists it is possible to track changes via an rss feed ta da list allows lists to be shared either publicly or with specific people by sending an automated email containing a private uri ta da list is based on the lists in both basecamp and backpack backpack is a web based personal information manager and intranet for small business the application has two main functions user created pages and an icalendar format calendar features of the user created pages include to do lists inline photo galleries notes and file attachments and page sharing features of the calendar include support for icalendar email sms reminders color coding of calendars and icalendar sharing writeboard is a free collaborative text editor which allows creation of an unlimited number of web based text documents each writeboard has a separate user name and password and changes can be monitored via an rss feed writeboard supports diff allowing users to compare changes made to the document and textile for easy to use and simple formatting writeboard has since been integrated with both basecamp and backpack campfire is a web based business oriented online chat service it was released on february 16 2006 the application uses ajax technology for real time communication and supports optional 128 bit ssl encryption to use the application users must either create a new chat room or be invited to one unless a chat room is specifically chosen to be off the record browsable transcripts of chats and uploaded files are stored for future reference one of the main features of the application referred to by 37signals as live image previews is that image uploads in gif png or jpeg formats are represented as thumbnails and automatically shown in the chat highrise is a web based application focusing mainly on shared contact management and basic crm tasks the application was released on march 20 2007 features of highrise include person pages and company pages which can contain images notes company info contact details etc to do lists much like those seen in basecamp or backpack and cases which are pages categories within which related notes images and people can be kept the other main features of highrise are its support for interaction via e mail carbon copy or email forwarding and support for importing data from vcards microsoft outlook act or data from basecamp accounts since june 25 2007 37signals basecamp application has offered a single sign on authentication option to users using the openid authentication service since then both highrise and backpack have had openid open bar support added open bar effectively an openid implementation is set up by the user logging into their basecamp backpack highrise account and entering their unique openid identification uri in their account settings if a user with multiple accounts were to do this when logged in to any of the accounts configured with openid a black open bar would appear at the top of the page and by hovering over the product name a drop down list of all the user s accounts on that product would appear also if the user has openid configured on both their basecamp highrise and or backpack accounts both product names will appear in the open bar and allow switching between applications without having to be logged in to each one individually open bar support was added to backpack on july 27 2007 whilst not primarily an online advertising company 37signals is the creator and maintainer of an advertising system the 37signals job board in october 2006 the company introduced a separate gig board for one off jobs but it often displayed few postings and quietly disappeared in february 2009 the company is a founding member of the deck a small online advertising network with 19 members 37signals often advertises its products on the deck and occasional job positions on the job board 
delta encoding is a way of storing or transmitting data in the form of differences between sequential data rather than complete files delta encoding is sometimes called delta compression particularly where archival histories of changes are required the differences are recorded in discrete files called deltas or diffs after the unix file comparison utility diff because changes are often small delta encoding greatly reduces data redundancy collections of unique deltas are substantially more space efficient than their non encoded equivalents from a logical point of view the difference between two data values is the information required to obtain one value from the other the difference between identical values is often called 0 or the neutral element a good delta should be minimal or ambiguous unless one element of a pair is present perhaps the simplest example is storing values of bytes as differences between sequential values rather than the values themselves so instead of 2 4 6 9 7 we would store 2 2 2 3 2 this is not very useful when used alone but it can help further compression of data in which sequential values occur often iff 8svx sound format applies this encoding to raw sound data before applying compression to it unfortunately not even all 8 bit sound samples compress better when delta encoded and the usability of delta encoding is even smaller for 16 bit and better samples therefore compression algorithms often choose to delta encode only when the compression is better than without however in video compression delta frames can considerably reduce frame size and are used in virtually every video compression codec a delta can be defined in 2 ways symmetric delta and directed delta a symmetric delta can be expressed as where v1 and v2 represent two successive versions a directed delta also called a change is a sequence of change operations which when applied to one version v1 yields another version v2 a variation of delta encoding which encodes differences between the prefixes or suffixes of strings is called incremental encoding it is particularly effective for sorted lists with small differences between strings such as a list of words from a dictionary in delta encoded transmission over a network where only a single copy of the file is available at each end of the communication channel special error control codes are used to detect which parts of the file has changed since its previous version the nature of the data to be encoded influences the effectiveness of a particular compression algorithm delta encoding performs best when data has small or constant variation for an unsorted data set there may be little to no compression possible with this method the following c code performs a simple form of delta encoding and decoding another instance of use of delta encoding is rfc 3229 delta encoding in http which proposes that http servers should be able to send updated web pages in the form of differences between versions which should decrease internet traffic as most pages change slowly over time rather than being completely rewritten repeatedly this document describes how delta encoding can be supported as a compatible extension to http 1 1 many http requests cause the retrieval of slightly modified instances of resources for which the client already has a cache entry research has shown that such modifying updates are frequent and that the modifications are typically much smaller than the actual entity in such cases http would make more efficient use of network bandwidth if it could transfer a minimal description of the changes rather than the entire new instance of the resource 
sas is an integrated system of software products provided by sas institute that enables the programmer to perform in addition sas has many business solutions that enable large scale software solutions for areas such as it management human resource management financial management business intelligence customer relationship management and more sas is driven by sas programs that define a sequence of operations to be performed on data stored as tables although non programmer graphical user interfaces to sas exist most of the time these guis are just a front end to automate or facilitate generation of sas programs sas components expose their functionalities via application programming interfaces in the form of statements and procedures a sas program is composed of three major parts the data step procedure steps and a macro language sas library engines and remote library services allow access to data stored in external data structures and on remote computer platforms the data step section of a sas program like other database oriented fourth generation programming languages such as sql or focus assumes a default file structure and automates the process of identifying files to the operating system opening the input file reading the next record opening the output file writing the next record and closing the files this allows the user programmer to concentrate on the details of working with the data within each record in effect working almost entirely within an implicit program loop that runs for each record all other tasks are accomplished by procedures that operate on the data set as a whole typical tasks include printing or performing statistical analysis and may just require the user programmer to identify the data set procedures are not restricted to only one behavior and thus allow extensive customization controlled by mini languages defined within the procedures sas also has an extensive sql procedure allowing sql programmers to use the system with little additional knowledge there are macro programming extensions that allow for rationalization of repetitive sections of the program proper imperative and procedural programming constructs can be simulated by use of the open code macros or the sas iml component macro code in a sas program if any undergoes preprocessing at runtime data steps are compiled and procedures are interpreted and run in the sequence they appear in the sas program a sas program requires the sas software to run compared to general purpose programming languages this structure allows the user programmer to be less familiar with the technical details of the data and how it is stored and relatively more familiar with the information contained in the data this blurs the line between user and programmer appealing to individuals who fall more into the business or research area and less in the information technology area since sas does not enforce a structured centralized approach to data and infrastructure management sas runs on ibm mainframes unix machines openvms alpha and microsoft windows and code is almost transparently moved between these environments older versions have supported pc dos the apple macintosh vms vm cms data general aos and os 2 sas was conceived by anthony j barr in 1966 as a north carolina state university graduate student from 1962 to 1964 barr had created an analysis of variance modeling language inspired by the notation of statistician maurice kendall followed by a multiple regression program that generated machine code for performing algebraic transformations of the raw data drawing on those programs and his experience with structured data files he created sas placing statistical procedures into a formatted file framework from 1966 to 1968 barr developed the fundamental structure and language of sas in january 1968 barr and james goodnight collaborated integrating new multiple regression and analysis of variance routines developed by goodnight into barr s framework goodnight s routines made the handling of basic statistical analysis more robust and his later implementation of the general linear model greatly increased the analytical power of the system by 1971 sas was gaining popularity within the academic community and by 1972 industry was making use of sas one strength of the system was analyzing experiments with missing data which was useful to the pharmaceutical and agricultural industries among others in 1973 john sall joined the project making extensive programming contributions in econometrics time series and matrix algebra other participants in the early years included caroll g perkins jolayne w service and jane t helwig perkins made programming contributions service and helwig created the early documentation in 1976 sas institute inc was incorporated by barr goodnight sall and helwig sas consists of a number of components which organizations separately license and install as required where many other languages refer to tables rows and columns fields sas uses the terms data sets observations and variables respectively this usage derives from its statistical heritage and is shared by spss another statistical package there are only two kinds of variables in sas numeric and character by default all numeric variables are stored as real it is possible to reduce precision in external storage only date and datetime variables are numeric variables that inherit the c tradition and are stored as either the number of days or seconds from an epoch of 1960 01 01 00 00 00 sas uses data steps and procedures to analyze and manipulate data by default a data step iterates through each observation in a data set this data step creates a new data set bbb that includes those observations from data set aaa that had charges greater than 100 procedures that can summarize data are available in sas the proc freq procedure shows a frequency distribution of a given variable in a data set sas also allows direct subsetting of rows and or columns of the data used as input to a procedure the two previous examples could be replaced by the following the same program could produce a data set containing the frequency distribution the sas macro language enables such features as conditional execution of sas language components either across multiple data steps and proc steps or within a single such step it is best considered as a code generator although it can also be used merely to establish static values that can be reused throughout the program and altered as needed for instance the above example could be re used in many pieces of code by rewriting it as a macro and further other macro variables could be used for both conditional execution as well as modification of the functionality of the step as shown below the first procedure is modified to include a new parameter limitobs which if used subsets the data before performing the frequency analysis a second macro provides overall program control functionality including a flag indicating whether the frequency analysis should be performed at all sas also features sql which can be used to create modify or query sas datasets or external database tables accessed with a sas libname engine for example duplicate records could be extracted from a table for analysis the proc print procedure allows the user to display information in ways not possible using only the sql select statement sas features scl which can be used to create object oriented programs scl programs provide a robust library of features not available in base sas or the sas macro languagesas 71 was the first limited release of the system the first manual for sas was printed at this time approximately 60 pages long the data step was implemented regression and analysis of variance were the main uses of the program this more robust release was the first to achieve wide distribution it included a substantial user s guide 260 pages in length the merge statement was introduced in this release adding the ability to perform a database join on two data sets this release also introduced the comprehensive handling of missing data sas 76 was a complete system level rewrite featuring an open architecture for adding and extending procedures and for extending the compiler the input and infile statements were significantly enhanced to read virtually all data formats in use on the ibm mainframe report generation was added through the put and file statements the capacity to analyze general linear models was added 1980 saw the addition of sas graph a graphing component and sas ets for econometric and time series analysis in 1981 sas fsp followed providing full screen interactive data entry editing browsing retrieval and letter writing in 1983 full screen spreadsheet capabilities were introduced for ibm mainframes sas 82 no longer required sas databases to have direct access organization because sas 82 removed location dependent information from databases this permitted sas to work with datasets on tape and other media besides disk in the early 1980s sas institute released version 4 the first version for non ibm computers it was written mostly in a subset of the pl i language to run on several minicomputer manufacturers operating systems and hardware data general s aos vs digital equipment s vax vms and prime computer s primos the version was colloquially called portable sas because most of the code was portable i e the same code would run under different operating systems version 6 represented a major milestone for sas while it was superficially similar to the user the major change was under the hood where the software was rewritten from its fortran origins followed by pl i and mainframe assembly language in version 6 sas was rewritten in c to provide enhanced portability between operating systems as well as access to an increasing pool of c programmers compared to the shrinking pool of pl i programmers this was the first version to run on unix ms dos and windows platforms the dos versions were incomplete implementations of the version 6 spec some functions and formats were unavailable as were sql and related items such as indexing and where subsetting dos memory limitations restricted the size of some user defined items the mainframe version of sas 6 changed the physical format of sas databases from direct files to flat files the practical benefit of this change is that a sas 6 database can be copied from any media with any copying tool in 1984 a project management component was added in 1985 sas af software econometrics and time series analysis component and interactive matrix programming software was introduced ms dos sas was introduced along with a link to mainframe sas in 1986 statistical quality improvement component is added sas iml and sas stat software is released for personal computers 1987 saw concurrent update access provided for sas data sets with sas share software database interfaces are introduced for db2 and sql ds in 1988 multivendor architecture concept is introduced sas access software is released support for unix based hardware announced sas assist software for building user friendly front end menus is introduced new sas cpe software establishes sas as innovator in computer performance evaluation version 6 03 for ms dos is released 6 06 for mvs cms and openvms is announced in 1990 the same year the last ms dos version is released data visualization capabilities added in 1991 with sas insight software in 1992 sas calc sas toolkit sas ph clinical and sas lab software is released in 1993 software for building customized executive information systems is introduced release 6 08 for mvs cms vms vse os 2 and windows is announced 1994 saw the addition of odbc support plus sas spectraview and sas share net components 6 09 saw the addition of a data step debugger 6 09e for mvs 6 10 in 1995 was a microsoft windows release and the first release for the apple macintosh version 6 was the first and last series to run on the macintosh jmp also produced by the sas institute is the software package the company produces for the macintosh also in 1995 6 11 was released for windows 95 windows nt and unix 6 12 in 1996 sas announces web enablement of sas software scalable performance data server is introduced in 1997 sas warehouse administrator and sas intrnet software goes into production 1998 sees sas introduce a customer relationship management solution and an erp access interface sas access interface for sap r 3 sas is also the first to release ole db for olap and releases holap solution balanced scorecard sas enterprise reporter and hr vision are released first release of sas enterprise miner 1999 sees the releases of hr vision software the first end to end decision support system for human resources reporting and analysis and risk dimensions software an end to end risk management solution ms dos versions are abandoned because of y2k issues and lack of continued demand in 2000 sas shipped enterprise guide and ported its software to linux the output delivery system debuted in version 7 as did long variable names storage of long character strings in variables and a much improved built in text editor the enhanced editor version 7 saw the synchronisation of features between the various platforms for a particular version number version 7 was a precursor to version 8 it was believed sas institute released a snapshot from their development on version 8 to meet a deadline promise sas institute recommended that sites wait until version 8 before deploying the new software released about 1999 8 0 8 1 8 2 were unix microsoft windows cms and z os releases key features long variable names output delivery system sas 8 1 was released in 2000 sas 8 2 was released in 2001 in version 9 sas institute added the sas management console parallel processing javaobj ods oo and national language support again the sas institute recommended sites delay deployment until 9 1 sas version 9 is running on windows unix linux and z os support for cms was dropped sas 9 1 was released in 2003 sas 9 1 2 was released in 2004 sas 9 1 3 was released in 2005 sas 9 2 is the latest release 9 2 is the only version that works on microsoft vista and was demonstrated at sas global forum 2008 a list of features added to this release of sas can be seen at the what s new in sas web page 
sas 9 2 will be released incrementally in three phases 
1 mva based products eg sas base sas stat sas graph nothing that relies on metadata limited availability from march 2008 because most users rely on the metadata server or products released in phased 3 
2 enterprise intelligence platform metadata server for business intelligence and data integration availability from around august 2008 
3 client software for metadata driven analytics and business solutions enterprise miner text miner model manager solutions include financial retail health life science availability unknown probably 2nd quarter 2009 there are several important additions to base sas in version 9 the new hash object now allows functionality similar to the merge statement without sorting data or building formats the function library was enlarged and many functions have new parameters perl regular expressions are now supported as opposed to the old regular expression facility which was incompatible with most other implementations of regular expressions long format names are now supported sas had been criticized for its relatively poor graphics when compared with other statistical software packages with the release of the output delivery system for statistical graphics extension in sas 7 the graphics have improved significantly critics also cite the existence of free alternatives that have similar statistics functionality the development tools provided which include the enhanced text editor log data step debugger scl debugger are also outdated compared to what other development environments provide debugging tools are especially lacking finding bugs in modern sas programs that use many macros can be complex sas will often not note the correct line number of execution when reporting an error wuu sas systems
the zachman framework is a framework for enterprise architecture which provides a formal and highly structured way of viewing and defining an enterprise the framework in practice is used for organizing enterprise architectural artifacts in a way that takes into account both these artifacts may include design documents specifications and models the framework is in essence a matrix it is named after its creator john zachman who first developed the concept in the 1980s at ibm it has been updated several times since the term zachman framework has multiple meanings it can refer to any of the frameworks proposed by john zachman in other sources the zachman framework is introduced as a framework originated by and named after john zachman represented in numerous ways see image this framework is explained as for example beside the frameworks developed by john zachman numerous extensions and or applications have been developed which are also sometimes called zachman frameworks the zachman framework summarizes a collection of perspectives involved in enterprise architecture these perspectives are represented in a two dimensional matrix that defines along the rows the type of stakeholders and with the columns the aspects of the architecture the framework does not define a process for an architecture rather the matrix is a template that must be filled in by the processes specifically required by the organization if these processes do not exist already the framework helps identify these gaps in the architecture the framework is a simple and logical structure for classifying and organizing the descriptive representation of an enterprise it is significant to both the management of the enterprise and the actors involved in the development of enterprise s systems while the framework is focused on the application oriented area of enterprise architecture its scope includes non it components such as people processes and time making it an appropriate addition to the overall it strategy toolkit for cios furthermore the zachman framework provides a common context for understanding a complex structure the framework enables communication among the various participants involved in developing or changing the structure architecture is the glue that holds the structure together the framework defines sets of architectures that contain the development pieces of the structure in the 1980s john zachman had been involved at ibm in the development of business system planning a method for analyzing defining and designing an information architecture of organizations in 1982 zachman had already concluded that these analyses could reach far beyond automating systems design and managing data into the realms of strategic business planning and management science in general it may be employed to the in that time considered more esoteric issues of enterprise architecture to data driven systems design to data classification criteria and some more in the 1987 article a framework for information systems architecture zachman noted that the term architecture was used loosely by information systems professionals and meant different things to planners designers programmers communication specialists and others in searching for an objective independent basis upon which to develop a framework for information systems architecture zachman looked at the field of classical architecture and a variety of complex engineering projects in industry he saw a similar approach and concluded that architectures exist on many levels and involves at least three perspectives raw material or data function of processes and location or networks in the 1992 article extending and formalizing the framework for information systems architecture john f sowa and john zachman presents the framework and its recent extensions and shows how it can be formalized in the notation of conceptual graphs according to stan locke 2008 later during the 1990seventually the chosen terms in the framework are a move towards a more generic business language on the enterprise framework as shown in the diagram below in the 1997 paper concepts of the framework for enterprise architecture zachman explained that the framework should be referred to as a framework for enterprise architecture and should have from the beginning in the early 1980s however according to zachman there was little interest in the idea of enterprise reengineering or enterprise modeling and the use of formalisms and models was generally limited to some aspects of application development within the information systems community in 2008 zachman enterprise introduced the zachman framework a concise definition as a new zachman framework standard since the 1990s several extended frameworks have been proposed such as the basic idea behind the zachman framework is that the same complex thing or item can be described for different purposes in different ways using different types of descriptions the zachman framework provides the thirty six necessary categories for completely describing anything especially complex things like manufactured goods constructed structures and enterprises the framework provides six increasingly detailed views or levels of abstraction from six different perspectives it allows different people to look at the same thing from different perspectives this creates a holistic view of the environment an important capability illustrated in the figure each row represents a total view of the solution from a particular perspective an upper row or perspective does not necessarily have a more comprehensive understanding of the whole than a lower perspective nor does an upper row decompose into greater detail in a lower row each row represents a distinct unique perspective however the deliverables from each perspective must provide sufficient detail to define the solution at the level of perspective and must translate to the next lower row explicitly each perspective must take into account the requirements of the other perspectives and the restraint those perspectives impose the constraints of each perspective are additive for example the constraints of higher rows affect the rows below the constraints of lower rows can but do not necessarily affect the higher rows understanding the requirements and constraints necessitates communication of knowledge and understanding from perspective to perspective the framework points the vertical direction for that communication between perspectives in the 1997 zachman enterprise architecture framework the rows are described as follows in summary each perspective focuses attention on the same fundamental questions then answers those questions from that viewpoint creating different descriptive representations which translate from higher to lower perspectives the basic model for the focus remains constant the basic model of each column is uniquely defined yet related across and down the matrix in addition the six categories of enterprise architecture components and the underlying interrogatives that they answer form the columns of the zachman enterprise architecture framework and these are in zachman s opinion the single factor that makes his framework unique is that each element on either axis of the matrix is explicitly distinguishable from all the other elements on that axis the representations in each cell of the matrix are not merely successive levels of increasing detail but actually are different representations different in context meaning motivation and use because each of the elements on either axis is explicitly different from the others it is possible to define precisely what belongs in each cell the kinds of models or architectural descriptive representations are made explicit at the intersections of the rows and columns an intersection is referred to as a cell because a cell is created by the intersection of a perspective and a focus each is distinctive and unique since each cell is distinctive and unique the contents of the cell are normalized and explicit per the perspective s focus since the product development in each cell or the problem solution embodied by the cell is the answer to a question from a perspective typically the models or descriptions are higher level depictions or the surface answers of the cell the refined models or designs supporting that answer are the detailed descriptions within the cell decomposition takes place within each cell if a cell is not made explicit it is implicit if it is implicit the risk of making assumptions about these cells exists if the assumptions are valid then time and money are saved if however the assumptions are invalid it is likely to increase costs and exceed the schedule for implementation the framework comes with a set of rules the framework is generic in that it can be used to classify the descriptive representations of any physical object as well as conceptual objects such as enterprises it is also recursive in that it can be used to analyze the architectural composition of itself although the framework will carry the relation from one column to the other it is still a fundamental structural representation of the enterprise and not a flow representation since the 1990s the zachman framework is widely used as a means of providing structure for information engineering style enterprise modeling the zachman framework can be applied both in commercial companies and in government agencies within a government organization the framework can be applied it an entire agency at an abstract level or it can be applied to various departments offices programs subunits and even to basic operational entities zachman framework is applied in customized frameworks such as the teaf build around the similiar frameworks the teaf matrix teaf matrix of views and perspectives framework for ea direction description and accomplishment overview teaf products teaf work products for ea direction description and accomplishment other sources zachman framework is also used as a framework to describe standards for example standards for healthcare and healthcare information system each cell of the framework contains such a series of standards for healthcare and healthcare information system an other application of the zachman framework is as reference model for other enterprise architectures see for example these four eap mapped to the zachman framework 1999mapping the c4isr 1999dod products map to the zachman framework cells 2003 mapping a part of the dodaf 2007 other examples less obvious are the ways the original zachman framework has stimulated the development of other enterprise architecture frameworks such as in the nist enterprise architecture model the c4isr ae the doe ae and the dodaf nist enterprise architecture model c4isr ae 1997 doe ae 1998 dodaf 2003 the zachman framework methodology has for example been used in the us va department to develop and maintain its one va enterprise architecture in 2001 this methodology required them to define all aspects of the va enterprise from a business process data technical location personnel and requirements perspective the next step in implementing the zachman methodology has been to define all functions related to each business process and identify associated data elements once identified duplication of function and inconsistency in data definition can be identified the hard job then followed to de conflict the data definitions and resolve duplicative implementations of the same business function integrated process flow for va it projects va zachman framework portalva ea repository introduction a tutorial on the zachman architecture frameworkthe department of veterans affairs in the new millennium was planning to implement a enterprise architecture full based on the zachman framework eventually a enterprise architecture repository is created at the macro level by the zachman framework and at a cell level by the meta model outlined below the primary strength of the zachman framework is that it explicitly shows that many views needs to be addressed by enterprise architecture it also has some potential problems 
united nations electronic data interchange for administration commerce and transport is the international edi standard developed under the united nations the work of maintenance and further development of this standard is done through the united nations centre for trade facilitation and electronic business under the un economic commission for europe in the finance domain working group un cefact tbg5 edifact has been adopted by the international organization for standardization as the iso standard iso 9735 the edifact standard providessee below for an example of an edifact message used to answer to a product availability request note the line breaks after each segment in this example have been added for readability there are typically no line breaks in edi data unh 1 paores 93 1 ia this is the header segment which is required at the start of every message this code specifies that the message name and version is paores 93 revision 1 and it was defined by the organisation ia ift 3 no more flights this is an interactive free text segment containing the text no more flights unt 13 1 this is the tail segment it indicated that the message sent contains 13 segments edifact has a hierarchical structure where the top level is referred to as an interchange and lower levels contain multiple messages which consist of segments which in turn consist of composites the final iteration is an element which is derived from the united nations trade data element directory and are normalised throughout the edifact standard a group or segment can be mandatory or conditional and can be specified to repeat for example c99 indicates between 0 and 99 repetitions of a segment or group while m99 signifies between 1 and 99 repetitions a group like a message is a sequence of segments or groups the first segment or group beneath a group must be mandatory and the group should be made conditional if the logic of the situation demands it there is an apparent battle between xml and edifact an equivalent xml message has a larger file size than an edifact message but it is easier for users to read another possible explanation is that compatibility is being favored over performance since more tools exist to work with xml data than with edifact edifact messages can be as much as one tenth the size of xml messages that makes xml less attractive for very high volume applications an advantage of edifact is the availability of agreed message contents which xml must leverage to develop its own similar agreed contents rosettanet is one of the emerging xml standards and is widely used in semiconductors and high tech industries ubl is another currently being adopted by scandinavian governments as a legally required standard for sending invoices to governments and was enforced in february 2005 that all invoices to the danish government must be sent in an electronic format ebxml is another xml standard built by un cefact and is often seen as a standard best suited for small and medium enterprises however edifact is likely to remain the most widely used in high tech civil aviation retail and tourism industries due to the amount of software that leverages the standard and the need for integration between new systems and legacy systems europe has a large edifact installed base because it adopted the technology early while the asian region adopted b2b in later implementations and is therefore using more xml standards edifact will grow further in europe s energy market where it is a current requirement 
ski combinator calculus is a computational system that is a reduced untyped version of lambda calculus all operations in lambda calculus are expressed in ski as binary trees whose leaves are one of the three symbols s k and i in fact the symbol i is added only for convenience and just the other two suffice for all of the purposes of the ski system although the most formal representation of the objects in this system requires binary trees they are usually represented for typesettability as parenthesized expressions either with all the subtrees parenthesized or only the right side children subtrees parenthesized so the tree whose left subtree is the tree ks and whose right subtree is the tree sk is usually typed as or more simply as ks instead of being fully drawn as a tree informally and using programming language jargon a tree can be thought of as a function x applied to an argument y when evaluated the tree returns a value i e transforms into another tree of course all three of the function the argument and the value are either combinators or binary trees and if they are binary trees they too may be thought of as functions whenever the need arises the evaluation operation is defined as follows i returns its argument k when applied to any argument x yields a one argument constant function kx which when applied to any argument returns x s is a substitution operator it takes three arguments and then returns the first argument applied to the third which is then applied to the result of the second argument applied to the third more clearly example computation sksk evaluates to kk by the s rule then if we evaluate kk we get k by the k rule as no further rule can be applied the computation halts here note that for all trees x and all trees y skxy will always evaluate to y in two steps and the ultimate result of evaluating skxy will always equal the result of evaluating y we say that skx and i are functionally equivalent because they always yield the same result when applied to any y note that from these definitions it can be shown that ski calculus is not the minimum system that can fully perform the computations of lambda calculus as all occurrences of i in any expression can be replaced by or or and the resulting expression will yield the same result so the i is merely syntactic sugar in fact it is possible to define a complete system using only one combinator an example is chris barker s iota combinator defined as follows the terms and derivations in this system can also be more formally defined terms the set t of terms is defined recursively by the following rules derivations a derivation is a finite sequence of terms defined recursively by the following rules assuming a sequence is a valid derivation to begin with it can be extended using these rules sii is an expression that takes an argument and applies that argument to itself one interesting property of this is that it makes the expression sii irreducible another thing that results from this is that it allows you to write a function that applies something to the self application of something else this function can be used to achieve recursion if  is the function that applies  to the self application of something else then self applying  performs  recursively on  more clearly if then s k reverses the following two terms ski combinator calculus can also implement boolean logic in the form of an if then else structure an if then else structure consists of a boolean expression that is either t or f and two arguments such that andthe key is in defining the two boolean expressions the first works just like one of our basic combinators the second is also fairly simple once true and false are defined all boolean logic can be implemented in terms of if then else structures boolean not works the same as the if then else structure with false and true as the second and third values so it can be implemented as a postfix operation if this is put in an if then else structure it can be shown that this has the expected resultboolean or works the same as an if then else structure with true as the second value so it can be implemented as an infix operation if this is put in an if then else structure it can be shown that this has the expected result boolean and works the same as an if then else structure with false as the third value so it can be implemented as a postfix operation if this is put in an if then else structure it can be shown that this has the expected result because this defines true false not or and and in terms of ski notation this proves that the ski system can fully express boolean logic the combinators k and s correspond to two well known axioms of sentential logic ak a 
as   function application corresponds to the rule modus ponens mp from a and a b infer b the axioms ak and as and the rule mp are complete for the implicational fragment of intuitionistic logic in order for combinatory logic to have as a model 
ole for process control which stands for object linking and embedding for process control is the original name for a standards specification developed in 1996 by an industrial automation industry task force the standard specifies the communication of real time plant data between control devices from different manufacturers after the initial release the opc foundation was created to maintain the standard since then standards have been added and names have been changed as of june 2006 opc is a series of standards specifications  the first standard is now called the data access specification or opc data access or opc data access specification while opc originally stood for ole for process control the official stance of the opc foundation is that opc is no longer an acronym and the technology is simply known as opc one of the reasons behind this is while opc is heavily used within the process industries it can be and is widely used in discrete manufacturing as well hence opc is known for more than just its applications within process control the opc specification was based on the ole com and dcom technologies developed by microsoft for the microsoft windows operating system family the specification defined a standard set of objects interfaces and methods for use in process control and manufacturing automation applications to facilitate interoperability opc was designed to bridge windows based applications and process control hardware and software applications standard defines consistent method of accessing field data from plant floor devices this method remains the same regardless of the type and source of data opc servers provide a method for many different software packages to access data from a process control device such as a plc or dcs traditionally any time a package needed access to data from a device a custom interface or driver had to be written the purpose of opc is to define a common interface that is written once and then reused by any business scada hmi or custom software packages once an opc server is written for a particular device it can be reused by any application that is able to act as an opc client opc servers use microsoft s ole technology to communicate with clients com technology permits a standard for real time information exchange between software applications and process hardware to be defined the opc unified architecture has been specified and is being tested and implemented through its early adopters program it can be implemented with java microsoft net or c eliminating the need to use a microsoft windows based platform of earlier opc versions ua combines the functionality of the existing opc interfaces with new technologies such as xml and web services to deliver higher level mes and erp support it looks to become the standard for exchanging industrial data replacing factorytalk archestra some modbus applications and opcda 
objective c is a reflective object oriented programming language which adds smalltalk style messaging to c today it is used primarily on mac os x iphone os and gnustep three environments based on the openstep standard and is the primary language used for the nextstep openstep and cocoa application frameworks generic objective c programs which do not make use of these libraries can also be compiled for any system supported by gcc which includes an objective c compiler in the early 1980s common software engineering practice was based on structured programming structured programming was implemented in order to help break down programs into smaller parts primarily to make them easier to work on as they grew increasingly large however as the problems being solved grew in size structured programming became less useful as more and more procedures had to be written leading to complex control structures and a low level of code reuse many saw object oriented programming as a potential solution to the problem in fact smalltalk had already addressed many of these engineering issues some of the most complex systems in the world were smalltalk environments on the downside smalltalk used a virtual machine the virtual machine interpreted an object memory called an image containing all development tools the smalltalk image was very large and tended to require huge amounts of memory for the time and ran very slowly partly due to the lack of useful hardware vm container support objective c was created primarily by brad cox and tom love in the early 1980s at their company stepstone both had been introduced to smalltalk while at itt s programming technology center in 1981 cox had become interested in the problems of true reusability in software design and programming he realized that a language like smalltalk would be invaluable in building powerful development environments for system developers at itt cox began by modifying the c compiler to add some of the capabilities of smalltalk he soon had a working implementation of an object oriented extension to the c language which he called oopc for object oriented programming in c love meanwhile was hired by schlumberger research in 1982 and had the opportunity to acquire the first commercial copy of smalltalk 80 which further influenced development of their brainchild in order to demonstrate that real progress could be made cox showed that making interchangeable software components really needed only a few practical changes to existing tools specifically they needed to support objects in a flexible manner come supplied with a usable set of libraries and allow for the code to be bundled into a single cross platform format love and cox eventually formed a new venture productivity products international to commercialize their product which coupled an objective c compiler with powerful class libraries in 1986 cox published the main description of objective c in its original form in the book object oriented programming an evolutionary approach although he was careful to point out that there is more to the problem of reusability than just the language objective c often found itself compared feature for feature with other languages in 1988 next the company started by steve jobs after he left apple licensed objective c from stepstone and released their own objective c compiler and libraries on which the nextstep user interface and interface builder were based although the next workstations failed to make much of an impact in the marketplace the tools were widely lauded in the industry this led next to drop hardware production and focus on software tools selling nextstep as a platform for custom programming the gnu project started work on their free clone of nextstep named gnustep based on the openstep standard dennis glatting wrote the first gnu objc runtime in 1992 the gnu objective c runtime which has been in use since 1993 is the one developed by kresten krab thorup when he was a university student in denmark kresten also worked at next from 1993 to 1996 after acquiring next in 1996 apple used openstep in its new operating system mac os x this included objective c and next s objective c based developer tool project builder as well as its interface design tool interface builder most of apple s present day cocoa api is based on openstep interface objects and is the most significant objective c environment being used for active development objective c is a very thin layer on top of c objective c is a strict superset of c that is it is possible to compile any c program with an objective c compiler objective c derives its syntax from both c and smalltalk most of the syntax is inherited from c while the syntax for object oriented features was created to enable smalltalk style messaging objective c syntax offers alternatives to a few kludges in c syntax but more importantly supports object oriented programming the objective c model of object oriented programming is based on sending messages to sovereign objects this is unlike the simula style programming model used by c and this distinction is semantically important the basic difference is that in objective c one does not call a method one sends a message in objective c the receiver of a message can opt to refuse it both styles carry their own strengths and weaknesses simula style oop allows multiple inheritance and faster execution by using compile time binding whenever possible but does not support dynamic binding by default it also forces all methods to have a corresponding implementation unless they are virtual smalltalk style oop allows messages to go unimplemented and is dynamically bound but in some cases runs slower and some programmers feel that it is a hassle to debug an object with method method is said to respond to the message method sending the message method to the object pointed to by the pointer obj would require the following code in c which in objective c is written as follows this mechanism allows messages to be sent to an object defined first at runtime something statically typed languages such as c are incapable of per the current standards for such languages c will be able to support messaging per ansi standard if the boost library is standardized qt provides this capability to c and other languages by adding the connect function as well as a large array of classes that afford and support this functionality binding objective c has a few features in message passing that relate to how it handles oop objective c messages do not need to execute because they are dynamically bound if a message is implemented by an object it will execute if not it will not execute yet the code will still compile and run so for example every object is sent an awakefromnib message but those objects don t necessarily have to implement awakefromnib to compile if an object does implement awakefromnib then that code will be executed when the message is sent otherwise the message is ignored messages can also be sent to the object that implements them or to the superclass that an object is derived from these can be accessed using the self and super object pointers also messages can be sent to nil objects objective c requires the interface and implementation of a class be in separately declared code blocks by convention the interface is put in a header file and the implementation in a code file the header files normally suffixed h are similar to c header files the implementation files normally suffixed m can be very similar to c code files the interface of the class is usually defined in a header file a common convention is to name the header file after the name of the class the interface for class class would thus be found in the file class h the interface declaration of the form plus signs denote class methods minus signs denote instance methods class methods have no access to instance variables if you are translating from c c97 the above code is somewhat equivalent to note that instancemethod2 demonstrates objective c s named parameter capability for which there is no direct equivalent in c c return types can be any standard c type a pointer to a generic objective c object or a pointer to a specific type of object such as nsarray nsimage or nsstring the default return type is the generic objective c type id method arguments begin with a colon followed by the expected argument type in parentheses followed by the argument name in some cases it is useful to add descriptive text before each parameter the interface only declares the class interface and not the methods themselves the actual code is written in the implementation implementation files normally have the file extension m methods are written as with their interface declarations comparing objective c and c the syntax allows pseudo naming of arguments internal representations of this method vary between different implementations of objective c if mycolor is of the class color internally instance method changecolortored green blue might be labeled _i_color_changecolortored_green_blue the i is to refer to an instance method with the class and then method names appended colons translated to underscores as the order of parameters is part of the method name it cannot be changed to suit coding style or expression as in true named parameters however internal names of the function are rarely used directly and generally messages are converted to function calls defined in the objective c runtime library it s not necessarily known at link time which method will be called the class of the receiver need not be known until runtime once an objective c class is written it can be instantiated this is done by first allocating the memory for a new object and then by initializing it an object isn t fully functional until both steps have been completed these steps are typically accomplished with a single line of code the alloc call allocates enough memory to hold all the instance variables for an object and the init call can be overridden to set instance variables to specific values on creation the init method is often written as follows objective c was extended at next to introduce the concept of multiple inheritance of specification but not implementation through the introduction of protocols this is a pattern achievable either as an abstract multiply inherited base class in c or as an interface objective c makes use of ad hoc protocols called informal protocols and compiler enforced protocols called formal protocols an informal protocol is a list of methods which a class can implement it is specified in the documentation since it has no presence in the language informal protocols often include optional methods where implementing the method can change the behavior of a class for example a text field class might have a delegate which should implement an informal protocol with an optional autocomplete method the text field discovers whether the delegate implements that method and if so calls it to support autocomplete a formal protocol is similar to an interface in java or c it is a list of methods which any class can declare itself to implement versions of objective c before 2 0 required that a class must implement all methods in a protocol it declares itself as adopting the compiler will emit an error if the class does not implement every method of its declared protocols however objective c 2 0 added support for marking certain methods in a protocol optional the compiler will not enforce that such methods are implemented the objective c concept of protocols is different from the java or c concept of interfaces in that a class may implement a protocol without being declared to implement that protocol the difference is not detectable from outside code formal protocols cannot provide any implementations they simply assure callers that classes which conform to the protocol will provide implementations in the next apple library protocols are frequently used by the distributed objects system to represent the capabilities of an object executing on a remote system the syntaxdenotes that there is the abstract idea of locking which is useful and when stated in a class definitiondenotes that instances of someclass will provide an implementation for the two instance methods using whatever means they want this abstract specification is particularly useful to describe the desired behaviors of plug ins for example without constraining at all what the implementation hierarchy should be objective c like smalltalk can use dynamic typing an object can be sent a message that is not specified in its interface this can allow for increased flexibility in objective c an object can capture this message and depending on the object can send the message off again to a different object this behavior is known as message forwarding or delegation alternatively an error handler can be used instead in case the message cannot be forwarded if the object does not forward the message handle the error or respond to it a runtime error occurs static typing information may also optionally be added to variables this information is then checked at compile time in the following statements increasingly specific type information is provided the statements are equivalent at runtime but the additional information allows the compiler to warn the programmer if the passed argument does not match the type specified in the first statement the object may be of any class in the second statement the object must conform to the aprotocol protocol and in the third it must be a member of the nsnumber class dynamic typing can be a powerful feature when implementing container classes using statically typed languages without generics like pre 1 5 java the programmer is forced to write a container class for a generic type of object and then cast back and forth between the abstract generic type and the real type casting however breaks the discipline of static typing if you put in an integer and read out a string you get an error one way of alleviating the problem is to resort to generic programming but then container classes must be homogeneous in type this need not be the case with dynamic typing since objective c permits the sending of a message to an object which might not respond to it the object has a number of things it can do with the message one of these things could be to forward the message on to an object which can respond to it forwarding can be used to implement certain design patterns such as the observer pattern or the proxy pattern very simply the objective c runtime specifies a pair of methods in objectand as such an object wishing to implement forwarding needs only to override the forwarding method to define the forwarding behaviour the action methods performv need not be overridden as this method merely performs the method based on the selector and arguments here is an example of a program which demonstrates the basics of forwarding if we were to compile the program the compiler would reportthe compiler is reporting the point made earlier that forwarder does not respond to hello messages in certain circumstances such a warning can help us find errors but in this circumstance we can safely ignore this warning since we have implemented forwarding if we were to run the programcox s main concern was the maintainability of large code bases experience from the structured programming world had shown that one of the main ways to improve code was to break it down into smaller pieces objective c borrowed and extended the concept of categories to help with this process from smalltalk implementations a category collects method implementations into separate files the programmer can place groups of related methods into a category to make them more readable for instance one could create a spellchecking category on the string object collecting all of the methods related to spell checking into a single place furthermore the methods within a category are added to a class at runtime thus categories permit the programmer to add methods to an existing class without the need to recompile that class or even have access to its source code for example if the system you are supplied with does not contain a spell checker in its string implementation you can add it without modifying the string source code methods within categories become indistinguishable from the methods in a class when the program is run a category has full access to all of the instance variables within the class including private variables categories provide an elegant solution to the fragile base class problem for methods if you declare a method in a category with the same method signature as an existing method in a class the category s method is adopted thus categories can not only add methods to a class but also replace existing methods this feature can be used to fix bugs in other classes by rewriting their methods or to cause a global change to a class behavior within a program if two categories have methods with the same method signature it is undefined which category s method is adopted other languages have attempted to add this feature in a variety of ways tom took the objective c system a step further and allowed for the addition of variables as well other languages have instead used prototype oriented solutions the most notable being self this example builds up an integer class by defining first a basic class with only accessor methods implemented and adding two categories arithmetic and display which extend the basic class whilst categories can access the base class private data members it is often good practice to access these private data members through the accessor methods which helps keep categories more independent from the base class this is one typical usage of categories the other is to use categories to add or replace certain methods in the base class compilation is performed for example byone can experiment by omitting the import arithmetic h and lines and omit arithmetic m in compilation the program will still run this means that it is possible to mix and match added categories if necessary if one does not need to have some capability provided in a category one can simply not compile it in objective c permits a class to wholly replace another class within a program the replacing class is said to pose as the target class note class posing was declared deprecated with mac os x v10 5 and unavailable in the 64 bit runtime for the versions still supporting posing all messages sent to the target class are instead received by the posing class there are several restrictions posing similarly to categories allows globally augmenting existing classes posing permits two features absent from categories for example this intercepts every invocation of setmainmenu to nsapplication in the c language the include pre compile directive allows for the insertion of entire files before any compilation actually begins objective c adds the import directive which does the same thing except that it knows not to insert a file which has already been inserted for example if file a includes files x and y but x and y each include the file q then q will be inserted twice into the resultant file causing duplicate definition compile errors but if file q is included using the import directive only the first inclusion of q will occur all others will be ignored a few compilers including gcc support import for c programs too its use is discouraged on the basis that the user of the header file has to distinguish headers which should be included only once from headers designed to be used multiple times it is argued that this burden should be placed on the implementor to this end the implementor may place the directive pragma once in the header file or use the traditional include guard technique if a header file uses guards or pragma once it makes no difference whether it is included or imported the same objection to import actually applies to objective c as well and many objective c programs also use guards in their headers objective c s features often allow for flexible and often easy solutions to programming issues objective c is a front end to the gnu compiler collection which can compile source files which use a combination of c and objective c syntax objective c adds to c the extensions objective c adds to c as nothing is done to unify the semantics behind the various language features certain restrictions apply at the 2006 worldwide developers conference apple announced the forthcoming release of objective c 2 0 a revision of the objective c language to include modern garbage collection syntax enhancements runtime performance improvements and 64 bit support mac os x v10 5 released in october 2007 included an objective c 2 0 compiler it is not yet known when these language improvements will be available in the gnu runtime objective c 2 0 provides an optional conservative yet generational garbage collector when run in backwards compatible mode the runtime turns reference counting operations such as retain and release into no ops all objects are subject to garbage collection when garbage collection is enabled regular c pointers may be qualified with __strong to also trigger the underlying write barrier compiler intercepts and thus participate in garbage collection a zero ing weak subsystem is also provided such that pointers marked as __weak are set to zero when the object is collected the garbage collector does not exist on the iphone implementation of objective c 2 0 objective c 2 0 introduces a new syntax to declare instance variables as properties with optional attributes to configure the generation of accessor methods properties are in a sense public instance variables that is declaring an instance variable as a property provides external classes with access to that property a property may be declared as readonly and may be provided with storage semantics such as assign copy or retain by default properties are considered atomic which results in a lock preventing multiple threads from accessing them at the same time a property can be declared as nonatomic which removes this lock properties are implemented by way of the synthesize keyword which generates getter and setter methods according to the property declaration alternately the dynamic keyword can be used to indicate that accessor methods will be provided by other means properties can be accessed using the traditional message passing syntax dot notation or by name via the valueforkey setvalue forkey methods in order to use dot notation to invoke property accessors within an instance method the self keyword should be used a class or protocol s properties may be dynamically introspected instead of using an enumerator object to iterate through a collection objective c 2 0 offers the fast enumeration syntax the following two loops are equivalent in objective c 2 0 fast enumeration generates more efficient code than standard enumeration because methods calls to enumerate over objects are replaced by pointer arithmetic using the nsfastenumeration protocol all objective c applications developed for mac os x that make use of the above improvements for objective c 2 0 are incompatible with all operating systems prior to 10 5 even using fast enumeration which one might expect to generate the exact same binaries as standard enumeration will cause an application to crash on os x version 10 4 or earlier objective c today is often used in tandem with a fixed library of standard objects such as cocoa or gnustep these libraries often come with the operating system the gnustep libraries often come with linux distributions and cocoa comes with mac os x the programmer is not forced to inherit functionality from the existing base class objective c allows for the declaration of new root classes which do not inherit any existing functionality originally objective c based programming environments typically offered an object class as the base class from which almost all other classes inherited with the introduction of openstep next created a new base class named nsobject which offered additional features over object almost all classes in cocoa inherit from nsobject not only did the renaming serve to differentiate the new default behavior of classes within the openstep api but it allowed code which used object the original base class used on nextstep to co exist in the same runtime with code which used nsobject as well the introduction of the two letter prefix became a sort of simplistic form of namespaces which objective c lacks using a prefix to create an informal packaging identifier became an informal coding standard in the objective c community and continues to this day besides the gcc next apple implementation which added several extensions to the original stepstone implementation there exists another free open source objective c implementation which implements a slightly different set of extensions the portable object compiler implements among other things also smalltalk like blocks for objective c objective c implementations use a thin runtime written in c which adds little to the size of the application in contrast most oo systems at the time that it was created used large virtual machine runtimes programs written in objective c tend to be not much larger than the size of their code and that of the libraries in contrast to smalltalk systems where a large amount of memory was used just to open a window objective c applications tend to be larger than similar c or c applications because objective c dynamic typing does not allow methods to be stripped or inlined likewise the language can be implemented on top of existing c compilers rather than as a new compiler this allows objective c to leverage the huge existing collection of c code libraries tools and mindshare existing c libraries can be wrapped in objective c wrappers to provide an oo style interface all of these practical changes lowered the barrier to entry likely the biggest problem for the widespread acceptance of smalltalk in the 1980s the first versions of objective c did not support garbage collection at the time this decision was a matter of some debate and many people considered long dead times to render the entire system unusable some 3rd party implementations have added this feature and apple have implemented it as of mac os x v10 5 another common criticism is that objective c does not have language support for namespaces instead programmers are forced to add prefixes to their class names which are traditionally shorter than namespace names and thus more prone to collisions as of 2007 all mac os x classes and functions in the cocoa programming environment are prefixed with ns to identify them as belonging to the mac os x core the ns derives from the names of the classes as defined during the development of nextstep since objective c is a strict superset of c it does not treat c primitive types as first class objects either unlike c objective c does not support operator overloading also unlike c objective c allows an object to directly inherit only from one class however categories and protocols may be used as alternative functionality to multiple inheritance because objective c uses dynamic runtime typing and because all method calls are function calls many common performance optimizations cannot be applied to objective c methods this limits the performance of objective c abstractions relative to similar abstractions in languages such as c proponents of objective c claim that it should not be used for low level abstraction in the way that c or java are used because objective c is known to have a high runtime cost the design and implementation of c and objective c represent different approaches to extending c in addition to c s style of procedural programming c directly supports object oriented programming generic programming and metaprogramming c also comes with a large standard library which includes several container classes objective c on the other hand adds only object oriented features to c objective c in its purest fashion does not contain the same number of standard library features but in most places where objective c is used it is used with an openstep like library such as openstep cocoa or gnustep which provide similar functionality to some of c s standard library one notable difference is that objective c provides runtime support for some reflective features whereas c adds only a small amount of runtime support to c in objective c an object can be queried about its own properties for example whether it will respond to a certain message in c this is not possible without the use of external libraries the use of reflection is part of the wider distinction between dynamic features versus static features of a language although objective c and c each employ a mix of both features objective c is decidedly geared toward run time decisions while c is geared toward compile time decisions the tension between dynamic and static programming involves many of the classic trade offs in programming 
microsoft sql server is a relational database management system produced by microsoft its primary query languages are t sql entity sql and ansi sql the code base for ms sql server originated in sybase sql server and was microsoft s entry to the enterprise level database market competing against oracle ibm and later sybase itself microsoft sybase and ashton tate originally teamed up to create and market the first version named sql server 1 0 for os 2 which was essentially the same as sybase sql server 3 0 on unix vms etc microsoft sql server 4 2 was shipped around 1992 later microsoft sql server 4 21 for windows nt was released at the same time as windows nt 3 1 microsoft sql server v6 0 was the first version designed for nt and did not include any direction from sybase about the time windows nt was released sybase and microsoft parted ways and each pursued their own design and marketing schemes microsoft negotiated exclusive rights to all versions of sql server written for microsoft operating systems later sybase changed the name of its product to adaptive server enterprise to avoid confusion with microsoft sql server until 1994 microsoft s sql server carried three sybase copyright notices as an indication of its origin since parting ways several revisions have been done independently sql server 7 0 was the first true gui based database server and was a rewrite from the legacy sybase code it was succeeded by sql server 2000 which was the first edition to be launched in a variant for the ia 64 architecture in the eight years since release of microsoft s previous sql server product advancements have been made in performance the client ide tools and several complementary systems that are packaged with sql server 2005 these include an etl tool a reporting server an olap and data mining server and several messaging technologies specifically service broker and notification services microsoft sql server 2000 is a rdbms that offers enough administrative tools for database development maintenance and administration 1 enterprise manager is the main administrative console for sql server installations it provides tree view of all of the sql server installations on network we can perform high level administrative functions that affect one or more servers schedule common maintenance tasks or create and modify the structure of individual databases 2 query analyzer is a quick method for performing queries against any of one of the sql server databases it s a great way to quickly get information out of a database in response to a user request test queries before implementing them in other applications we can execute administration taks create modify stored procedures functions and views etc this is most commonly used version by the developers it has all generic features for microsoft applications for microsoft visual studio 2003 this is having enough features but for microsoft visual studio 2005 some more features required namely long text any way this is acceptable for most cases sql server 2005 released in october 2005 is the successor to sql server 2000 it included native support for managing xml data in addition to relational data for this purpose it defined an xml data type that could be used either as a data type in database columns or as literals in queries xml columns can be associated with xsd schemas xml data being stored is verified against the schema xml is converted to an internal binary data type before being stored in the database specialized indexing methods were made available for xml data xml data is queried using xquery sql server 2005 added some extensions to the t sql language to allow embedding xquery queries in t sql in addition it also defines a new extension to xquery called xml dml that allows query based modifications to xml data sql server 2005 also allows a database server to be exposed over web services using tds packets encapsulated within soap requests when the data is accessed over web services results are returned as xml for relational data t sql has been augmented with error handling features and support for recursive queries sql server 2005 has also been enhanced with new indexing algorithms and better error recovery systems data pages are checksummed for better error resiliency and optimistic concurrency support has been added for better performance permissions and access control have been made more granular and the query processor handles concurrent execution of queries in a more efficient way partitions on tables and indexes are supported natively so scaling out a database onto a cluster is easier sql clr was introduced with sql server 2005 to let it integrate with the net framework the current version of sql server sql server 2008 was released on august 6 2008 and aims to make data management self tuning self organizing and self maintaining with the development of sql server always on technologies to provide near zero downtime sql server 2008 will also include support for structured and semi structured data including digital media formats for pictures audio video and other multimedia data in current versions such multimedia data can be stored as blobs but they are generic bitstreams intrinsic awareness of multimedia data will allow specialized functions to be performed on them according to paul flessner senior vice president server applications microsoft corp sql server 2008 can be a data storage backend for different varieties of data xml email time calendar file document spatial etc as well as perform search query analysis sharing and synchronization across all data types other new data types include specialized date and time types and a spatial data type for location dependent data better support for unstructured and semi structured data is provided using the new filestream data type which can be used to reference any file stored on the file system structured data and metadata about the file is stored in sql server database whereas the unstructured component is stored in the file system such files can be accessed both via win32 file handling apis as well as via sql server using t sql doing the latter accesses the file data as a blob backing up and restoring the database backs up or restores the referenced files as well sql server 2008 also natively supports hierarchical data and includes t sql constructs to directly deal with them without using recursive queries the full text search functionality has been integrated with the database engine which simplifies management and improves performance spatial data will be stored in two types a flat earth data type represents geospatial data which has been projected from its native spherical coordinate system into a plane a round earth data type uses an ellipsoidal model in which the earth is defined as a single continuous entity which does not suffer from the singularities such as the international dateline poles or map projection zone edges approximately 70 methods are available to represent spatial operations for the open geospatial consortium simple features for sql version 1 1 sql server includes better compression features which also helps in improving scalability it also includes resource governor that allows reserving resources for certain users or workflows it also includes capabilities for transparent encryption of data as well as compression of backups sql server 2008 supports the ado net entity framework and the reporting tools replication and data definition will be built around the entity data model sql server reporting services will gain charting capabilities from the integration of the data visualization products from dundas data visualization inc which was acquired by microsoft on the management side sql server 2008 includes the declarative management framework which allows configuring policies and constraints on the entire database or certain tables declaratively the version of sql server management studio included with sql server 2008 supports intellisense for sql queries against a sql server 2008 database engine sql server 2008 also makes the databases available via windows powershell providers and management functionality available as cmdlets so that the server and all the running instances can be managed from windows powershell microsoft makes sql server available in multiple versions with different feature sets and targeting different users these versions are protocol layer implements the external interface to sql server all operations that can be invoked on sql server are communicated to it via a microsoft defined format called tabular data stream tds is an application layer protocol used to transfer data between a database server and a client initially designed and developed by sybase inc for their sybase sql server relational database engine in 1984 and later by microsoft in microsoft sql server tds packets can be encased in other physical transport dependent protocols including tcp ip named pipes and shared memory consequently access to sql server is available over these protocols in addition the sql server api is also exposed over bando web services the main unit of data storage is a database which is a collection of tables with typed columns sql server supports different data types including primary types such as integer float decimal char varchar binary text among others it also allows user defined composite types to be defined and used sql server also makes server statistics available as virtual tables and views a database can also contain other objects including views stored procedures indexes and constraints in addition to tables along with a transaction log a sql server database can contain a maximum of 231 objects and can span multiple os level files with a maximum file size of 220 tb the data in the database are stored in primary data files with an extension mdf secondary data files identified with an ndf extension are used to store optional metadata log files are identified with the ldf extension storage space allocated to a database is divided into sequentially numbered pages each 8 kb in size a page is the basic unit of i o for sql server operations a page is marked with a 96 byte header which stores metadata about the page including the page number page type free space on the page and the id of the object that owns it page type defines the data contained in the page data stored in the database index allocation map which holds information about how pages are allocated to tables and indexes change map which holds information about the changes made to other pages since last backup or logging or contain large data types such as image or text while page is the basic unit of an i o operation space is actually managed in terms of an extent which consists of 8 pages a database object can either span all 8 pages in an extent or share an extent with up to 7 more objects a row in a database table cannot span more than one page so is limited to 8 kb in size however if the data exceeds 8 kb and the row contains varchar or varbinary data the data in those columns are moved to a new page and replaced with a pointer to the data for physical storage of a table its rows are divided into a series of partitions the partition size is user defined by default all rows are in a single partition a table is split into multiple partitions in order to spread a database over a cluster rows in each partition are stored in either b tree or heap structure if the table has an associated index to allow fast retrieval of rows the rows are stored in order according to their index values with a b tree providing the index the data is in the leaf node of the leaves and other nodes storing the index values for the leaf data reachable from the respective nodes if the index is non clustered the rows are not sorted according to the index keys an indexed view has the same storage structure as an indexed table a table without an index is stored in an unordered heap structure both heaps and b trees can span multiple allocation units sql server buffers pages in ram to minimize disc i o any 8 kb page can be buffered in memory and the set of all pages currently buffered is called the buffer cache the amount of memory available to sql server decides how many pages will be cached in memory the buffer cache is managed by the buffer manager either reading from or writing to any page copies it to the buffer cache subsequent reads or writes are redirected to the in memory copy rather than the on disc version the page is updated on the disc by the buffer manager only if the in memory cache has not been referenced for some time while writing pages back to disc asynchronous i o is used whereby the i o operation is done in a background thread so that other operations do not have to wait for the i o operation to complete each page is written along with its checksum when it is written when reading the page back its checksum is computed again and matched with the stored version to ensure the page has not been damaged or tampered with in the meantime sql server ensures that any change to the data is acid compliant i e it uses transactions to ensure that any operation either totally completes or is undone if fails but never leaves the database in an intermediate state using transactions a sequence of actions can be grouped together with the guarantee that either all actions will succeed or none will sql server implements transactions using a write ahead log any changes made to any page will update the in memory cache of the page simultaneously all the operations performed will be written to a log along with the transaction id which the operation was a part of each log entry is identified by an increasing log sequence number which ensure that no event overwrites another sql server ensures that the log will be written onto the disc before the actual page is written back this enables sql server to ensure integrity of the data even if the system fails if both the log and the page were written before the failure the entire data is on persistent storage and integrity is ensured if only the log was written then the actions can be read from the log and repeated to restore integrity if the log wasn t written then integrity is also maintained although the database state remains unchanged as if the transaction never occurred if it was only partially written then the actions associated with the unfinished transaction are discarded since the log was only partially written the page is guaranteed to have not been written again ensuring data integrity removing the unfinished log entries effectively undoes the transaction sql server ensures consistency between the log and the data every time an instance is restarted sql server allows multiple clients to use the same database concurrently as such it needs to control concurrent access to shared data to ensure data integrity when multiple clients update the same data or clients attempt to read data that is in the process of being changed by another client sql server provides two modes of concurrency control pessimistic concurrency and optimistic concurrency when pessimistic concurrency control is being used sql server controls concurrent access by using locks locks can be either shared or exclusive exclusive lock grants the user exclusive access to the data no other user can access the data as long as the lock is held shared locks are used when some data is being read multiple users can read from data locked with a shared lock but not acquire an exclusive lock the latter would have to wait for all shared locks to be released locks can be applied on different levels of granularity on entire tables pages or even on a per row basis on tables for indexes it can either be on the entire index or on index leaves the level of granularity to be used is defined on a per database basis by the database administrator while a fine grained locking system allows more users to use the table or index simultaneously it requires more resources so it does not automatically turn into higher performing solution sql server also includes two more lightweight mutual exclusion solutions latches and spinlocks which are less robust than locks but are less resource intensive sql server uses them for dmvs and other resources that are usually not busy sql server also monitors all worker threads that acquire locks to ensure that they do not end up in deadlocks in case they do sql server takes remedial measures which in many cases is to kill one of the threads entangled in a deadlock and rollback the transaction it started to implement locking sql server contains the lock manager the lock manager maintains an in memory table that manages the database objects and locks if any on them along with other metadata about the lock access to any shared object is mediated by the lock manager which either grants access to the resource or blocks it sql server also provides the optimistic concurrency control mechanism which is similar to the multiversion concurrency control used in other databases the mechanism allows a new version of a row to be created whenever the row is updated as opposed to overwriting the row i e a row is additionally identified by the id of the transaction that created the version of the row both the old as well as the new versions of the row are stored and maintained though the old versions are moved out of the database into a system database identified as tempdb when a row is in the process of being updated any other requests are not blocked but are executed on the older version of the row if the other request is an update statement it will result in two different versions of the rows both of them will be stored by the database identified by their respective transaction ids the main mode of retrieving data from an sql server database is querying for it the query is expressed using a variant of sql called t sql a dialect microsoft sql server shares with sybase sql server due to its legacy the query declaratively specifies what is to be retrieved it is processed by the query processor which figures out the sequence of steps that will be necessary to retrieve the requested data the sequence of actions necessary to execute a query is called a query plan there might be multiple ways to process the same query for example for a query that contains a join statement and a select statement executing join on both the tables and then executing select on the results would give the same result as selecting from each table and then executing the join but result in different execution plans in such case sql server chooses the plan that is supposed to yield the results in the shortest possible time this is called query optimization and is performed by the query processor itself sql server includes a cost based query optimizer which tries to optimize on the cost in terms of the resources it will take to execute the query given a query the query optimizer looks at the database schema the database statistics and the system load at that time it then decides which sequence to access the tables referred in the query which sequence to execute the operations and what access method to be used to access the tables for example if the table has an associated index whether the index should be used or not if the index is on a column which is not unique for most of the columns it might not be worthwhile to use the index to access the data finally it decides whether to execute the query concurrently or not while a concurrent execution is more costly in terms of total processor time because the execution is actually split to different processors might mean it will execute faster once a query plan is generated for a query it is temporarily cached for further invocations of the same query the cached plan is used unused plans are discarded after some time sql server also allows stored procedures to be defined stored procedures are parameterized t sql queries that are stored in the server itself stored procedures can accept values sent by the client as input parameters and send back results as output parameters they can also call other stored procedures and can be selectively provided access to unlike other queries stored procedures have an associated name which is used at runtime to resolve into the actual queries also because the code need not be sent from the client every time it reduces network traffic and somewhat improves performance execution plans for stored procedures are also cached as necessary microsoft sql server 2005 includes a component named sql clr via which it integrates with net framework unlike most other applications that use net framework sql server itself hosts the net framework runtime i e memory threading and resource management requirements of net framework are satisfied by sqlos itself rather than the underlying windows operating system sqlos provides deadlock detection and resolution services for net code as well with sql clr stored procedures and triggers can be written in any managed net language including c and vb net managed code can also be used to define udt s which can persist in the database managed code is compiled to net assemblies and after being verified for type safety registered at the database after that they can be invoked like any other procedure however only a subset of the base class library is available when running code under sql clr most apis relating to user interface functionality are not available when writing code for sql clr data stored in sql server databases can be accessed using the ado net apis like any other managed application that accesses sql server data however doing that creates a new database session different from the one in which the code is executing to avoid this sql server provides some enhancements to the ado net provider that allows the connection to be redirected to the same session which already hosts the running code such connections are called context connections and are set by setting context connection parameter to true in the connection string sql server also provides several other enhancements to the ado net api including classes to work with tabular data or a single row of data as well as classes to work with internal metadata about the data stored in the database it also provides access to the xml features in sql server including xquery support these enhancements are also available in t sql procedures in consequence of the introduction of the new xml datatype sql server also includes an assortment of add on services while these are not essential for the operation of the database system these provide value added services on top of the core database management system these services either run as a part of some sql server component or out of process as windows service and presents their own api to control and interact with them the service broker which runs as a part of the database engine provides a reliable messaging and message queuing platform for sql server applications used inside an instance it is used to provide an asynchronous programming environment for cross instance applications service broker communicates over tcp ip and allows the different components to be synchronized together via exchange of messages sql server replication services are used by sql server to replicate and synchronize database objects either in entirety or a subset of the objects present across replication agents which might be other database servers across the network or database caches on the client side replication follows a publisher subscriber model i e the changes are sent out by one database server and are received by others sql server supports three different types of replication sql server analysis services adds olap and data mining capabilities for sql server databases the olap engine supports molap rolap and holap storage modes for data analysis services supports the xml for analysis standard as the underlying communication protocol the cube data can be accessed using mdx queries data mining specific functionality is exposed via the dmx query language analysis services includes various algorithms decision trees clustering algorithm naive bayes algorithm time series analysis sequence clustering algorithm linear and logistic regression analysis and neural networks for use in data mining sql server reporting services is a report generation environment for data gathered from sql server databases it is administered via a web interface reporting services features a web services interface to support the development of custom reporting applications reports are created as rdl files reports can be designed using recent versions of microsoft visual studio with business intelligence development studio installed or with the included report builder once created rdl files can be rendered in a variety of formats including excel pdf csv xml tiff and html web archive originally introduced as a post release add on for sql server 2000 notification services was bundled as part of the microsoft sql server platform for the first and only time with sql server 2005 with sql server 2005 sql server notification services is a mechanism for generating data driven notifications which are sent to notification services subscribers a subscriber registers for a specific event or transaction when the event occurs notification services can use one of three methods to send a message to the subscriber informing about the occurrence of the event these methods include smtp soap or by writing to a file in the filesystem sql server integration services is used to integrate data from different data sources it is used for the etl capabilities for sql server for data warehousing needs integration services includes gui tools to build data extraction workflows integration various functionality such as extracting data from various sources querying data transforming data including aggregating duplication and merging data and then loading the transformed data onto other sources or sending e mails detailing the status of the operation sql server full text search service is a specialized indexing and querying service for unstructured text stored in sql server databases the full text search index can be created on any column with character based text data it allows for words to be searched for in the text columns while it can be performed with the sql like operator using sql server full text search service can be more efficient full text search allows for inexact matching of the source string indicated by a rank value which can range from 0 to 1000 a higher rank means a more accurate match it also allows linguistic matching i e linguistic variants of a word will also be a match for a given word proximity searches are also supported i e if the words searched for do not occur in the sequence they are specified in the query but are near each other they are also considered a match t sql exposes special operators that can be used to access the fts capabilities the full text search engine is divided into two processes the filter daemon process and the search process these processes interact with the sql server the search process includes the indexer and the full text query processor the indexer scans through text columns in the database it can also index through binary columns and use ifilters to extract meaningful text from the binary blob the ifilters are hosted by the filter daemon process once the text is extracted the filter daemon process breaks it up into a sequence of words and hands it over to the indexer the indexer filters out noise words i e words like a and etc which occur frequently and are not useful for search with the remaining words an inverted index is created associating each word with the columns they were found in sql server itself includes a gatherer component that monitors changes to tables and invokes the indexer in case of updates when a full text query is received by the sql server query processor it is handed over to the fts query processor in the search process the fts query processor breaks up the query into the constituent words filters out the noise words and uses an inbuilt thesaurus to find out the linguistic variants for each word the words are then queried against the inverted index and a rank of their accurateness is computed the results are returned to the client via the sql server process sqlcmd is a command line application that comes with microsoft sql server and exposes the management features of sql server it allows sql queries to be written and executed from the command prompt it can also act as a scripting language to create and run a set of sql statements as a script such scripts are stored as a sql file and are used either for management of databases or to create the database schema during the deployment of a database sqlcmd was introduced with sql server 2005 and this continues with sql server 2008 its predecessor for earlier versions was osql which is functionally equivalent and many of the command line parameters are identical microsoft visual studio includes native support for data programming with microsoft sql server it can be used to write and debug code to be executed by sql clr it also includes a data designer that can be used to graphically create view or edit database schemas queries can be created either visually or using code ssms 2008 onwards provides intellisense for sql queries as well sql server management studio is a gui tool included with sql server 2005 and later for configuring managing and administering all components within microsoft sql server the tool includes both script editors and graphical tools that work with objects and features of the server sql server management studio replaces enterprise manager as the primary management interface for microsoft sql server since sql server 2005 a version of sql server management studio is also available for sql server express edition for which it is known as sql server management studio express a central feature of sql server management studio is the object explorer which allows the user to browse select and act upon any of the objects within the server it can be used to visually observe and analyze query plans and optimize the database performance among others sql server management studio can also be used to create a new database alter any existing database schema by adding or modifying tables and indexes or analyze performance it includes the query windows which provide a gui based interface to write and execute queries business intelligence development studio is the ide from microsoft used for developing data analysis and business intelligence solutions utilizing the microsoft sql server analysis services reporting services and integration services it is based on the microsoft visual studio development environment but customizes with the sql server services specific extensions and project types including tools controls and projects for reports cubes and data mining structures t sql is the primary means of programming and managing sql server it exposes keywords for the operations that can be performed on sql server including creating and altering database schemas entering and editing data in the database as well as monitoring and managing the server itself client applications both which consume data or manage the server leverage sql server functionality by sending t sql queries and statements which are then processed by the server and results returned to the client application sql server allows it to be managed using t sql for this it exposes read only tables from which server statistics can be read management functionality is exposed via system defined stored procedures which can be invoked from t sql queries to perform the management operation sql native client is the native client side data access library for microsoft sql server version 2005 onwards it natively implements support for the sql server features including the tabular data stream implementation support for mirrored sql server databases full support for all data types supported by sql server asynchronous operations query notifications encryption support as well as receiving multiple result sets in a single database session sql native client is used under the hood by sql server plug ins for other data access technologies including ado or ole db it is also used by the native sql server ado net provider sqlclient the sql native client can also be directly used bypassing the generic data access layers sql server 2008 is the latest version introduced and it has many new features most of the enterprise applications are migrated to sql server2008
eclipse is a multi language software development platform comprising an ide and a plug in system to extend it it is written primarily in java and is used to develop applications in this language and by means of the various plug ins in other languages as well c c cobol python perl php and more the initial codebase originated from visualage in its default form it is meant for java developers consisting of the java development tools users can extend its capabilities by installing plug ins written for the eclipse software framework such as development toolkits for other programming languages and can write and contribute their own plug in modules language packs provide translations into over a dozen natural languages released under the terms of the eclipse public license eclipse is free and open source software eclipse employs plug ins in order to provide all of its functionality on top of the runtime system in contrast to some other applications where functionality is typically hard coded the runtime system of eclipse is based on equinox an osgi standard compliant implementation this plug in mechanism is a lightweight software componentry framework in addition to allowing eclipse to be extended using other programming languages such as c and python the plug in framework allows eclipse to work with typesetting languages like latex networking applications such as telnet and database management systems the plug in architecture supports writing any desired extension to the environment such as for configuration management java and cvs support is provided in the eclipse sdk the key to the seamless integration of tools with eclipse is the plug in with the exception of a small run time kernel everything in eclipse is a plugin this means that every plugin developed integrates with eclipse in exactly the same way as other plugins in this respect all features are created equal eclipse provides plugins for a wide variety of features some of which are through third parties using both free and commercial models examples of plugins include uml plug in for sequence and other uml diagrams plug in for database explorer and many others the eclipse sdk includes the eclipse java development tools offering an ide with a built in incremental java compiler and a full model of the java source files this allows for advanced refactoring techniques and code analysis the ide also makes use of a workspace in this case a set of metadata over a flat filespace allowing external file modifications as long as the corresponding workspace resource is refreshed afterwards the visual editor project allows interfaces to be created interactively thus allowing eclipse to be used as a rad tool eclipse s widgets are implemented by a widget toolkit for java called swt unlike most java applications which use the java standard abstract window toolkit or swing eclipse s user interface also uses an intermediate gui layer called jface which simplifies the construction of applications based on swt eclipse provides the eclipse rich client platform for developing general purpose applications the following components constitute the rich client platform eclipse began as an ibm canada project it was developed by oti as a java based replacement for the smalltalk based visualage family of ide products which itself had been developed by oti in november 2001 a consortium was formed to further the development of eclipse as open source in january 2004 the eclipse foundation was created eclipse 3 0 selected the osgi service platform specifications as the runtime architecture eclipse was originally released under the common public license but was later relicensed under the eclipse public license the free software foundation has said that both licenses are free software licenses but are incompatible with the gnu general public license mike milinkovich of the eclipse foundation commented that moving to the gpl would be considered when version 3 of the gpl was released according to ibm chief technology officer lee nackman the name eclipse was chosen to target microsoft s visual studio product since 2006 the eclipse foundation has coordinated an annual simultaneous release each release includes the eclipse platform as well as a number of other eclipse projects the purpose is to provide a distribution of eclipse software with static features and versions ostensibly this simplifies deployment and maintenance for enterprise systems and others may simply find it convenient so far each simultaneous release has been named after one of the moons of jupiter and has occurred at the end of june 
representational state transfer is a style of software architecture for distributed hypermedia systems such as the world wide web as such it is not strictly a method for building web services the terms representational state transfer and rest were introduced in 2000 in the doctoral dissertation of roy fielding one of the principal authors of the hypertext transfer protocol specification rest refers in the strictest sense to a collection of network architecture principles which outline how resources are defined and addressed the term is often used more loosely to describe any simple interface which transmits domain specific data over http without an additional messaging layer such as soap or session tracking via http cookies these two meanings can conflict as well as overlap it is possible to design a software system in accordance with fielding s rest architectural style without using http and without interacting with the world wide web it is also possible to design simple xml http interfaces which do not conform to rest principles and instead follow a model of remote procedure call the difference between the uses of the term rest therefore causes some confusion in technical discussions systems which follow fielding s rest principles are often referred to as restful proponents of rest argue that the web s scalability and growth are a direct result of a few key design principles fielding describes rest s effect on scalability thus rest s client server separation of concerns simplifies component implementation reduces the complexity of connector semantics improves the effectiveness of performance tuning and increases the scalability of pure server components layered system constraints allow intermediaries proxies gateways and firewalls to be introduced at various points in the communication without changing the interfaces between components thus allowing them to assist in communication translation or improve performance via large scale shared caching rest enables intermediate processing by constraining messages to be self descriptive interaction is stateless between requests standard methods and media types are used to indicate semantics and exchange information and responses explicitly indicate cacheability an important concept in rest is the existence of resources each of which is referenced with a global identifier in order to manipulate these resources components of the network communicate via a standardized interface and exchange representations of these resources for example a resource which is a circle may accept and return a representation which specifies a center point and radius formatted in svg but may also accept and return a representation which specifies any three distinct points along the curve as a comma separated list any number of connectors can mediate the request but each does so without seeing past its own request thus an application can interact with a resource by knowing two things the identifier of the resource and the action required it does not need to know whether there are caches proxies gateways firewalls tunnels or anything else between it and the server actually holding the information the application does however need to understand the format of the information returned which is typically an html xml or json document of some kind although it may be an image plain text or any other content many of the statements below refer to rest in the specific context of web services as opposed to soap rest was originally defined in fielding s dissertation in the context of information and media access fielding did not originally contrast rest with rpc some benefits with rest one benefit that s obvious with regards to web based applications is that a restful implementation allows a user to bookmark specific queries and allows those to be conveyed to others across e mail instant messages or to be injected into wikis etc thus this representation of a path or entry point into an application state becomes highly portable the world wide web is the key example of restful design much of it conforms to rest principles the web consists of the hypertext transfer protocol content types including the hypertext markup language and other internet technologies such as the domain name system html can include javascript and applets to support code on demand and has implicit support for hyperlinks http has a uniform interface for accessing resources which consists of uris methods status codes headers and content distinguished by mime type the most important http methods are post get put and delete these are often respectively associated with the create read update delete operations associated with database technologies the following table associates several common http verbs with similar database operations however the meaning of the http verbs do not correspond directly with a single database operation for example an http put is used to set the value of a resource and may result in either a creation or replacement as needed some restful services will extend the post method to include the operations of updating and deleting by including additional arguments however in doing so the service is moving the operation out of http and inside the request data the http standard states that post is intended to create a new subordinate of the resource identified while the put operation is intended to create a new resource stored under the supplied request uri based on the enclosed entity in the request and in the case that the supplied request uri exists the enclosed entity should be considered as a modified version of the one residing on the origin server http separates the notions of a web server and a web browser this allows the implementation of each to vary from the other based on the client server principle when used restfully http is stateless each message contains all the information necessary to understand the request when combined with state at the resource as a result neither the client nor the server needs to remember any communication state between messages any state retained by the server must be modeled as a resource the statelessness constraint can be violated in http using cookies to maintain sessions fielding notes the risks of privacy leaks and security complications which often arise through the use of cookies and the confusions and bugs which can result from interactions between cookies and the back button in a browser http provides mechanisms to control caching and permits a conversation between web browser and web cache to occur using the same mechanisms as between web browser and web server no layer can access any conversation other than the one it is immediately involved with html links only produce http get requests and html forms allow get and post methods the other http methods mentioned here are not available in html 4 01 or xhtml 1 0 webdav makes use of other http verbs in a web context for this reason some restful services will overload the post method to make it perform the operation updating and deleting a resource a restful web service is a simple web service implemented using http and the principles of rest such a web service can be thought about as a collection of resources the definition of such a web service can be thought of as comprising three aspects members of the collection are addressed by id using uris of the form baseuri id the id can be any unique identifier for example if a restful web service representing a collection of cars for sale might have the uri http example com resources cars if the service uses the car registration number as the id then a particular car might be present in the collection as http example com resources cars yxz123 the following table shows how the http verbs are typically used to implement a web service the statements below refer to rest in the context of web services specifically as opposed to soap note that fielding s dissertation presents rest in the context of information and media access not web services it does not contrast rest to rpc although it does contrast rpc to http a restful web application requires a different design approach from an rpc application an rpc application is exposed as one or more network objects each with an often unique set of functions which can be invoked before a client communicates with the application it must have knowledge of the object identity in order to locate it and must also have knowledge of the object type in order to communicate with it restful design constrains the aspects of a resource which define its interface this leads to the definition of fewer types on the network than an rpc based application but more resource identifiers rest design seeks to define a set of resources with which clients can interact uniformly and to provide hyperlinks between resources which clients can navigate without requiring knowledge of the whole resource set server provided forms can also be used in a restful environment to describe how clients should construct a url in order to navigate to a particular resource an rpc application might define operations such as the following client code to access this application may look something like this with rest on the other hand the emphasis is on the diversity of resources or nouns for example a rest application might define the following resourcesclient code to access this application may look something like this each resource has its own identifier noun clients start at a single resource such as the user resource which represents themselves and navigate to location resources and other user resources clients work with each resource through standard operations such as get to download a copy of the resource s representation put to paste a changed copy over the top of the original or delete to remove the data or state associated with the resource post is sometimes used interchangeably with put but can also be seen as a paste after rather than a paste over request post is generally used for actions with side effects such as requesting the creation of a purchase order or adding some data to a collection note how each object has its own url and can easily be cached copied and bookmarked the uniform interface allows clients to access data from a range of resources without special code to deal with each one so long as it is actually uniform the content returned from a user resource could be the globally standard and restful html a less restful industry standard representation such as userml or an unrestful application specific data format which content is returned can be negotiated at request time the content could even be a combination of these representations html can be marked up with microformats which have general or industry specific appeal and these microformats can be extended with application specific information uniform interfaces reduce the cost of client software by ensuring it is only written once rather than once per application it has to deal with both rest and rpc designs may try to maximise the uniformity of the interface they expose by conforming to industry or global standards in the rpc model these standards are primarily in the form of standard type definitions and standard choreography in rest it is primarily the choice of standard content types and verbs which controls uniformity it is possible to claim an enormous number of restful applications on the web taken more narrowly in its sense as an alternative to both web services generally and the rpc style specifically rest can be found in a number of places on the public web note that wsdl version 2 0 now offers support for binding to all the http request methods some interfaces referred to as being restful do not intentionally respect rest s architectural constraints rest advocate mark baker uses the term accidentally restful to describe interfaces that partially follow rest s architectural constraints for example flickr s interface can be considered restful in its use of standalone get operations but it does not attempt to support the full range of a rest interface other interfaces that use http to tunnel function calls or which offer a pox http endpoint are also sometimes referred to as rest interfaces implementation is hampered by limited support for http put and delete in popular development platforms for example in the lamp platform support for put must be added as a module web searches offer few examples of how to implement updating database driven content using put for example it is nontrivial to create a php script to update http example com thing 1 with a put message when thing php will serve a get request with xml generated from a database most published patterns for updating entities use the post method just as much of the web can be seen as restful or nearly restful a number of existing protocols and architectures have restful characteristics software which may interact with a number of different kinds of objects or devices can do so by virtue of a uniform agreed interface many of these uniform interfaces follow document oriented rest patterns rather than object oriented patterns modbus is a protocol which allows memory ranges within plcs to be addressed ranges can be written and read effectively as put and get operations javabeans and other systems which perform property based editing follow the put and get model of the rest architectural style rather than write object specific editor code the code is written once and can interact with various object types resources in this model are defined by the combination of an object identifier and a property name the snmp protocol and its object model which predate the web share some characteristics with restful systems a strict verb discipline follows from the protocol s small operator set and the resources are addressed with a uniform global scheme of object identifiers most interaction occurs in client server fashion and the clients and servers can be deployed and evolved independently each request response pair can be understood in isolation however movement through the space of object identifiers is not assisted by hyperlinks nor is it considered as traversal through states in a state machine rather the manager uses prior knowledge of the management information bases supported by this particular agent to request or change the information it is interested in snmp is focused on providing data about known elements of a device or entity in a lan or limited access wan scope rather than issues of internet scaling and links between independently authored content the cmip protocol was designed to allow the control of network resources by presenting their manageable characteristics as object graphs the objects have parent child relationships which are identified using distinguished names and attributes which are read and modified by a set of crud operations the notable non restful aspect of cmip is the m_action operation although wherever possible mib designers would typically endeavour to represent controllable and stateful aspects of network equipment through attributes 
the jacquard loom is a mechanical loom invented by joseph marie jacquard in 1801 that simplifies the process of manufacturing textiles with complex patterns such as brocade damask and matelasse the loom is controlled by punchcards with punched holes each row of which corresponds to one row of the design multiple rows of holes are punched on each card and the many cards that compose the design of the textile are strung together in order it is based on earlier inventions by the frenchmen basile bouchon jean falcon and jacques vaucanson each hole in the card corresponds to a bolus hook which can either be up or down the hook raises or lowers the harness which carries and guides the warp thread so that the weft will either lie above or below it the sequence of raised and lowered threads is what creates the pattern each hook can be connected via the harness to a number of threads allowing more than one repeat of a pattern a loom with a 400 hook head might have four threads connected to each hook resulting in a fabric that is 1600 warp ends wide with four repeats of the weave going across the term jacquard loom is a misnomer it is the jacquard head that adapts to a great many dobby looms such as the dornier brand that allow the weaving machine to then create the intricate patterns often seen in jacquard weaving jacquard looms whilst relatively common in the textile industry are not as ubiquitous as dobby looms which are usually faster and much cheaper to operate however unlike jacquard looms they are not capable of producing so many different weaves from one warp modern jacquard looms are controlled by computers in place of the original punched cards and can have thousands of hooks the threading of a jacquard loom is so labor intensive that many looms are threaded only once subsequent warps are then tied in to the existing warp with the help of a knotting robot which ties each new thread on individually even for a small loom with only a few thousand warp ends the process of re threading can take days the jacquard loom was the first machine to use punch cards to control a sequence of operations although it did no computation based on them it is considered an important step in the history of computing hardware the ability to change the pattern of the loom s weave by simply changing cards was an important conceptual precursor to the development of computer programming specifically charles babbage planned to use cards to store programs in his analytical engine this article was originally based on material from the free on line dictionary of computing which is licensed under the gfdl 
bytecode is a term which has been used to denote various forms of instruction sets designed for efficient execution by a software interpreter as well as being suitable for further compilation into machine code since instructions are processed by software they may be arbitrarily complex but are nonetheless often akin to traditional hardware instructions stack machines are common for instance different parts may often be stored in separate files similar to object modules but dynamically loaded during execution the name bytecode stems from instruction sets which have one byte opcodes followed by optional parameters intermediate representations such as bytecode may be output by programming language implementations to ease interpretation or it may be used to reduce hardware and operating system dependence by allowing the same code to run on different platforms bytecode may often be either directly executed on a virtual machine or it may be further compiled into machine code for better performance unlike human readable source code bytecodes are compact numeric codes constants and references which encode the result of parsing and semantic analysis of things like type scope and nesting depths of program objects they therefore allow much better performance than direct interpretation of source code a bytecode program is normally executed by parsing the instructions one at a time this kind of bytecode interpreter is very portable some systems called dynamic translators or just in time compilers translate bytecode into machine language as necessary at runtime this makes the virtual machine unportable but doesn t lose the portability of the bytecode itself for example java and smalltalk code is typically stored in bytecoded format which is typically then jit compiled to translate the bytecode to machine code before execution this introduces a delay before a program is run when bytecode is compiled to native machine code but improves execution speed considerably compared to interpretation normally by several times because of its performance advantage today many language implementations execute a program in two phases first compiling the source code into bytecode and then passing them to the virtual machine therefore there are virtual machines for java python php forth and tcl the current reference implementation of perl and ruby instead work by walking an abstract syntax tree representation derived from the source code 
in mathematics matrix multiplication is the operation of multiplying a matrix with either a scalar or another matrix this article gives an overview of the various ways to perform matrix multiplication this is the most often used and most important way to multiply matrices it is defined between two matrices only if the number of columns of the first matrix is the same as the number of rows of the second matrix formally forthenwhere the elements of a b are given byfor each pair i and j with 1 i m and 1 j p the algebraic system of matrix units summarises the abstract properties of this kind of multiplication the picture to the left shows how to calculate the element and the element of ab if a is a 3 2 matrix and b is a 2 3 matrix elements from each matrix are paired off in the direction of the arrows each pair is multiplied and the products are added the location of the resulting number in ab corresponds to the row and column that were considered 
for example ba this matrix multiplication can also be considered from a slightly different viewpoint it adds vectors together after being multiplied by different coefficients if a and b are matrices given by thenthe example revisited the rows in the matrix on the left can be thought of as the list of coefficients and the matrix on the right as the list of vectors in the example the first row is and thus we take 1 times the first vector 0 times the second vector and 2 times the third vector the equation can be simplified further by using outer products the terms of this sum are matrices of the same shape each describing the effect of one column of a and one row of b on the result the columns of a can be seen as a coordinate system of the transform i e given a vector x we have where xi are coordinates along the ai axes the terms aibi are like aixi except that bi contains the ith coordinate for each column vector of b each of which is transformed independently in parallel the example revisited the vectors and have been transformed to and in parallel one could also transform them one by one with the same steps the ordinary matrix product can be thought of as a dot product of a column list of vectors and a row list of vectors if a and b are matrices given by wherethen
in other words is the dot product of the xth row vector of a and the yth column vector of b the complexity of matrix multiplication if carried out naively is o but more efficient algorithms do exist strassen s algorithm devised by volker strassen in 1969 and often referred to as fast matrix multiplication is based on a clever way of multiplying two 2 2 matrices which requires only 7 multiplications at the expense of several additional addition and subtraction operations applying this trick recursively gives an algorithm with a multiplicative cost of strassen s algorithm is awkward to implement compared to the naive algorithm and it lacks numerical stability nevertheless it is beginning to appear in libraries such as blas where it is computationally interesting for matrices with dimensions n 100 and is very useful for large matrices over exact domains such as finite fields where numerical stability is not an issue the algorithm with the lowest known exponent which was presented by don coppersmith and shmuel winograd in 1990 has an asymptotic complexity of o it is similar to strassen s algorithm a clever way is devised for multiplying two k k matrices with fewer than k3 multiplications and this technique is applied recursively it improves on the constant factor in strassen s algorithm reducing it to 4 537 however the constant term implied in the o result is so large that the coppersmith winograd algorithm is only worthwhile for matrices that are too big to handle on present day computers since any algorithm for multiplying two n n matrices has to process all 2 n entries there is an asymptotic lower bound of  operations raz proves a lower bound of  for bounded coefficient arithmetic circuits over the real or complex numbers cohn et al put methods such as the strassen and coppersmith winograd algorithms in an entirely different group theoretic context they show that if families of wreath products of abelian with symmetric groups satisfying certain conditions exists matrix multiplication algorithms with essential quadratic complexity exist most researchers believe that this is indeed the case because of the nature of matrix operations and the layout of matrices in memory it is typically possible to gain substantial performance gains through use of parallelisation and vectorisation it should therefore be noted that some lower time complexity algorithms on paper may have indirect time complexity costs on real machines matrices offer a concise way of representing linear transformations between vector spaces and matrix multiplication corresponds to the composition of linear transformations this will be illustrated here by means of an example using three vector spaces of specific dimensions but the correspondence applies equally to any other choice of dimensions let x y and z be three vector spaces with dimensions 4 2 and 3 respectively all over the same field for example the real numbers the coordinates of a point in x will be denoted as xi for i 1 to 4 and analogously for the other two spaces two linear transformations are given one from y to x which can be expressed by the system of linear equationsand one from z to y expressed by the systemthese two transformations can be composed to obtain a transformation from z to x by substituting in the first system the right hand sides of the equations of the second system for their corresponding left hand sides the xi can be expressed in terms of the zk these three systems can be written equivalently in matrix vector notation thereby reducing each system to a single equation as follows representing these three equations symbolically and more concisely asinspection of the entries of matrix c reveals that c ab this can be used to formulate a more abstract definition of matrix multiplication given the special case of matrix vector multiplication the product ab of matrices a and b is the matrix c such that for all vectors z of the appropriate shape cz a the scalar multiplication of a matrix a and a scalar r gives a product ra of the same size as a the entries of ra are given byfor example ifthenif we are concerned with matrices over a more general ring then the above multiplication is the left multiplication of the matrix a with scalar p while the right multiplication is defined to bewhen the underlying ring is commutative for example the real or complex number field the two multiplications are the same however if the ring is not commutative such as the quaternions they may be different for examplefor two matrices of the same dimensions we have the hadamard product also known as the entrywise product and the schur product 
formally forthenwhere the elements of are given bynote that the hadamard product is a submatrix of the kronecker product the hadamard product is commutative the hadamard product appears in lossy compression algorithms such as jpeg for any two arbitrary matrices a and b we have the direct product or kronecker product a b defined asif a is an m by n matrix and b is a p by q matrix then their kronecker product a b is an mp by nq matrix the kronecker product is not commutative for exampleif a and b represent linear transformations v1 w1 and v2 w2 respectively then a b represents the tensor product of the two maps v1 v2 w1 w2 if a b and c are matrices with appropriate dimensions defined over a commutative field e g where c is a scalar in that field then for all three types of multiplication the frobenius inner product sometimes denoted a b is the component wise inner product of two matrices as though they are vectors in other words it is the sum of the entries of the hadamard product that is this inner product induces the frobenius norm square matrices can be multiplied by themselves repeatedly in the same way that ordinary numbers can this repeated multiplication can be described as a power of the matrix using the ordinary notion of matrix multiplication the following identities hold for an n by n matrix a a positive integer k and a scalar c the naive computation of matrix powers is to multiply k times the matrix a to the result starting with the identity matrix just like the scalar case this can be improved using the binary representation of k a method commonly used to scalars an even better method is to use the eigenvalue decomposition of a calculating high powers of matrices can be very time consuming but the complexity of the calculation can be dramatically decreased by using the cayley hamilton theorem which takes advantage of an identity found using the matrices characteristic polynomial and gives a much more effective equation for ak which instead raises a scalar to the required power rather than a matrix 
posix threads or pthreads is a posix standard for threads the standard defines an api for creating and manipulating threads pthreads are most commonly used on unix like posix systems such as linux mac os x and solaris but microsoft windows implementations also exist for example the pthreads w32 is available and supports a subset of the pthread api for the windows 32 bit platform pthreads defines a set of c programming language types functions and constants it is implemented with a pthread h header and a thread library programmers can use pthreads to create manipulate and manage threads as well as synchronize between threads using mutexes and signals an example of using pthreads in c an example of using pthreads in c this program creates a new thread that prints lines containing b while the main thread prints lines containing a the output is interleaved between a and b as a result of execution switching between the two threads or simultaneous execution on a multicore system in any case the pattern of a s and b s does not strictly alternate between each letter and can vary even between different runs on the same machine 
structured programming can be seen as a subset or subdiscipline of procedural programming one of the major programming paradigms it is most famous for removing or reducing reliance on the goto statement historically several different structuring techniques or methodologies have been developed for writing structured programs the most common are the two latter meanings for the term structured programming are more common and that is what this article will discuss years after dijkstra object oriented programming was developed to handle very large or complex programs at a low level structured programs are often composed of simple hierarchical program flow structures these are sequence selection and repetition some languages such as dijkstra s original guarded command language emphasise the unity of these structures with a syntax which completely encloses the structure as in if fi in others such as c this is not the case which increases the risk of misunderstanding and incorrect modification a language is described as block structured when it has a syntax for enclosing structures between bracketed keywords such as an if statement bracketed by if fi as in algol 68 or a code section bracketed by begin end as in pl i however a language is described as comb structured when it has a syntax for enclosing structures within an ordered series of keywords a comb structured language has multiple structure keywords to define separate sections within a block analogous to the multiple teeth or prongs in a comb separating sections of the comb for example in ada a block is a 4 pronged comb with keywords declare begin exception end and the if statement in ada is a 4 pronged comb with keywords if then else end if structured programming is often associated with a top down approach to design it is possible to do structured programming in any programming language though it is preferable to use something like a procedural programming language since about 1970 when structured programming began to gain popularity as a technique most new procedural programming languages have included features to encourage structured programming some of the better known structured programming languages are algol pascal pl i and ada the structured program theorem provides the theoretical basis of structured programming it states that three ways of combining programs sequencing selection and iteration are sufficient to express any computable function this observation did not originate with the structured programming movement these structures are sufficient to describe the instruction cycle of a central processing unit as well as the operation of a turing machine therefore a processor is always executing a structured program in this sense even if the instructions it reads from memory are not part of a structured program however authors usually credit the result to a 1966 paper by bhm and jacopini possibly because dijkstra cited this paper himself the structured program theorem does not address how to write and analyze a usefully structured program these issues were addressed during the late 1960s and early 1970s with major contributions by dijkstra robert w floyd tony hoare and david gries p j plauger an early adopter of structured programming described his reaction to the structured program theorem in 1967 a letter from dijkstra appeared in communications of the acm with the heading goto statement considered harmful the letter which cited the bhm and jacopini proof called for the abolishment of unconstrained goto from high level languages in the interest of improving code quality this letter is usually cited as the beginning of the structured programming debate although as plauger mentioned many programmers unfamiliar with the theorem doubted its claims the more significant dispute in the ensuing years was whether structured programming could actually improve software s clarity quality and development time enough to justify training programmers in it dijkstra claimed that limiting the number of structures would help to focus a programmer s thinking and would simplify the task of ensuring the program s correctness by dividing analysis into manageable steps in his 1969 notes on structured programming dijkstra wrote donald knuth accepted the principle that programs must be written with provability in mind but he disagreed with abolishing the goto statement in his 1974 paper structured programming with goto statements he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability knuth proposed a looser structural constraint it should be possible to draw a program s flow chart with all forward branches on the left all backward branches on the right and no branches crossing each other many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs structured programming theorists gained a major ally in the 1970s after ibm researcher harlan mills applied his interpretation of structured programming theory to the development of an indexing system for the new york times research file the project was a great engineering success and managers at other companies cited it in support of adopting structured programming although dijkstra criticized the ways that mills s interpretation differed from the published work as late as 1987 it was still possible to raise the question of structured programming in a computer science journal frank rubin did so in that year with a letter goto considered harmful considered harmful numerous objections followed including a response from dijkstra that sharply criticized both rubin and the concessions other writers made when responding to him by the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming high level programming languages that originally lacked programming structures such as fortran cobol and basic now have them although there is almost never a reason to have multiple points of entry to a subprogram multiple exits are often used to reflect that a subprogram may have no more work to do or may have encountered circumstances that prevent it from continuing a typical example of a simple procedure would be reading data from a file and processing it the stop and inform may be achieved by throwing an exception second return from the procedure labelled loop break or even a goto as the procedure has 2 exit points it breaks the rules of dijkstra s structured programming coding it in accordance with single point of exit rule would be very cumbersome if there were more possible error conditions with different cleanup rules single exit point procedure would be extremely hard to read and understand very likely even more so than an unstructured one with control handled by goto statements on the other hand structural programming without such a rule would result in very clean and readable code most languages have adopted the multiple points of exit form of structural programming c allows multiple paths to a structure s exit newer languages have also labelled breaks and exceptions some programs particularly parsers and communications protocols have a number of states that follow each other in a way that is not easily reduced to the basic structures it is possible to structure these systems by making each state change a separate subprogram and using a variable to indicate the active state however some programmers prefer to implement the state changes with a jump to the new state in the 1960s language design was often based on textbook examples of programs which were generally small however when programs became very large the focus changed in small programs the most common statement is generally the assignment statement however in large programs the most common statement is typically the procedure call to a subprogram ensuring parameters are correctly passed to the correct subprogram becomes a major issue many small programs can be handled by coding a hierarchy of structures however in large programs the organization is more a network of structures and insistence on hierarchical structuring for data and procedures can produce cumbersome code with large amounts of tramp data for example a text display program that allows dynamically changing the font size of the entire screen would be very cumbersome if coded by passing font size data through a hierarchy instead a subsystem could be used to control the font data through a set of accessor functions that set or retrieve data from a common area controlled by that font data subsystem databases are a common way around tramping the fortran language has used labelled common blocks to separate global program data into subsystems to allow program wide network style access to data such as font size but only by specifying the particular common block name confusion could occur in fortran by coding alias names and changing data types when referencing the same labelled common block yet mapping alternate variables to overlay the same area of memory regardless the labelled common concept was very valuable in organizing massive software systems and lead to the use of object oriented programming to define subsystems of centralized data controlled by accessor functions changing data into other data types was performed by explicitly converting or casting data from the original variables global subprogram names were recognized as just as dangerous than global variables or blank common and subsystems were limited to isolated groups of subprogram names such as naming with unique prefixes or using java package names although structuring a program into a hierarchy might help to clarify some types of software even for some special types of large programs a small change such as requesting a user chosen new option could cause a massive ripple effect with changing multiple subprograms to propagate the new data into the program s hierarchy the object oriented approach is allegedly more flexible by separating a program into a network of subsystems with each controlling their own data algorithms or devices across the entire program but only accessible by first specifying named access to the subsystem object class not just by accidentally coding a similar global variable name rather than relying on a structured programming hierarchy chart object oriented programming needs a call reference index to trace which subsystems or classes are accessed from other locations modern structured systems have tended away from deep hierarchies found in the 1970s and tend toward event driven architectures where various procedural events are designed as relatively independent tasks structured programming as a forerunner to object oriented programming noted some crucial issues such as emphasizing the need for a single exit point in some types of applications as in a long running program with a procedure that allocates memory and should deallocate that memory before exiting and returning to the calling procedure memory leaks that cause a program to consume vast amounts of memory could be traced to a failure to observe a single exit point in a subprogram needing memory deallocation similarly structured programming in warning of the rampant use of goto statements led to a recognition of top down discipline in branching typified by ada s goto that cannot branch to a statement label inside another code block however goto wrapup became a balanced approach to handling a severe anomaly without losing control of the major exit point to ensure wrap up when a severe issue interrupts complex multi level processing and wrap up code must be performed before exiting the various concepts behind structured programming can help to understand the many facets of object oriented programming 
brian wilson kernighan is a computer scientist who worked at bell labs alongside unix creators ken thompson and dennis ritchie and contributed greatly to unix and its school of thought he is also coauthor of the awk and ampl programming languages the k of k r c and the k in awk both stand for kernighan kernighan s name became widely known through co authorship of the first book on the c programming language with dennis ritchie kernighan has said that he had no part in the design of the c language he authored many unix programs including ditroff and cron for version 7 unix in collaboration with shen lin he devised well known heuristics for two np complete optimization problems graph partitioning and the travelling salesman problem brian was the software editor for prentice hall international his software tools series spread the essence of c unix thinking with makeovers for basic fortran and pascal and most notably his ratfor was put in the public domain he has said that if stranded on an island with only one programming language it would have to be c he received his bachelor s degree in engineering physics from the university of toronto he received his phd in electrical engineering from princeton university where he has held a professorship in the department of computer science since 2000 each fall he teaches a course called computers in our world which introduces the fundamentals of computing to non majors he has on occasion revealed it was his own pun which led to the use of the name unix for the operating system ken thompson and dennis ritchie were working on 
reification is a process through which a computable addressable object a resource is created in a system as a proxy for a non computable addressable object by means of reification something that was previously implicit unexpressed and possibly unexpressible is explicitly formulated and made available to conceptual manipulation informally reification is often referred to as making something a first class citizen within the scope of a particular system some aspect of a system can be reified at language design time this is related to reflection in programming languages reification can be applied as a stepwise refinement step at system design time reification is one of the most frequently used techniques of conceptual analysis and knowledge representation the etymology of the term reification is from latin res facere reification can be translated as thing making the turning of something abstract into a concrete thing or object in linguistics reification corresponds to a well known phenomenon called nominalization which consists in turning a verb into a noun for example move movement be a member membership nominalization often makes it easier to supply additional clauses to the verb in the context of programming languages reification is the process by which a user program or any aspect of a programming language which were implicit in the translated program and the run time system are expressed in the language itself and made available to the program which can inspect them as ordinary data in reflective languages reification data is causally connected to the related reified aspect such that a modification to one of them affects the other therefore the reification data is always a faithful representation of the related reified aspect reification data is often said to be made first class object reification at least partially has been experienced in many languages to date in early lisp dialects and in current prolog dialects programs have been treated as data although the causal connection has often been left to the responsibility of the programmer in smalltalk 80 the compiler from the source text to bytecode has been part of the run time system since the very first implementations of the language data reification involves finding a more concrete representation of the abstract data types used in a formal specification data reification is the terminology of the vienna development method for what most other people would call data refinement that is the taking of a step towards an implementation by replacing a data representation without a counterpart in the intended implementation language by one that does have a counterpart or at least one which is closer to having a counterpart such as sequences the vdm community prefers the word reification over refinement because the process has more to do with making an idea concrete than with making it more refined for similar usages see reification reification is widely used in conceptual modeling reifying a relationship consists in viewing it as an entity the purpose of reifying a relationship is to make it explicit when additional information needs to be added to it consider the relationship type ismemberof an instance of ismemberof is a relationship that represents the fact that a person is a member of a committee the figure below shows an example population of ismemberof relationship is tabular form person p1 is a member of committees c1 and c2 person p2 is a member of committee c1 only the same fact however could also be viewed as an entity when we view a relationship as an entity we say that the entity reifies the relationship the reification of a relationship consists in viewing it as an entity like any other this entity must be an instance of an entity type in the present example the entity type has been named membership for each instance of ismemberof there is one and only one instance of membership and vice versa now it becomes possible to add more information to the original relationship for example we can express the fact that person p1 was nominated to be the member of committee c1 by person p2 reified relationship membership can be used as the source of a new relationship isnominatedby for related usages see reification uml provides an association class construct for defining reified relationship types the association class is a single model element that is both a kind of association and a kind of a class the association and the entity type that reifies it are both the same model element note that attributes cannot be reified the figure below shows the uml class diagram for the membership example in semantic web languages such as resource description framework and web ontology language a statement is a binary relation it is used to link two individuals or an individual and a value applications sometimes need to describe other rdf statements using rdf for instance to record information about when statements were made who made them or other similar information for example we may want to represent properties of a relation such as our certainty about it severity or strength of a relation relevance of a relation and so on the example from the conceptual modeling section describes a particular person with uriref person p1 who is a member of the committee c1 the rdf triple from that description is and it might be useful to record who nominated this particular person to this committee rdf provides a built in vocabulary intended for describing rdf statements a description of a statement using this vocabulary is called a reification of the statement the rdf reification vocabulary consists of the type rdf statement and the properties rdf subject rdf predicate and rdf object using the reification vocabulary a reification of the statement about the person s membership would be given by assigning the statement a uriref such as committee membership12345 and then describing the statement using the statements these statements say that the resource identified by the uriref committee membership12345 is an rdf statement that the subject of the statement refers to the resource identified by person p1 the predicate of the statement refers to the resource identified by committee ismemberof and the object of the statement refers to the resource committee c1 assuming that the original statement is actually identified by committee membership12345 it should be clear by comparing the original statement with the reification that the reification actually does describe it the conventional use of the rdf reification vocabulary always involves describing a statement using four statements in this pattern the four statements are sometimes referred to as a reification quad for this reason using reification according to this convention we could record the fact that person p2 made the nomination described by the original statement by adding statement it is important to note that in the conventional use of reification the subject of the reification triples is assumed to identify a particular instance of a triple in a particular rdf document rather than some arbitrary triple having the same subject predicate and object this particular convention is used because reification is intended for expressing properties such as dates of composition and source information as in the examples given already and these properties need to be applied to specific instances of triples the power of the reification vocabulary in rdf is restricted by the lack of a built in means for assigning urirefs to statements so in order to express provenance information of this kind in rdf one has to use some mechanism to assign uris to individual rdf statements then make further statements about those individual statements using their uris to identify them of all the constructs in a xml topic maps only the topic is allowed to have names and occurrences and to play roles in associations in other words one can only make assertions about a subject which is represented by a topic those assertions themselves are not topics and so we cannot make assertions about assertions reification is the process by which a topic may be constructed to represent the assertion made by some other construct in the topic map this process enables a name to be given to a particular occurrence of a topic or documentation of an association to be attached to the association itself thus when an association is reified a new topic is created that can be used in higher level associations an rdf statement carries the same type of semantic information than a xtm association does in semantic web languages such as rdf and owl a property is a binary relation it is used to link two individuals or an individual and a value however in some cases the natural and convenient way to represent certain concepts is to use relations to link an individual to more than just one individual or value these relations are called n ary relations for example is representing relations among multiple individuals such as a committee a person who is a committee member and another person who has nominated the first person to become the committee member for example a buyer a seller and an object that was bought when describing a purchase of a book a more general approach to reification is to create an explicit new class and n new properties to represent an n ary relation an instance of the relation linking the n individuals is then an instance of this class this approach can be also used to represented provenance information and other properties for an individual relation instance it is also important to note that the interpretation of reification described here is not the same as quotation as found in some languages instead the reification describes the relationship between a particular instance of a triple and the resources the triple refers to the reification can be read intuitively as saying this rdf triple talks about these things rather than this rdf triple has this form for instance in the reification example used in this section the triple describing the rdf subject of the original statement says that the subject of the statement is the resource identified by the uriref person p1 it does not say that the subject of the statement is the uriref itself as quotation would do 
in the c programming language virtual inheritance is a kind of inheritance that solves some of the problems caused by multiple inheritance by clarifying ambiguity over which ancestor class members to use it is used when inheritance is representing restrictions of a set rather than composition of parts a multiply inherited base class is denoted as virtual with the virtual keyword consider the following class hierarchy but how does bat eat as declared above a call to bat eat is ambiguous one would have to call either bat wingedanimal animal eat or bat mammal animal eat this situation is sometimes referred to as diamond inheritance because the inheritance diagram is in the shape of a diamond virtual inheritance can help to solve this problem before going further it is helpful to consider how classes are represented in c in particular inheritance is simply a matter of putting parent and child class one after the other in memory thus bat is really which makes animal duplicated causing the ambiguity we can redeclare our classes as follows now the animal portion of bat wingedanimal is the same animal as the one used by bat mammal which is to say that a bat has only one animal in its representation and so a call to bat eat is unambiguous this is implemented by providing mammal and wingedanimal with a vtable pointer since e g the memory offset between the beginning of a mammal and of its animal part is unknown until runtime thus bat becomes two vtable pointers per object so the object size increased by two pointers but now there is only one animal and no ambiguity there are two vtables pointers one per inheritance hierarchy that virtually inherits animal one for mammal and one for wingedanimal all objects of type bat will have the same vtable s but each bat object will contain its own unique animal object if another class inherits mammal such as squirrel then the vtable in the mammal object in a squirrel will be different from the vtable in the mammal object in a bat although they can still be essentially the same in the special case that the squirrel part of the object has the same size as the bat part because then the distance from the mammal to the animal part is the same the vtables are not really the same but all essential information in them is 
in computer science reflection is the process by which a computer program can observe and modify its own structure and behaviour the programming paradigm driven by reflection is called reflective programming it is a particular kind of metaprogramming in many computer architectures program instructions are stored as data hence the distinction between instruction and data is merely a matter of how the information is treated by the computer and programming language normally instructions are executed and data is processed however in some languages programs can also treat instructions as data and therefore make reflective modifications reflection is most commonly used in high level virtual machine programming languages like smalltalk and scripting languages and less commonly used in manifestly typed and or statically typed programming languages such as java and c reflection oriented programming or reflective programming is a functional extension to the object oriented programming paradigm reflection oriented programming includes self examination self modification and self replication however the emphasis of the reflection oriented paradigm is dynamic program modification which can be determined and executed at runtime some imperative approaches such as procedural and object oriented programming paradigms specify that there is an exact predetermined sequence of operations with which to process data the reflection oriented programming paradigm however adds that program instructions can be modified dynamically at runtime and invoked in their modified state that is the program architecture itself can be decided at runtime based upon the data services and specific operations that are applicable at runtime programming sequences can be classified in one of two ways atomic or compound atomic operations are those that can be viewed as completing in a single logical step such as the addition of two numbers compound operations are those that require a series of multiple atomic operations a compound statement in classic procedural or object oriented programming can lose its structure once it is compiled the reflective programming paradigm introduces the concept of meta information which keeps knowledge of program structure meta information stores information such as the name of the contained methods the name of the class the name of parent classes and or what the compound statement is supposed to do using this stored information as an object is consumed it can be reflected upon to find out the operations that it supports the operation that issues in the required state via the desired state transition can be chosen at run time without hard coding it reflection can be used for observing and or modifying program execution at runtime a reflection oriented program component can monitor the execution of an enclosure of code and can modify itself according to a desired goal related to that enclosure this is typically accomplished by dynamically assigning program code at runtime reflection can also be used to adapt a given program to different situations dynamically for example consider an application that uses two different classes x and y interchangeably to perform similar operations without reflection oriented programming the application might be hard coded to call method names of class x and class y however using the reflection oriented programming paradigm the application could be designed and written to utilize reflection in order to invoke methods in classes x and y without hard coding method names reflection oriented programming almost always requires additional knowledge framework relational mapping and object relevance in order to take advantage of more generic code execution hard coding can be avoided to the extent that reflection oriented programming is used reflection is also a key strategy for metaprogramming a language supporting reflection provides a number of features available at runtime that would otherwise be very obscure or impossible to accomplish in a lower level language some of these features are the abilities to these features can be implemented in different ways in moo reflection forms a natural part of everyday programming idiom when verbs are called various variables such as verb and this are populated to give the context of the call security is typically managed by accessing the caller stack programmatically since callers is a list of the methods by which the current verb was eventually called performing tests on callers allows the verb to protect itself against unauthorised use compiled languages rely on their runtime system to provide information about the source code a compiled objective c executable for example records the names of all methods in a block of the executable providing a table to correspond these with the underlying methods compiled into the program in a compiled language that supports runtime creation of functions such as common lisp the runtime environment must include a compiler or an interpreter reflection can be implemented for languages not having built in reflection facilities by using a program transformation system to define automated source code changes here is an example in c here is an equivalent example in common lisp here is an equivalent example in ecmascript the following is an example in java using the java package java lang reflect here is an equivalent example in perl here is an equivalent example in php here is an equivalent example from the python shell here is an equivalent example in ruby here is an equivalent example in smalltalk 
software architect is a general term with many accepted definitions which refers to a broad range of roles generally accepted terminology and certifications began appearing in connection with this role near the beginning of the 21st century with the increased popularity of multi tier application development the choices of how an application can be built have also increased given that expansion the risk that a software development project may inadvertently create an end product that in essence already exists has grown markedly a new software architect role became necessary during software development the software architect concept began to take hold when object oriented programming was coming into more widespread use oop allowed ever larger and more complex applications to be built which in turn required increased high level application and system oversight the main responsibilities of a software architect include software architects can also in order to perform these responsibilities effectively software architects often use unified modeling language and oop uml has become an important tool for software architects to use in communicating the overall system design to developers and other team members comparable to the drawings made by building architects despite the lack of an accepted overall definition the role of software architect generally has certain common traits architects address the technological aspects of business needs by considering what a given technology can contribute to the overall functions of the system as distinct from how that technology will perform its own functions this encourages opportunities for re use of the technology and ultimately contributes to the organization s efficiency as a result an architect s decisions will often differ from those that a developer or a project manager might make in many ways the architect acts as a technically savvy business owner would where a developer will construct a software component based purely on technical specifications designed for creating software the software architect will integrate many such components into a coherent and viable whole architects deal with the interactions of systems whether between components written in different languages at different times and at different locations or between components of the same software system that use the same coding language one recent approach to this facet known as service oriented architecture offers new ways to define the apis of systems the architect makes high level design choices much more often than low level choices in addition the architect may sometimes dictate technical standards including coding standards tools or platforms so as to advance business goals rather than to place arbitrary restrictions on the choices of developers note that software architects rarely deal with the physical architecture of the hardware environment confining themselves to the design methodology of the code architects also have to communicate effectively not only to understand the business needs but also to advance their own architectural vision they can do so verbally in writing and through various software architectural models that specialize in communicating architecture the enterprise architect handles business related software decisions that frequently can involve multiple software systems within an organization spanning several projects teams and often at more than one site the enterprise architect may seldom see or interact with source code an application architect works with a single software application this may be a full or a part time role the application architect is almost always an active software developer other similar titles in use but without consensus on their exact meaning include the table below indicates many of the differences between various kinds of software architects in the software industry as the table above suggests the various versions of architect do not always have the same goals the term software architect came into being because of the perceived similarities between the creation of software and the creation of buildings the sudden popularity of the term in the world of information technology most likely stems from bill gates relinquishing of the title president and ceo of microsoft to assume the role of chief software architect the phrase reflected his new role as an overseer of many software development projects at microsoft although a simplified construction metaphor may be flawed the term is still meaningful in the sense that it describes the design aspect of the job when architects become too disconnected from the actual developers they are often dismissively termed ivory tower architects this is partly due to the limited usefulness of the construction metaphor moreover many waterfall model development methodologies of the past encouraged this working method application or solutions architects work at a level of detail that demands involvement in actual coding and will function best with a substantial background in software development a school of thought holds that enterprise architects should also have a development background so as to avoid the issues that can arise from an ivory tower approach 
in computer networking and databases the two phase commit protocol is a distributed algorithm that lets all nodes in a distributed system agree to commit a transaction the protocol results in either all nodes committing the transaction or aborting even in the case of network failures or node failures however the protocol will not handle more than one random site failure at a time the two phases of the algorithm are the commit request phase in which the coordinator attempts to prepare all the cohorts and the commit phase in which the coordinator completes the transactions note that the two phase commit protocol shouldn t be confused with two phase locking a concurrency control locking protocol the protocol works in the following manner one node is designated the coordinator which is the master site and the rest of the nodes in the network are designated the cohorts the protocol assumes that there is stable storage at each node with a write ahead log that no node crashes forever that the data in the write ahead log is never lost or corrupted in a crash and that any two nodes can communicate with each other the last assumption is not too restrictive as network communication can typically be rerouted the first two assumptions are much stronger if a node is totally destroyed then data can be lost the protocol is initiated by the coordinator after the last step of the transaction has been reached the cohorts then respond with an agreement message or an abort message depending on whether the transaction has been processed successfully at the cohort if the coordinator received an agreement message from all cohorts during the commit request phase if any cohort sent an abort message during the commit request phase the greatest disadvantage of the two phase commit protocol is the fact that it is a blocking protocol a node will block while it is waiting for a message this means that other processes competing for resource locks held by the blocked processes will have to wait for the locks to be released a single node will continue to wait even if all other sites have failed if the coordinator fails permanently some cohorts will never resolve their transactions this has the effect that resources are tied up forever the algorithm can block indefinitely in the following way if a cohort has sent an agreement message to the coordinator it will block until a commit or rollback is received if the coordinator is permanently down the cohort will block indefinitely unless it can obtain the global commit abort decision from some other cohort when the coordinator has sent query to commit to the cohorts it will block until all cohorts have sent their local decision yet if a cohort is permanently down the coordinator will not block indefinitely since the coordinator is the one to decide whether the decision is commit or abort permanent blocking can be avoided by introducing a timeout if the coordinator has not received all awaited messages when the timeout is over it will decide for abort this conservative behaviour of the protocol is another disadvantage it is biased to the abort case rather than the complete case a lot of database research has been done on ways to get most of the benefits of the two phase commit protocol without the costs in many cases the 2pc protocol is utilized in distributed environments the protocol is easily distributed in a network by implementing multiple dedicated 2pc components similar to each other typically named transaction managers that carry out the protocol s execution for each transaction the databases involved with a distributed transaction the participants both the coordinator and cohorts register to close tms for terminating that transaction using 2pc each distributed transaction has an ad hoc set of tms the tms to which the transaction participants register a leader the coordinator tm exists for each transaction to coordinate 2pc for it typically the tm of the coordinator database however the coordinator role can be transferred to another tm for performance or reliability reasons rather than exchanging 2pc messages among themselves the participants exchange the messages with their respective tms the relevant tms communicate among themselves to execute the 2pc protocol schema above representing the respective participants for terminating that transaction with this architecture the protocol is fully distributed and scales up with number of network nodes effectively this common architecture is also effective for the distribution of other atomic commitment protocols besides 2pc a common variant of 2pc in a distributed system which better utilizes the underlying communication infrastructure is the tree 2pc protocol in this variant the coordinator is the root of a communication tree while the cohorts are the other nodes messages from the coordinator are propagated down the tree while messages to the coordinator are collected by a cohort from all the cohorts below it before it sends the appropriate message up the tree the dynamic two phase commit protocol is a variant of tree 2pc with no predetermined coordinator agreement messages start to propagate from all the leaves each leaf when completed its tasks on behalf of the transaction and the coordinator is determined dynamically by racing agreement messages at the place where they collide they collide either on a transaction tree node or on an edge in the latter case one of the two edge s nodes is elected as a coordinator d2pc is time optimal it commits the coordinator and each cohort in minimum possible time allowing earlier release of locked resources 
a programming paradigm is a fundamental style of computer programming paradigms differ in the concepts and abstractions used to represent the elements of a program and the steps that compose a computation a programming language can support multiple paradigms for example programs written in c or object pascal can be purely procedural or purely object oriented or contain elements of both paradigms software designers and programmers decide how to use those paradigm elements in object oriented programming programmers can think of a program as a collection of interacting objects while in functional programming a program can be thought of as a sequence of stateless function evaluations when programming computers or systems with many processors process oriented programming allows programmers to think about applications as sets of concurrent processes acting upon logically shared data structures just as different groups in software engineering advocate different methodologies different programming languages advocate different programming paradigms some languages are designed to support one particular paradigm while other programming languages support multiple paradigms many programming paradigms are as well known for what techniques they forbid as for what they enable for instance pure functional programming disallows the use of side effects structured programming disallows the use of the goto statement partly for this reason new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to earlier styles avoiding certain techniques can make it easier to prove theorems about a program s correctness or simply to understand its behavior initially computers were hard wired or soft wired and then later programmed using binary code that represented control sequences fed to the computer cpu this was difficult and error prone programs written in binary are said to be written in machine code which is a very low level programming paradigm to make programming easier assembly languages were developed these replaced machine code functions with mnemonics and absolute memory addresses with symbolic labels assembly language programming is considered a low level paradigm although it is a second generation paradigm even 1960 s assembly languages actually supported library copy and quite sophisticated conditional macro generation and pre processsing capabilities they also supported modular programming features such as call external variables and common sections enabling significant code re use although a significant improvement over machine code it is claimed that assembler it is still more prone to errors difficult to debug and maintain than alternative languages paradigms 
the next advance was the development of procedural languages these third generation languages use vocabulary related to the problem being solved for example were developed mainly for commercial or scientific and engineering problems although one of the ideas behind the development of algol was that it was an appropriate language to define algorithms write programs all these languages follow the procedural paradigm that is they describe step by step exactly the procedure that should be followed to solve a problem familiar to most people from other disciplines these new languages did not replace assembler and were deleloped more or less in parallel assembler was and still is used for time critical systems and frequently in embedded systems it is claimed by some that the problem with procedural languages is that it can be difficult to reuse codeand to modify solutions when better methods of solution are developed in order to address these perceived problems object oriented languages were developed in these languages data and methods of manipulating the data are kept as a single unit called an object the only way that a user can access the data is via the object s methods this means that once an object is fully working it cannot be corrupted by the user it also means that the internal workings of an object may be changed without affecting any code that uses the object since object oriented programming is a paradigm not a language it is possible to create even an object oriented assembler language high level assembly is an example of this that fully supports advanced data types and object oriented assembly language programming a further advance was made when declarative programming paradigms were developed in these languages the computer is told what the problem is not how to solve the problem the program is structured as a collection of properties to find in the expected result not as a procedure to follow given a database or a set of rules the computer tries to find a solution matching all the desired properties the archetypical example of a declarative language is the fourth generation language sql as well as the family of functional languages functional programming is a subset of declarative programming programs written using this paradigm use functions blocks of code intended to behave like mathematical functions functional languages discourage changes in the value of variables through assignment making a great deal of use of recursion instead 
defensive programming is a form of defensive design intended to ensure the continuing function of a piece of software in spite of unforeseeable usage of said software the idea can be viewed as reducing or eliminating the prospect of murphy s law having effect defensive programming techniques are used especially when a piece of software could be misused mischievously or inadvertently to catastrophic effect defensive programming is an approach to improve software and source code in terms of defensive programming is sometimes referred to as secure programming by computer scientists who state this approach minimizes bugs software bugs can be potentially used by a cracker for a code injection denial of service attack or other attack a difference between defensive programming and normal practices is that few assumptions are made by the programmer who attempts to handle all possible error states in short the programmer never assumes a particular function call or library will work as advertised and so handles it in the code an example follows the function will crash when the input is over 1000 characters many mainstream programmers may not feel that this is a problem because surely no one will enter that long of an input a programmer practicing defensive programming would not allow the bug because if the application contains a known bug murphy s law dictates that the bug will occur in use this particular bug demonstrates a vulnerability which enables buffer overflow exploits here is a solution to this example here are some defensive programming techniques suggested by some leading computer scientists to avoid creating security problems and software bugs these computer scientists state that while this process can improve general quality of code it is not sufficient to ensure security see the articles computer insecurity and secure computing for more information defensive software programming principles described by leading proponents include never make code more complex than necessary complexity breeds bugs including security problems this goal can conflict with the goal of writing programs that can recover from any error and handle any user input handling all unexpected occurrences in a program requires the programmer to add extra code which may also contain bugs a source code review is where someone other than the original author performs a code audit a do it yourself security audit is insufficient the review must be made by a non author just as when writing a book it must be proofread by someone other than the author simply making the code available for others to read is insufficient there is no guarantee that the code will ever be looked at let alone that it will be rigorously reviewed software testing should include both whether the software works as intended and what is supposed to happen when deliberately bad input is supplied testing tools can capture keystrokes associated with normal operations then the captured keystroke strings can be copied and edited to try out all permutations of combinations then extended for later tests after any modifications proponents of key logging state that programmers who use this method should make sure that the people whose keystrokes are being captured are aware of this and for what purpose to avoid accusations of privacy violation if possible reuse code instead of writing from scratch the idea is to capture the benefits of well written and well tested source code instead of creating unnecessary bugs however re using code is not always the best way to go forward particularly when business logic is involved reuse in this case may cause serious business process bugs before reusing old source code libraries apis configurations and so forth it must be considered if the old work is valid for reuse or if it is likely to be prone to legacy problems legacy problems are problems inherent when old designs are expected to work with today s requirements especially when the old designs were not developed or tested with those requirements in mind many software products have experienced problems with old legacy source code for example notable examples of the legacy problem managing input is a difficult problem which is detailed in secure input and output handling crackers are likely to invent new kinds of representations of incorrect data for example if you checked if a requested file is not etc passwd a cracker might pass another variant of this file name like etc passwd to avoid bugs due to non canonical input employ canonicalization apis employ principle of least privilege avoid having software running in a privileged mode if possible assume that code constructs that appear to be problem prone are bugs and potential security flaws the basic rule of thumb is i m not aware of all types of security exploits i must protect against those i do know of and then i must be proactive 
coordinated universal time fr temps universel coordonn is a time standard based on international atomic time with leap seconds added at irregular intervals to compensate for the earth s slowing rotation leap seconds are used to allow utc to closely track ut1 which is mean solar time at the royal observatory greenwich the difference between utc and ut1 is not allowed to exceed 0 9 seconds so if high precision is not required the general term universal time may be used in casual use greenwich mean time can be considered equivalent to utc or ut1 when fractions of a second are not important owing to the ambiguity as to whether utc or ut1 is meant gmt is generally avoided in technical contexts time zones around the world can be expressed as positive or negative offsets from utc utc replaced gmt as the basis for the main reference time scale or civil time in various regions on january 1 1972 coordinated universal time is abbreviated utc the international telecommunication union wanted coordinated universal time to have a single abbreviation for all languages english speakers and french speakers each wanted the initials of their respective language s terms to be used internationally cut for coordinated universal time and tuc for temps universel coordonn this resulted in the final compromise of using utc utc also has the benefit that it fits in with the pattern for the abbreviations of variants of universal time ut0 ut1 ut1r and others exist so appending c for coordinated to the base ut is very satisfactory for those who are familiar with the other types of ut utc has been erroneously expanded into universal time code or universal time convention as a time scale utc divides up time into days hours minutes and seconds days are conventionally identified using the gregorian calendar but julian day numbers can also be used each day contains 24 hours and each hour contains 60 minutes but the number of seconds in a minute can be 60 or sometimes 61 or 59 thus in the utc time scale the second and all smaller time units are of constant duration but the minute and all larger time units are of variable duration most utc days contain exactly 86 400 si seconds with exactly 60 seconds in each minute however since the mean solar day is slightly longer than 86 400 si seconds occasionally the last minute of a utc day will have 61 seconds the extra second is called a leap second it accounts for the grand total of the extra length of all the mean solar days since the previous leap second the last minute of a utc day is allowed to contain 59 seconds to cover the remote possibility of the earth rotating faster but that has not yet been necessary since utc was introduced the irregular day lengths mean that fractional julian days do not work properly with utc utc is derived from international atomic time which is a coordinate time scale tracking notional proper time on the rotating surface of the earth at any particular time utc proceeds as a linear function of tai from 1972 onward utc ticks at the same rate as tai but earlier utc ticked at a different rate from tai in order to remain a close approximation of ut1 utc occasionally has discontinuities where it changes from one linear function of tai to another these discontinuities take the form of leaps implemented by a utc day of irregular length and changes to the rate at which utc ticks relative to tai discontinuities in utc have only ever occurred at the end of a gregorian month the international earth rotation and reference systems service tracks and publishes the difference between utc and universal time dut1 ut1 utc and introduces discontinuities into utc to keep dut1 in the range 0 9s dut1 0 9s since 1972 the discontinuities have consisted only of a leap of one second at the end of 30 june or 31 december the iers publishes its decision on whether to have a leap second on each of these dates a few months in advance in bulletin c in principle leap seconds can also occur on 31 march or 30 september but the iers has never found this necessary as with tai utc is only known with the highest precision in retrospect the international bureau of weights and measures publishes monthly tables of differences between canonical tai utc and tai utc as estimated in real time by participating laboratories the local mean solar time at the royal observatory greenwich england was chosen at the 1884 international meridian conference to define the universal day counted from zero hours at mean midnight in recognition of the widespread use of greenwich mean time which in 1884 was used for two thirds of all charts and maps as their prime meridian in 1928 the term universal time was introduced by the international astronomical union to refer to gmt with the day starting at midnight until the 1950s broadcast time signals were based on ut and hence on the rotation of the earth in 1955 the caesium atomic clock was invented this provided a form of timekeeping that was both more stable and more convenient than astronomical observations in 1956 the u s national bureau of standards started to use atomic frequency standards in generating the wwv time signals named for the shortwave radio station which broadcasts them in a controversial decision the frequency of the signals was initially set to match the rate of ut but then kept at the same frequency by the use of atomic clocks and deliberately allowed to drift away from ut when the divergence grew significantly the signal was phase shifted by 20 ms to bring it back into agreement with ut many such steps were used the signal frequency was changed less often in 1958 the international atomic time service started it was based on the frequency for the caesium transition newly established that was later used to redefine the second in 1967 at a length practically equal to the second of ephemeris time the wwv time signal s frequency was set to a simple offset from the tai frequency initially an offset of 1 0 10 8 so that wwv ticked exactly one second for every 1 00000001s of tai despite the initial controversy it became clear that basing time signals on atomic clocks was an improvement over the prior system however it was widely desired to keep civil time synchronised with the earth s rotation and many uses of time signals relied on their closely matching universal time wwv s compromise approach was copied by other agencies worldwide such as the royal greenwich observatory it then became a concern that time signals should be synchronised with each other rather than independently determining their own frequency offsets and phase shifts in 1960 an international agreement was made on atomic based time signals a frequency offset of 1 5 10 8 was adopted by all the participating institutions matching the then current rate of ut2 a version of ut1 smoothed from seasonal variations ad hoc phase shifts were used to synchronise the time signals as far as possible it was determined that the bureau international de l heure should henceforth choose the frequency offsets and coordinate the time steps it was also decided to use larger jumps of 50ms instead of 20ms utc was officially initiated at the start of 1961 the tai instant 1 january 1961 00 00 01 422818 exactly was identified as utc instant 1 january 1961 00 00 00 000000 exactly and utc ticked exactly one second for every 1 000000015s of tai time steps occurred every few months thereafter and frequency changes at the end of each year the jumps increased in size to 100ms with only one 50ms jump having ever occurred this utc was intended to permit a very close approximation of ut2 within around 0 1s in 1967 the si second was redefined in terms of the frequency supplied by a caesium atomic clock the length of second so defined was practically equal to the second of ephemeris time this was the frequency that had been provisionally used in tai since 1958 it was soon recognised that having two types of second with different lengths namely the utc second and the si second used in tai was a bad idea it was thought that it would be better for time signals to maintain a consistent frequency and that that frequency should match the si second thus it would be necessary to rely on time steps alone to maintain the approximation of ut this was tried experimentally in a service known as stepped atomic time which ticked at the same rate as tai and used jumps of 200ms to stay synchronised with ut2 there was also dissatisfaction with the frequent jumps in utc in 1968 louis essen the inventor of the caesium atomic clock and g m r winkler both independently proposed that steps should be of 1s only this system was eventually approved along with the idea of maintaining the utc second equal to the tai second at the end of 1971 there was a final irregular jump of 0 107758 tai seconds exactly so that 1 january 1972 00 00 00 utc was 1 january 1972 00 00 10 tai exactly making the difference between utc and tai an integer number of seconds at the same time the tick rate of utc was changed to exactly match tai utc also started to track ut1 rather than ut2 some time signals started to broadcast the dut1 correction for applications which required a closer approximation of ut1 than utc now provided the first leap second occurred on june 30 1972 since then leap seconds have occurred on average about once every 19 months always on june 30 or december 31 as of 2009 there have been 24 leap seconds in total all positive putting utc 34 seconds behind tai it seems unlikely that a negative leap second will ever occur but there is a small chance of one due to the acceleration of the earth s crust in the 2000s this acceleration has already led to the longest ever period without a leap second from 1999 01 01 to 2005 12 31 the earth s rotational speed is very slowly decreasing due to tidal deceleration causing the mean solar day to increase in length the length of the si second was calibrated on the basis of the second of ephemeris time and can now be seen to have a relationship with the mean solar day observed between 1750 and 1892 analysed by simon newcomb as a result the si second is close to 1 86400 of a mean solar day in around 1820 in earlier centuries the mean solar day was shorter than 86400 si seconds and in later centuries it is longer than 86400 seconds at the end of the 20th century the length of the mean solar day was approximately 86 400 002s for this reason ut is now slower than tai the excess of the lod over the nominal 86 400s accumulates over time causing the utc day initially synchronised with the mean sun to become desynchronised and run ahead of it at the end of the 20th century with the lod at 2ms above the nominal value utc ran faster than ut by 2ms per day getting a second ahead roughly every 500 days thus leap seconds were inserted at approximately this interval retarding utc to keep it synchronised in the long term note that the actual rotational period varies on unpredictable factors such as tectonic motion and has to be observed rather than computed the insertion of a leap second every 500 days does not mean that the mean solar day is getting longer by a second every 500 days it will take approximately 50 000 years for a mean solar day to lengthen by one second this is a mean rate within the range of 1 7 2 3ms cy the rate due to tidal friction alone is about 2 3ms cy but the uplift of canada and scandinavia by several metres since the last ice age has temporarily reduced this to 1 7ms cy over the last 2700 years the correct reason for leap seconds is not the current difference between actual and nominal lod but rather the accumulation of this difference over a period of time in the late twentieth century this difference was about 1 500 of a second per day so it accumulated to 1 second after about 500 days for example assume you start counting the seconds from the unix epoch of 1970 01 01t00 00 00 utc with an atomic clock at midnight on that day your counter registers 0s after earth has made one full rotation with respect to the mean sun your counter will register approximately 86400 002s based on your counter you can calculate that the date is 1970 01 02t00 00 00 ut1 after 500 rotations your counter will register 43 200 001s since 86 400s 500 is 43 200 000s you will calculate that the date is 1971 05 16t00 00 01 utc while it is only 1971 05 16t00 00 00 ut1 if you had added a leap second on december 31 1970 retarding your counter by 1s then the counter would have a value of 43 200 000s at 1971 05 16t00 00 00 ut1 and allow you to calculate the correct date in the graph of dut1 above the excess of lod above the nominal 86 400s corresponds to the downward slope of the graph between vertical segments vertical position on the graph corresponds to the accumulation of this difference over time and the vertical segments correspond to leap seconds introduced to match this accumulated difference leap seconds are timed to keep dut1 within the vertical range depicted by this graph the frequency of leap seconds therefore corresponds to the slope of the diagonal graph segments and thus to the excess lod as the earth s rotation continues to slow positive leap seconds will be required more frequently the long term rate of change of lod is approximately 1 7ms per century at the end of the 21st century lod will be roughly 86 400 004s requiring leap seconds every 250 days over several centuries the frequency of leap seconds will become problematic sometime in the 22nd century two leap seconds will be required every year the current use of only the leap second opportunities in june and december will be insufficient and the march and september options will have to be used in the 25th century four leap seconds will be required every year so the current quarterly options will be insufficient thereafter there will need to be the possibility of leap seconds at the end of any month in about two thousand years even that will become insufficient and there will have to be leap seconds that are not at the end of a month in a few tens of thousands of years lod will exceed 86 401s causing the current form of utc to break down due to requiring more than one leap second per day it would be possible to then continue with double leaps but this becomes increasingly untenable both the one leap second per month and one leap second per day milestones are considered to mark the theoretical limit of the applicability of utc the actual number of leap seconds to keep track of time would become unwieldy by current standards well before these but presumably if utc were to continue then horological systems would be redesigned to cope with regular leap seconds much better than current systems do there is a proposal to redefine utc and abolish leap seconds such that sundials would slowly get further out of sync with civil time the resulting gradual shift of the sun s movements relative to civil time is analogous to the shift of seasons relative to the yearly calendar that results from the calendar year not precisely matching the tropical year length this would be a major practical change in civil timekeeping but would take effect slowly over several centuries an itu study group was to have voted on this possibility during 2008 possibly leading to official approval by the world radio conference in 2011 and the cessation of leap seconds in 2013 there is also a proposal that the present form of utc could be improved to track ut1 more closely by allowing greater freedom in scheduling leap seconds utc is the time system used for many internet and world wide web standards in particular the network time protocol which is designed to synchronise the clocks of many computers over the internet uses utc those who transmit on the amateur radio bands often log the time of their radio contacts in utc as transmissions can go worldwide on some frequencies in the past the fcc required all amateur radio operators in the united states to log their radio conversations international broadcasters such as the bbc world service also use utc when publishing their schedules and announcing times during broadcasts utc is also the time system used in aviation weather forecastings flight plans air traffic control clearances and maps all use utc to avoid confusion about time zones and daylight saving time because of time dilation a standard clock not on the geoid or in rapid motion will not maintain synchronicity with utc therefore telemetry from clocks with a known relation to the geoid is used to provide utc when required on locations such as that of spacecraft utc is a discontinuous timescale so it is not possible to compute the exact time interval elapsed between two utc timestamps without consulting a table that describes how many leap seconds occurred during that interval therefore many scientific applications that require precise measurement of long intervals use tai instead tai is also commonly used by systems that can not handle leap seconds a fixed 19 second offset from tai also gives gps time for most common and legal trade purposes the fractional second difference between utc and ut is inconsequentially small so utc is often called gmt for example by the bbc although that usage is ambiguous time zones usually differ from utc by an integral number of hours although the laws of each jurisdiction would have to be consulted if sub second accuracy was required several jurisdictions established time zones that differ by an integer number of half hours or quarter hours from ut1 or utc the utc time zone is sometimes denoted by the letter z a reference to the equivalent nautical time zone which has been denoted by a z since about 1950 the letter also refers to the zone description of zero hours which has been used since 1920 since the nato phonetic alphabet and amateur radio word for z is zulu utc is sometimes known as zulu time this is especially true in aviation where zulu is the universal standard this ensures all pilots regardless of location are using the same 24 hour clock thus avoiding confusion when flying between time zones clock horology history of timekeeping devices astrarium marine chronometer sundial water clockcalendar day week month year tropical year julian gregorian islamicarrow of time chronon fourth dimension planck epoch planck time time domainterrestrial time geocentric coordinate time barycentric coordinate timecivil time minute hour 12 hour clock 24 hour clock iso 8601theory of relativity time dilation gravitational time dilation coordinate time proper timeastronomical julian gregorian islamic lunisolar solar lunar epact intercalation leap year
in object oriented computer programming a null object is an object with defined neutral behavior the null object design pattern describes the uses of such objects and their behavior it was first published in the pattern languages of program design book series in most object oriented languages such as java references may be null these references need to be checked to ensure they are not null before invoking any methods because one can t invoke anything on a null reference instead of using a null reference to convey absence of an object one uses an object which implements the expected interface but whose method body is empty the advantage of this approach over a working default implementation is that a null object is very predictable and has no side effects it does nothing for example the processing of binary search trees including operations such as insertion deletion and lookup involves many nullity check of the encountered pointers these checks can be implemented within the special object pattern the null object pattern can also be used to act as a stub for testing if a certain feature such as a database is not available for testing it can be regarded as a special case of the state pattern and the strategy pattern it is not a pattern from design patterns but is mentioned in martin fowler s refactoring and joshua kerievsky s book on refactoring in the insert null object refactoring chapter 17 is dedicated to the pattern in robert cecil martin s agile software development principles patterns and practices
z order morton order or morton code first proposed in 1966 by g m morton is a space filling curve which is often used in computer science due to its good locality preserving behaviour it is used in data structures for mapping multidimensional data to one dimension the z value of a point in multidimensions is simply calculated by interleaving the binary representations of its coordinate values once the data are sorted into this ordering any one dimensional data structure can be used such as binary search trees b trees skip lists or hash tables the resulting ordering can equivalently be described as the order would get from a depth first traversal of a quadtree because of its close connection with quadtrees the z ordering can be used to efficiently construct quadtrees and related higher dimensional data structures the figure below shows the z values for the two dimensional case with integer coordinates 0 x 7 0 y 7 interleaving the binary coordinate values yields binary z values as shown connecting the z values in their numerical order produces the recursively z shaped curve although well locality preserving for efficient range searches an algorithm is necessary for calculating from a point encountered in the data structure the next z value which is in the multidimensional search range in this example the range being queried is indicated by the dotted rectangle its highest z value is 45 in this example the value f 19 is encountered when searching a data structure in increasing z value direction so we would have to search in the interval between f and max to speed up the search one would calculate the next z value which is in the search range called bigmin and only search in the interval between bigmin and max thus skipping most of the hatched area searching in decreasing direction is analogous with litmax which is the highest z value in the query range lower than f the bigmin problem has first been stated and its solution shown in this solution is also used in ub trees as the approach does not depend on the one dimensional data structure chosen there is still free choice of structuring the data so well known methods such as balanced trees can be used to cope with dynamic data similarly this independence makes it easier to incorporate the method into existing databases applying the method hierarchically optionally in both increasing and decreasing direction yields highly efficient multidimensional range search which is important in both commercial and technical applications e g as a procedure underlying nearest neighbour searches z order is one of the few multidimensional access methods that has found its way into commercial database systems transbase 2000 already in 1966 g m morton has proposed z order for file sequencing of a static two dimensional geographical database areal data units are contained in one or a few quadratic frames represented by their sizes and lower right corner z values the sizes complying with the z order hierarchy at the corner position with high probability changing to an adjacent frame is done with one or a few relatively small scanning steps as an alternative the hilbert curve has been suggested as it has a better order preserving behaviour but here the calculations are much more complicated leading to significant processor overhead bigmin source code for both z curve and hilbert curve were described in a patent by h tropf for a recent overview on multidimensional data processing including e g nearest neighbour searches see hanan samet s textbook the strassen algorithm for matrix multiplication is based on splitting the matrices in four blocks and then recursively each of these blocks in four smaller blocks until the blocks are single elements arranging the matrix elements in z order then improves locality and has the additional advantage that the subroutine for multiplying two blocks does not need to know the total size of the matrix but only the size of the blocks and their location in memory 
postscript is a dynamically typed concatenative programming language created by john warnock and charles geschke in 1982 postscript is best known for its use as a page description language in the electronic and desktop publishing areas the concepts of the postscript language were seeded in 1976 when john warnock was working at evans sutherland a famous computer graphics company at that time john warnock was developing an interpreter for a large three dimensional graphics database of new york harbor warnock conceived the design system language to process the graphics concurrently researchers at xerox parc had developed the first laser printer and had recognized the need for a standard means of defining page images in 1975 76 a team led by bob sproull developed the press format which was eventually used in the xerox star system to drive laser printers but press a data format rather than a language lacked flexibility and parc mounted the interpress effort to create a successor in 1978 evans and sutherland asked warnock to move from the san francisco bay area to their main headquarters in utah but he was not interested in moving he then joined xerox parc to work with martin newell they rewrote design system to create jam which was used for vlsi design and the investigation of type and graphics printing this work later evolved and expanded into the interpress language warnock left with chuck geschke and founded adobe systems in december 1982 they created a simpler language similar to interpress called postscript which went on the market in 1984 at about this time they were visited by steve jobs who urged them to adapt postscript to be used as the language for driving laser printers in march 1985 the apple laserwriter was the first printer to ship with postscript sparking the desktop publishing revolution in the mid 1980s the combination of technical merits and widespread availability made postscript a language of choice for graphical output for printing applications for a time an interpreter for the postscript language was a common component of laser printers into the 1990s however the cost of implementation was high computers output raw ps code that would be interpreted by the printer into a raster image at the printer s natural resolution this required high performance microprocessors and ample memory the laserwriter used a 12 mhz motorola 68000 making it faster than any of the macintosh computers it attached to when the laser printer engines themselves cost over a thousand dollars the added cost of ps was worthwhile but as printer mechanisms fell in price the cost of implementing ps became increasingly expensive once the de facto standard for electronic distribution of final documents meant for publication postscript is steadily being supplanted in this area by one of its own descendants the portable document format or pdf by 2001 there were fewer printer models which came with support for postscript largely due to the growing competition from much cheaper non postscript ink jet printers and new software based methods to render postscript images on the computer making them suitable for any printer the use of a postscript laser printer still can however significantly reduce the cpu workload involved in printing documents transferring the work of rendering postscript images from the computer to the printer ps is still an option on most high end models the postscript language has had two major upgrades the first version known as postscript level 1 was introduced in 1984 postscript level 2 was introduced in 1991 and included several improvements improved speed and reliability support for in rip separations image decompression support for composite fonts and the form mechanism for caching reusable content postscript 3 came at the end of 1997 and along with many new dictionary based versions of older operators introduced better color handling and new filters postscript 3 was significant in terms of replacing the existing proprietary color electronic prepress systems then widely used for magazine production through the introduction of smooth shading operations with up to 4096 shades of grey as well as devicen a color space that allowed the addition of additional ink colors into composite color pages prior to the introduction of postscript printers were designed to print character output given the text typically in ascii as input there were a number of technologies for this task but most shared the property that the glyphs were physically difficult to change as they were stamped onto typewriter keys bands of metal or optical plates this changed to some degree with the increasing popularity of dot matrix printers the characters on these systems were drawn as a series of dots the proper dots to use defined as a font table inside the printer as they grew in sophistication dot matrix printers started including several built in fonts from which the user could select and some models allowed users to upload their own custom glyphs into the printer dot matrix printers also introduced the ability to print raster graphics the graphics were interpreted by the computer and sent as a series of dots to the printer using a series of escape sequences these printer control languages varied from printer to printer requiring program authors to create numerous drivers vector graphics printing was left to special purpose devices called plotters plotters did share a common command language hpgl but were of limited use for anything other than printing graphics in addition they tended to be expensive and slow and thus rare laser printers combine the best features of both printers and plotters like plotters laser printers offer high quality line art and like dot matrix printers they are able to generate pages of text and raster graphics unlike either printers or plotters however a laser printer makes it possible to position high quality graphics and text on the same page postscript made it possible to fully exploit these characteristics by offering a single control language that could be used on any brand of printer postscript went beyond the typical printer control language and was a complete programming language of its own many applications can transform a document into a postscript program whose execution will result in the original document this program can be sent to an interpreter in a printer which results in a printed document or to one inside another application which will display the document on screen since the document program is the same regardless of its destination it is called device independent postscript is noteworthy for implementing on the fly rasterization everything even text is specified in terms of straight lines and cubic bzier curves which allows arbitrary scaling rotating and other transformations when the postscript program is interpreted the interpreter converts these instructions into the dots needed to form the output for this reason postscript interpreters are also sometimes called postscript raster image processors or rips almost as complex as postscript itself was its handling of fonts the rich font system used the ps graphics primitives to draw glyphs as line art which could then be rendered at any resolution though this sounds like a reasonably straightforward concept there were a number of typographic issues that had to be considered one issue is that fonts do not actually scale linearly at small sizes features of the glyphs will become proportionally too large or small and they start to look wrong postscript avoided this problem with the inclusion of hints which could be saved along with the font outlines basically they are additional information in horizontal or vertical bands that help identify the features in each letter that are important for the rasterizer to maintain the result was significantly better looking fonts even at low resolution it had formerly been believed that hand tuned bitmap fonts were required for this task at the time the technology for including these hints in fonts was carefully guarded and the hinted fonts were compressed and encrypted into what adobe called a type 1 font type 1 was effectively a simplification of the ps system to store outline information only as opposed to being a complete language adobe would then sell licenses to the type 1 technology to those wanting to add hints to their own fonts those who did not license the technology were left with the type 3 font type 3 fonts allowed for all the sophistication of the postscript language but without the standardized approach to hinting other differences further added to the confusion type 2 was designed to be used with the compact font format and were implemented for a compact representation of the glyph description procedures to reduce the overall font file size the cff type2 format later became the basis for type 1 opentype fonts cid keyed font format was also designed to solve the problems in the ocf type 0 fonts for addressing the complex asian language encoding and very large character set issues cid keyed font format can be used with the type 1 font format for standard cid keyed fonts or type 2 for cid keyed opentype fonts adobe s rates were widely considered to be prohibitively high and it was this issue that led apple to design their own system truetype around 1991 immediately following the announcement of truetype adobe published the specification for the type 1 font format retail tools such as altsys fontographer added the ability to create type 1 fonts since then many free type 1 fonts have been released for instance the fonts used with the tex typesetting system are available in this format in the early 1990s there were several other systems for storing outline based fonts developed by bitstream and metafont for instance but none included a general purpose printing solution and they were therefore not widely used as a result in the late 1990s adobe joined microsoft in developing opentype essentially a functional superset of the type 1 and truetype formats when printed to a postscript output device the unneeded parts of the opentype font are omitted and what is sent to the device by the driver is the same as it would be for a truetype or type 1 font depending on which kind of outlines were present in the opentype font in the 1980s adobe drew most of their revenue from the licensing fees for their implementation of postscript for printers known as a raster image processor or rip as a number of new risc based platforms became available in the mid 1980s some found adobe s support of the new machines to be lacking this and issues of cost led to third party implementations of postscript becoming common particularly in low cost printers or in high end typesetting equipment at one point microsoft and apple teamed up to try to unseat adobe s laser printer monopoly microsoft licensing to apple a postscript compatible interpreter it had bought called trueimage and apple licensing to microsoft its new font format truetype today third party postscript compatible interpreters are widely used in printers and multifunction peripherals for example zoran corporation s ips ps3 interpreter formerly known as phoenixpage is standard in many printers and mfps including those developed by hewlett packard and sold under the laserjet and color laserjet lines other third party postscript solutions used by print and mfp manufacturers include jaws and harlequin both provided by global graphics still some basic inexpensive laser printers don t support postscript instead coming with drivers that simply rasterize the platform s native graphics formats rather than converting them to postscript first when postscript support is needed for such a printer a free postscript compatible interpreter called ghostscript can be used ghostscript prints postscript documents on non postscript printers using the cpu of the host computer to do the rasterization sending the result as a single large bitmap to the printer ghostscript can also be used to preview postscript documents on a computer monitor and to convert postscript pages into raster graphics such as tiff and png and vector formats such as pdf very high resolution devices such as imagesetters or ctp platesetters in which resolutions exceeding 2500 dpi are common still require external rips with large amounts of memory and hard drive space very high end laser printer systems also use an external rip to separate the more readily upgradable computer from the specialized printing hardware companies such as efi and xitron specialize in such rip software postscript became commercially successful due to the introduction of the graphical user interface allowing designers to directly lay out pages for eventual output on laser printers however the gui s own graphics systems were generally much less sophisticated than postscript apple s quickdraw for instance supported only basic lines and arcs not the complex b splines and advanced region filling options of postscript in order to take full advantage of postscript printing applications on the computers had to re implement those features using the host platform s own graphics system this led to numerous issues where the on screen layout would not exactly match the printed output due to differences in the implementation of these features as computer power grew it became possible to host the ps system in the computer rather than the printer this led to the natural evolution of ps from a printing system to one that could also be used as the host s own graphics language there were numerous advantages to this approach not only did it help eliminate the possibility of different output on screen and printer but it also provided a powerful graphics system for the computer and allowed the printers to be dumb at a time when the cost of the laser engines was falling in a production setting using postscript as a display system meant that the host computer could render low resolution to the screen higher resolution to the printer or simply send the ps code to a smart printer for offboard printing however postscript was written with printing in mind and had numerous features that made it unsuitable for direct use in an interactive display system in particular ps was based on the idea of collecting up ps commands until the showpage command was seen at which point all of the commands read up to that point were interpreted and output in an interactive system this was clearly not appropriate nor did ps have any sort of interactivity built in supporting hit detection for mouse interactivity obviously did not apply when it was being used on a printer when steve jobs left apple and started next he pitched adobe on the idea of using ps as the display system for his new workstation computers the result was display postscript or dps dps added basic functionality to improve performance by changing many string lookups into 32 bit integers adding support for direct output with every command and adding functions to allow the gui to inspect the diagram additionally a set of bindings was provided to allow ps code to be called directly from the c programming language next used these bindings in their nextstep system to provide an object oriented graphics system although dps was written in conjunction with next adobe sold it commercially and it was a common feature of most unix workstations in the 1990s sun microsystems took another approach creating news instead of dps s concept of allowing ps to interact with c programs news instead extended ps into a language suitable for running the entire gui of a computer sun added a number of new commands for timers mouse control interrupts and other systems needed for interactivity and added data structures and language elements to allow it to be completely object oriented internally a complete gui three in fact were written in news and provided for a time on their workstations however the ongoing efforts to standardize the x11 system led to its introduction and widespread use on sun systems and news never became widely used postscript is a turing complete programming language belonging to the concatenative group typically postscript programs are not produced by humans but by other programs however it is possible to write computer programs in postscript just like any other programming language postscript is an interpreted stack based language similar to forth but with strong dynamic typing data structures inspired by those found in lisp scoped memory and since language level 2 garbage collection the language syntax uses reverse polish notation which makes the order of operations unambiguous but reading a program requires some practice because one has to keep the layout of the stack in mind most operators take their arguments from the stack and place their results onto the stack literals have the effect of placing a copy of themselves on the stack sophisticated data structures can be built on the array and dictionary types but cannot be declared to the type system which sees them all only as arrays and dictionaries so any further typing discipline to be applied to such user defined types is left to the code that implements them the character  is used to introduce comments in postscript programs as a general convention every postscript program should start with the characters so that all devices will properly interpret it as postscript a hello world program the customary way to show a small example of a complete program in a given language might look like this in postscript or if the output device has a consolepostscript uses the point as its unit of length however unlike other versions of the point postscript uses exactly 72 points to the inch thus for example in order to draw horizontal line of 4cm length it is sufficient to type however for draft graphics the number of significant digits may be reduced this article was originally based on material from the free on line dictionary of computing which is licensed under the gfdl 
david lorge parnas is a canadian early pioneer of software engineering who developed the concept of information hiding in modular programming which is an important element of object oriented programming today he is also noted for his advocacy of precise documentation david earned his ph d at carnegie mellon university in electrical engineering parnas also earned a professional engineering license in canada and was one of the first to apply traditional engineering principles to software design he worked there as a professor for many years he also taught at the university of north carolina at chapel hill the technische hochschule darmstadt the university of victoria and queen s university in 1991 he then went to mcmaster university in hamilton ontario between 2002 and 2008 david parnas worked at the university of limerick in limerick ireland david parnas received several awards and honorsin modular design his double dictum of high cohesion within modules and loose coupling between modules is fundamental to modular design in software however in parnas s seminal 1972 paper on the criteria to be used in decomposing systems into modules this dictum is expressed in terms of information hiding and the terms cohesion and coupling are not used dr parnas took a public stand against the us strategic defense initiative in the mid 1980s arguing that it would be impossible to write an application of sufficient quality that it could be trusted to prevent a nuclear attack he has also been in the forefront of those urging the professionalization of software engineering dr parnas is also a heavy promoter of ethics in the field of software engineering dr parnas has joined the group of scientists which openly criticize the number of publications based approach towards ranking academic production on his november 2007 paper stop the numbers game he elaborates on several reasons on why the current number based academic evaluation system used in many fields by universities all over the world is flawed and instead of generating more advance of the sciences it leads to knowledge stagnation 
the buddhabrot is a special rendering of the mandelbrot set which when traditionally oriented resembles to some extent certain depictions of the buddha when viewed upside down it vaguely resembles a human face with large triangular glasses or goggles over its eyes the buddhabrot rendering technique was discovered and later described in a 1993 usenet post to sci fractals by melinda greenprevious researchers had come very close to finding the precise buddhabrot technique in 1988 linas vepstas relayed images of the buddhabrot to cliff pickover for inclusion in pickover s forthcoming book computers pattern chaos and beauty this led directly to the discovery of pickover stalks these researchers did not filter out non escaping trajectories required to produce the ghostly forms typically reminiscent of hindu art green first named it ganesh since an indian co worker instantly recognized it as the god ganesha which is the one with the head of an elephant the name buddhabrot was coined later by lori gardi mathematically the mandelbrot set consists of the set of points c in the complex number plane for which the iteratively defined sequencewith z0 0 does not tend to infinity however the buddhabrot is rendered by creating a 2 dimensional array of counters each counter corresponding to the final pixel of the image then a random sampling of points c is iterated through the mandelbrot function for points which do escape within a chosen number of iterations and are thus not in the mandelbrot set their values are sent through the mandelbrot function again and this time every counter that is hit by z value as it is iterated is incremented by 1 after a large number of values c have been iterated image colors are then chosen based on the values recorded in the array because rendering buddhabrot involves potentially iterating twice over each sample it is more computationally intensive than standard mandlebrot rendering techniques to add to this rendering highly zoomed areas requires even more computation as the path of an escaping point may enter the portion being rendered from outside without resorting to more complex probabilistic techniques rendering zoomed portions of buddhabrot consists of merely cropping a large full sized render the number of iterations chosen has a large effect on the image higher values give sparser more detailed appearance as a few of the points pass through a large number of pixels before they escape resulting in their paths being more prominent if a lower number of iterations was used these points would not escape in time and would be regarded as not escaping at all it is also possible to create a composite from three images with different numbers of iterations and different colours for example combining a red image with 2 000 iterations a green image with 200 and a blue image with 20 a technique similar to how astronomers produce false color images some have labelled this the nebulabrot as it results in a very nebula like image another technique which it is natural to consider is to plot the paths for points c which are in the mandelbrot set a sort of anti buddhabrot 
office open xml is a file format for representing spreadsheets charts presentations and word processing documents an office open xml document file contains mainly xml based files compressed within a zip package the office open xml format specification in 2006 became a free and open ecma international standard in november 2008 after incorporating some of the proposed changes from iso iec members during standardization process of office open xml a revised version of the specification was published as a multi part international standard iso iec 29500 2008 information technology office open xml formats and as ecma 376 office open xml file formats 2nd edition iso iec 29500 2008 is a 4 part standard specification that can be freely downloaded microsoft originally developed the specification as a successor to its earlier binary and office 2003 xml file formats the specification was later handed over to ecma international to be developed as the ecma 376 standard under the stewardship of ecma international technical committee tc45 ecma 376 was published in december 2006 and can be freely downloaded from ecma international an amended version of the format received the necessary votes for approval as an iso iec standard as the result of a jtc 1 fast tracking standardization process that concluded in april 2008 starting with microsoft office 2007 the office open xml file formats have become the default file format of microsoft office the currently market leading office suite microsoft office 14 will be the first version to implement the iso iec is 29500 compliant version of office open xml prior to the 2007 edition the core applications of the microsoft office software suite by default stored their data in binary files historically these files were difficult for other applications to interoperate with due to the lack of publicly available information before 2007 microsoft offered these binary format specifications under a royalty free license and since 2007 the formats are directly downloadable from their site under a covenant not to sue as part of its open specification promise due to microsoft keeping their prior file formats secret other office software had great difficulty obtaining full levels of interoperability microsoft came under increasing pressure to adopt an open file format in particular several nations adopted rules that official documents should be in an open format in 2000 microsoft released an initial version of an xml based format for microsoft excel which was incorporated in office xp in 2002 a new file format for microsoft word followed the excel and word formats known as the office 2003 xml formats were later incorporated into the 2003 release of microsoft office in may 2004 governments and the european union recommended to microsoft that they publish and standardize their xml office formats through a standardization organization microsoft announced in november 2005 that it would standardize the new version of their xml based formats through ecma international as ecma office open xml office open xml uses a file package conforming to the open packaging convention this format uses mechanisms from the zip file format and contains the individual files that form the basis of the document in addition to office markup the package can also include embedded files such as images videos or other documents an office open xml file may contain several documents encoded in specialized markup languages corresponding to applications within the microsoft office product line office open xml defines multiple vocabularies using 27 namespaces and 89 schema modules the primary markup languages are shared markup language materials include in addition to the above markup languages custom xml schemas can be used to extend office open xml the xml schema of office open xml emphasizes reducing load time and improving parsing speed in a test with applications current in april 2007 xml based office documents were slower to load than binary formats to enhance performance office open xml uses very short element names for common elements and spreadsheets save dates as index numbers in order to be systematic and generic office open xml typically uses separate child elements for data and metadata rather than using multiple attributes which allows structured properties office open xml does not use mixed content but uses elements to put a series of text runs into paragraphs the result is terse and highly nested in contrast to html for example which is fairly flat designed for humans to write in text editors and is more congenial for humans to read office math markup language is a mathematical markup language which can be embedded in wordprocessingml with intrinsic support for including word processing markup like revision markings footnotes comments images and elaborate formatting and styles the omml format is different from the world wide web consortium mathml recommendation that does not support those office features but is partially compatible through relatively simple xsl transformations the following office mathml example defines the fraction drawingml is the vector graphics markup language used in office open xml documents its major features are the graphics rendering of text elements graphical vector based shape elements graphical tables and charts the drawingml table is the third table model in office open xml and is optimized for graphical effects and its main use is in presentations created with presentationml markup drawingml contains graphics effects that can be used on the different graphical elements that are used in drawingml in drawingml you can also create 3d effects for instance to show the different graphical elements through a flexible camera viewpoint it is possible to create separate drawingml theme parts in an office open xml package these themes can then be applied to graphical elements throughout the office open xml package drawingml is unrelated to the other vector graphics formats such as svg these can be converted to drawingml to include natively in an office open xml document this is a different approach to that of the opendocument format which uses a subset of svg and includes vector graphics as separate files a drawingml graphic s dimensions are specified in english metric units this unit is defined as 1 360 000 of a centimeter and thus there are 914 400 emus per inch and 12 700 emus per point this unit was chosen so that integers can be used to accurately represent most dimensions encountered in a document floating point cannot accurately represent a fraction that is not a sum of powers of two and the error is magnified when the fractions are added together many times resulting in misalignment as an inch is 2 54 centimeters or 127 50 the inch must be divided by 127 so that both 1 inch and 1 centimeter are an integer to accurately represent decimal to 2 digits a divisor of 100 is needed to accurately represent a point a divisor of 72 is needed which also allows divisions of 2 3 4 6 8 9 12 18 24 and 36 to be accurate multiplying these together gives 127 72 100 914 400 units per inch according to rick jelliffe programmer and standards activist emus are a rational solution to a particular set of design criteria office open xml documents are stored in open packaging convention packages which are zip files containing xml and other data files along with a specification of the relationships between them depending on the type of the document the packages have different internal directory structures and names an application will use the relationships files to locate individual sections with each having accompanying metadata in particular mime metadata a basic package contains an xml file called xml at the root along with three directories _rels docprops and a directory specific for the document type the word directory contains the document xml file which is the core content of the document an example relationship file is as such images referenced in the document can be found in the relationship file by looking for all relationships that are of type http schemas microsoft com office 2006 relationships image to change the used image edit the relationship the following code shows an example of inline markup for a hyperlink in this example the uniform resource locator is represented by rid2 the actual url is in the accompanying relationships file located by the corresponding rid2 item linked images templates and other items are referenced in the same way pictures can be embedded or linked using a tag this is the reference to the image file all references are managed via relationships for example a document xml has a relationship to the image there is a _rels directory in the same directory as document xml inside _rels is a file called document xml rels in this file there will be a relationship definition that contains type id and location the id is the referenced id used in the xml document the type will be a reference schema definition for the media type and the location will be an internal location within the zip package or an external location defined with a url office open xml uses the dublin core metadata element set and dcmi metadata terms to store document properties dublin core is a standard for cross domain information resource description and is defined in iso 15836 2003 an example document properties file that uses dublin core metadata is to aid the reader s understanding the office open xml specification contains both normative material and informative material the iso iec standard is structured into four parts each of which are independent standards the standard specifies six levels of document and application conformance strict and transitional for each of wordprocessingml presentationml and spreadsheetml the standard also specifies applications descriptions of base and full the ecma standard is structured in five parts to meet the needs of different audiences ecma international provides specifications that can be freely copied by all interested parties without restrictions under the ecma code of conduct in patent matters participating and approving member organisations are required to make available their patent rights on a reasonable and non discriminatory basis while making patent rights available on a rand basis is considered a common minimum patent condition for a standard international standardization has a clear preference for royalty free patent licensing that is why microsoft a main contributor to the standard provided a covenant not to sue for its patent licensing the covenant received a mixed reception with some identifying problems and others endorsing it microsoft also added the format to their open specification promise in which microsoft irrevocably promises not to assert any microsoft necessary claims against you for making using selling offering for sale importing or distributing any implementation to the extent it conforms to a covered specification subject to certain restrictions office open xml can therefore be used under the covenant not to sue or the open specification promise the open specification promise was included in documents submitted to iso in support of the ecma 376 fast track submission ecma international asserted that the osp enables both open source and commercial software to implement in support of the licensing arrangements microsoft commissioned an analysis from the london legal firm baker mckenzie several standards and oss licensing experts expressed support in 2006 of the osp a 2006 article in cover pages quotes lawrence rosen an attorney and lecturer at stanford law school as saying i m pleased that this osp is compatible with free and open source licenses in 2006 mark webbink a lawyer and member of the board of the software freedom law center and former employee of linux vendor red hat has said red hat believes that the text of the osp gives sufficient flexibility to implement the listed specifications in software licensed under free and open source licenses we commend microsoft s efforts to reach out to representatives from the open source community and solicit their feedback on this text and microsoft s willingness to make modifications in response to our comments standards lawyer andy updegrove said in 2006 the open specification promise was what i consider to be a highly desirable tool for facilitating the implementation of open standards in particular where those standards are of interest to the open source community on march 12 2008 the software freedom law center which provides services to protect and advance free software and open source software has warned of problems with the open specification promise as it relates to office open xml and the gnu general public license in a published analysis of the promise it states thatmicrosoft amended the osp faq to specifically assure gpl license users that the open licensing of its covered formats through the open specification promise applies to users of the gpl license when implementing covered implementations the osp provides the assurance that microsoft will not assert its necessary claims against anyone who make use sell offer for sale import or distribute any covered implementation under any type of development or distribution model including the gpl legal experts and academics have confirmed that the licensing is similar to the licensing terms offered by ibm and to a lesser extent sun and adobe systems on their office formats but warned that ambiguous legal jargon contained in microsoft s open specification promise although understood in the terminology of the specialist patent and intellectual property law community makes it hard for small developers to determine with certainty that microsoft will not be entitled to sue them for using ooxml they criticize microsoft for not explicitly defining which parts of the specification contains intellectual property the osp has never been tested in a court of law regarding intellectual property rights and the osp does not name which court or jurisdiction a dispute will be heard in david vaile executive director of the cyberspace law and policy centre at the university of new south wales acknowledges that the microsoft approach to its open specification promise is similar although not identical to other approaches used by ibm and to a lesser extent adobe and sun and that these represent a substantial advance on the past practice of negotiating long case by case agreements office open xml is an ecma standard ecma 376 was created using as a basis a new version of the microsoft office 2003 xml file format donated by microsoft which was being created for microsoft office 12 the specification entered fast track standardization within iso iec as dis 29500 in a september 2007 vote by iso iec member bodies the draft text was not approved as an international standard a ballot resolution process in march 2008 amended the text on april 2 2008 iso and iec officially stated that the dis 29500 had been approved for acceptance as an iso iec standard pending any appeals in accordance with the jtc 1 directives the project editor created a new version with the final text within a month after the brm to be published as iso iec 29500 after review and corrections this text has been distributed to the members of sc34 on may 21 2008 microsoft announced that it will be an active participant in the future evolution of the office open xml standard in october 2008 iso iec committee jtc1 sc34 created a working group for maintenance of the iso iec 29500 standard within iso iec in november 2008 the new international standard was published as iso iec 29500 2008 information technology office open xml formats in december 2008 ecma international published ecma 376 office open xml file formats 2nd edition an updated version of ecma 376 that is identical to iso iec 29500 2008 the microsoft office applications word excel and powerpoint 2007 and later promote the office open xml format as it is used as their default file format several groups and companies support the office open xml document format for example the odf alliance uk action group has stated that with opendocument an iso standard for office files already exists further they argue that the office open xml file format is heavily based on microsoft s own office applications and is thus not vendor neutral and that it has inconsistencies with existing iso standards such as time and date formats and color codes belgium s federal public service for information and communication technology is evaluating the adoption of the office open xml format federal administrations may use the file format for creating saving and exchanging office documents the us state of massachusetts has been examining its options for implementing xml based document processing in early 2005 eric kriss secretary of administration and finance in massachusetts was the first government official in the united states to publicly connect open formats to a public policy purpose it is an overriding imperative of the american democratic system that we cannot have our public documents locked up in some kind of proprietary format perhaps unreadable in the future or subject to a proprietary system license that restricts access since 2007 massachusetts classifies office open xml as open format and has amended its approved technical standards list the enterprise technical reference model to include office open xml massachusetts now formally endorses office open xml formats for its public records in june 2007 the danish ministry of science technology and innovation recommended that beginning with january 1 2008 public authorities must support at least one of the two word processing document formats office open xml and odf in all new it solutions where appropriate in germany the office open xml standard is currently under observation by the governmental office for standards in public it see saga 4 0 it may be used to exchange complex documents when further processing is required lithuanian standards board has adopted the iso iec 29500 2008 office open xml format standard as lithuanian national standard the decision was made by technical committee 4 information technology on march 5 2009 the proposal to adopt the office open xml format standard was submitted by lithuanian archives department under the government of the republic of lithuania norway s ministry of government administration and reform is evaluating the adoption of the office open xml format the ministry put the document standard under observation in december 2007 in july 2007 the swiss federal council announced adherence saga ch e government standards mandatory for its departments as well as for cantons cities and municipalities the latest version of saga ch includes office open xml file formats the ecma 376 1st edition standard is supported in several office suites from various vendors the newest version of office open xml is formally known as either iso iec 29500 2008 or as ecma 376 2nd edition microsoft has stated that microsoft office 14 will be the first version of microsoft office to support iso iec 29500 though no release date has been announced on july 28 2008 murray sargent a software development engineer in the microsoft office team confirmed that word 2007 will have a service pack release that enables it to read and write iso standard ooxml files microsoft whose products currently only support the ecma 376 standard version of office open xml has committed to using the iso iec 29500 standard in their products and has also committed to participate in the maintenance of this standard in a zdnet article alex brown leader of the iso iec group in charge of deciding maintenance processes for any iso iec 29500 standard stated i am hoping that microsoft office will shortly be brought into line with the 29500 specification and will stay that way on march 13 2008 doug mahugh a senior product manager at microsoft specializing in office client interoperability and the open xml file formats confirmed that version 1 0 of the open xml format sdk will definitely be 100 compliant with the final iso iec 29500 spec including the changes accepted at the brm in a computerworld interview from 2008 doug mahugh said that microsoft would continue to update the sdk to make sure that applications built with it remained compliant with an open xml standard as changes were made in the future 
a hash function is any well defined procedure or mathematical function which converts a large possibly variable sized amount of data into a small datum usually a single integer that may serve as an index into an array the values returned by a hash function are called hash values hash codes hash sums or simply hashes hash functions are mostly used to speed up table lookup or data comparison tasks such as finding items in a database detecting duplicated or similar records in a large file finding similar stretches in dna sequences and so on hash functions are related to checksums check digits fingerprints randomizing functions error correcting codes and cryptographic hash functions although these concepts overlap to some extent each has its own uses and requirements the hashkeeper database maintained by the national drug intelligence center for instance is more aptly described as a catalog of file fingerprints than of hash values hash functions are mostly used in hash tables to quickly locate a data record given its search key specifically the hash function is used to map the search key to the index of a slot in the table where the corresponding record is supposedly stored in general a hashing function may map several different keys to the same hash value therefore each slot of a hash table contains a set of records rather than a single record for this reason each slot of a hash table is often called a bucket and hash values are also called bucket indices thus the hash function only hints at the record s location it only tells where one should start looking for it still in a half full table a good hash function will typically narrow the search down to only one or two entries in the java programming language for example the object parent class provides a standard hashcode method that is required to generate a 32 bit integer hash value of its object this method is used in several hash table based classes such as hashmap and hashset to find duplicated records in a large unsorted file one may use a hash function to map each file record to an index into a table t and collect in each bucket t a list of the numbers of all records with the same hash value i once the table is complete any two duplicate records will end up in the same bucket the duplicates can then be found by scanning every bucket t which contains two or more members fetching those records and comparing them with a table of appropriate size this method is likely to be much faster than any alternative approach hash functions can also be used to locate table records whose key is similar but not identical to a given key or pairs of records in a large file which have similar keys for that purpose one needs a hash function that maps similar keys to hash values that differ by at most m where m is a small integer if one builds a table of t of all record numbers using such a hash function then similar records will end up in the same bucket or in nearby buckets then one need only check the records in each bucket t against those in buckets t where k ranges between m and m this class includes the so called acoustic fingerprint algorithms that are used to locate similar sounding entries in large collection of audio files for this application the hash function must be as insensitive as possible to data capture or transmission errors and to trivial changes such as timing and volume changes compression etc the same techniques can be used to find equal or similar stretches in a large collection of strings such as a document repository or a genomic database in this case the input strings are broken into many small pieces and a hash function is used to detect potentially equal pieces as above the rabin karp algorithm is a relatively fast string searching algorithm that works in o time on average it is based on the use of hashing to compare strings this principle is widely used in computer graphics computational geometry and many other disciplines to solve many proximity problems in the plane or in three dimensional space such as finding closest pairs in a set of points similar shapes in a list of shapes similar images in an image database and so on in these applications the set of all inputs is some sort of metric space and the hashing function can be interpreted as a partition of that space into a grid of cells the table is often an array with two or more indices and the hash function returns an index tuple this special case of hashing is known as geometric hashing or the grid method geometric hashing is also used in telecommunications to encode and compress multi dimensional signals good hash functions in the original sense of the term are usually required to satisfy certain properties listed below note that different requirements apply to the other related concepts the cost of computing a hash function must be small enough to make a hashing based solution more advantageous over other approaches for instance binary search can locate an item in a sorted table of n items with log2 n key comparisons therefore a hash table solution will be more efficient than binary search only if computing the hash function for one key costs less than performing log2 n key comparisons a hash procedure must be deterministic meaning that for a given input value it must always generate the same hash value in other words it must be a function of the hashed data in the mathematical sense of the term this requirement excludes hash functions that depend on external variable parameters such as pseudo random number generators that depend on the time of day it also excludes functions that depend on the memory address of the object being hashed if that address may change during processing a good hash function should map the expected inputs as evenly as possible over its output range that is every hash value in the output range should be generated with roughly the same probability the reason for this last requirement is that the cost of hashing based methods goes up sharply as the number of collisions pairs of inputs that are mapped to the same hash value increases basically if some hash values are more likely to occur than others a larger fraction of the lookup operations will have to search through a larger set of colliding table entries note that this criterion only requires the value to be uniformly distributed not random in any sense a good randomizing function is usually good for hashing but the converse need not be true hash tables often contain only a small subset of the valid inputs for instance a club membership list may contain only a hundred or so member names out of the very large set of all possible names in these cases the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table not just for the global set of all possible entries in other words if a typical set of m records is hashed to n table slots the probability of a bucket receiving many more than m n records should be vanishingly small in particular if m is less than n very few buckets should have more than one or two records in many applications the range of hash values may be different for each run of the program or may change along the same run in those situations one needs a hash function which takes two parameters the input data z and the number n of allowed hash values a common solution is to compute a fixed hash function with a very large range and thentake the result remainder modulo n if n is itself a power of 2 this can be done by bit masking and bit shifting if this approach is used the hash function must be chosen so that the result has fairly uniform distribution between 0 and n 1 for any n in some applications the input data may contain features that are irrelevant for comparison purposes when looking up a personal name for instance it may be desirable to ignore the distinction between upper and lower case letters for such data one must use a hash function that is compatible with the data equivalence criterion being used that is any two inputs that are considered equivalent must yield the same hash value a hash function that is used to search for similar data must be as continuous as possible two inputs that differ by a little should be mapped to equal or nearly equal hash values note that continuity is usually considered a fatal flaw for checksums cryptographic hash functions and other related concepts continuity is desirable for hash functions only in some applications such as hash tables that use linear search the choice of a hashing function depends strongly on the nature of the input data and their probability distribution in the intended application if the data to be hashed is small enough one can use the data itself as the hashed value the cost of computing this trivial hash function is effectively zero the meaning of small enough depends on how much memory is available for the hash table a typical pc might have a gigabyte of available memory meaning that hash values of up to 30 bits could be accommodated however there are many applications that can get by with much less for example when mapping character strings between upper and lower case one can use the binary encoding of each character interpreted as an integer to index a table that gives the alternative form of that character if each character is stored in 8 bits the table has only 28 256 entries in the case of unicode characters the table would have 216 65536 entries the same technique can be used to map two letter country codes like us or za to country names 5 digit zip codes like 13083 to city names etc invalid data values may be left undefined in the table or mapped to some appropriate null value the ideal hashing function should be injective that is it should map each valid input to a different hash value such a function would directly locate the desired entry in a hash table without any additional search an injective hash function whose range is all integers between 0 and n 1 where n is the number of valid inputs is said to be perfect besides providing single step lookup a perfect hash function also results in a compact hash table without any vacant slots unfortunately injective and perfect hash functions exist only in very few special situations and even then they are often too complicated or expensive to be of practical use indeed hash functions are typically required to map a large set of valid potential inputs to a much smaller range of hash values and therefore cannot be injective if the inputs are bounded length strings and each input may independently occur with uniform probability then a hash function need only map roughly the same number of inputs to each hash value for instance suppose that each input is an integer z in the range 0 to n 1 and the output must be an integer h in the range 0 to n 1 where n is much larger than n then the hash function could be h z mod n or h n or many other formulas these simple formulas will not do if the input values are not equally likely or are not independent for instance most patrons of a supermarket will live in the same geographic area so their telephone numbers are likely to begin with the same 3 to 4 digits in that case if n is 10000 or so the division formula n which depends mainly on the leading digits will generate a lot of collisions whereas the remainder formula z mod n which is quite sensitive to the trailing digits may still yield a fairly even distribution of hash values when the data values are long character strings such as personal names web page addresses or mail messages their distribution is usually very uneven with complicated dependencies for example text in any natural language has highly non uniform distributions of characters and character pairs very characteristic of the language for such data it is prudent to use a hash function that depends on all characters of the string and depends on each character in a different way a fairly common scheme for hashing such data is to break the input into a sequence of small units and combine all the units b b  b sequentially as followsthis schema is also used in many text checksum and fingerprint algorithms the state variable s may be a 32 or 64 bit unsigned integer in that case s0 can be 0 and g can be just s mod n the best choice of f is a complex issue and depends on the nature of the data if the units b are single bits then f could be for instancehere highbit denotes the most significant bit of s the  operator denotes unsigned integer multiplication with lost overflow is the bitwise exclusive or operation applied to words and p is a suitable fixed word in many cases one can design a special purpose hash function that yields many fewer collisions than a good general purpose hash function for example suppose that the input data are file names such as file0000 chk file0001 chk file0002 chk etc with mostly sequential numbers for such data a function that extracts the numeric part k of the file name and returns k mod n would be nearly optimal needless to say a function that is exceptionally good for a specific kind of data may have dismal performance on data with different distribution one can obtain good general purpose hash functions for string data by adapting certain checksum or fingerprinting algorithms some of those algorithms will map arbitrary long string data z with any typical real world distribution no matter how non uniform and dependent to a fixed length bit string with a fairly uniform distribution this string can be interpreted as a binary integer k and turned into a hash value by the formula h k mod n this method will produce a fairly even distribution of hash values as long as the hash range size n is small compared to the range of the checksum function bob jenkins lookup3 algorithm uses a 32 bit checksum a 64 bit checksum should provide adequate hashing for tables of any feasible size some cryptographic hash functions such as sha 1 have even stronger uniformity guarantees than checksums or fingerprints and thus can provide very good general purpose hashing functions however the uniformity advantage may be too small to offset their much higher cost the term hash comes by way of analogy with its standard meaning in the physical world to chop and mix indeed typical hash functions like the mod operation chop the input domain into many sub domains that get mixed into the output range donald knuth notes that hans peter luhn of ibm appears to have been the first to use the concept in a memo dated january 1953 and that robert morris used the term in a survey paper in cacm which elevated the term from technical jargon to formal terminology 
the open dynamics engine is a physics engine its two main components are a rigid body dynamics simulation engine and a collision detection engine it is free software licensed both under the bsd license and the lgpl ode was started in 2001 and has already been used in many applications and games such as bloodrayne 2 call of juarez s t a l k e r world of goo x moto and opensimulator an open source second life simulator the open dynamics engine is used for simulating the dynamic interactions between bodies in space it is not tied to any particular graphics package it supports several geometries box sphere capsule trimesh cylinder and heightmap games using ode to simulate physics higher level environments that allow non programmers access to ode
the spiral model is a software development process combining elements of both design and prototyping in stages in an effort to combine advantages of top down and bottom up concepts also known as the spiral lifecycle model it is a systems development method used in information technology this model of development combines the features of the prototyping model and the waterfall model the spiral model is intended for large expensive and complicated projects the spiral model was defined by barry boehm in his 1988 article a spiral model of software development and enhancement this model was not the first model to discuss iterative development but it was the first model to explain why the iteration matters as originally envisioned the iterations were typically 6 months to 2 years long each phase starts with a design goal and ends with the client reviewing the progress thus far analysis and engineering efforts are applied at each phase of the project with an eye toward the end goal of the project the steps in the spiral model can be generalized as follows the spiral model is used most often in large projects for smaller projects the concept of agile software development is becoming a viable alternative the us military has adopted the spiral model for its future combat systems program the spiral model promotes quality assurance through prototyping at each stage in systems development 
histogram equalization is a method in image processing of contrast adjustment using the image s histogram this method usually increases the global contrast of many images especially when the usable data of the image is represented by close contrast values through this adjustment the intensities can be better distributed on the histogram this allows for areas of lower local contrast to gain a higher contrast without affecting the global contrast histogram equalization accomplishes this by effectively spreading out the most frequent intensity values the method is useful in images with backgrounds and foregrounds that are both bright or both dark in particular the method can lead to better views of bone structure in x ray images and to better detail in photographs that are over or under exposed a key advantage of the method is that it is a fairly straightforward technique and an invertible operator so in theory if the histogram equalization function is known then the original histogram can be recovered the calculation is not computationally intensive a disadvantage of the method is that it is indiscriminate it may increase the contrast of background noise while decreasing the usable signal in scientific imaging where spatial correlation is more important than intensity of signal the small signal to noise ratio usually hampers visual detection histogram equalization provides better detectability of fragment size distributions with savings in dna replication toxic fluorescent markers and strong uv source requirements whilst improving chemical and radiation risks in laboratory settings and even allowing the use of otherwise unavailable techniques for reclaiming those dna fragments unaltered by the partial fluorescent marking process histogram equalization often produces unrealistic effects in photographs however it is very useful for scientific images like thermal satellite or x ray images often the same class of images that user would apply false color to also histogram equalization can produce undesirable effects when applied to images with low color depth for example if applied to 8 bit image displayed with 8 bit gray scale palette it will further reduce color depth of the image histogram equalization will work the best when applied to images with much higher color depth than palette size like continuous data or 16 bit gray scale images there are two ways to think about and implement histogram equalization either as image change or as palette change the operation can be expressed as p where i is the original image m is histogram equalization mapping operation and p is a palette if we define new palette as p p and leave image i unchanged than histogram equalization is implemented as palette change on the other hand if palette p remains unchanged and image is modified to i m than the implementation is by image change in most cases palette change is better as it preserves the original data generalizations of this method use multiple histograms to emphasize local contrast rather than overall contrast examples of such methods include adaptive histogram equalization and contrast limiting adaptive histogram equalization or clahe histogram equalization also seems to be used in biological neural networks so as to maximize the output firing rate of the neuron as a function of the input statistics this has been proved in particular in the fly retina histogram equalization is a specific case of the more general class of histogram remapping methods these methods seek to adjust the image to make it easier to analyze or improve visual quality the back projection of a histogrammed image is the re application of the modified histogram to the original image functioning as a look up table for pixel brightness values consider a discrete grayscale image x and let ni be the number of occurrences of gray level i the probability of an occurrence of a pixel of level i in the image isl being the total number of gray levels in the image n being the total number of pixels in the image and px being in fact the image s histogram normalized to let us also define the cumulative distribution function corresponding to px aswhich is also the image s accumulated normalized histogram we would like to create a transformation of the form y t to produce a new image y such that its cdf will be linearized across the value range i e for some constant k the properties of the cdf allow us to perform such a transform it is defined asnotice that the t maps the levels into the range in order to map the values back into their original range the following simple transformation needs to be applied on the result the above describes histogram equalization on a greyscale image however it can also be used on color images by applying the same method separately to the red green and blue components of the rgb color values of the image still it should be noted that applying the same method on the red green and blue components of an rgb image may yield dramatic changes in the image s color balance since the relative distributions of the color channels change as a result of applying the algorithm however if the image is first converted to another color space lab color space or hsl hsv color space in particular then the algorithm can be applied to the luminance or value channel without resulting in changes to the hue and saturation of the image the following is the same 8x8 subimage as used in jpeg the 8 bit greyscale image shown has the following values the histogram for this image is shown in the following table pixel values that have a zero count are excluded for the sake of brevity the cumulative distribution function is shown below again pixel values that do not contribute to an increase in the cdf are excluded for brevity this cdf shows that the minimum value in the subimage is 52 and the maximum value is 154 the cdf of 64 for value 154 coincides with the number of pixels in the image the cdf must be normalized to the general histogram equalization formula is where cdfmin is the minimum value of the cumulative distribution function m n gives the image s number of pixels and l is the number of grey levels used the equalization formula for this particular example is for example the cdf of 78 is 46 the normalized value becomesonce this is done then the values of the equalized image are directly taken from the normalized cdf to yield the equalized values notice that the minimum value is now 0 and the maximum value is now 255 
this article lists unified modeling language tools classified by their proprietary or non proprietary status potential users can freely download versions of most of the following tools such versions usually impose limits in capability and or by a time period structure actor attribute artifact class component interface object packagebehavior activity event message method state usecaserelationships aggregation association composition dependency generalization 
william wright is an american computer game designer and co founder of the game development company maxis now part of electronic arts the first computer game wright designed was raid on bungeling bay in 1984 but it was simcity that brought him to prominence the game was released by maxis a company wright formed with jeff braun and he built upon the game s theme of computer simulation with numerous other titles including simearth and simant wright s greatest success to date came as the original designer for the sims games series which as of 2008 is the best selling pc game in history the game spawned multiple sequels and expansions and wright earned many awards for his work his latest work spore was released in september 2008 and features gameplay based upon the model of evolution the game sold over 1 000 000 copies within three weeks of its release wright was born in atlanta the son of bill wright sr and beverlye wright edwards his father was a graduate of georgia tech s engineering school and was an entrepreneur in the field of plastic packing materials in the early 1960s wright sr founded a successful company which allowed the wrights to live comfortably in atlanta beverlye was an amateur musician and actress wright was educated at a local montessori school where he enjoyed its emphasis on creativity problem solving and self motivation wright admitted to having been inspired to create certain elements of simcity from his experiences in the school montessori taught me the joy of discovery it showed you can become interested in pretty complex theories like pythagorean theory say by playing with blocks it s all about learning on your terms rather than a teacher explaining stuff to you simcity comes right out of montessori if you give people this model for building cities they will abstract from it principles of urban design wright later described himself as obsessive in his pursuits i would usually get very obsessed with some subject or area of interest for six months or a year and just totally learn everything about it as a child wright was an avid builder of models ships cars planes i loved to do that he told the new yorker in october 2006 at 10 he built a scale model in balsa wood of the uss enterprise s flight deck wright later found these early experiences to be formative in his vision of game design well one thing i ve always really enjoyed is making things out of whatever it started with modeling as a kid building models i think when i started doing games i really wanted to carry that to the next step to the player so that you give the player a tool so that they can create things and then you give them some context for that creation wright would discuss with his father the possibility of life on other worlds nasa and the stars his ambition was to be an astronaut and form colonies in space to relieve overpopulation his father was sympathetic to his ambitions he was also a fan of avalon hill s board games which he enjoyed particularly for their propensity to descend into a form of rules lawyering when will jr was nine his father died of leukemia his mother moved the family to her hometown of baton rouge louisiana wright was enrolled in the local episcopal high school he enjoyed it for the chance to debate the faculty during his time at the school he became an atheist overall he found the methods of the school inferior to the montessori and came off with a bad impression of conventional schooling in general in 1994 he declared in a wired interview i ve always been somewhat disillusioned with the educational system some people have said it was originally based on the idea that we re training factory workers so it was very important to teach them to do some repetitive task for eight hours a day what s going to be really exciting is when this nintendo generation gets a little bit older and starts becoming teachers in schools i think that s going to make a bigger difference than any kind of educational reform ever will in the future a lot more learning will happen in the home after graduating at 16 from episcopal high school he enrolled in louisiana state university transferring two years later to louisiana tech beginning with a start at an architecture degree followed by mechanical engineering he fell into computers and robotics he excelled in subjects he was interested in architecture economics mechanical engineering and military history but was held back by his impractical goals his earlier dream of space colonization remained and was joined by a love for robotics after another two years at louisiana tech in the fall of 1980 wright moved on to the new school in manhattan he lived in an apartment over balducci s in greenwich village and spent his spare time searching for spare parts in local electronics surplus stores after one year at the new school wright returned to baton rouge without his degree concluding five years of collegiate study during a summer break from college he met his first wife joell jones an artist currently living in california on vacation to her hometown of baton rouge in an interview published in february 2003 will claims that games were absorbing so much of his time he decided that perhaps making games was the way to go wright s first game was the helicopter action game raid on bungeling bay for the commodore 64 wright found that he had more fun creating levels with his level editor for raid on bungeling bay than he had while actually playing the game he created a new game that would later evolve into simcity but he had trouble finding a publisher the structuralist dynamics of the game were in part inspired by the work of two architectural and urban theorists christopher alexander and jay forrester i m interested in the process and strategies for design the architect christopher alexander in his book a pattern language formalized a lot of spatial relationships into a grammar for design i d really like to work toward a grammar for complex systems and present someone with tools for designing complex things wright in an interview with the times expressed belief that computers extend the imagination and posits the emergence of the metabrain stating any human institutional system that draws on the intelligence of all its members is a metabrain up to now we have had high friction between the neurons of the metabrain technology is lowering that friction tremendously computers are allowing us to aggregate our intelligence in ways that were never possible before if you look at spore people are making this stuff and computers collect it then decide who to send it to the computer is the broker what they are really exploring is the collective creativity of millions of people they are aggregating human intelligence into a system that is more powerful than we thought artificial intelligence was going to be in 1986 he met jeff braun an investor interested in entering the computer game industry at what wright calls the world s most important pizza party together they formed maxis the next year in orinda california simcity was a hit and has been credited as one of the most influential computer games ever made wright himself has been widely featured in several computer magazines particularly pc gamer which has listed wright in its annual game gods feature alongside such notables as roberta williams and peter molyneux following the success of simcity wright designed simearth and simant he co designed simcity 2000 with fred haslam and in the meantime maxis produced other sim games wright s next game was simcopter although none of these games were as successful as simcity they further cemented wright s reputation as a designer of software toys games that cannot be won or lost in 1992 wright and his family moved to orinda california maxis went public in 1995 with revenue of usd 38 million the stock reached 50 a share and then dropped as maxis posted a loss electronic arts bought maxis in june 1997 wright had been thinking about making a virtual doll house ever since the early 90s similar to simcity but focused on individual people originally conceived of as an architectural design game called home tactics wright s idea changed when someone suggested the player should be rated on the quality of life experience by the homeowners it was a difficult idea to sell to ea because already 40 of maxis s employees had been laid off ea published the sims in february 2000 and it became wright s biggest success yet it eventually surpassed myst as the best selling computer game of all time and spawned numerous expansion packs and other games he designed a massively multiplayer version of the game called the sims online which was not as popular as the original in a presentation at the game developers conference on march 11 2005 he announced his latest game spore he used the current work on this game to demonstrate methods that can be used to reduce the amount of content that needs to be created by the game developers wright hopes to inspire others to take risks in game creation wright was given a lifetime achievement award at the game developers choice awards in 2001 in 2002 he became the fifth person to be inducted into the academy of interactive arts and sciences hall of fame until 2006 he was the only person to have been honored this way by both of these industry organizations in 2007 the british academy of film and television arts awarded him a fellowship the first given to a game designer he has been called one of the most important people in gaming technology and entertainment by publications such as entertainment weekly time pc gamer discover and gamespy wright was also awarded the pc magazine lifetime achievement award in january 2005 in 1980 along with co driver and race organizer rick doherty wright participated in the u s express a cross country race that was the successor to the cannonball run wright and doherty drove a specially outfitted mazda rx 7 from brooklyn new york to santa monica california in 33 39 winning the illegal race wright only competed once in the race which continued until 1983 since 2003 in his spare time wright has collected leftovers from the soviet space program including a 100 pound hatch from a space shuttle a seat from a soyuz control panels from the mir and the control console of the soyuz 23 as well as dolls dice and fossils during e3 2004 he passed off an old lapel pin commemorating the soviet space program to a reporter i m uncollecting i buy collections on ebay and i disperse them out to people again i have to be like an entropic force to collectors otherwise all of this stuff will get sorted he once built competitive robots for battlebots with his daughter but no longer does so as of november 2006 wright still had remnant bits of machined metal left over from his battlebots days strewn about the garage of his oakland home wright was a former robot wars champion in the berkeley based robotics workshop the stupid fun club one of wright s bots designed with the help of wright s daughter cassidy kitty puff puff fought against its opponents by sticking a roll of gauze onto its armature and circling around them encapsulating them and denying them movement the technique cocooning was eventually banned following his work in battlebots he has taken steps into the field of human robot interactions we build these robots and we take them down to berkeley and study the interactions that people have with the robots says wright we built this newer one that has a rapid fire pingpong cannon it will fire about 10 per second so we give people this plastic bat and we say it s set up to play baseball do you want to play baseball it s going to shoot a little ball and you try to hit it and all of a sudden it s like da da da da and it s pelting them with balls according to the huffington post search engine will wright has donated to many republican party fundraising committees 
gregory john chaitin is an argentine american mathematician and computer scientist beginning in the late 1960s chaitin made contributions to algorithmic information theory and metamathematics in particular a new incompleteness theorem in reaction to gdel s incompleteness theorem he attended the bronx high school of science and city college of new york where he developed the theories that led to his independent discovery of kolmogorov complexity chaitin has defined chaitin s constant  a real number whose digits are equidistributed and which is sometimes informally described as an expression of the probability that a random program will halt  has the mathematical property that it is definable but not computable chaitin s early work on algorithmic information theory paralleled the earlier work of kolmogorov chaitin also writes about philosophy especially metaphysics and philosophy of mathematics in metaphysics chaitin claims that algorithmic information theory is the key to solving problems in the field of biology and neuroscience indeed in recent writings he defends a position known as digital philosophy in the epistemology of mathematics he claims that his findings in mathematical logic and algorithmic information theory show there are mathematical facts that are true for no reason they re true by accident they are random mathematical facts chaitin proposes that mathematicians must abandon any hope of proving those mathematical facts and adopt a quasi empirical methodology chaitin is also the originator of using graph coloring to do register allocation in compiling a process known as chaitin s algorithm in 1995 he was given the degree of doctor of science honoris causa by the university of maine in 2002 he was given the title of honorary professor by the university of buenos aires in argentina where his parents were born and where chaitin spent part of his youth he is a research staff member at ibm s thomas j watson research center and also a visiting professor at the computer science department of the university of auckland and on the international committee of the valparaso complex systems institute some philosophers and logicians strongly disagree with the philosophical conclusions that chaitin has drawn from his theorems the logician torkel franzn criticizes chaitin s interpretation of gdel s incompleteness theorem and the alleged explanation for it that chaitin s work represents 
streaming api for xml is an application programming interface to read and write xml documents in the java programming language traditionally xml apis are either both have advantages the former allows for random access to the document the latter requires a small memory footprint and is typically much faster these two access metaphors can be thought of as polar opposites a tree based api allows unlimited random access and manipulation while an event based api is a one shot pass through the source document stax was designed as a median between these two opposites in the stax metaphor the programmatic entry point is a cursor that represents a point within the document the application moves the cursor forward pulling the information from the parser as it needs this is different from an event based api such as sax which pushes data to the application requiring the application to maintain state between events as necessary to keep track of location within the document stax has its roots in a number of incompatible pull apis for xml most notably xmlpull the authors of which collaborated with amongst others bea systems oracle sun breeze factor and james clark from jsr 173 specification final v1 0 quote competing and complementary ways to process xml in java 
in computer networking a port is an application specific or process specific software construct serving as a communications endpoint used by transport layer protocols of the internet protocol suite such as transmission control protocol and user datagram protocol a specific port is identified by its number commonly known as the port number the ip address it is associated with and the protocol used for communication transport layer protocols such as tcp udp sctp and dccp specify a source and destination port number in their packet headers a port number is a 16 bit unsigned integer thus ranging from 0 to 65535 a process associates with a particular port to send and receive data meaning that it will listen for incoming packets whose destination port number and ip destination address match that port and or send outgoing packets whose source port number is set to that port processes may bind to multiple ports applications implementing common services will normally listen on specific port numbers which are defined by convention for use with the given protocol see list of tcp and udp port numbers typically these will be low port numbers in many unix like operating systems only processes owned by the superuser can create ports with numbers from 0 to 1023 this is for security purposes to prevent untrusted processes from providing system services conversely the client end of a connection will typically use a high port number allocated for short term use because the port number is contained in the packet header it is readily interpreted not only by the sending and receiving computers but also by other components of the networking infrastructure in particular firewalls are commonly configured to differentiate between packets depending on their source and or destination port numbers port forwarding is an example application of this processes implement connections to transport protocol ports by means of sockets a socket is the software structure used as the transport end point it is created by the process and bound to a socket address which consists of a combination of a port number and an ip address sockets may be set to send receive data in one direction at a time or simultaneously in both directions because different services commonly listen on different port numbers the practice of attempting to connect in sequence to a wide range of services on a single computer is commonly known as port scanning this is usually associated either with malicious cracking attempts or with a search for possible vulnerabilities to help prevent such attacks port connection attempts are frequently monitored and logged by computers the technique of port knocking uses a series of port connections from a client computer to enable a server connection an example for the use of ports is the internet mail system a server used for sending and receiving e mail provides both an smtp service and a pop3 service these are handled by different server processes and the port number is used to determine which data is associated with which process by convention the smtp server listens on port 25 while pop3 listens on port 110 the concept of ports can be readily explained with an analogy think of ip addresses as the street address of an apartment building and the port number as the number of a particular apartment within that building if a letter is sent to the apartment building without an apartment number on it then nobody knows whom it is intended for in order for the delivery to be successful the sender needs to include an apartment number along with the address to ensure the letter gets to the right domicile another way to explain this concept is to imagine the ip address is your house address and the port number indicates whether the package goes to your post box or milk box or newspaper box or delivered to a particular person we could say therefore that once the packet arrives at your ip address the port number decides whether the packet should be sent to the mailing application or other application port numbers can occasionally be seen in the urls of websites or other services by default http uses port 80 and https uses port 443 but a url like http www example com 8000 blah would try to connect to an http server on port 8000 instead of the default port 80 the internet assigned numbers authority is responsible for the global coordination of the dns root ip addressing and other internet protocol resources this includes the registration of commonly used port numbers for well known internet services the port numbers are divided into three ranges the well known ports the registered ports and the dynamic or private ports the well known ports are those from 0 through 1023 the registered ports are those from 1024 through 49151 a list of registered ports can be found on the iana website http www iana org assignments port numbers
progressive disclosure is an interaction design technique often used in human computer interaction to help maintain the focus of a user s attention by reducing clutter confusion and cognitive workload this improves usability by presenting only the minimum data required for the task at hand the principle is used in journalism s inverted pyramid style learning s spiral approach and the game twenty questions progressive disclosure is an interaction design technique that sequences information and actions across several screens in order to reduce feelings of overwhelm for the user by disclosing information progressively you reveal only the essentials and help the user manage the complexity of feature rich sites or applications progressive disclosure follows the typical notion of moving from abstract to specific only it may mean sequencing interactions and not necessarily level of detail in other words progressive disclosure is not just about displaying abstract then specific information but rather about ramping up the user from simple to more complex actions in its most formal definition progressive disclosure means to move complex and less frequently used options out of the main user interface and into secondary screens an example of progressive disclosure is the print dialog where you can initially choose how many copies to print the printer to use and whether you want to print the full document or only certain pages on the secondary screen one can then modify the full set of options in its purest format progressive disclosure is about offering a good teaser a good teaser can include the following progressive disclosure says make more information available within reach but don t overwhelm the user with all the features and possibilities an example for staged disclosure is an online news article that is spread across four screens this use of progressive disclosure serves advertising objectives and not the user s task another example would be a site that explains a product by making the user click through 4 5 pages of overview benefits information before revealing the price of the product the idea here is that if the user reads the product information they will accept the price more easily the problem with that approach is that it does not accommodate free form exploration a typical behavior on the web progressive disclosure is a concept that has been around since at least the early 1980s the technique caught the attention of user interface specialists with jack carroll s lab work at ibm where he found that hiding advanced functionality early on led to an increased success of its use later on the approach dubbed training wheels is one of the few references validating the technique carroll and rosson pointed out that no empirical evidence exists regarding the effectiveness of progressive disclosure and that the training wheels approach only studied a single computer application and a single interface style while independent usability studies and consultancy research has shown that appropriate usage of the technique is valuable more empirical research is clearly required historically progressive disclosure is a concept that came from the software usability experience it is clearly easier to apply to software than it is on the web which is probably why you don t hear all that much about how to do it effectively on the web in software the interaction is between dialogues and fixed state interactions on the web interactions are chaotic randomized and dynamic due to the fact that hypertext is a non linear media in the software world the audience is predictable and targeted making learning styles more predictable on the web it s anybody s guess who might be using the site the website visitor might be a particle physicist a teen or a grandma learning styles comfort levels and expectations differ greatly this is perhaps why you hear a lot of references to progressive disclosure in conversations and interviews but rarely any ideas about how to apply it effectively usability guru jakob nielsen mentions progressive disclosure regularly nielsen has stated good usability includes ideas like progressive disclosure where you show a small number of features to the less experienced user to lower the hurdle of getting started and yet have a larger number of features available for the expert to call up progressive disclosure is the best tool so far show people the basics first and once they understand that allow them to get to the expert features but don t show everything all at once or you will only confuse people and they will waste endless time messing with features that they don t need yet the marketing approach to progressive disclosure is to get excited about the features and force users to partake of the excitement by making them wade through them all there s only one problem with that if you want to get someone excited creating a feeling of overwhelm does not strike me as a good way to get someone excited instead you want to roll a small snowball down a hill and hope it gets bigger and bigger leading to an avalanche this is the goal of progressive disclosure from a marketing standpoint the best way to think about progressive disclosure on the web is only show information that is relevant to the task the user wants to focus on on any given page context sensitive advertising has finally figured this one out with regard to the text ads that google pioneered progressive disclosure is an interaction design technique that emerges out of the insights gained during task analysis observing users in the field allows you to understand their workflow outside of your technology this insight gives you the data you need to prioritize and sequence content functionality the main thing to remember about progressive disclosure is that you will be able to use it correctly if you have conducted task analysis with your user base observing users in their native problem solving environment gives you the insight about how they interact with the information by observing someone s eating habits you ll know whether they typically look at the desert and drinks menu at the start in the middle or at the end of the meal you ll discover whether they like to eat the main course or their salad first and whether they drink before a meal or at the end of a meal 
remote procedure call is an inter process communication technology that allows a computer program to cause a subroutine or procedure to execute in another address space without the programmer explicitly coding the details for this remote interaction that is the programmer would write essentially the same code whether the subroutine is local to the executing program or remote when the software in question is written using object oriented principles rpc may be referred to as remote invocation or remote method invocation note that there are many different technologies commonly used to accomplish this which are often incompatible the idea of rpc goes back at least as far as 1976 when it was described in rfc 707 one of the first business uses of rpc was by xerox under the name courier in 1981 the first popular implementation of rpc on unix was sun s rpc used as the basis for sun s nfs onc rpc is still widely used today on several platforms another early unix implementation was apollo computer s network computing system ncs later was used as the foundation of dce rpc in the osf s distributed computing environment a decade later microsoft adopted dce rpc as the basis of their microsoft rpc mechanism and implemented dcom atop it around the same time xerox parc s ilu and the object management group s corba offered another rpc paradigm based on distributed objects with an inheritance mechanism rpc is an obvious and popular paradigm for implementing the client server model of distributed computing an rpc is initiated by the client sending a request message to a known remote server in order to execute a specified procedure using supplied parameters a response is returned to the client where the application continues along with its process there are many variations and subtleties in various implementation resulting in a variety of different rpc protocols while the server is processing the call the client is blocked an important difference between remote procedure calls and local calls is that remote calls can fail because of unpredictable network problems also callers generally must deal with such failures without knowing whether the remote procedure was actually invoked idempotent procedures are easily handled but enough difficulties remain that code which calls remote procedures is often confined to carefully written low level subsystems in order to allow servers to be accessed by differing clients a number of standardized rpc systems have been created most of these use an interface description language to allow various platforms to call the rpc the idl files can then be used to generate code to interface between the client and server the most common tool used for this is rpcgen rpc analogues found elsewhere 
this article was originally based on material from the free on line dictionary of computing which is licensed under the gfdl 
the buddy memory allocation technique is a memory allocation technique that divides memory into partitions to try to satisfy a memory request as suitably as possible this system makes use of splitting memory into halves to try to give a best fit according to donald knuth the buddy system was invented in 1963 by harry markowitz who won the 1990 nobel memorial prize in economics and was independently developed by knowlton compared to the memory allocation techniques that modern operating systems use the buddy memory allocation is relatively easy to implement and does not have the hardware requirement of an mmu thus it can be implemented for example on intel 80286 and below computers in comparison to other simpler techniques such as dynamic allocation the buddy memory system has little external fragmentation and has little overhead trying to do compaction of memory however because of the way the buddy memory allocation technique works there may be a moderate amount of internal fragmentation memory wasted because the memory requested is a little larger than a small block but a lot smaller than a large block internal fragmentation is where more memory than necessary is allocated to satisfy a request thereby wasting memory external fragmentation is where enough memory is free to satisfy a request but it is split into two or more chunks none of which is big enough to satisfy the request the buddy memory allocation technique allocates memory in powers of 2 i e 2x where x is an integer thus the programmer has to decide on or to write code to obtain the upper limit of x for instance if the system had 2000k of physical memory the upper limit on x would be 10 since 210 is the biggest allocatable block this results in making it impossible to allocate everything in as a single chunk the remaining 976k of memory would have to be taken in smaller blocks after deciding on the upper limit the programmer has to decide on the lower limit i e the smallest memory block that can be allocated this lower limit is necessary so that the overhead of storing used and free memory locations is minimized if this lower limit did not exist and many programs request small blocks of memory like 1k or 2k the system would waste a lot of space trying to remember which blocks are allocated and unallocated typically this number would be a moderate number small enough to minimize wasted space but large enough to avoid excessive overhead let s call this lower limit l now that we have our limits let us see what happens when a program makes requests for memory let s say in this system l 6 which results in blocks 26 64k in size and u 10 which results in a largest possible allocatable block 210 1024k in size the following shows a possible state of the system after various memory requests this allocation could have occurred in the following manneras you can see what happens when a memory request is made is as follows this method of freeing memory is rather efficient as compaction is done relatively quickly with the maximal number of compactions required equal to log2 log2 typically the buddy memory allocation system is implemented with the use of a binary tree to represent used or unused split memory blocks however there still exists the problem of internal fragmentation in many situations it is essential to minimize the amount of internal fragmentation this problem can be solved by slab allocation one possible version of the buddy allocation algorithm was described in detail by donald knuth in the art of computer programming this is a complicated process 
a kludge is a workaround an ad hoc engineering solution a clumsy or inelegant solution to a problem typically using parts that are cobbled together kludges are particularly widespread in computer programs where processing speed is such that they may not make a big difference in performance the etymology of kludge is itself a kludge since there are many overlapping reports as to the word s origin spelling and pronunciation although the term may have been in use as early as the 1940s in the united kingdom the first printed usage given in the oxford english dictionary is from an article by jackson granholm titled how to design a kludge which appeared in the american computer magazine datamation in february 1962 mr granholm goes on to say the word kludge is derived from the same root as the german klug  originally meaning smart or witty  kludge eventually came to mean not so smart or pretty ridiculous it seems probable however that negative associations were intended from the beginning since the words fudge and botch sound similar and have similar connotations granholm himself alluded to this less noble dimension in eulogistic terms the building of a kludge  is not work for amateurs there is a certain indefinable masochistic finesse that must go into true kludge building the jargon file dictionary of computer slang derives the word from scottish gaelic via british military slang kludge or kludgie meaning a common toilet another etymology which has been suggested derives from klumsy lame ugly dumb but good enough this appears to be a folk etymology or backronym most dictionaries have the spelling kludge as the headword with kluge listed as an alternate spelling the jargon file however claims that kluge was the original spelling and kludge is in fact a variant the german word klug is pronounced approximately like clook in naval slang of the world war ii era a kludge is any piece of electronics that works well on shore but consistently fails at sea a shaggy dog story told in the us navy in the 1940s tells of murgatroyd the kluge maker who when enlisted into the navy gives kluge maker as his occupation because none of the officers knows what a kluge is murgatroyd ascends through the ranks eventually becoming kluge maker first class when an admiral demands that murgatroyd build him a kluge he constructs a strange object with springs in all directions he then drops it over the side of the ship into the ocean where it goes kkluuge the punchline suggests that the intended pronunciation within the story was clug which is closer to the sound of an object hitting the water than clooge or cludge the idea of a jury rig also marine is close many of rube goldberg s or heath robinson s machines were kludgey jury rigs and evoked considerable amusement from newspaper and magazine readers worldwide in aerospace design a kluge was a temporary design using separate commonly available components that were not flight worthy to proof the design and enable concurrent software development while the integrated components were developed and manufactured the term was in common enough use to appear in a fictional movie about the us space program perhaps the ultimate kludge was the first us space station skylab its two major components the saturn workshop and the apollo telescope mount began their development as separate projects later the sws and atm were folded into the apollo applications program but the components were to have been launched separately then docked together in orbit in the final design the sws and atm were launched together but for the single launch concept to work the atm had to pivot 90 degrees on a truss structure from its launch position to its on orbit orientation clearing the way for the crew to dock its apollo command service module at the axial docking port of the multiple docking adapter the airlock module s manufacturer mcdonnell douglas even recycled the hatch design from its gemini spacecraft and kludged what was originally designed for the conical gemini command module onto the cylindrical skylab airlock module the skylab project managed by the national aeronautics and space administration s marshall space flight center was seen by the manned spacecraft center as an invasion of its historical role as the nasa center for manned spaceflight thus msc personnel missed no opportunity to disparage the skylab project calling it the kludge in modern computing terminology a kludge is a solution to a problem doing a task or fixing a system that is inefficient inelegant or even unfathomable but which nevertheless works it has been suggested as a folk etymology or backronym that it means klumsy lame ugly dumb but good enough which rather captures the point to kludge around something is to avoid a bug or some difficult condition by building a kludge perhaps relying on properties of the bug itself to assure proper operation it is somewhat similar in spirit to a workaround only without the grace a kludge is often used to change the behavior of a system after it is finished without having to make fundamental changes sometimes to keep backwards compatibility but often simply because it is easier that something was often originally a crock which is why it must now be hacked to make it work note that a hack might be a kludge but that hack could be at least in computing ironic praise for a quick fix solution to a frustrating problem a kludge is often used to fix an unanticipated problem in an earlier kludge this is essentially a kind of cruft something might be a kludge if it fails in corner cases but this is a less common sense as such situations are not expected to come up in typical usage more commonly a kludge is a poorly working heuristic which was expected to work well an intimate knowledge of the context is typically required to build a corner case kludge as a consequence they are sometimes ironically praised an anecdotal example of a kludge involved a computer part supposedly manufactured in the soviet union during the 1960s the part needed slightly delayed receipt of a signal to work rather than setting up a timing system the kludge was to make the internal wires extra long increasing the distance and thus increasing the time the electrical signal took to reach its destination a variation on this use of kludge is evasion of an unknown problem or bug in a computer program rather than continue to struggle to find out exactly what is causing the bug and how to fix it the programmer may hack the problem by the simple kludge of writing new code which compensates for example if a variable keeps ending up doubled in a certain code area add code which divides by two when it is used after the original code has been executed in computer networking use of nat or pat to cope with the shortage of ipv4 addresses is an example of a kludge in this case an awkward fix for a fundamental design flaw another common example are tsrs like the openoffice org quickstarter real player quickstarter and adobe quickstarter in the science fiction television series andromeda genetically engineered human beings called nietzscheans use the term disparagingly to refer to genetically unmodified humans in scotland kludge refers to an outside toilet 
the following is a list of user interface markup languages categorizeduiml is the earliest pioneer in user interface markup languages it is an open standard where implementation is not restricted to a single vendor however it doesn t attract much attention the primary interface language of mozilla foundation products is xul xul documents are rendered by the gecko engine which also renders xhtml and svg documents it cooperates with many existing standards and technologies including css javascript dtd and rdf which makes it relatively easy to learn for people with background of web programming and design extensible application language is the markup language of nexaweb s enterprise web 2 0 suite developers can use this language to define applications that will run as a java client or an ajax client scalable vector graphics is a markup language for graphics proposed by the w3c that can support rich graphics for web and mobile applications while svg is not a user interface language it includes support for vector raster graphics animation interaction with the dom and css embedded media events and scriptability when these features are used in combination rich user interfaces are possible svg can also be super imposed upon another xml compatible user interface markup language such as xul and xforms as the graphics rich portion of a user interface xaml is a markup system that underlies user interface components of microsoft s net framework 3 0 and above its scope is more ambitious than that of most user interface markup languages since program logic and styles are also embedded in the xaml document functionally it can be seen as a combination of xul svg css and javascript into a single xml schema some people are critical of this design as many standards exist for doing these things however it is expected to be developed with a visual tool where developers do not even need to understand the underlying markups i3ml is a proprietary thin client application delivery mechanism developed by cokinetic systems corp with client support provided by a browser plugin that will render windows like applications over an http infrastructure with minimal bandwidth needs openlaszlo is a runtime environment that comprises a runtime environment and an interface definition language lzx is a declarative user interface language which defines the widgets application layout and scripting elements to create your application lzx is runtime agnostic with the currently supported runtime being within macromedia adobe flash an experimental runtime called laszlo legals that will allow openlaszlo applications run in multiple runtime environments such as dhtml ajax hierarchical model view controller user interface language is an xml markup user interface language which supports the creation and chaining of atomic mvc triad components used in constructing hmvc gui applications the associated runtime provides methods which enable configuration of properties data binding and events of each of the mvc triad elements the runtime accomplishes this by mapping xml elements defined in an hmvcul file to objects inside the framework attributes to properties or to events chaining is accomplished by following the tree structure described inside the hmvcul file wasabixml is an xml markup language that is used to define the graphical interface in wasabi powered applications it is most commonly used with winamp for making winamp skins wasabixml had been developed by nullsoft for winamp but it is also usable with other applications with the wasabi sdk the root element in wasabixml is wasabixml the skininfo element shows the information for a skin the graphical interface is held by the container element and the basic viewable gui element is layout following is an example for a simple gui with a button element wasabixml supports many gui elements including groupdef allows the developer to define a group of gui objects which can be re used anywhere in the skin wasabi also supports xui s which are nothing but groups powered by a maki script allowing developers to create gui components of their own adding to modularity wasabixml has an xml namespace wasabi which defines common gui s without having the need to declare their image paths other markup languages incorporated into existing frameworks are some of these are compiled into binary forms in avionics the arinc 661 standard prescribes a binary format to define user interfaces in glass cockpits 
in computer science a library is a collection of subroutines or classes used to develop software libraries contain code and data that provide services to independent programs this allows code and data to be shared and changed in a modular fashion some executables are both standalone programs and libraries but most libraries are not executables executables and libraries make references known as links to each other through the process known as linking which is typically done by a linker most modern operating systems provide libraries that implement the majority of system services such libraries have commoditized the services a modern application expects an os to provide as such most code used by modern applications is provided in these libraries the earliest programming concepts analogous to libraries were intended to separate data definitions from the program implementation the compool concept was brought to popular attention by jovial in 1959 although it borrowed the idea from the large system sage software following the computer science principles of separation of concerns and information hiding comm pool s purpose was to permit the sharing of system data among many programs by providing a centralized data description cobol also included primitive capabilities for a library system in 1959 but jean sammet described them as inadequate library facilities in retrospect another major contributor to the modern library concept was the subprogram innovation of fortran fortran subprograms can be compiled independently of each other but the compiler lacks a linker so type checking between subprograms is impossible finally the influential simula 67 cannot be overlooked simula was the first object oriented programming language and its classes are nearly identical to the modern concept as used in java c and c the class concept of simula was also a progenitor of the package in ada and the module of modula 2 even when originally developed in 1965 simula classes could be included in library files and added at compile time historically libraries could only be static a static library also known as an archive consists of a set of routines which are copied into a target application by the compiler linker or binder producing object files and a stand alone executable file this process and the stand alone executable file are known as a static build of the target application actual addresses for jumps and other routine calls are stored in a relative or symbolic form which cannot be resolved until all code and libraries are assigned final static addresses the linker resolves all of the unresolved addresses into fixed or relocatable addresses by loading all code and libraries into actual runtime memory locations this linking process can take as much or more time than the compilation process and must be performed when any of the modules is recompiled most compiled languages have a standard library but programmers can also create their own custom libraries commercial compiler publishers provide both standard and custom libraries with their compiler products a linker may work on specific types of object files and thus require specific types of libraries collecting object files into a static library may ease their distribution and use a client either a program or a library subroutine accesses a library object by referencing just its name the linking process resolves references by searching the libraries in the order given usually it is not considered an error if a name can be found multiple times in a given set of libraries in some programming languages a feature called smart linking may be used where the linker is aware of or integrated with the compiler such that the linker understands how external references are used and code in a library that is never actually used even though internally referenced can be deleted from the compiled application for example a program that only uses integers for arithmetic or does no arithmetic operations at all can exclude the floating point library routines this smart linking feature can lead to smaller application file sizes and reduced memory usage dynamic linking means that the subroutines of a library are loaded into an application program at runtime rather than being linked in at compile time and remain as separate files on disk only a minimum amount of work is done at compile time by the linker it only records what library routines the program needs and the index names or numbers of the routines in the library the majority of the work of linking is done at the time the application is loaded or during execution the necessary linking code called a loader is actually part of the underlying operating system at the appropriate time the loader finds the relevant libraries on disk and adds the relevant data from the libraries to the process s memory space some operating systems can only link in a library at loadtime before the process starts executing others may be able to wait until after the process has started to execute and link in the library just when it is actually referenced the latter is often called delay loading or deferred loading in either case such a library is called a dynamically linked library the nature of dynamic linking makes it a common boundary in software licenses plugins are one common usage of dynamically linked libraries which is especially useful when the libraries can be replaced by other libraries with a similar interface but different functionality software may be said to have a plugin architecture if it uses libraries for core functionality with the intention that they can be replaced note however that the use of dynamically linked libraries in an application s architecture does not necessarily mean that they may be replaced dynamic linking was originally developed in the multics operating system starting in 1964 it was also a feature of mts built in the late 1960s in microsoft windows dynamically linked libraries are called dynamic link libraries or dlls one wrinkle that the loader must handle is that the actual location in memory of the library data cannot be known until after the executable and all dynamically linked libraries have been loaded into memory this is because the memory locations used depend on which specific dynamic libraries have been loaded it is not possible to depend on the absolute location of the data in the executable nor even in the library since conflicts between different libraries would result if two of them specified the same or overlapping addresses it would be impossible to use both in the same program however in practice the shared libraries on most systems do not change often therefore it is possible to compute a likely load address for every shared library on the system before it is needed and store that information in the libraries and executables if every shared library that is loaded has undergone this process then each will load at their predetermined addresses which speeds up the process of dynamic linking this optimization is known as prebinding in mac os x and prelinking in linux disadvantages of this technique include the time required to precompute these addresses every time the shared libraries change the inability to use address space layout randomization and the requirement of sufficient virtual address space for use the library itself contains a jump table of all the methods within it known as entry points calls into the library jump through this table looking up the location of the code in memory then calling it this introduces overhead in calling into the library but the delay is usually so small as to be negligible dynamic linkers loaders vary widely in functionality some depend on explicit paths to the libraries being stored in the executable any change to the library naming or layout of the filesystem will cause these systems to fail more commonly only the name of the library is stored in the executable with the operating system supplying a system to find the library on disk based on some algorithm one of the biggest disadvantages of dynamic linking is that the executables depend on the separately stored libraries in order to function properly if the library is deleted moved or renamed or if an incompatible version of the dll is copied to a place that is earlier in the search the executable would fail to load on windows this is commonly known as dll hell most unix like systems have a search path specifying file system directories in which to look for dynamic libraries on some systems the default path is specified in a configuration file in others it is hard coded into the dynamic loader some executable file formats can specify additional directories in which to search for libraries for a particular program this can usually be overridden with an environment variable although it is disabled for setuid and setgid programs so that a user can t force such a program to run arbitrary code developers of libraries are encouraged to place their dynamic libraries in places in the default search path on the downside this can make installation of new libraries problematic and these known locations quickly become home to an increasing number of library files making management more complex microsoft windows will check the registry to determine the proper place to find an activex dll but for other dlls it will check the directory that the program was loaded from the current working directory any directories set by calling the setdlldirectory function the system32 system and windows directories and finally the directories specified by the path environment variable applications written for the net framework framework also check the global assembly cache as the primary store of shared dll files to remove the issue of dll hell openstep used a more flexible system collecting a list of libraries from a number of known locations when the system first starts moving libraries around causes no problems at all although there is a time cost when first starting the system under amigaos generic system libraries are stored in a directory defined by the libs path assignment and application specific libraries can be stored in the same directory as the application s executable amigaos will search these locations when an executable attempts to launch a shared library an application may also supply an explicit path when attempting to launch a library in addition to being loaded statically or dynamically libraries are also often classified according to how they are shared among programs dynamic libraries almost always offer some form of sharing allowing the same library to be used by multiple programs at the same time static libraries by definition cannot be shared the term linker comes from the process of copying procedures or subroutines which may come from relocatable libraries and adjusting or linking the machine address to the final locations of each module the shared library term is slightly ambiguous because it covers at least two different concepts first it is the sharing of code located on disk by unrelated programs the second concept is the sharing of code in memory when programs execute the same physical page of ram mapped into different address spaces it would seem that the latter would be preferable and indeed it has a number of advantages for instance on the openstep system applications were often only a few hundred kilobytes in size and loaded almost instantly the vast majority of their code was located in libraries that had already been loaded for other purposes by the operating system there is a cost however shared code must be specifically written to run in a multitasking environment in some older environments such as 16 bit windows or mpe for the hp 3000 only stack based data was allowed or other significant restrictions were placed on writing a dll ram sharing can be accomplished by using position independent code as in unix which leads to a complex but flexible architecture or by using position dependent code as in windows and os 2 these systems make sure by various tricks like pre mapping the address space and reserving slots for each dll that code has a great probability of being shared windows dlls are not shared libraries in the unix sense the rest of this section concentrates on aspects common to both variants in most modern operating systems shared libraries can be of the same format as the regular executables this allows two main advantages first it requires making only one loader for both of them rather than two secondly it allows the executables also to be used as dlls if they have a symbol table typical executable dll formats are elf and mach o and pe in windows the concept was taken one step further with even system resources such as fonts being bundled in the dll file format the same is true under openstep where the universal bundle format is used for almost all system resources the term dll is mostly used on windows and os 2 products on unix and unix like platforms the term shared library or shared object is more commonly used consequently the most common filename extension for shared library files is so usually followed by another dot and a version number this is technically justified in view of the different semantics more explanations are available in the position independent code article in some cases an operating system can become overloaded with different versions of dlls which impedes its performance and stability such a scenario is known as dll hell most modern operating systems after 2001 have clean up methods to eliminate such situations dynamic loading is a subset of dynamic linking where a dynamically linked library loads and unloads at run time on request such a request may be made implicitly at compile time or explicitly at run time implicit requests are made at compile time when a linker adds library references that include file paths or simply file names explicit requests are made when applications make direct calls to an operating system s api at runtime most operating systems that support dynamically linked libraries also support dynamically loading such libraries via a run time linker api for instance microsoft windows uses the api functions loadlibrary loadlibraryex freelibrary and getprocaddress with microsoft dynamic link libraries posix based systems including most unix and unix like systems use dlopen dlclose and dlsym some development systems automate this process another solution to the library issue is to use completely separate executables and call them using a remote procedure call over a network to another computer this approach maximizes operating system re use the code needed to support the library is the same code being used to provide application support and security for every other program additionally such systems do not require the library to exist on the same machine but can forward the requests over the network the downside to such an approach is that every library call requires a considerable amount of overhead rpc calls are much more expensive than calling a shared library which has already been loaded on the same machine this approach is commonly used in a distributed architecture which makes heavy use of such remote calls notably client server systems and application servers such as enterprise javabeans although dynamic linking was originally developed in the 1960s it did not reach consumer operating systems until the late 1980s it was generally available in some form in most operating systems by the early 1990s it was during this same period that object oriented programming was becoming a significant part of the programming landscape oop with runtime binding requires additional information that traditional libraries don t supply in addition to the names and entry points of the code located within they also require a list of the objects on which they depend this is a side effect of one of oop s main advantages inheritance which means that the complete definition of any method may be defined in a number of places this is more than simply listing that one library requires the services of another in a true oop system the libraries themselves may not be known at compile time and vary from system to system at the same time another common area for development was the idea of multi tier programs in which a display running on a desktop computer would use the services of a mainframe or minicomputer for data storage or processing for instance a program on a gui based computer would send messages to a minicomputer to return small samples of a huge dataset for display remote procedure calls already handled these tasks but there was no standard rpc system it was not long before the majority of the minicomputer and mainframe vendors were working on projects to combine the two producing an oop library format that could be used anywhere such systems were known as object libraries or distributed objects if they supported remote access microsoft s com is an example of such a system for local use dcom a modified version that support remote access for some time object libraries were the next big thing in the programming world there were a number of efforts to create systems that would run across platforms and companies competed to try to get developers locked into their own system examples include ibm s system object model sun microsystems distributed objects everywhere next s portable distributed objects digital s objectbroker microsoft s component object model and any number of corba based systems in the end it turned out that oop libraries were not the next big thing with the exception of microsoft s com and next s pdo all of these efforts have since ended the jar file format is mainly used for object libraries in the java programming language it consists of classes in bytecode format and is loaded by a java virtual machine or special class loaders 
in mathematics the mandelbrot set named after benot mandelbrot is a set of points in the complex plane the boundary of which forms a fractal mathematically the mandelbrot set can be defined as the set of complex values of c for which the orbit of 0 under iteration of the complex quadratic polynomial zn 1 zn2 c remains bounded that is a complex number c is in the mandelbrot set if when starting with z0 0 and applying the iteration repeatedly the absolute value of zn never exceeds a certain number however large n gets for example letting c 1 gives the sequence 0 1 2 5 26 which tends to infinity as this sequence is unbounded 1 is not an element of the mandelbrot set on the other hand c i gives the sequence 0 i  i  i which is bounded and so i belongs to the mandelbrot set when computed and graphed on the complex plane the mandelbrot set is seen to have an elaborate boundary which does not simplify at any given magnification this qualifies the boundary as a fractal the mandelbrot set has become popular outside mathematics both for its aesthetic appeal and for being a complicated structure arising from a simple definition benot mandelbrot and others worked hard to communicate this area of mathematics to the public the mandelbrot set has its place in complex dynamics a field first investigated by the french mathematicians pierre fatou and gaston julia at the beginning of the 20th century the first pictures of it were drawn in 1978 by robert brooks and peter matelski as part of a study of kleinian groups mandelbrot studied the parameter space of quadratic polynomials in an article that appeared in 1980 the mathematical study of the mandelbrot set really began with work by the mathematicians adrien douady and john h hubbard who established many of its fundamental properties and named the set in honour of mandelbrot the mathematicians heinz otto peitgen and peter richter became well known for promoting the set with stunning photographs books and an internationally touring exhibit of the german goethe institut the cover article of the august 1985 scientific american introduced the algorithm for computing the mandelbrot set to a wide audience the cover featured an image created by peitgen et al the work of douady and hubbard coincided with a huge increase in interest in complex dynamics and abstract mathematics and the study of the mandelbrot set has been a centerpiece of this field ever since an exhaustive list of all the mathematicians who have contributed to the understanding of this set since then is beyond the scope of this article but such a list would notably include mikhail lyubich curt mcmullen john milnor mitsuhiro shishikura and jean christophe yoccoz the mandelbrot set m is defined by a family of complex quadratic polynomialsgiven bywhere c is a complex parameter for each c one considers the behavior of the sequence obtained by iterating pc starting at critical point z 0 which either escapes to infinity or stays within a disk of some finite radius the mandelbrot set is defined as the set of all points c such that the above sequence does not escape to infinity more formally if denotes the nth iterate of pc composed with itself n times the mandelbrot set is the subset of the complex plane given bymathematically the mandelbrot set is just a set of complex numbers a given complex number c either belongs to m or it does not a picture of the mandelbrot set can be made by colouring all the points c which belong to m black and all other points white the more colourful pictures usually seen are generated by colouring points not in the set according to how quickly or slowly the sequence diverges to infinity see the section on computer drawings below for more details the mandelbrot set can also be defined as the connectedness locus of the family of polynomials pc that is it is the subset of the complex plane consisting of those parameters c for which the julia set of pc is connected the mandelbrot set is a compact set contained in the closed disk of radius 2 around the origin in fact a point c belongs to the mandelbrot set if and only if for all in other words if the absolute value of ever becomes larger than 2 the sequence will escape to infinity the intersection of m with the real axis is precisely the interval the parameters along this interval can be put in one to one correspondence with those of the real logistic family the correspondence is given byin fact this gives a correspondence between the entire parameter space of the logistic family and that of the mandelbrot set the area of the mandelbrot set is estimated to be 1 506 591 77 0 000 000 08 douady and hubbard have shown that the mandelbrot set is connected in fact they constructed an explicit conformal isomorphism between the complement of the mandelbrot set and the complement of the closed unit disk mandelbrot had originally conjectured that the mandelbrot set is disconnected this conjecture was based on computer pictures generated by programs which are unable to detect the thin filaments connecting different parts of m upon further experiments he revised his conjecture deciding that m should be connected the dynamical formula for the uniformisation of the complement of the mandelbrot set arising from douady and hubbard s proof of the connectedness of m gives rise to external rays of the mandelbrot set these rays can be used to study the mandelbrot set in combinatorial terms and form the backbone of the yoccoz parapuzzle the boundary of the mandelbrot set is exactly the bifurcation locus of the quadratic family that is the set of parameters c for which the dynamics changes abruptly under small changes of c it can be constructed as the limit set of a sequence of plane algebraic curves the mandelbrot curves of the general type known as polynomial lemniscates the mandelbrot curves are defined by setting p0 z pn pn 12 z and then interpreting the set of points pn 1 in the complex plane as a curve in the real cartesian plane of degree 2n 1 in x and y upon looking at a picture of the mandelbrot set one immediately notices the large cardioid shaped region in the center this main cardioid is the region of parameters c for which pc has an attracting fixed point it consists of all parameters of the formfor some  in the open unit disk to the left of the main cardioid attached to it at the point c  3 4 a circular shaped bulb is visible this bulb consists of those parameters c for which pc has an attracting cycle of period 2 this set of parameters is an actual circle namely that of radius 1 4 around 1 there are infinitely many other bulbs tangent to the main cardioid for every rational number with p and q coprime there is such a bulb that is tangent at the parameterthis bulb is called the bulb of the mandelbrot set it consists of parameters which have an attracting cycle of period q and combinatorial rotation number more precisely the q periodic fatou components containing the attracting cycle all touch at a common point if we label these components in counterclockwise orientation then pc maps the component uj to the component the change of behavior occurring at is known as a bifurcation the attracting fixed point collides with a repelling period q cycle as we pass through the bifurcation parameter into the bulb the attracting fixed point turns into a repelling fixed point and the period q cycle becomes attracting all the bulbs we encountered in the previous section were interior components of the mandelbrot set in which the maps pc have an attracting periodic cycle such components are called hyperbolic components it is conjectured that these are the only interior regions of m this problem known as density of hyperbolicity may be the most important open problem in the field of complex dynamics hypothetical non hyperbolic components of the mandelbrot set are often referred to as queer components for real quadratic polynomials this question was answered positively in the 1990s independently by lyubich and by graczyk and witek not every hyperbolic component can be reached by a sequence of direct bifurcations from the main cardioid of the mandelbrot set however such a component can be reached by a sequence of direct bifurcations from the main cardioid of a little mandelbrot copy it is conjectured that the mandelbrot set is locally connected this famous conjecture is known as mlc by the work of adrien douady and john h hubbard this conjecture would result in a simple abstract pinched disk model of the mandelbrot set in particular it would imply the important hyperbolicity conjecture mentioned above the celebrated work of jean christophe yoccozestablished local connectivity of the mandelbrot set at all finitely renormalizable parameters that is roughly speaking those which are contained only in finitely many small mandelbrot copies since then local connectivity has been proved at many other points of m but the full conjecture is still open the mandelbrot set is self similar under magnification in the neighborhoods of the misiurewicz points it is also conjectured to be self similar around generalized feigenbaum points in the sense of converging to a limit set the mandelbrot set in general is not strictly self similar but it is quasi self similar as small slightly different versions of itself can be found at arbitrarily small scales the little copies of the mandelbrot set are all slightly different mostly because of the thin threads connecting them to the main body of the set the hausdorff dimension of the boundary of the mandelbrot set equals 2 as determined by a result of mitsuhiro shishikura it is not known whether the boundary of the mandelbrot set has positive planar lebesgue measure in the blum shub smale model of real computation the mandelbrot set is not computable but its complement is computably enumerable however many simple objects are also not computable in the bss model at present it is unknown whether the mandelbrot set is computable in models of real computation based on computable analysis which correspond more closely to the intuitive notion of plotting the set by a computer hertling has shown that the mandelbrot set is computable in this model if the hyperbolicity conjecture is true as a consequence of the definition of the mandelbrot set there is a close correspondence between the geometry of the mandelbrot set at a given point and the structure of the corresponding julia set this principle is exploited in virtually all deep results on the mandelbrot set for example shishikura proves that for a dense set of parameters in the boundary of the mandelbrot set the julia set has hausdorff dimension two and then transfers this information to the parameter plane similarly yoccoz first proves the local connectivity of julia sets before establishing it for the mandelbrot set at the corresponding parameters adrien douady phrases this principle asplough in the dynamical plane and harvest in parameter space recall that for every rational number where p and q are relatively prime there is a hyperbolic component of period q bifurcating from the main cardioid the part of the mandelbrot set connected to the main cardioid at this bifurcation point is called the p q limb computer experiments suggest that the diameter of the limb tends to zero like the best current estimate known is the famous yoccoz inequality which states that the size tends to zero like a period q limb will have q 1 antennae at the top of its limb we can thus determine the period of a given bulb by counting these antennas the mandelbrot set shows more intricate detail the closer one looks or magnifies the image usually called zooming in the following example of an image sequence zooming to a selected c value gives an impression of the infinite richness of different geometrical structures and explains some of their typical rules the magnification of the last image relative to the first one is about 10 000 000 000 to 1 relating to an ordinary monitor it represents a section of a mandelbrot set with a diameter of 4 million kilometres its border would show an inconceivable number of different fractal structures start mandelbrot set with continuously coloured environment regardless of the extent to which one zooms in on a mandelbrot set there is always additional detail to see during the twelve second zoom in the animation at left the set becomes magnified eleven million fold thus assuming the first frame is life size at 45mm across a carbon atom would comprise 36 pixels in the final frame for general families of holomorphic functions the boundary of the mandelbrot set generalizes to the bifurcation locus which is a natural object to study even when the connectedness locus is not useful multibrot sets are bounded sets found in the complex plane for members of the general monic univariate polynomial family of recursionsof particular interest is the tricorn fractal the connectedness locus of the anti holomorphic familythe tricorn was encountered by milnor in his study of parameter slices of real cubic polynomials it is not locally connected this property is inherited by the connectedness locus of real cubic polynomials another non analytic generalization is the burning ship fractal which is obtained by iterating the mappingmultibrot set with d changing from 0 to 7multibrot sets of degrees 3 and 4the tricorn fractalthe burning ship fractalthe multibrot set is obtained by varying the value of the exponent d the video shows its development from d 0 to 7 at which point there are 6 i e lobes around the perimeter a similar development with negative exponents results in d 1 clefts on the inside of a ring algorithms every algorithm can be implemented in sequential or parallel version mirror symmetry can be used to speed up calculations the simplest algorithm for generating a representation of the mandelbrot set is known as the escape time algorithm a repeating calculation is performed for each x y point in the plot area and based on the behaviour of that calculation a colour is chosen for that pixel the x and y locations of each point are used as starting values in a repeating or iterating calculation the result of each iteration is used as the starting values for the next the values are checked during each iteration to see if they have reached a critical escape condition or bailout if that condition is reached the calculation is stopped the pixel is drawn and the next x y point is examined for some starting values escape occurs quickly after only a small number of iterations for starting values very close to but not in the set it may take hundreds or thousands of iterations to escape for values within the mandelbrot set escape will never occur the programmer or user must choose how much iteration or depth they wish to examine the higher the maximum number of iterations the more detail and subtlety emerge in the final image but the longer time it will take to calculate the fractal image escape conditions can be simple or complex because no complex number with a real or imaginary part greater than 2 can be part of the set a common bailout is to escape when either coefficient exceeds 2 a more computationally complex method but which detects escapes sooner is to compute the distance from the origin using the pythagorean theorem and if this distance exceeds two the point has reached escape more computationally intensive rendering variations such as buddhabrot detect an escape then use values iterated along the way the colour of each point represents how quickly the values reached the escape point often black is used to show values that fail to escape before the iteration limit and gradually brighter colours are used for points that escape this gives a visual representation of how many cycles were required before reaching the escape condition the definition of the mandelbrot set together with its basic properties suggests a simple algorithm for drawing a picture of the mandelbrot set the region of the complex plane we are considering is subdivided into a certain number of pixels to color any such pixel let c be the midpoint of that pixel we now iterate the critical value c under pc checking at each step whether the orbit point has modulus larger than 2 if this is the case we know that the midpoint does not belong to the mandelbrot set and we color our pixel otherwise we keep iterating for a certain number of steps after which we decide that our parameter is probably in the mandelbrot set or at least very close to it and color the pixel black in pseudocode this algorithm would look as follows where relating the pseudocode to c z and pc and so as can be seen in the pseudocode in the computation of x and y to get colorful images of the set the assignment of a color to each value of the number of executed iterations can be made using one of a variety of functions one practical way to do it without slowing down the calculations is to use the number of executed iterations as an entry to a look up color palette table initialized at startup if the color table has for instance 500 entries then you can use nmod500 where n is the number of iterations to select the color to use you can initialize the color palette matrix in various different ways depending on what special feature of the escape behavior you want to emphasize graphically the escape time algorithm is popular for its simplicity however it creates bands of color which as a type of aliasing can detract from an image s aesthetic value this can be improved using the normalized iteration count algorithm which provides a smooth transition of colors between iterations the algorithm associates a real number  with each value of z using the equationwhere n is the number of iterations for z to escape zn is the value after n iterations b is the bailout radius and p is the power for which z is raised to in the mandelbrot set equation another formula for this isnote that this new formula is simpler than the first but it requies a large bailout radius and doesn t work for generalized mandelbrot sets 
while this algorithm is relatively simple to implement there are a few things that need to be taken into consideration first the two formulae return a continuous stream of numbers however it is up to the programmer to decide on how the return values will be converted into a color some type of method for casting these numbers onto a gradient should be developed second it is recommended that a few extra iterations are done so that z can grow if iterations cease as soon as z escapes there is the possibility that the smoothing algorithm will not work one can compute distance from point c to nearest point on the boundary of mandelbrot set the proof of the connectedness of the mandelbrot set in fact gives a formula for the uniformizing map of the complement of m by the koebe 1 4 theorem one can then estimate the distance between the mid point of our pixel and the mandelbrot set up to a factor of 4 in other words provided that the maximal number of iterations is sufficiently high one obtains a picture of the mandelbrot set with the following properties the distance estimate of a pixel c from the mandelbrot set is given bywherefrom a mathematician s point of view this formula only works in limit where n goes to infinity but very reasonable estimates can be found with just a few additional iterations after the main loop exits once b is found by the koebe 1 4 theorem we know there s no point of the mandelbrot set with distance from c smaller than b 4 it is also possible to estimate the distance of a limitly periodic point to the boundary of the mandelbrot set the estimate is given bywheregiven p and z0 and its derivatives can be evaluated by analogous to the exterior case once b is found we know that all points within the distance of b 4 from c are inside the mandelbrot set there are two practical problems with the interior distance estimate first we need to find z0 precisely and second we need to find p precisely the problem with z0 is that the convergence to z0 by iterating pc requires theoretically an infinite number of operations the problem with period is that sometimes due to rounding errors a period is falsely identified to be an integer multiple of the real period in such case the distance is overestimated i e the reported radius could contain points outside the mandelbrot set one way to improve calculations is to find out beforehand whether the given point lies within the cardioid or in the period 2 bulb to prevent having to do huge numbers of iterations for other points in the set one can do periodicity checking which means check if a point reached in iterating a pixel has been reached before if so the pixel cannot diverge and must be in the set this is most relevant for fixed point calculations where there is a relatively high chance of such periodicity a full floating point implementation would rarely go into such a period periodicity checking is of course a trade off the need to remember points costs memory and data management instructions whereas it saves computational instructions 
in the field of logic in computer science model checking refers to the following problem given a simplified model of a system test automatically whether this model meets a given specification typically the systems one has in mind are hardware or software systems and the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash in order to solve such a problem algorithmically both the model of the system and the specification are formulated in some precise mathematical language to this end it is formulated as a task in logic namely to check whether a given structure satisfies a given logical formula the concept is general and applies to all kinds of logics and suitable structures a simple model checking problem is verifying whether a given formula in the propositional logic is satisfied by a given structure an important class of model checking methods have been developed for checking models of hardware and software designs where the specification is given by a temporal logic formula pioneering work in the model checking of temporal logic formulae was done by e m clarke and e a emerson and by j p queille and j sifakis clarke emerson and sifakis shared the 2007 turing award for their work on model checking model checking is most often applied to hardware designs for software because of undecidability the approach cannot be fully algorithmic typically it may fail to prove or disprove a given property the structure is usually given as a source code description in an industrial hardware description language or a special purpose language such a program corresponds to a finite state machine i e a directed graph consisting of nodes and edges a set of atomic propositions is associated with each node typically stating which memory elements are one the nodes represent states of a system the edges represent possible transitions which may alter the state while the atomic propositions represent the basic properties that hold at a point of execution formally the problem can be stated as follows given a desired property expressed as a temporal logic formula p and a structure m with initial state s decide if if m is finite as it is in hardware model checking reduces to a graph search model checking tools face a combinatorial blow up of the state space commonly known as the state explosion problem that must be addressed to solve most real world problems there are several approaches to combat this problem model checking tools were initially developed to reason about the logical correctness of discrete state systems but have since been extended to deal with real time and limited forms of hybrid systems this article was originally based on material from the free on line dictionary of computing which is licensed under the gfdl 
in computing protected mode also called protected virtual address mode is an operational mode of x86 compatible central processing units it was first added to the x86 architecture in 1982 with the release of intel s 80286 processor and later extended with the release of the 80386 in 1985 protected mode allows system software to utilize features such as virtual memory paging safe multi tasking and other features designed to increase an operating system s control over application software when a processor that supports x86 protected mode is powered on it begins executing instructions in real mode in order to maintain backwards compatibility with earlier x86 processors protected mode may only be entered after the system software sets up several descriptor tables and enables the protection enable bit in the control register 0 due to the enhancements added by protected mode it has become widely adopted and has become the foundation for all subsequent enhancements to the x86 architecture the 8086 the predecessor to the 286 was originally designed with a 20 bit memory address bus this allowed the processor to access 220 bytes of memory equivalent to 1 megabyte at the time 1 megabyte was considered a relatively large amount of memory so the designers of the ibm personal computer reserved the first 640 kilobytes for application and the operating system usage and the remaining 384 kilobytes were reserved for the basic input output system and memory for add on devices as time progressed the cost of memory continuously decreased and utilization increased the 1 mb limitation eventually became a significant problem intel intended to solve this limitation along with others with the release of the 286 the initial protected mode released with the 286 was not widely used several shortcomings such as the inability to access the bios and the inability to switch back to real mode without resetting the processor prevented widespread usage this was hampered by the fact that the 286 only allowed memory access in 16 bit segments meaning only 216 bytes equivalent to 64 kilobytes could be accessed at a time the 286 maintained backwards compatibility with the previous 8086 by initially entering real mode on power up real mode functions identically to the 8086 allowing older software to run unmodified on the newer 286 to access the extended functionality of the 286 the operating system would set the processor into protected mode this enabled 24 bit addressing which allowed the processor to access 224 bytes of memory equivalent to 16 megabytes with the release of the 386 in 1985 many of the issues preventing widespread adoption of the previous protected mode were addressed the 386 was released with an address bus size of 32 bits which allows for 232 bytes of memory accessing equivalent to 4 gigabytes the segment sizes were also increased to 32 bits meaning that the full address space of 4 gigabytes could be accessed without the need to switch between multiple segments in addition to the increased size of the address bus and segment registers many other new features were added with the intention of increasing operational security and stability protected mode is now used in virtually all modern operating systems which run on the x86 architecture such as microsoft windows linux and many others with the release of the 386 the following additional features were added to protected mode the 32 bit physical address space is not present on the 80386sx and other 386 processor variants which use the older 286 bus until the release of the 386 protected mode did not offer a direct method to switch back into real mode once protected mode was entered ibm created a workaround which involved resetting the cpu via the keyboard controller and saving the system registers stack pointer and often the interrupt mask in the real time clock chip s ram this allowed the bios to restore the cpu to a similar state and begin executing code before the reset later a triple fault was used to reset the 286 cpu which was a lot faster and cleaner than the keyboard controller method to enter protected mode the global descriptor table must first be created with a minimum of three entries a null descriptor a code segment descriptor and data segment descriptor the 21st address line also must be enabled to allow the use of all the address lines so that the cpu can access beyond 1 megabyte of memory after performing those two steps the pe bit must be set in the cr0 register and a far jump must be made to clear the prefetch input queue with the release of the 386 protected mode could be exited by loading the segment registers with real mode values disabling the a20 line and clearing the pe bit in the cr0 register without the need to perform the initial setup steps required with the 286 protected mode has a number of features designed to enhance an operating system s control over application software in order to increase security and system stability these additions allow the operating system to function in a way that would be significantly more difficult or even impossible without proper hardware support in protected mode there are four privilege levels or rings numbered from 0 to 3 with ring 0 being the most privileged and 3 being the least the use of rings allows for system software to restrict tasks from accessing data call gates or executing privileged instructions in most environments the operating system and some device drivers run in ring 0 and applications run in ring 3 according to the intel 80286 programmer s reference manual for the most part the binary compatibility with real mode code the ability to access up to 16 mb of physical memory and 1 gb of virtual memory were the most apparent changes to application programmers this was not without its limitations if an application utilized or relied on any of the techniques below it wouldn t run in reality almost all dos application programs violated these rules due to these limitations virtual 8086 mode was created and released with the 386 despite such potential setbacks windows 3 x and its successors can take advantage of the binary compatibility with real mode to run many windows 2 x applications which run in real mode in windows 2 x in protected mode with the release of the 386 protected mode offers what the intel manuals call virtual 8086 mode virtual 8086 mode is designed to allow code previously written for the 8086 to run unmodified and concurrently with other tasks without compromising security or system stability virtual 8086 mode although is not completely backwards compatible with all programs programs that require segment manipulation privileged instructions direct hardware access or use self modifying code will generate an exception and not be executable in addition applications running in virtual 8086 mode generate a trap with the use of instructions that involve input output which can negatively impact performance due to these limitations many programs originally designed to run on the 8086 can not be run in virtual 8086 mode as a result system software is forced to either compromise system security or backwards compatibility when dealing with legacy software an example of such a compromise can be seen with the release of windows nt which dropped backwards compatibility for many dos applications in real mode each logical address points directly into physical memory location every logical address consists of two 16 bit parts the segment part of the logical address contains the base address of a segment with a granularity of 16 bits i e a segments may start at physical address 0 16 32  220 16 the offset part of the logical address contains an offset inside the segment i e the physical address can be calculated as
physical_address segment_part 16 offset 
respectively mod 216 every segment has a size of 216 bytes in protected mode the segment_part is replaced by a 16 bit selector the 13 upper bits of the selector contains the index of an entry inside a descriptor table the descriptor table entry containsthe segment address inside the descriptor table entry has a length of 24 bits so every byte of the physical memory can be defined as bound of the segment the limit value inside the descriptor table entry has a length of 16 bits so segment length can be between 1 byte and 216 byte the calculated linear address equals the physical memory address the segment address inside the descriptor table entry is expanded to 32 bits so every byte of the physical memory can be defined as bound of the segment the limit value inside the descriptor table entry is expanded to 20 bitsand completed with a granularity flag if paging is off the calculated linear address equals the physical memory address if paging is on the calculated linear address is used as input of paging the 386 processor also uses 32 bit values for the address offset 
for maintaining compatibility with 286 protected mode a new default flag was added if d bit of a code segment is off all commands inside this segment will be interpreted as 16 bit commands columns b byte offset inside entry
column bits first range bit offset inside entry
column bits second range bit offset inside bytein addition to adding virtual 8086 mode the 386 also added paging to protected mode through paging system software can restrict and control a task s access to pages which are sections of memory in many operating systems paging is used to create an independent virtual address space for each task this prevents one task from manipulating the memory of another paging also allows for pages to be moved out of primary storage and onto a slower and larger secondary storage such as a hard disk this allows for more memory to be used than physically available in primary storage the x86 architecture allows control of pages through two arrays page directories and page tables originally a page directory was the size of one page 4 kilobytes and contained 1 024 page directory entries although subsequent enhancements to the x86 architecture have added the ability to use larger page sizes each pde contained a pointer to a page table a page table was also originally 4 kilobytes in size and contained 1 024 page table entries each pte contained a pointer to the actual page s physical address and are only used when 4 kilobyte pages are used at any given time only one page directory may be in active use through the use of the rings privileged call gates and the task state segment introduced with the 286 preemptive multitasking was made possible on the x86 architecture the tss allows general purpose registers segment selector fields and stacks to all be modified without affecting those of another task the tss also allows a task s privilege level and i o port permissions to be independent of another task s in many operating systems the full features of the tss are not used this is commonly due to portability concerns or due to the performance issues created with hardware task switches as a result many operating systems use both hardware and software to create a multitasking system operating systems like os 2 1 x try to switch the processor between protected and real modes this is both slow and unsafe because a real mode program can easily crash a computer os 2 1 x defines restrictive programming rules allowing a family api or bound program to run in either real or protected mode some early unix operating systems os 2 1 x and windows used this mode windows 3 0 was able to run real mode programs in 16 bit protected mode windows 3 0 when switching to protected mode decided to preserve the single privilege level model that was used in real mode which is why windows applications and dlls can hook interrupts and do direct hardware access that lasted through the windows 9x series if a windows 1 x or 2 x program is written properly and avoids segment arithmetic it will run the same way in both real and protected modes windows programs generally avoid segment arithmetic because windows implements a software virtual memory scheme moving program code and data in memory when programs are not running so manipulating absolute addresses is dangerous programs should only keep handles to memory blocks when not running starting an old program while windows 3 0 is running in protected mode triggers a warning dialog suggesting to either run windows in real mode or to obtain an updated version of the application updating well behaved programs using the mark utility with the memory parameter avoids this dialog it is not possible to have some gui programs running in 16 bit protected mode and other gui programs running in real mode in windows 3 1 real mode disappeared today 16 bit protected mode is still used for running legacy applications eg dpmi compatible dos extender programs or windows 3 x applications and certain classes of device drivers in os 2 2 0 and later all under control of a 32 bit kernel 
the mozilla application framework is a collection of cross platform software components that make up the mozilla applications it was originally known as xpfe an abbreviation of cross platform front end it was also known as xptoolkit to avoid confusion it is now best referred to as the mozilla application framework while similar to generic cross platform application frameworks like gtk qt and wxwidgets the intent is to provide a subset of cross platform functionality suitable for building network applications like web browsers leveraging the cross platform functionality already built into the gecko layout engine the following are the various components of the framework 
this list complements the software engineering article giving more details and examples for an alphabetical listing of topics please see list of software engineering topics skilled software engineers use technologies and practices from a variety of fields to improve their productivity in creating software and to improve the quality of the delivered product many technologies and practices are confined to software engineering though many of these are shared with computer science a platform combines computer hardware and an operating system as platforms grow more powerful and less costly applications and tools grow more widely available skilled software engineers know a lot of computer science including what is possible and impossible and what is easy and hard for software discrete mathematics is a key foundation of software engineering otherdeliverables must be developed for many se projects software engineers rarely make all of these deliverables themselves they usually cooperate with the writers trainers installers marketers technical support people and others who make many of these deliverables many people made important contributions to se technologies practices or applications see alsosee also software engineers affect society by creating applications these applications produce value for users and sometimes produce disasters software engineers build software that people use applications influence software engineering by pressuring developers to solve problems in new ways for example consumer software emphasizes low cost medical software emphasizes high quality and internet commerce software emphasizes rapid development software has played a role in many high profile disasters 
ascii85 is a form of binary to text encoding developed by paul e rutter for the btoa utility by using five ascii characters to represent four bytes of binary data it is more efficient than uuencode or base64 which use four characters to represent three bytes of data its main modern use is in adobe s postscript and portable document format file formats the basic need for a binary to text encoding comes from a need to communicate arbitrary binary data over preexisting communications protocols that were designed to carry only human readable text they may support only 7 bit ascii and within that reserve certain of the ascii control codes for their own use may require line breaks at certain maximum intervals and may do careless things with whitespace thus only the 94 printable ascii characters are safe to use to convey data 4 bytes can represent 232 4 294 967 296 possible values 5 radix 85 digits provide 855 4 437 053 125 possible values enough to provide for a unique representation for each possible 32 bit value 32 bits is a popular computer word size and 85 fits nicely into the 94 available ascii characters when encoding each group of 4 bytes is taken as a 32 bit binary number most significant byte first this is converted by repeatedly dividing by 85 and taking the remainder into 5 radix 85 digits then the digits are encoded as printable characters by adding 33 to them giving the ascii characters 33 to 117 because all zero data is quite common a special exception is made for the sake of data compression and an all zero group is encoded as a single character z instead of  groups of characters that decode to a value greater than 232 1 will cause a decoding error as will z characters in the middle of a group whitespace between the characters is ignored and may occur anywhere to accommodate line length limitations the original btoa program always encoded full groups with a prefix line of xbtoa begin and suffix line of xbtoa end followed by the original file length and three 32 bit checksums the decoder needs to use the file length to see how much of the group was padding this program also introduced the special z short form for an all zero group version 4 2 added a y exception for a group of all ascii space characters adobe adopted the basic btoa encoding but with slight changes and gave it the name ascii85 in particular adobe use the delimiters and to mark the beginning and end of an ascii85 encoded string and represent the length by truncating the final group while it is simple to pad the final 1 3 bytes with zero bytes and then output only the first 2 4 characters of the encoded output decoding this requires some care truncating the encoded characters has a rounding down effect to decode the final block it is necessary to round up by padding it with u characters then you can decode the block and truncate the unused trailing bytes in this form a group consisting of only one character is illegal adobe s specification does not support the y exception published on april 1 1996 and thus presumably not meant to be taken completely seriously rfc 1924 a compact representation of ipv6 addresses by robert elz suggests a base 85 encoding of ipv6 addresses this differs from the scheme used above in that he proposes a different set of 85 ascii characters and proposes to do all arithmetic on the 128 bit number converting it to a single 20 digit base 85 number rather than breaking it into four 32 bit groups the proposed character set is in order 0 9 a z a z and then the 23 characters   _  the highest possible representable address 2128 1 74 8519 53 8518 5 8517  would be encoded as r54lj nuuo hi c2ym0 a quote from thomas hobbes s leviathan encoded in ascii85 is as follows 
the common language runtime is a core component of microsoft s net initiative it is microsoft s implementation of the common language infrastructure standard which defines an execution environment for program code the clr runs a form of bytecode called the common intermediate language developers using the clr write code in a language such as c or vb net at compile time a net compiler converts such code into cil code at runtime the clr s just in time compiler converts the cil code into code native to the operating system alternatively the cil code can be compiled to native code in a separate step prior to runtime this speeds up all later runs of the software as the cil to native compilation is no longer necessary although some other implementations of the common language infrastructure run on non windows operating systems microsoft s implementation runs only on microsoft windows operating systems the clr allows programmers to ignore many details of the specific cpu that will execute the program it also provides other important services including the following 
nlphistory
nlp and science
positive negativemeta model
milton model
metaphor
reframing
rep systems
meta programs
submodalitiestherapyrichard bandler
john grinder
robert dilts
judith delozier
stephen gilliganfritz perls
milton h erickson
virginia satir
syntax
gregory bateson
alfred korzybski
noam chomskytopics
bibliography
studiesneuro linguistic programming is defined in the oxford english dictionary as a model of interpersonal communication chiefly concerned with the relationship between successful patterns of behaviour and the subjective experiences underlying them and a system of alternative therapy based on this which seeks to educate people in self awareness and effective communication and to change their patterns of mental and emotional behaviour the co founders richard bandler and linguist john grinder claimed it would be instrumental in finding ways to help people have better fuller and richer lives they coined the title to denote a supposed theoretical connection between neurological processes language and behavioral patterns that have been learned through experience and that can be organised to achieve specific goals in life nlp was originally promoted by its founders bandler and grinder in the 1970s as an extraordinarily effective and rapid form of psychological therapy capable of addressing the full range of problems which psychologists are likely to encounter such as phobias depression habit disorder psychosomatic illnesses learning disorders it also espoused the potential for self determination through overcoming learned limitations and emphasized well being and healthy functioning later it was promoted as a science of excellence derived from the study or modeling of how successful or outstanding people in different fields obtain their results it was claimed that these skills can be learned by anyone to improve their effectiveness both personally and professionallybecause of the absence of any firm empirical evidence supporting its sometimes extravagant claims nlp has enjoyed little or no support from the scientific community it continues to make no impact on mainstream academic psychology and only limited impact on mainstream psychotherapy and counselling however it has some influence among private psychotherapists including hypnotherapists to the extent that they claim to be trained in nlp and use nlp in their work it has also had an enormous influence in management training life coaching and the self help industry nlp originated when richard bandler a student at university of california santa cruz was transcribing taped therapy sessions of the gestalt therapist fritz perls as a project for the psychiatrist robert spitzer bandler believed he recognized particular word and sentence structures which facilitated the acceptance of perls positive suggestions bandler took this idea to one of his university lecturers john grinder a linguist and together they produced what they termed the meta model a model of what they believed to be influential word structures and how they work they also modeled the therapeutic sessions of the family therapist virginia satir they published an account of their work in the structure of magic in 1975 when bandler was 25 the main theme of the book was that it was possible to analyse and codify the therapeutic methods of satir and perls exceptional therapy even when it appears magical has a discernible structure which anyone could learn some of the book was based on previous work by grinder on transformational grammar the chomskyan generative syntax that was current at the time some considered the importation of transformational grammar to psychotherapy to be bandler and grinder s main contribution to the field of psychotherapy bandler and grinder also made use of ideas of gregory bateson who was influenced by alfred korzybski particularly his ideas about human modeling and that the map is not the territory impressed by bandler and grinder s work with fritz perls and virgina satir the british anthropologist gregory bateson agreed to write the preface to bandler and grinder s structure of magic series bateson also introduced them to milton erickson who was selected as the third model for nlp erickson an american psychiatrist and founding member of the american society for clinical hypnosis was well known for his unconventional approach to therapy for his ability to utilize anything about a patient to help him or her change including his or her beliefs favorite words cultural background personal history or even neurotic habits and for treating the unconscious mind as creative solution generating and often positive at that time the californian human potential movement was developing into an industry its founders claimed that in addition to being a therapeutic method it was also a study of communication and by the 1970s grinder and bandler were marketing it as a business tool claiming that if any human being can do anything so can you after 150 students paid 1 000 each for a ten day workshop in santa cruz bandler and grindler gave up academic writing to produce popular books from seminar transcripts such as frogs into princes which sold more than 270 000 copies according to court documents bandler s nlp business made more than 800 000 in 1980 in the early 1980s nlp was hailed as an important advance in psychotherapy and counseling and attracted some interest in counseling research and clinical psychology in the mid 1980s reviews in the journal of counseling psychology and by the national research council committee found little or no empirical basis for the claims about preferred representational systems or assumptions of nlp since then nlp has been regarded with suspicion or outright hostility by the academic psychiatric and medical professions in the 1980s shortly after publishing neuro linguistic programming volume i with robert dilts and judith delozier grinder and bandler fell out amidst acrimony and intellectual property lawsuits the nlp brand was adopted by other training organisations some time afterwards john grinder collaborated with various people to develop a form of nlp called the new code of nlp which claimed to restore a whole mind body systemic approach to nlp richard bandler also published new processes based on submodalities and ericksonian hypnosis in july 1996 after many years of legal controversy bandler filed a lawsuit against john grinder and others claiming retrospective sole ownership of nlp and also the sole right to use the term under trademark at the same time tony clarkson successfully asked the uk high court to revoke bandler s uk registered trademark of nlp in order to clarify legally that nlp was a generic term rather than intellectual property despite the nlp community s being splintered most nlp material acknowledges the early work of co founders bandler and grinder and also the development group that surrounded them in the 1970s in 2001 the lawsuits were settled with bandler and grinder agreeing to be known as co founders of nlp since 1978 a 20 day nlp practitioner certification program had been in existence for training therapists to apply nlp as an adjunct to their professional qualifications as nlp evolved and the applications began to be extended beyond therapy new ways of training were developed and the course structures and design changed course lengths and style vary from institute to institute in the 1990s following attempts to put nlp on a regulated footing in the uk other governments began certifying nlp courses and providers such as in australia for example where neuro linguistic programming is accredited under the australian qualifications framework however nlp continues to be an open field of training with no official best practice with different authors individual trainers and practitioners having developed their own methods concepts and labels often branding them as nlp the training standards and quality differ greatly the multiplicity and general lack of controls has led to difficulty discerning the comparative level of competence skill and attitude in different nlp trainings according to peter schtz the length of training in europe varies from 2 3 days for the hobbyist to 35 40 days over at least nine months to achieve a professional level of competence in europe the european nlp therapy association has been promoting its training in line with european therapy standards in 2001 an off shoot application of nlp neuro linguistic psychotherapy was recognized by united kingdom council for psychotherapy as an experimental constructivist form of psychotherapy today nlp is a lucrative industry and many variants of the practice are found in seminars workshops books and audio programs in the form of exercises and principles intended to influence behavioral and emotional change in self and others there is great variation in the depth and breadth of training and standards of practitioners and some disagreement exists between those in the field about which patterns are or are not nlp at the time it was introduced nlp was heralded as a breakthrough in therapy and advertisements for training workshops videos and books began to appear in trade magazines the workshops provided certification however controlled studies shed such a poor light on the practice and those promoting the intervention made such extreme and changeable claims that researchers began to question the wisdom of researching the area further there are three main criticisms of nlp a research committee working for united states national research council led by daniel druckman came to two conclusions first the committee found little if any evidence to support nlp s assumptions or to indicate that it is effective as a strategy for social influence it assumes that by tracking another s eye movements and language an nlp trainer can shape the person s thoughts feelings and opinions there is no scientific support for these assumptions secondly the committee were impressed with the modeling approach used to develop the technique the technique was developed from careful observations of the way three master psychotherapists conducted their sessions emphasizing imitation of verbal and nonverbal behaviors this then led the committee to take up the topic of expert modeling in the second phase of its work these studies marked a decline in research interest in nlp generally and particularly in matching sensory predicates and its use in counsellor client relationship in counseling psychology beyerstein argued that nlp was based on outmoded scientific theories and that its explanation of the relationship between cognitive style and brain function was no more than crude analogy nlp began with the studies of three master psychotherapists fritz perls virginia satir and milton erickson grinder and bandler reviewed many hours of audio and video of the three therapists and spent months imitating how they worked with clients in order to replicate or model the communication patterns which supposedly made these individuals more successful than their peers the studies were an attempt to identify why particular psychotherapists were so effective with their patients rather than take a purely theoretical approach bandler and grinder sought to observe what the therapists were doing categorize it and model it bandler and grinder aimed to learn and codify the know how or know why that set these experts apart from their peers the expert therapists knew what they were doing but there were tacit aspects of this knowledge in the initial phase of the modeling process bandler and grinder spent months observing in person and via recordings and imitating how their models worked with clients the initial part of the modeling process involved putting aside prior knowledge or expectations while the style and approach of these psychotherapists were apparently different bandler and grinder believed that all experts in human communication have patterns in common that could be learned by others when you watch and listen to virginia satir and milton erickson do therapy they apparently could not be more different people also report that the experiences of being with them are profoundly different however if you examine their behavior and the essential key patterns and sequences of what they do they are similar the same was true of fritz perls when he was operating in what i consider a powerful and effective way he was using the same sequences of patterns that you will find in their work bandler and grinder 1979 they claimed that there were a few common traits expert communicators whether top therapists top executives or top salespeople all seemed to share as a result they claimed that there were only three behaviour patterns underlying successful communication in therapy business and sales the methods of observation and imitation bandler and grinder used to learn and codify the initial models of nlp came to be known as modeling proponents maintain that nlp modeling is not confined to therapy but can be applied to all human learning another aspect of nlp modeling is understanding the patterns of one s own behaviors in order to model the more successful parts of oneself the meta model can be seen as a heuristic that responds to the words and phrases that reveal unconscious limitations and faulty thinking the distortions generalizations and deletions in language bandler and grinder observed similar patterns in the communication of fritz perls and virginia satir the meta model seeks to recover unspoken information and to challenge generalization the other distorted messages that involve restrictive thinking and beliefs the intent is to help someone develop new choice in thinking and behavior by listening to and carefully responding to the distortions in a client s sentences the practitioner seeks to respond to the syntactic form of the sentence rather than the content itself for example if someone said everyone must love me the message is overly general as it does not specify any particular person or group of people examples of meta model responses include which people specifically or all people and questions to define the criteria that would be acceptable for this person to know when he or she is experiencing the state of love the practitioner also understands that words such as must also indicates necessity or lack of choice on the part of the speaker a meta model response might be what would happen if they did didn t practitioners choose when to respond and when not to using softeners and linkage phrases from the milton model to maintain rapport in contrast to the meta model of nlp which seeks to specify information is the milton erickson inspired milton model described by bandler and grinder as artfully vague in it the communicator makes statements that seem specific but allow the listener to fill in their own meaning for what is being said it makes use of pacing and leading ambiguity metaphor embedded suggestion and multiple meaning sentence structures it has been described as a way of using language to induce and maintain trance in order to contact the hidden resources of our personality the milton model has three primary aspects first to assist in building and maintaining rapport with the client second to overload and distract the conscious mind so that unconscious communication can be cultivated third to allow for interpretation in the words offered to the client after spending months closely studying erickson s language and imitating the way that erickson worked with clients bandler and grinder published the milton model in 1976 1977 under the title the patterns of milton h erickson volumes i ii in the preface erickson said although this book is far from being a complete description of my methodologies as they so clearly state it is a much better explanation of how i work than i myself can give i know what i do but to explain how i do it is much too difficult for me erickson was known for his use of unconventional approaches including the use of stories and for deeply entering the world of his clients the milton model is a way of communicating based on the hypnotic language patterns of milton erickson the basic assumption of nlp is that internal mental processes such as problem solving memory and language consist of visual auditory kinesthetic representations that are engaged when people think about problems tasks or activities or engage in them internal sensory representations are constantly being formed and activated whether making conversation talking about a problem reading a book kicking a ball or riding a horse internal representations have an impact on performance nlp techniques generally aim to change behavior through modifying the internal representations examining the way a person represents a problem and building desirable representations of alternative outcomes or goals in addition bandler and grinder claimed that the representational system use could be tracked using eye movements gestures breathing sensory predicates and other cues in order to improve rapport and social influence some of these ideas of sensory representations and associated therapeutic ideas appear to have been imported from gestalt therapy shortly after its creation in the 1970s bandler and grinder claimed that matching and responding to the representational systems people use to think is generally beneficial for enhancing rapport and influence in communication they proposed several models for this purpose including eye accessing cues and sensory predicates the direction of eye accesses was considered an indicator of the type of internal mental process the sensory predicates breathing posture and gestures were also considered important in the sensory predicate model if someone said these verbal cues are often coupled with posture changes eye movements skin color or breathing shifts essentially it was claimed that the practitioner could ascertain the current sensory mode of thinking from external cues such as the direction of eye movements posture breathing tone of voice and the use of sensory based predicates the majority of research focused on bandler and grinder s claim that a preferred representational system exists and is effective in counseling client influence put simply they claimed that some people prefer visual auditory or kinesthetic processing further a therapist could be more influential by matching the other s preferred system christopher sharpley s review of counselling psychology literature on prs found that it could not be reliably assessed it was not certain that it even existed and it could not be demonstrated to reliably assist counselors buckner found some support for the notion that eye movements can indicate visual and auditory components of thought in that moment while some nlp training programs and books still feature prs many have modified or dropped it richard bandler for example de emphasized its importance in an interview with the enhancing human performance subcommittee john grinder in the new code of nlp emphasizes individual calibration and sensory acuity precluding such a rigidly specified model as the one described above responding directly to sensory experience requires an immediacy which respects the importance of context grinder also stated in an interview that a representational system diagnosis lasts about 30 seconds submodalities are the fine details of sensory representational systems or modalities in the late 1970s the use of visual imagery was common in goal setting sports psychology and meditation not only did bandler and grinder begin to explore imagery in all sensory modalities they also were interested in the qualities properties of internal representations the submodalities bandler and grinder observed that for some people by increasing the brightness changing the color or location of an internal imagery intensity of their state also increased they observed similar patterns in different sensory modalities in other people and changes depending on context this work with submodalities inspired a number of novel interventions within nlp therapeutic and personal development settings for example the swish pattern is proposed to reduce unwanted habits it involves first deciding on a positive alternative the desired alternative may be in the form of a representation of the self resourceful and happy the internal representations that previously triggered unwanted behavior are identified and recoded in the form of something that is uninteresting to the participant typically small and dark the desirable outcome recoded in a form of something that is highly motivating typically bright colourful and large after the initial preparation the participant is asked to bring to mind the representation of the unwanted behavior as this is brought to mind the participant immediately makes it small and dark and brings forth an image of the desired alternative the process is repeated and revised as required to test it the participant then put himself into the context where the old behavior used to be triggered the process is considered successful if the participant remains resourceful when recalling the context where the unwanted behavior used to occur and automatically thinks of the desired alternative nlp proposed a number of simple techniques involving matching pacing and leading for establishing rapport with people there are a number of techniques explored in nlp that are supposed to be beneficial in building and maintaining rapport such as matching and pacing non verbal behavior and matching speech and body rhythms of others anchoring is the process by which a particular state or response is associated with a unique anchor an anchor is most often a gesture voice tone or touch but could be any unique visual auditory kinesthetic olfactory or gustatory stimulus it is claimed that by recalling past resourceful states one can anchor those states to make them available in new situations a psychotherapist might anchor positive states like calmness and relaxation or confidence in the treatment of phobias and anxiety such as in public speaking proponents state that anchors are capable of being formed and reinforced by repeated stimuli and thus are analogous to classical conditioning anchoring appears to have been imported into nlp from family systems therapy as part of the model of virginia satir swish is a novel visualization technique for reducing unwanted habits the process involves disrupting a pattern of thought that usually leads to an unwanted behavior such that it leads to a desired alternative the process involves visualizing the trigger or cue image that normally leads to the unwanted behavior pattern such as a smoker s hand with a cigarette moving towards the face the cue image is then switched a number of times with a visualization of a desired alternative such as a self image looking resourceful and fulfilled the swish is tested by having the person think of the original cue image that used to lead to the undesired behavior or by presenting the actual cue such as a cigarette to the client while observing the responses if the client stays resourceful then the process is complete the name swish comes from the sound made by the practitioner trainer as the visualizations are switched swish also makes use of submodalities for example the internal image of the unwanted behavior is typically shrunk to a small and manageable size and the desired outcome is enhanced by making it brighter and larger than normal the swish was first published by richard bandler in nlp reframing is the process whereby an element of communication is presented so as to transform an individual s perception of the meanings or frames attributed to words phrases and events by changing the way the event is perceived responses and behaviors will also change reframing with language allows you to see the world in a different way and this changes the meaning reframing is the basis of jokes myths legends fairy tales and most creative ways of thinking the concept was common to a number of therapies prior to nlp for example it appeared in the approaches of virginia satir fritz perls and milton erickson and in strategic therapy of paul watzlawick there are examples in children s literature pollyanna for example would play the glad game whenever she felt downhearted to remind herself of the things that she could do and not worry about the things that she could not change an example of reframing is found in the six step reframe which involves distinguishing between an underlying intention and the consequent behaviors for the purpose of achieving the intention by different and more successful behaviors it is based on the notion that there is a positive intention behind all behaviors but that the behaviors themselves may be unwanted or counterproductive in other ways nlp uses this staged process to identify the intention and create alternative choices to satisfy that intention ecology in nlp deals with the relationships between a client and his or her natural social and created environments and how a proposed goal or change might retreat to his or her relationships and environment it is a frame within which the desired outcome is checked against the consequences client s life and mind as systemic processes it treats the client s relationship with self as a system and his or her relationship with others as subsystems that interact so when someone considers a change it is important therefore to take into account the consequences on the system as a whole like gestalt therapy a goal of nlp is to help the client choose goals and make changes that achieve a sense of personal congruency and integrity with personal and other aspects of the client s life parts integration creates a metaphor of different aspects of ourselves which are in conflict due to different goals perceptions and beliefs parts integration is the process of identifying these parts and negotiating with each of these parts separately together with a goal of resolving internal conflict successful parts negotiation occurs by listening to and providing opportunities to meet the needs of each part and adequately addressing each part s interests so that they are each satisfied with the desired outcome it often involves negotiating with the conflicting parts of a person to achieve resolution parts integration appears to be modeled on parts from family therapy and has similarities to ego state therapy in psychoanalysis in that it seeks to resolve conflicts that constitute a family of self within a single individual in contrast to mainstream psychotherapy nlp does not concentrate on diagnosis treatment and assessment of mental and behavioral disorders instead it focuses on helping clients to overcome their own self perceived or subjective problems it seeks to do this while respecting their own capabilities and wisdom to choose additional goals for the intervention as they learn more about their problems and to modify and specify those goals further as a result of extended interaction with a therapist the two main therapeutic uses of nlp are use as an adjunct by therapists practicing in other therapeutic disciplines or as a specific therapy called neurolinguistic psychotherapy which is a recognized by the ukcp while the main goals of neuro linguistic programming are therapeutic the patterns have also been adapted for use outside of psychotherapy including business communication management training sales sports and interpersonal influence for some the techniques such as anchoring reframing therapeutic metaphor and hypnotic suggestion were intended to be used in the therapeutic setting research in counseling psychology found rapport to be no more effective than existing listening skills taught to counselors furthermore druckman found weak empirical support for prs and little theoretical support in counseling psychology and the experimental literature for nlp as a technique for social influence sharpley concluded that most of the other techniques available in nlp were already available in counseling outside of psychotherapy the meta model for example is seen by some as a promising business management communication technique he does not translate unconscious communication into conscious form whatever the patient says in metaphoric form erickson responds in kind by parables by interpersonal action and by directives he works within the metaphor to bring about change he seems to feel that the depth and swiftness of that change can be prevented if the person suffers a translation of the communication haley uncommon therapy 1973 1986 p 28 textbooks from wikibooks
 quotations from wikiquote
 source texts from wikisource
 images and media from commons
 news stories from wikinews
general purpose computing on graphics processing units is the technique of using a gpu which typically handles computation only for computer graphics to perform computation in applications traditionally handled by the cpu it is made possible by the addition of programmable stages and higher precision arithmetic to the rendering pipelines which allows software developers to use stream processing on non graphics data gpu functionality has traditionally been very limited in fact for many years the gpu was only used to accelerate certain parts of the graphics pipeline some improvements were needed before gpgpu became feasible programmable vertex and fragment shaders were added to the graphics pipeline to enable game programmers to generate even more realistic effects vertex shaders allow the programmer to alter per vertex attributes such as position color texture coordinates and normal vector fragment shaders are used to calculate the color of a fragment or per pixel programmable fragment shaders allow the programmer to substitute for example a lighting model other than those provided by default by the graphics card typically simple gouraud shading shaders have enabled graphics programmers to create lens effects displacement mapping and depth of field the programmability of the pipelines have trended according to microsoft s directx specification with directx8 introducing shader model 1 1 directx8 1 pixel shader models 1 2 1 3 and 1 4 and directx9 defining shader model 2 x and 3 0 each shader model increased the programming model flexibilities and capabilities ensuring the conforming hardware follows suit the directx10 specification introduces shader model 4 0 which unifies the programming specification for vertex geometry and fragment processing allowing for a better fit for unified shader hardware thus providing a single computational pool of programmable resource pre directx9 graphics cards only supported paletted or integral color types various formats are available each containing a red element a green element and a blue element sometimes an additional alpha value is added to be used for transparency common formats are for early fixed function or limited programmability graphics this was sufficient because this is also the representation used in displays this representation does have certain limitations however given sufficient graphics processing power even graphics programmers would like to use better formats such as floating point data formats in order to obtain effects such as high dynamic range imaging many gpgpu applications require floating point accuracy which came with graphics cards conforming to the directx9 specification directx9 shader model 2 x suggested the support of two precision types full and partial precision full precision support could either be fp32 and fp24 or greater while partial precision was fp16 ati s r300 series of gpus supported fp24 precision only in the programmable fragment pipeline while nvidia s nv30 series supported both fp16 and fp32 other vendors such as s3 graphics and xgi supported a mixture of formats up to fp24 shader model 3 0 altered the specification increasing full precision requirements to a minimum of fp32 support in the fragment pipeline ati s shader model 3 0 compliant r5xx generation supports just fp32 throughout the pipeline while nvidia s nv4x and g7x series continued to support both fp32 full precision and fp16 partial precisions although not stipulated by shader model 3 0 both ati and nvidia s shader model 3 0 gpus introduced support for blendable fp16 render targets easier facilitating the support for high dynamic range rendering the implementations of floating point on nvidia gpus are mostly ieee compliant however this is not true across all vendors this has implications for correctness which are considered important to some scientific applications while 64 bit floating point values are commonly available on cpus these are not universally supported on gpus some gpu architectures sacrifice ieee compliance while others lack double precision altogether there have been efforts to emulate double precision floating point values on gpus however the speed tradeoff negates any benefit to offloading the computation onto the gpu in the first place most operations on the gpu operate in a vectorized fashion a single operation can be performed on up to four values at once for instance if one color r1 g1 b1 is to be modulated by another color r2 g2 b2 the gpu can produce the resulting color r1 r2 g1 g2 b1 b2 in a single operation this functionality is useful in graphics because almost every basic data type is a vector examples include vertices colors normal vectors and texture coordinates many other applications can put this to good use and because of their higher performance vector instructions have long been available on cpus in november 2006 nvidia launched cuda a sdk and api that allows a programmer to use the c programming language to code algorithms for execution on geforce 8 series gpus amd offers a similar sdk for their ati based gpus and that sdk and technology is called stream sdk designed to compete directly with nvidia s cuda ctm provides a thin hardware interface amd has also announced the amd firestream product line compared for example to traditional floating point accelerators such as the 64 bit csx700 boards from clearspeed that are used in today s supercomputers current top end gpus from nvidia and amd emphasize single precision computation double precision computation executes much more slowly gpus are designed specifically for graphics and thus are very restrictive in terms of operations and programming because of their nature gpus are only effective at tackling problems that can be solved using stream processing and the hardware can only be used in certain ways gpus can only process independent vertices and fragments but can process many of them in parallel this is especially effective when the programmer wants to process many vertices or fragments in the same way in this sense gpus are stream processors processors that can operate in parallel by running a single kernel on many records in a stream at once a stream is simply a set of records that require similar computation streams provide data parallelism kernels are the functions that are applied to each element in the stream in the gpus vertices and fragments are the elements in streams and vertex and fragment shaders are the kernels to be run on them since gpus process elements independently there is no way to have shared or static data for each element we can only read from the input perform operations on it and write to the output it is permissible to have multiple inputs and multiple outputs but never a piece of memory that is both readable and writable arithmetic intensity is defined as the operations performed per word of memory transferred it is important for gpgpu applications to have high arithmetic intensity or memory access latency will limit computation speed ideal gpgpu applications have large data sets high parallelism and minimal dependency between data elements there are a variety of computational resources available on the gpu in fact the programmer can substitute a write only texture for output instead of the framebuffer this is accomplished either through render to texture render to backbuffer copy to texture or the more recent stream out the most common form for a stream to take in gpgpu is a 2d grid because this fits naturally with the rendering model built into gpus many computations naturally map into grids matrix algebra image processing physically based simulation and so on since textures are used as memory texture lookups are then used as memory reads certain operations can be done automatically by the gpu because of this kernels can be thought of as the body of loops for example if the programmer were operating on a grid on the cpu they might have code that looked like this on the gpu the programmer only specifies the body of the loop as the kernel and what data to loop over by invoking geometry processing in sequential code it is possible to control the flow of the program using if then else statements and various forms of loops such flow control structures have only recently been added to gpus conditional writes could be accomplished using a series of simpler instructions but looping and conditional branching were not possible recent gpus allow branching but usually with a performance penalty branching should generally be avoided in inner loops whether in cpu or gpu code and various techniques such as static branch resolution pre computation and z cull can be used to achieve branching when hardware support does not exist the map operation simply applies the given function to every element in the stream a simple example is multiplying each value in the stream by a constant the map operation is simple to implement on the gpu the programmer generates a fragment for each pixel on screen and applies a fragment program to each one the result stream of the same size is stored in the output buffer some computations require calculating a smaller stream from a larger stream this is called a reduction of the stream generally a reduction can be accomplished in multiple steps the results from the previous step are used as the input for the current step and the range over which the operation is applied is reduced until only one stream element remains stream filtering is essentially a non uniform reduction filtering involves removing items from the stream based on some criteria the scatter operation is most naturally defined on the vertex processor the vertex processor is able to adjust the position of the vertex which allows the programmer to control where information is deposited on the grid other extensions are also possible such as controlling how large an area the vertex affects the fragment processor cannot perform a direct scatter operation because the location of each fragment on the grid is fixed at the time of the fragment s creation and cannot be altered by the programmer however a logical scatter operation may sometimes be recast or implemented with an additional gather step a scatter implementation would first emit both an output value and an output address an immediately following gather operation uses address comparisons to see whether the output value maps to the current output slot the fragment processor is able to read textures in a random access fashion so it can gather information from any grid cell or multiple grid cells as desired the sort operation transforms an unordered set of elements into an ordered set of elements the most common implementation on gpus is using sorting networks the search operation allows the programmer to find a particular element within the stream or possibly find neighbors of a specified element the gpu is not used to speed up the search for an individual element but instead is used to run multiple searches in parallel a variety of data structures can be represented on the gpu the following are some of the areas where gpus have been used for general purpose computing while gpgpu can achieve a 100 250x speedup vs a single cpu only embarrassingly parallel applications will see this kind of benefit a single gpu processing core is not equivalent to a single processing core found in a desktop cpu 
lucene is a free open source information retrieval library originally created in java by doug cutting it is supported by the apache software foundation and is released under the apache software license lucene has been ported to other programming languages including delphi perl c c python ruby and php while suitable for any application which requires full text indexing and searching capability lucene has been widely recognized for its utility in the implementation of internet search engines and local single site searching at the core of lucene s logical architecture is the idea of a document containing fields of text this flexibility allows lucene s api to be independent of the file format text from pdfs html microsoft word and opendocument documents as well as many others can all be indexed so long as their textual information can be extracted lucene itself is just an indexing and search library and does not contain crawling and html parsing functionality the apache project nutch is based on lucene and provides this functionality the apache project solr is a fully featured search server based on lucene compass is a java search engine framework built on the top of lucene 
in artificial intelligence strips is an automated planner developed by richard fikes and nils nilsson in 1971 the same name was later used to refer to the formal language of the inputs to this planner this language is the base for most of the languages for expressing automated planning problem instances in use today this article only describes the language not the planner a strips instance is composed of mathematically a strips instance is a quadruple in which each component has the following meaning a plan for such a planning instance is a sequence of operators that can be executed from the initial state and that leads to a goal state formally a state is a set of conditions a state is represented by the set of conditions that are true in it transitions between states are modeled by a transition function which is a function mapping states into new states that result from the execution of actions since states are represented by sets of conditions the transition function relative to the strips instance is a functionwhere 2p is the set of all subsets of p and is therefore the set of all possible states the transition function can be defined as follows in the assumption that actions can always be executed but they do not have effects if their preconditions are not met the function can be extended to sequences of actions by the following recursive equations a plan for a strips instance is a sequence of actions such that the state that results from executing the actions in order from the initial state satisfies the goal conditions formally is a plan for if satisfies the following two conditions the above language is actually the propositional version of strips in practice conditions are often about objects for example that the position of a robot can be modeled by a predicate at and at means that the robot is in room1 in this case actions can have free variables which are implicitly existentially quantified in other words an action represents all possible propositional actions that can be obtained by replacing each free variable with a value the initial state is considered fully known in the language described above conditions that are not in i are all assumed false this is often a limiting assumption as there are natural examples of planning problems in which the initial state is not fully known extensions of strips have been developed to deal with partially known initial states other extensions exist a monkey is in a lab the monkey wants some bananas there are three locations in the lab locations a b and c the monkey is at location a there is a box in location c there are some bananas in location b but they are hanging from the ceiling the monkey needs the box to get to the bananas deciding the existence of a plan for a propositional strips instance is pspace complete various restrictions can be enforced on the instances to make the problem np complete 
an educational programming language is a programming language that is designed primarily as a learning instrument and not so much as a tool for writing real world application programs many educational programming languages position themselves inside a learning path that is a sequence of languages each designed to build on the others moving a student from easy to understand and entertaining environments to full professional environments some of the better known are presented below as part of the olpc project small talk has developed a flow of languages each designed to act as an introduction to the other the structure is scratch to etoys to squeak to any smalltalk each provides graphical environments which may be used not just to teach programming concepts to kids but also physics and mathematics simulations story telling exercises etc through the use of constructive learning squeak as well as other smalltalks are a fully featured application development language that has been around and well respected for decades while scratch is a children s learning tool sun s recommended path is greenfoot to bluej to netbeans bluej to netbeans javalisp is the second oldest family of computer languages in use today and as such has a host of dialects and implementations at a wide range of difficulties lisp was originally created as a practical mathematical notation for computer programs based on lambda calculus which makes it particularly well suited for teaching theories of computation as one of the earliest programming languages lisp pioneered many ideas in computer science including tree data structures automatic storage management dynamic typing object oriented programming and the self hosting compiler all of which are useful for learning computer science the name lisp derives from list processing language linked lists are one of lisp languages major data structures and lisp source code is itself made up of lists as a result lisp programs can manipulate source code as a data structure giving rise to the macro systems that allow programmers to create new syntax or even new domain specific programming languages embedded in lisp so lisps are useful for learning language design and creating custom languages a reasonable learning path would be logo followed by any educational variant such as scheme or newlisp followed by a professional variant such as common lisp over a short period many universities in the us and around the world switched their rst course to sicp and scheme the book became a major bestseller for mit press along with sicp the scheme programming language basic is a language invented in 1964 to provide computer access to non science students it became popular on mini computers during the 1960s and became the standard computing language for microcomputers during the late 1970s and early 1980s the goals of basic were focused on the needs of learning to program easily be easy for beginners to use be interactive provide clear and friendly error messages respond quickly do not require an understanding of computer hardware or operating systems what made basic particularly useful for education was the small size of programs useful programs to illustrate a concept could be written in a dozen lines at the same time basic did not require mathematical or computer science sophistication basic continues to this day to be a language which is frequently self taught with excellent tutorials and implementations see list of basic dialects by platform for a complete list basic offers a learning path from learning oriented basics such as microsoft small basic basic 256 and simple to more full featured basics like visual basic net and gambas the following chart helps to summarize the information above for parents and teachers 
in telecommunication a convolutional code is a type of error correcting code in which each m bit information symbol to be encoded is transformed into an n bit symbol where m n is the code rate and the transformation is a function of the last k information symbols where k is the constraint length of the code convolutional codes are often used to improve the performance of digital radio mobile phones and satellite links these applications typically cannot tolerate delays and can tolerate minor glitches in the data stream to convolutionally encode data start with k memory registers each holding 1 input bit unless otherwise specified all memory registers start with a value of 0 the encoder has n modulo 2 adders and n generator polynomials one for each adder an input bit m1 is fed into the leftmost register using the generator polynomials and the existing values in the remaining registers the encoder outputs n bits now bit shift all register values to the right and wait for the next input bit if there are no remaining input bits the encoder continues output until all registers have returned to the zero state the figure below is a rate 1 3 encoder with constraint length of 3 generator polynomials are g1  g2  and g3  therefore output bits are calculated as follows the encoder on the picture above is a non recursive encoder here s an example of a recursive one one can see that the input being encoded is included in the output sequence too such codes are referred to as systematic otherwise the code is called non systematic recursive codes are almost always systematic and conversely non recursive codes are non systematic it isn t a strict requirement but a common practice a convolutional encoder is called so because it performs a convolution of the input stream with encoder s impulse responses where is an input sequence is a sequence from output and is an impulse response for output a convolutional encoder is a discrete linear time invariant system every output of an encoder can be described by its own transfer function which is closely related to a generator polynomial an impulse response is connected with a transfer function through z transform transfer functions for the first encoder are transfer functions for the second encoder are define bywhere for any rational function then is the maximum of the polynomial degrees of the and the constraint length is defined as for instance in the first example the constraint length is 3 and in the second the constraint length is 4 a convolutional encoder is a finite state machine an encoder with n binary cells will have 2n states imagine that the encoder has 1 in the left memory cell and 0 in the right one  we will designate such a state as 10 according to an input bit the encoder at the next turn can convert either to the 01 state or the 11 state one can see that not all transitions are possible all possible transitions can be shown as below an actual encoded sequence can be represented as a path on this graph one valid path is shown in red as an example this diagram gives us an idea about decoding if a received sequence doesn t fit this graph then it was received with errors and we must choose the nearest correct sequence the real decoding algorithms exploit this idea a free distance is a minimal hamming distance between different encoded sequences a correcting capability of a convolutional code is a number of errors that can be corrected by the code it can be calculated assince a convolutional code doesn t use blocks processing instead a continuous bitstream the value of t applies to a quantity of errors located relatively near to each other that is multiple groups of t errors can usually be fixed when they are relatively far free distance can be interpreted as a minimal length of an erroneous burst at the output of a convolutional decoder the fact that errors appears as bursts should be accounted for when designing a concatenated code with an inner convolutional code the popular solution for this problem is to interleave data before convolutional encoding so that the outer block code can correct most of the errors several algorithms exist for decoding convolutional codes for relatively small values of k the viterbi algorithm is universally used as it provides maximum likelihood performance and is highly parallelizable viterbi decoders are thus easy to implement in vlsi hardware and in software on cpus with simd instruction sets longer constraint length codes are more practically decoded with any of several sequential decoding algorithms of which the fano algorithm is the best known unlike viterbi decoding sequential decoding is not maximum likelihood but its complexity increases only slightly with constraint length allowing the use of strong long constraint length codes such codes were used in the pioneer program of the early 1970s to jupiter and saturn but gave way to shorter viterbi decoded codes usually concatenated with large reed solomon error correction codes that steepen the overall bit error rate curve and produce extremely low residual undetected error rates both viterbi and sequential decoding algorithms return hard decisions the bits that form the most likely codeword an approximate confidence measure can be added to each bit by use of the soft output viterbi algorithm maximum a posteriori soft decisions for each bit can be obtained by use of the bcjr algorithm an especially popular viterbi decoded convolutional code used at least since the voyager program has a constraint length k of 7 and a rate r of 1 2 puncturing is a technique used to make a m n rate code from a basic rate 1 2 code it is reached by deletion of some bits in the encoder output bits are deleted according to puncturing matrix the following puncturing matrices are the most frequently used for example if we want to make a code with rate 2 3 using the appropriate matrix from the above table we should take a basic encoder output and transmit every second bit from the first branch and every bit from the second one the specific order of transmission is defined by the respective communication standard punctured convolutional codes are widely used in the satellite communications for example in intelsat systems and digital video broadcasting punctured convolutional codes are also called perforated simple viterbi decoded convolutional codes are now giving way to turbo codes a new class of iterated short convolutional codes that closely approach the theoretical limits imposed by shannon s theorem with much less decoding complexity than the viterbi algorithm on the long convolutional codes that would be required for the same performance turbo codes have not yet been concatenated with solid reed solomon error correction codes however in the interest of planetary exploration this may someday be done 
in computing a system call is the mechanism used by an application program to request service from the operating system based on the monolithic kernel or to system servers on operating systems based on the microkernel structure a system call is a request made by any program to the operating system for performing tasks picked from a predefined set which the said program does not have required permissions to execute in its own flow of execution system calls provide the interface between a process and the operating system most operations interacting with the system require permissions not available to a user level process e g i o performed with a device present on the system or any form of communication with other processes requires the use of system calls the fact that improper use of the system call can easily cause a system crash necessitates some level of control the design of the microprocessor architecture on practically all modern systems offers a series of privilege levels the privilege level in which normal applications execute limits the address space of the program so that it cannot access or modify other running applications nor the operating system itself it also prevents the application from directly using devices but obviously many normal application needs these abilities thus it can call the operating system the operating system executes at the highest level of privilege and allows the applications to request services via system calls which are often implemented through interrupts if allowed the system enters a higher privilege level executes a specific set of instructions which the interrupting program has no direct control over then returns control to the former flow of execution this concept also serves as a way to implement security with the development of separate operating modes with varying levels of privilege a mechanism was needed for transferring control safely from lesser privileged modes to higher privileged modes less privileged code could not simply transfer control to more privileged code at any point and with any processor state to allow it to do so would allow it to break security for instance the less privileged code could cause the higher privileged code to execute in the wrong order or provide it with a bad stack generally systems provide a library that sits between normal programs and the operating system usually an implementation of the c library such as glibc this library handles the low level details of passing information to the operating system and switching to supervisor mode as well as any data processing and preparation which does not need to be done in privileged mode ideally this reduces the coupling between the os and the application and increases portability on exokernel based systems the library is especially important as an intermediary on exokernels os shield user applications from the very low level kernel api and provide abstractions and resource management on unix unix like and other posix systems popular system calls are open read write close wait exec fork exit and kill many of today s operating systems have hundreds of system calls for example linux has 319 different system calls freebsd has about the same number tools such as strace and truss allow a process to execute from start and report all system calls the process invokes or can attach to an already running process and intercept any system call made by said process if the operation does not violate the permissions of the user this special ability of the program is usually also implemented with a system call e g the gnu s strace is implemented with i a ptrace implementing system calls requires a control transfer which involves some sort of architecture specific feature a typical way to implement this is to use a software interrupt or trap interrupts transfer control to the os so software simply needs to set up some register with the system call number they want and execute the software interrupt for many risc processors this is the only feasible implementation but cisc architectures such as x86 support additional techniques one example is syscall sysret which is very similar to sysenter sysexit these are fast control transfer instructions that are designed to quickly transfer control to the os for a system call without the overhead of an interrupt linux 2 5 began using this on the x86 where available formerly it used the int instruction where the system call number was placed in the eax register before interrupt 0x80 was executed an older x86 mechanism is called a call gate and is a way for a program to literally call a kernel function directly using a safe control transfer mechanism the os sets up in advance this approach has been unpopular presumably due to the requirement of a far call which uses x86 memory segmentation and the resulting lack of portability it causes and existence of the faster instructions mentioned above this article was originally based on material from the free on line dictionary of computing which is licensed under the gfdl 
malbolge is a public domain esoteric programming language invented by ben olmstead in 1998 named after the eighth circle of hell in dante s inferno the malebolge the peculiarity of malbolge is that it was designed to be the most difficult and esoteric programming language however several of the tricks used to make understanding it difficult can be simplified away malbolge was so difficult to understand when it arrived that it took two years for the first malbolge program to appear the program was not even written by a human being it was generated by a beam search algorithm designed by andrew cooke and implemented in lisp later lou scheffer posted a cryptanalysis of malbolge and provided a program to copy its input to its output olmstead believed malbolge to be a linear bounded automaton there is a more interesting discussion about whether one can implement sensible loops in malbolge it took many years before the first non terminating one was introduced a correct 99 bottles of beer program which deals with non trivial loops and conditions was not announced for eight years the first correct one was by hisashi iizawa in 2007 this malbolge program displays hello world however without capitalization as no such version has yet been found malbolge is machine language for a ternary virtual machine the malbolge interpreter to aid in the writing of malbolge programs that run properly the way the standard interpreter works will be described below malbolge has three registers a c and d which are like variables in other languages when a program starts the value of all three registers is zero c is special it points to the current instruction d can hold a memory address is the value stored at that address is similar the virtual machine has 59049 memory locations that can each hold a ten digit ternary number each memory location has an address from 0 to 59048 and can hold a value from 0 to 59048 incrementing past this limit wraps back to zero before a malbolge program starts the first part of memory is filled with the program all whitespace in the program is ignored and to make programming more difficult everything else in the program must start out as one of the instructions below the rest of memory is filled by using the crazy operation on the previous two addresses  crz  memory filled this way will repeat every twelve addresses malbolge has eight instructions malbolge figures out which instruction to execute by taking the value at adding the value of c to it and taking the remainder when this is divided by 94 the final result tells the interpreter what to do after each instruction is executed the guilty instruction gets encrypted so that it won t do the same thing next time unless a jump just happened right after a jump malbolge will encrypt the innocent instruction just prior to the one it jumped to instead then the values of both c and d are increased by one and the next instruction is executed for each ternary digit of both inputs use the following table to get a ternary digit of the result for example crz0001112220 0120120120 gives 1001022211 after an instruction is executed the value at will be replaced with itself mod 94 then the result is encrypted with one of the following two equivalent methods find the result below store the ascii code of the character below it at find the result below store the encrypted version at lou scheffer s cryptanalysis of malbolge mentions six different cycles in the encryption they are listed here these cycles can be used to create loops that do different things each time and that eventually become repetitive lou scheffer used this idea to create a malbolge program that repeats anything the user inputs malbolge is not turing complete due to its memory limits several attempts have been made to create turing complete versions of malbolge 
 here be dragons is a phrase used to denote dangerous or unexplored territories in imitation of the infrequent medieval practice of putting sea serpents and other mythological creatures in blank areas of maps the earliest and only known use of this phrase is in the latin form hc svnt dracones on the lenox globe the term appeared on the east coast of asia earlier maps contain a variety of references to mythical and real creatures but the lenox globe is the only known surviving map to bear this phrase the classical phrase utilized by ancient roman and medieval cartographers used to be hic svnt leones when denoting unknown territories on maps dragons appear on a few other historical maps 
in computer programming a big ball of mud is a system or computer program that appears to have no distinguishable architecture it usually features other anti patterns the term was popularized in brian foote and joseph yoder s 1999 paper of the same name which defines the term thus a big ball of mud is a haphazardly structured sprawling sloppy duct tape and baling wire spaghetti code jungle these systems show unmistakable signs of unregulated growth and repeated expedient repair information is shared promiscuously among distant elements of the system often to the point where nearly all the important information becomes global or duplicated the overall structure of the system may never have been well defined if it was it may have eroded beyond recognition programmers with a shred of architectural sensibility shun these quagmires only those who are unconcerned about architecture and perhaps are comfortable with the inertia of the day to day chore of patching the holes in these failing dikes are content to work on such systems big ball of mud systems have usually been developed over a long period of time with different individuals working on various pieces and parts systems developed by people with no formal architecture or programming training often fall into this pattern foote and yoder do not universally condemn big ball of mud programming pointing out that this pattern is most prevalent because it works at least at the moment it is developed however programs of this pattern become maintenance nightmares programmers in control of a big ball of mud project are strongly encouraged to study it and to understand what it accomplishes and to use this as a loose basis for a formal set of requirements for a well designed system that could replace it technology shifts such as client server to web based or file based to database based may provide good reasons to start over from scratch in discussion of the lisp programming language the term big ball of mud is used differently in this case to describe the malleability of a lisp system in lisp it is generally possible to the programming language forth has also been described as a ball of mud because it too has many of these properties joel moses may have coined the phrase in the 1970s there is controversy over whether moses in fact said this and if he did whether he intended it to be derogatory explanatory or laudatory 
a sequence point in imperative programming defines any point in a computer program s execution at which it is guaranteed that all side effects of previous evaluations will have been performed and no side effects from subsequent evaluations have yet been performed they are often mentioned in reference to c and c because the result of some expressions can depend on the order of evaluation of their subexpressions adding one or more sequence points is one method of ensuring a consistent result because this restricts the possible orders of evaluation consider two functions f and g in c and c the operator is not a sequence point and therefore in the expression f g it is possible that either f or g will be executed first the comma operator is a sequence point and therefore in the code f g the order of evaluation is defined is called and then g is called the type and value of the whole expression are those of g the value of f is discarded sequence points also come into play when the same variable is modified more than once an often cited example is the expression i i which both assigns i to itself and increments i what is the final value of i language definitions might specify one of the possible behaviors or simply say the behavior is undefined in c and c evaluating such an expression yields undefined behavior in c and c sequence points occur in the following places 
in mathematics and computer science graph theory is the study of graphs mathematical structures used to model pairwise relations between objects from a certain collection a graph in this context refers to a collection of vertices or nodes and a collection of edges that connect pairs of vertices a graph may be undirected meaning that there is no distinction between the two vertices associated with each edge or its edges may be directed from one vertex to another see graph for more detailed definitions and for other variations in the types of graphs that are commonly considered the graphs studied in graph theory should not be confused with graphs of functions and other kinds of graphs please refer to glossary of graph theory for some basic definitions in graph theory the paper written by leonhard euler on the seven bridges of knigsberg and published in 1736 is regarded as the first paper in the history of graph theory this paper as well as the one written by vandermonde on the knight problem carried on with the analysis situs initiated by leibniz euler s formula relating the number of edges vertices and faces of a convex polyhedron was studied and generalized by cauchy and l huillier and is at the origin of topology more than one century after euler s paper on the bridges of knigsberg and while listing introduced topology cayley was led by the study of particular analytical forms arising from differential calculus to study a particular class of graphs the trees this study had many implications in theoretical chemistry the involved techniques mainly concerned the enumeration of graphs having particular properties enumerative graph theory then rose from the results of cayley and the fundamental results published by plya between 1935 and 1937 and the generalization of these by de bruijn in 1959 cayley linked his results on trees with the contemporary studies of chemical composition the fusion of the ideas coming from mathematics with those coming from chemistry is at the origin of a part of the standard terminology of graph theory in particular the term graph was introduced by sylvester in a paper published in 1878 in nature one of the most famous and productive problems of graph theory is the four color problem is it true that any map drawn in the plane may have its regions colored with four colors in such a way that any two regions having a common border have different colors this problem was first posed by francis guthrie in 1852 and its first written record is in a letter of de morgan addressed to hamilton the same year many incorrect proofs have been proposed including those by cayley kempe and others the study and the generalization of this problem by tait heawood ramsey and hadwiger led to the study of the colorings of the graphs embedded on surfaces with arbitrary genus tait s reformulation generated a new class of problems the factorization problems particularly studied by petersen and knig the works of ramsey on colorations and more specially the results obtained by turn in 1941 was at the origin of another branch of graph theory extremal graph theory the four color problem remained unsolved for more than a century a proof produced in 1976 by kenneth appel and wolfgang haken which involved checking the properties of 1 936 configurations by computer was not fully accepted at the time due to its complexity a simpler proof considering only 633 configurations was given twenty years later by robertson seymour sanders and thomas the autonomous development of topology from 1860 and 1930 fertilized graph theory back through the works of jordan kuratowski and whitney another important factor of common development of graph theory and topology came from the use of the techniques of modern algebra the first example of such a use comes from the work of the physicist gustav kirchhoff who published in 1845 his kirchhoff s circuit laws for calculating the voltage and current in electric circuits the introduction of probabilistic methods in graph theory especially in the study of erds and rnyi of the asymptotic probability of graph connectivity gave rise to yet another branch known as random graph theory which has been a fruitful source of graph theoretic results graphs are represented graphically by drawing a dot for every vertex and drawing an arc between two vertices if they are connected by an edge if the graph is directed the direction is indicated by drawing an arrow a graph drawing should not be confused with the graph itself as there are several ways to structure the graph drawing all that matters is which vertices are connected to which others by how many edges and not the exact layout in practice it is often difficult to decide if two drawings represent the same graph depending on the problem domain some layouts may be better suited and easier to understand than others there are different ways to store graphs in a computer system the data structure used depends on both the graph structure and the algorithm used for manipulating the graph theoretically one can distinguish between list and matrix structures but in concrete applications the best structure is often a combination of both list structures are often preferred for sparse graphs as they have smaller memory requirements matrix structures on the other hand provide faster access for some applications but can consume huge amounts of memory there is a large literature on graphical enumeration the problem of counting graphs meeting specified conditions some of this work is found in harary and palmer a common problem called the subgraph isomorphism problem is finding a fixed graph as a subgraph in a given graph one reason to be interested in such a question is that many graph properties are hereditary for subgraphs which means that a graph has the property if and only if all subgraphs have it too unfortunately finding maximal subgraphs of a certain kind is often an np complete problem a similar problem is finding induced subgraphs in a given graph again some important graph properties are hereditary with respect to induced subgraphs which means that a graph has a property if and only if all induced subgraphs also have it finding maximal induced subgraphs of a certain kind is also often np complete for example still another such problem the minor containment problem is to find a fixed graph as a minor of a given graph a minor or subcontraction of a graph is any graph obtained by taking a subgraph and contracting some edges many graph properties are hereditary for minors which means that a graph has a property if and only if all minors have it too a famous example another class of problems has to do with the extent to which various species and generalizations of graphs are determined by their point deleted subgraphs for example many problems have to do with various ways of coloring graphs for example there are numerous problems arising especially from applications that have to do with various notions of flows in networks for example covering problems are specific instances of subgraph finding problems and they tend to be closely related to the clique problem or the independent set problem applications of graph theory are primarily but not exclusively concerned with labeled graphs and various specializations of these structures that can be represented as graphs are ubiquitous and many problems of practical interest can be represented by graphs the link structure of a website could be represented by a directed graph the vertices are the web pages available at the website and a directed edge from page a to page b exists if and only if a contains a link to b a similar approach can be taken to problems in travel biology computer chip design and many other fields the development of algorithms to handle graphs is therefore of major interest in computer science there the transformation of graphs is often formalized and represented by graph rewrite systems they are either directly used or properties of the rewrite systems are studied a graph structure can be extended by assigning a weight to each edge of the graph graphs with weights or weighted graphs are used to represent structures in which pairwise connections have some numerical values for example if a graph represents a road network the weights could represent the length of each road a digraph with weighted edges in the context of graph theory is called a network networks have many uses in the practical side of graph theory network analysis within network analysis the definition of the term network varies and may often refer to a simple graph many applications of graph theory exist in the form of network analysis these split broadly into three categories firstly analysis to determine structural properties of a network such as the distribution of vertex degrees and the diameter of the graph a vast number of graph measures exist and the production of useful ones for various domains remains an active area of research secondly analysis to find a measurable quantity within the network for example for a transportation network the level of vehicular flow within any portion of it thirdly analysis of dynamical properties of networks graph theory is also used to study molecules in chemistry and physics in condensed matter physics the three dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph theoretic properties related to the topology of the atoms for example franzblau s shortest path rings in chemistry a graph makes a natural model for a molecule where vertices represent atoms and edges bonds this approach is especially used in computer processing of molecular structures ranging from chemical editors to database searching graph theory is also widely used in sociology as a way for example to measure actors prestige or to explore diffusion mechanisms notably through the use of social network analysis software 
clips is a public domain software tool for building expert systems the name is an acronym for c language integrated production system the syntax and name was inspired by charles forgy s ops the first versions of clips were developed starting in 1985 at nasa johnson space center until the mid 1990s when the development group s responsibilities ceased to focus on expert system technology the original name of the project was nasa s ai language clips is probably the most widely used expert system tool because it is fast efficient and free although it is now in the public domain it is still updated and supported by the original author gary riley clips incorporates a complete object oriented language cool for writing expert systems though it is written in c its interface more closely resembles that of the programming language lisp extensions can be written in c and clips can be called from c like other expert system languages clips deals with rules and facts various facts can make a rule applicable an applicable rule is then asserted facts and rules are created by first defining them as shown below in clips salience allows a user to assign priority to a rule descendants of the clips language include jess and fuzzyclips clips contains an extensive set of readable documentation and the following books are available 
in software engineering a directory is similar to a dictionary it enables the look up of a name and information associated with that name as a word in a dictionary may have multiple definitions in a directory a name may be associated with multiple different pieces of information likewise as a word may have different parts of speech and different definitions a name in a directory may have many different types of data based on this rudimentary explanation of a directory a directory service is simply the software system that stores organizes and provides access to information in a directory directories may be very narrow in scope supporting only a small set of node types and data types or they may be very broad supporting an arbitrary or extensible set of types in a telephone directory the nodes are names and the data items are telephone numbers in the dns the nodes are domain names and the data items are ip addresses in a directory used by a network operating system the nodes represent resources that are managed by the os including users computers printers and other shared resources many different directory services have been used since the advent of the internet but this article focuses mainly on those that have descended from the x 500 directory service a simple directory service called a naming service maps the names of network resources to their respective network addresses with the name service type of directory a user doesn t have to remember the physical address of a network resource providing a name will locate the resource each resource on the network is considered an object on the directory server information about a particular resource is stored as attributes of that object information within objects can be made secure so that only users with the available permissions are able to access it more sophisticated directories are designed with namespaces as subscribers services devices entitlements preferences content and so on this design process is highly related to identity management a directory service defines the namespace for the network a namespace in this context is the term that is used to hold one or more objects as named entries the directory design process normally has a set of rules that determine how network resources are named and identified the rules specify that the names be unique and unambiguous in x 500 and ldap the name is called the distinguished name and is used to refer to a collection of attributes which make up the name of a directory entry a directory service is a shared information infrastructure for locating managing administering and organizing common items and network resources which can include volumes folders files printers users groups devices telephone numbers and other objects a directory service is an important component of a nos in the more complex cases a directory service is the central information repository for a service delivery platform for example looking up computers using a directory service might yield a list of available computers and information for accessing them replication and distribution have very distinct meanings in the design and management of a directory service the term replication is used to indicate that the same directory namespace are copied to another directory server for redundancy and throughput reasons the replicated namespace is governed by the same authority the term distribution is used to indicate that multiple directory servers that hold different namespaces are interconnected to form a distributed directory service each distinct namespace can be governed by different authorities there are a number of things that distinguish a traditional directory service from a typical relational database of course there are exceptions but in general directory schemas are defined as object classes attributes name bindings and knowledge where an object class has directory services were part of an open systems interconnection initiative to get everyone in the industry to agree to common network standards to provide multi vendor interoperability in the 1980s the itu and iso came up with a set of standards x 500 for directory services initially to support the requirements of inter carrier electronic messaging and network name lookup the lightweight directory access protocol ldap is based on the directory information services of x 500 but uses the tcp ip stack and a string encoding scheme of the x 500 protocol dap giving it more relevance on the internet there have been numerous forms of directory service implementations from different vendors systems developed before the advent of x 500 include among the ldap x 500 based implementations are there are also plenty of open source tools to create directory services including openldap and the kerberos protocol and samba software which can act as a windows domain controller with kerberos and ldap backends 
anders hejlsberg is a prominent danish software engineer who co designed several popular and commercially successful programming languages and development tools he was the chief architect of delphi and currently works for microsoft as the lead architect of the c programming language hejlsberg was born in copenhagen denmark and studied engineering at the technical university of denmark but did not graduate while at the university in 1980 he began writing programs for the nascom microcomputer including a pascal compiler which was initially marketed as the blue label pascal compiler for the nascom 2 however he soon rewrote it for cp m and ms dos marketing it first as compas pascal and later as polypascal later the product was licensed to borland and integrated into an ide to become the turbo pascal system turbo pascal competed with polypascal the compiler itself was largely inspired by the tiny pascal compiler in niklaus wirth s algorithms data structures programs one of the most influential computer science books of the time anders and his partners ran a computer store in copenhagen and marketed accounting systems their company polydata was the distributor for microsoft products in denmark which put them at odds with borland philippe kahn and anders first met in 1986 for all those years niels jensen one of borland s founders and its majority shareholder had successfully handled the relationship between borland and polydata in borland s hands turbo pascal became the most commercially successful pascal compiler ever hejlsberg remained with polydata until the company came under financial stress at which time in 1989 he moved to california and became chief engineer at borland there he remained until 1996 during this time he developed turbo pascal further and eventually he became the chief architect for the team which produced the replacement for turbo pascal delphi in 1996 hejlsberg left borland and joined archrival microsoft one of his first achievements was the j programming language and the windows foundation classes he also became a microsoft distinguished engineer and technical fellow since 2000 he has been the lead architect of the team developing the c programming language he received the 2001 dr dobb s excellence in programming award for his work on turbo pascal delphi c and the microsoft net framework together with shon katzenberger scott wiltamuth todd proebsting erik meijer peter hallam and peter sollich anders was recently awarded a technical recognition award for outstanding technical achievement for their work on the c language a video about this is available at outstanding technical achievement c team 
reversi is a board game involving abstract strategy and play by two players parties on a board with 8 rows and 8 columns and a set of distinct pieces for each side pieces typically appear coin like with a light and a dark face each side representing one player the goal for each player is to make pieces of their color constitute a majority of the pieces on the board at the end of the game by turning over as many of their opponent s pieces as possible the modern version of the game is based on the game reversi that was invented in 1883 by the englishman lewis waterman and gained considerable popularity in england at the end of the 19th century the game is mentioned in an 1895 article in the new york times reversi is something like go bang and is played with 64 pieces in 1898 the well known german games publisher ravensburger started producing the game as one of its first titles the modern rule set now universally accepted originated in mito ibaraki japan in the 1970s the game was renamed othello and was registered as a trademark by the japanese game company tsukuda original the name is a reference to the shakespearean play othello the moor of venice referencing the conflict between the moor othello and iago who describes himself as two faced it can also be likened to a jealousy competition since players engulf the pieces of the opponent thereby turning them their possession while this rule set is the most widely accepted harrison heath the famous pioneer of disc based puzzles put forward his own version however this failed to break into the mainstream market a 2002 press release about the origins of the modern game makes no mention of the original version othello was invented by japanese game enthusiast goro hasegawa in 1971 he chose one of the leading toy inventors in the world james r becker to help him develop and market the game inspired by the ancient chinese strategy game go hasegawa sought to create a game that was rich in strategy but still approachable by the casual player becker simplified the game play coined the tagline a minute to learn a lifetime to master and named this new game after shakespeare s classic play because of the black and white chips othello was first introduced in japan in 1973 by tsukuda original co who at becker s suggestion organized the japanese othello association goro hasegawa who wrote how to win at othello popularized the game in japan in 1975 each of the two sides corresponds to one player they are referred to here as light and dark after the sides of othello pieces but heads and tails would identify them equally as well so long as each marker has sufficiently distinctive sides originally reversi did not have a defined starting position later it adopted othello s rules which state that the game begins with four markers placed in a square in the middle of the grid two facing light up two pieces with the dark side up the dark player makes the first move dark must place a piece with the dark side up on the board in such a position that there exists at least one straight occupied line between the new piece and another dark piece with one or more contiguous light pieces between them in the below situation dark has the following options indicated by transparent pieces after placing the piece dark turns over all light pieces lying on a straight line between the new piece and any anchoring dark pieces all reversed pieces now show the dark side and dark can use them in later moves unless light has reversed them back in the meantime if dark decided to put a piece in the topmost location one piece gets turned over so that the board appears thus now light plays this player operates under the same rules with the roles reversed light lays down a light piece causing one or more dark pieces to flip possibilities at this time appear thus light takes the bottom left option and reverses one piece players take alternate turns if one player cannot make a valid move play passes back to the other player when neither player can move the game ends this occurs when the grid has filled up or when one player has no more pieces on the board or when neither player can legally place a piece in any of the remaining squares the player with the most pieces on the board at the end of the game wins a beginner often looks for the move that will reverse the greatest possible number of pieces trying for immediate numerical advantage for unsophisticated players this strategy works quite well and will win a majority of games as long as the player thinks at least a few turns in advance as the experience of the opponent increases this strategy becomes ineffectual instead of numerical advantage the key elements of successful reversi strategy are corners mobility edge play parity endgame play and looking ahead corner positions once played remain immune to flipping for the rest of the game thus a player could use a piece in a corner of the board to anchor groups of pieces permanently therefore capturing a corner often proves an effective strategy when the opportunity arises more generally a piece is stable when along all four axes it is on a boundary in a filled row or next to a stable piece of the same color grabbing a corner prematurely may be a mistake however if in doing so the player leaves holes along the edge these holes can be filled by the opposing player and could result in capture of some or all of the pieces along that edge this renders occupying the corner largely useless an opponent playing with reasonable strategy will not so easily relinquish the corner or any other good moves so to achieve these good moves you must force your opponent to play moves which relinquish those good moves the best way to achieve this involves reducing the number of moves available to your opponent if you consistently restrict the number of legal moves your opponent can make then sooner or later they will have to make an undesirable move an ideal position involves having all your pieces in the center surrounded by your opponent s pieces in such situations you can dictate what moves your opponent can make when moves seem equal with respect to what moves you will leave yourself and your opponent playing a minimum piece strategy will tend to give you an advantage because minimizing your discs will tend to leave fewer discs for your opponent to flip in subsequent moves of the game one should not play the minimum disc strategy to an extreme however as this also can quickly lead to a lack of mobility while playing pieces to edges of the board may seem sound this strategy can often prove detrimental edge pieces can anchor flips that influence moves to all regions of the board this can poison later moves by causing players to flip too many pieces and open up many moves for the opponent however playing on edges where an opponent cannot easily respond drastically reduces possible moves for that opponent the square immediately diagonally adjacent to the corner when played in the early or middle game typically guarantees the loss of that corner nevertheless such a corner sacrifice is sometimes played for some strategic purpose playing to the edge squares adjacent to the corner can also be dangerous if it gives the opponent powerful forcing moves in general edge play in the early and middle game is to be avoided unless players can gain larger concessions in terms of mobility or a mass of unflippable pieces a good rule of thumb is to keep pieces grouped together in the middle of the board and minimize tangents formed by a player s own pieces this strategy leads to the greatest mobility parity is one of the most important parts of the strategy in short the concept of parity is about getting the last move in every empty region in the end game and thereby increasing the number of stable discs in the late 1980s the concept of parity was spread across europe the concept of parity led to a change in the perception of the game as it led to distinct strategies for playing black and white it forced black to play more aggressive moves and gave white the opportunity to stay calm and focus on keeping the parity as a result the opening books and mid game were focused on black being the attacker and white being the defender another side effect of parity is that black should try to complicate the game whereas white should seek to simplify it it is easier to maintain parity in a simple position the concept of parity also controls how edge positions are played and how edges interact as in any good strategy for chess or for checkers a player should not consider only the current situation on the board for each move you consider you must consider possible responses from your opponent then the subsequent responses you will make to those moves and so on the aspects of the current position may not remain relevant a few moves hence so when optimizing your mobility gaining corners or anything else you should consider how best to do this for the long term rather than just for the next move for the endgame the strategies will typically change special techniques such as sweeping gaining access and the details of move order can have a large impact on the outcome of the game at these late stages of the game no hard set rules exist the experienced player will try to look ahead and get a feel for what will lead to the best final outcome the best othello computer programs can easily defeat the best humans as early as 1980 the program the moor beat the reigning world champion in 1997 logistello defeated the human champion takeshi murakami in a score of 6 0 by comparison computers also easily win against the best human players of english draughts in chess the best computers are debatably stronger than the best humans and in go and arimaa even average human players can defeat the best computers human beings cannot generally win against computer intelligence in othello because computers can look ahead much further than humans can analysts have estimated the number of legal positions in othello is at most 1028 and it has a game tree complexity of approximately 1058 mathematically othello still remains unsolved experts have not yet figured out what the outcome of a game will be where both sides have perfect play however analysis of thousands of high quality games has led to the conclusion that on the standard 8 by 8 board perfect play on both sides results in a draw when generalizing the game to play on an n by n board the problem of determining if the first player has a winning move in a given position is pspace complete on 4 by 4 and 6 by 6 boards under perfect play the second player wins this rivalling monte carlo world championship is usually not considered to be an official world championship in official homepages it is called the first europe championship 
halftone is the reprographic technique that simulates continuous tone imagery through the use of dots varying either in size or in spacing halftone can also be used to refer specifically to the image that is produced by this process where continuous tone imagery contains an infinite range of colors or greys the halftone process reduces visual reproductions to a binary image that is printed with only one color of ink this binary reproduction relies on a basic optical illusion that these tiny halftone dots are blended into smooth tones by the human eye at a microscopic level developed black and white photographic film also consists of only two colors and not an infinite range of continuous tones for details see film grain just as color photography evolved with the addition of filters and film layers color printing is made possible by repeating the halftone process for each subtractive color most commonly using what is called the cmyk color model the semi opaque property of ink allows halftone dots of different colors to create another optical effect full color imagery the idea of halftone printing is due to william fox talbot in the early 1850s he suggested using photographic screens or veils in connection with a photographic intaglio process several different kinds of screens were proposed during the following decades one of the well known attempts was by stephen h horgan while working for the new york daily graphic the first printed photograph was an image of steinway hall in manhattan published on december 2 1873 the graphic then published the first reproduction of a photograph with a full tonal range in a newspaper on march 4 1880 with a crude halftone screen the first truly successful commercial method was patented by frederic ives of philadelphia in 1881 although he found a way of breaking up the image into dots of varying sizes he did not make use of a screen in 1882 the german george meisenbach patented a halftone process in england his invention was based on the previous ideas of berchtold and swan he used single lined screens which were turned during exposure to produce cross lined effects he was the first to achieve any commercial success with relief halftones shortly afterwards ives this time in collaboration with louis and max levy improved the process further with the invention and commercial production of quality cross lined screens the relief halftone process proved almost immediately to be a success the use of halftone blocks in popular journals became regular during the early 1890s the most common method of creating screens amplitude modulation produces a regular grid of dots that vary in size the other method of creating screens frequency modulation is used in a process named stochastic screening the resolution of a halftone screen is measured in lines per inch this is the number of lines of dots in one inch measured parallel with the screen s angle known as the screen ruling the resolution of a screen is written either with the suffix lpi or a hash mark e g 150lpi or 150 the higher the pixel resolution of a source file the greater the detail that can be reproduced however such increase also requires a corresponding increase in screen ruling or the output will suffer from posterization therefore file resolution is matched to the output resolution when different screens are combined a number of distracting visual effects can occur including the edges being overly emphasized as well as a moir pattern this problem can be reduced by rotating the screens in relation to each other this screen angle is another common measurement used in printing measured in degrees clockwise from a line running to the left halftoning is also commonly used for printing color pictures the general idea is the same by varying the density of the four primary printing colors cyan magenta yellow and black any particular shade can be reproduced in this case there is an additional problem that can occur in the simple case one could create a halftone using the same techniques used for printing shades of grey but in this case the different printing colors have to remain physically close to each other to fool the eye into thinking they are a single color to do this the industry has standardized on a set of known angles which result in the dots forming into small circles or rosettes the dots cannot easily be seen by the naked eye but can be discerned through a microscope or a magnifying glass digital halftoning has been replacing photographic halftoning since the 1970s when electronic dot generators were developed for the film recorder units linked to color drum scanners made by companies such as crosfield electronics hell and linotype paul in the 1980s halftoning became available in the new generation of imagesetter film and paper recorders that had been developed from earlier laser typesetters unlike pure scanners or pure typesetters imagesetters could generate all the elements in a page including type photographs and other graphic objects early examples were the widely used linotype linotronic 300 and 100 introduced in 1984 which were also the first to offer postscript rips in 1985 early laser printers from the late 1970s onward could also generate halftones but their original 300 dpi resolution limited the screen ruling to about 65 lpi this was improved as higher resolutions of 600 dpi and above and dithering techniques were introduced all halftoning uses a high frequency low frequency dichotomy in photographic halftoning the low frequency attribute is a local area of the output image designated a halftone cell each equal sized cell relates to a corresponding area of the continuous tone input image within each cell the high frequency attribute is a centered variable sized halftone dot composed of ink or toner the ratio of the inked area to the non inked area of the output cell corresponds to the luminance or graylevel of the input cell from a suitable distance the human eye averages both the high frequency apparent gray level approximated by the ratio within the cell and the low frequency apparent changes in gray level between adjacent equally spaced cells and centered dots digital halftoning uses a raster image or bitmap within which each monochrome picture element or pixel may be on or off ink or no ink consequently to emulate the photographic halftone cell the digital halftone cell must contain groups of monochrome pixels within the same sized cell area the fixed location and size of these monochrome pixels compromises the high frequency low frequency dichotomy of the photographic halftone method clustered multi pixel dots cannot grow incrementally but in jumps of one whole pixel in addition the placement of that pixel is slightly off center to minimize this compromise the digital halftone monochrome pixels must be quite small numbering from 600 to 2 540 or more pixels per inch however digital image processing has also enabled more sophisticated dithering algorithms to decide which pixels to turn black or white some of which yield better results than digital halftoning 
in mathematics the sieve of atkin is a fast modern algorithm for finding all prime numbers up to a specified integer it is an optimized version of the ancient sieve of eratosthenes but does some preliminary work and then marks off multiples of primes squared rather than multiples of primes it was created by a o l atkin and daniel j bernstein in the algorithm the following is pseudocode for a straightforward version of the algorithm this pseudocode is written for clarity repeated and wasteful calculations mean that it would run slower than the sieve of eratosthenes to improve its efficiency faster methods must be used to find solutions to the three quadratics at the least separate loops could have tighter limits than the algorithm completely ignores any numbers divisible by two three or five all numbers with modulo sixty remainder 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 or 58 are divisible by two and not prime all numbers with modulo sixty remainder 3 9 15 21 27 33 39 45 51 or 57 are divisible by three and not prime all numbers with modulo sixty remainder 5 25 35 or 55 are divisible by five and not prime all these remainders are ignored all numbers with modulo sixty remainder 1 13 17 29 37 41 49 or 53 have a modulo four remainder of 1 these numbers are prime if and only if the number of solutions to 4x2 y2 n is odd and the number is squarefree all numbers with modulo sixty remainder 7 19 31 or 43 have a modulo six remainder of 1 these numbers are prime if and only if the number of solutions to 3x2 y2 n is odd and the number is squarefree all numbers with modulo sixty remainder 11 23 47 or 59 have a modulo twelve remainder of 11 these numbers are prime if and only if the number of solutions to 3x2 y2 n is odd and the number is squarefree none of the potential primes are divisible by 2 3 or 5 so they can t be divisible by their squares this is why squarefree checks don t include 22 32 and 52 this sieve computes primes up to n using o operations with only n1 2 o bits of memory that is a little better than the sieve of eratosthenes which uses o operations and o log n bits of memory these asymptotic computational complexities include simple optimizations such as wheel factorization and splitting the computation to smaller blocks the first two equations used to determine if a number is prime after their respective modulo tests are equations for ellipses they can be rewritten into standard form for an ellipse by dividing both sides of the equation by n where n is the entry number being tested for primality using the equations in this form is easier to implement a test for various reasons see ellipse for more information 
intercal a programming language parody is the canonical esoteric programming language it was created by don woods and james m lyon two princeton university students in 1972 it satirizes aspects of the various programming languages at the time as well as the proliferation of proposed language constructs and notations in the 1960s consequently the humor may appear rather dated to modern readers brought up with c or java according to the original manual by the authors there are two currently maintained versions of intercal c intercal and clc intercal maintained by claudio calvelli intercal is intended to be completely different from all other computer languages common operations in other languages have cryptic and redundant syntax in intercal from the intercal reference manual do 1  0 256any sensible programmer would say that that was absurd since this is indeed the simplest method the programmer would be made to look foolish in front of his boss who would of course have happened to turn up as bosses are wont to do the effect would be no less devastating for the programmer having been correct the intercal reference manual contains many paradoxical nonsensical or otherwise humorous instructions the manual also contains a tonsil as explained in this footnote 4 since all other reference manuals have appendices it was decided that the intercal manual should contain some other type of removable organ intercal has many other features designed to make it even more aesthetically unpleasing to the programmer it uses statements such as read out ignore forget and modifiers such as please this last keyword provides two reasons for the program s rejection by the compiler if please does not appear often enough the program is considered insufficiently polite and the error message says this if too often the program could be rejected as excessively polite although this feature existed in the original intercal compiler it was undocumented the intercal manual gives unusual names to all non alphanumeric ascii characters single and double quotes are sparks and rabbit ears respectively the assignment operator represented as an equals sign in many other programming languages is in intercal a left arrow  referred to as gets and made up of an angle and a worm the original princeton implementation used punched cards and the ebcdic character set in order to allow intercal to run on computers using ascii substitutions for two characters had to be made substituted for as the mingle operator represent the increasing cost of software in relation to hardware and was substituted for as the unary exclusive or operator to correctly express the average person s reaction on first encountering exclusive or in recent versions of c intercal the older operators are supported as alternatives intercal programs may now be encoded in ascii latin 1 or utf 8 the usenet newsgroup alt lang intercal is devoted to the study and appreciation of intercal and other esoteric languages despite the language s being intentionally obtuse and wordy intercal is nevertheless turing complete given enough memory intercal can solve any problem that a universal turing machine can solve most implementations of intercal do this very slowly however a sieve of eratosthenes benchmark computing all prime numbers less than 65536 was tested on a sun sparcstation 1 in c it took less than half a second the same program in intercal took over seventeen hours it should be noted that almost any programming language allows notational horrors as great as or greater than intercal s as demonstrated in contests such as the international obfuscated c code contest however these are generally intentional efforts to create unreadable code in contrast to intercal s design which forces virtually all code to be unreadable according to the intercal manual the aim in designing intercal was to have no precedents supposedly neither in flow control features nor in data manipulation operators the designers were partially successful the only known precedent is a machine instruction in a soviet mainframe computer besm 6 released in 1967 that is effectively equivalent to intercal s select operator the original woods lyon intercal was very limited in its input output capabilities the only acceptable input were numbers with the digits spelled out and the only output was an extended version of roman numerals a while later there was an atari implementation about which notes are provided in the intercal reference manual it differs from the original princeton version primarily in the use of ascii rather than ebcdic the c intercal reimplementation being available on the internet has made the language more popular with devotees of esoteric programming languages the c intercal dialect has a few differences from original intercal and introduced a few new features such as a come from statement and a means of doing text i o based on the turing text model the authors of c intercal also created the triintercal variant based on the ternary numeral system and generalizing intercal s set of operators a more recent variant is threaded intercal which extends the functionality of come from to support multithreading intercal 72 had only four data types the 16 bit integer the 32 bit integer the array of 16 bit integers and the array of 32 bit integers there are 65535 available variables of each type numbered from 1 to 65535 for 16 bit integers for instance however each of these variables has its own stack on which it can be pushed and popped increasing the possible complexity of data structures and clc intercal implements many of its own data structures such as classes and lectures by making the basic data types store more information rather than adding new types arrays are dimensioned by assigning to them as if they were a scalar variable constants can also be used and are represented by a followed by the constant itself written as a decimal number only integer constants from 0 to 65535 are supported there are only five operators in intercal 72 implementations vary in which characters represent which operation and many accept more than one character so more than one possibility is given for many of the operators sources for this table contrary to most other languages and or and xor are unary operators which work on consecutive bits of their argument the most significant bit of the result is the operator applied to the most significant and least significant bits of the input the second most significant bit of the result is the operator applied to the most and second most significant bits the third most significant bit of the result is the operator applied to the second most and third most bits and so on the operator is placed between the punctuation mark specifying a variable name or constant and the number that specifies which variable it is or just inside grouping marks select and interleave are infix binary operators select takes the bits of its first operand that correspond to 1 bits of its second operand and removes the bits that correspond to 0 bits shifting towards the least significant bit and padding with zeroes select 21 is 5 mingle alternates bits from its first and second operands there is no operator precedence grouping marks must be used to disambiguate the precedence where it would otherwise be ambiguous which matches another spark and which matches another rabbit ears the programmer is responsible for using these in such a way that they make the expression unambiguous intercal statements all start with a statement identifier in intercal 72 this can be do please or please do all of which have the same meaning as far as the program is concerned or an inverted form backtracking intercal a modern variant also allows variants using maybe as a statement identifier which introduces a choice point before the identifier an optional line number can be given after the identifier a percent chance of the line executing can be given in the format 50 which defaults to 100 in intercal 72 the main control structures are next resume and forget do next branches to the line specified remembering the next line that would be executed if it weren t for the next on a call stack do forget expression removes expression entries from the top of the call stack and do resume expression removes expression entries from the call stack and jumps to the last line remembered c intercal also provides the come from instruction written do come from clc intercal and the most recent c intercal versions also provide computed come from and next from which is like come from but also saves a return address on the next stack alternative ways to affect program flow originally available in intercal 72 are to use the ignore and remember instructions on variables and the abstain and reinstate instructions on lines or on types of statement causing the lines to have no effect or to have an effect again respectively input and output do not use the usual formats in intercal 72 write in inputs a number written out as digits in english and read out outputs it in butchered roman numerals more recent versions have their own i o systems comments can be achieved by using the inverted statement identifiers involving not or n t these cause lines to be initially abstained so that they have no effect the traditional hello world program demonstrates how different intercal is from standard programming languages in c it could read as follows the equivalent program in c intercal is longer and harder to read in intercal 72 the equivalent program is not possible as it can only output as butchered roman numerals in the article a box darkly obfuscation weird languages and code aesthetics intercal is described under the heading abandon all sanity ye who enter here intercal the compiler and commenting strategy are among the weird features described 
in cryptography md5 is a widely used cryptographic hash function with a 128 bit hash value as an internet standard md5 has been employed in a wide variety of security applications and is also commonly used to check the integrity of files however it has been shown that md5 is not collision resistant as such md5 is not suitable for applications like ssl certificates or digital signatures that rely on this property an md5 hash is typically expressed as a 32 digit hexadecimal number md5 was designed by ron rivest in 1991 to replace an earlier hash function md4 in 1996 a flaw was found with the design of md5 while it was not a clearly fatal weakness cryptographers began recommending the use of other algorithms such as sha 1 in 2004 more serious flaws were discovered making further use of the algorithm for security purposes questionable in 2007 a group of researchers including arjen lenstra described how to create a pair of files that share the same md5 checksum in an attack on md5 published in december 2008 a group of researchers used this technique to fake ssl certificate validity md5 is one in a series of message digest algorithms designed by professor ronald rivest of mit when analytic work indicated that md5 s predecessor md4 was likely to be insecure md5 was designed in 1991 to be a secure replacement in 1993 den boer and bosselaers gave an early although limited result of finding a pseudo collision of the md5 compression function that is two different initialization vectors which produce an identical digest in 1996 dobbertin announced a collision of the compression function of md5 while this was not an attack on the full md5 hash function it was close enough for cryptographers to recommend switching to a replacement such as sha 1 or ripemd 160 the size of the hash 128 bits is small enough to contemplate a birthday attack md5crk was a distributed project started in march 2004 with the aim of demonstrating that md5 is practically insecure by finding a collision using a birthday attack md5crk ended shortly after 17 august 2004 when collisions for the full md5 were announced by xiaoyun wang dengguo feng xuejia lai and hongbo yu their analytical attack was reported to take only one hour on an ibm p690 cluster on 1 march 2005 arjen lenstra xiaoyun wang and benne de weger demonstrated construction of two x 509 certificates with different public keys and the same md5 hash a demonstrably practical collision the construction included private keys for both public keys a few days later vlastimil klima described an improved algorithm able to construct md5 collisions in a few hours on a single notebook computer on 18 march 2006 klima published an algorithm that can find a collision within one minute on a single notebook computer using a method he calls tunneling because md5 makes only one pass over the data if two prefixes with the same hash can be constructed a common suffix can be added to both to make the collision more reasonable because the current collision finding techniques allow the preceding hash state to be specified arbitrarily a collision can be found for any desired prefix that is for any given string of characters x two colliding files can be determined which both begin with x all that is required to generate two colliding files is a template file with a 128 byte block of data aligned on a 64 byte boundary that can be changed freely by the collision finding algorithm recently a number of projects have created md5 rainbow tables which are easily accessible online and can be used to reverse many md5 hashes into strings that collide with the original input usually for the purposes of password cracking however if passwords are combined with a salt before the md5 digest is generated rainbow tables become much less useful the use of md5 in some websites urls means that google can also sometimes function as a limited tool for reverse lookup of md5 hashes this technique is rendered ineffective by the use of a salt on december 30 2008 a group of researchers announced at the 25th chaos communication congress how they had used md5 collisions to create an intermediate certificate authority certificate which appeared to be legitimate when checked via its md5 hash the researchers used a cluster of sony playstation 3s at the epfl in lausanne switzerland to change a normal ssl certificate issued by rapidssl into a working ca certificate for that issuer which could then be used to create other certificates that would appear to be legitimate and issued by rapidssl verisign the issuers of rapidssl certificates said they stopped issuing new certificates using md5 as their checksum algorithm for rapidssl once the vulnerability was announced md5 digests have been widely used in the software world to provide some assurance that a transferred file has arrived intact for example file servers often provide a pre computed md5 checksum for the files so that a user can compare the checksum of the downloaded file to it unix based operating systems include md5 sum utilities in their distribution packages whereas windows users use third party applications however now that it is easy to generate md5 collisions it is possible for the person who created the file to create a second file with the same checksum so this technique cannot protect against some forms of malicious tampering also in some cases the checksum cannot be trusted in which case md5 can only provide error checking functionality it will recognize a corrupt or incomplete download which becomes more likely when downloading larger files md5 is widely used to store passwords to mitigate against the vulnerabilities mentioned above one can add a salt to the passwords before hashing them some implementations may apply the hashing function more than once see key strengthening s denotes a left bit rotation by s places s varies for each operation denotes addition modulo 232 md5 processes a variable length message into a fixed length output of 128 bits the input message is broken up into chunks of 512 bit blocks the message is padded so that its length is divisible by 512 the padding works as follows first a single bit 1 is appended to the end of the message this is followed by as many zeros as are required to bring the length of the message up to 64 bits fewer than a multiple of 512 the remaining bits are filled up with a 64 bit integer representing the length of the original message in bits the main md5 algorithm operates on a 128 bit state divided into four 32 bit words denoted a b c and d these are initialized to certain fixed constants the main algorithm then operates on each 512 bit message block in turn each block modifying the state the processing of a message block consists of four similar stages termed rounds each round is composed of 16 similar operations based on a non linear function f modular addition and left rotation figure 1 illustrates one operation within a round there are four possible functions f a different one is used in each round denote the xor and or and not operations respectively pseudocode for the md5 algorithm follows note instead of the formulation from the original rfc 1321 shown the following may be used for improved efficiency the 128 bit md5 hashes are typically represented as a sequence of 32 hexadecimal digits the following demonstrates a 43 byte ascii input and the corresponding md5 hash even a small change in the message will result in a completely different hash due to the avalanche effect for example adding a period to the end of the sentence the hash of the zero length string is the nessie project test vectors for md5widely used functions md5 sha  other gost has 160 haval lm hash mdc 2 md2 md4 n hash radiogatn ripemd snefru tiger whirlpool crypt dessha 3 candidates cubehash fugue grstl jh keccak lane sandstorm md6 skeinmac algorithms daa cbc mac hmac omac cmac pmac umac poly1305 aesauthenticated encryption modes ccm cwc eax gcm ocbattacks hash collision birthday attack preimage attack rainbow table side channel attack brute force attackmisc avalanche effect hash collision merkle damgrd construction standardization cryptrec nessie nist hash function competition
windbg is a multipurpose debugger for microsoft windows distributed on the web by microsoft it can be used to debug user mode applications drivers and the operating system itself in kernel mode it is a gui application but has little in common with the more well known but less powerful visual studio debugger windbg can be used for debugging kernel mode memory dumps created after what is commonly called the blue screen of death which occurs when a bug check is issued it can also be used to debug user mode crash dumps this is known as post mortem debugging windbg also has the ability to automatically load debugging symbol files from a server by matching various criteria this is a very helpful and time saving alternative to creating a symbol tree for a debugging target environment if a private symbol server is configured the symbols can be correlated with the source code for the binary this eases the burden of debugging problems that have various versions of binaries installed on the debugging target by eliminating the need for finding and installing specific symbols version on the debug host microsoft has a public symbol server that has most of the public symbols for windows 2000 and later versions of windows recent versions of windbg have been distributed as part of the free debugging tools for windows suite which shares a common debugging engine between windbg and command line debuggers like kd cdb and ntsd this means that most commands will work in all alternative versions without modification allowing users to use the style of interface with which they are most comfortable windbg allows the loading of extension dlls that can augment the debugger s supported commands and allow for help in debugging specific scenarios for example displaying an msxml document given an ixmldomdocument or debugging the common language runtime these extensions are a large part of what makes windbg such a powerful debugger windbg is used by the microsoft windows product team to build windows and everything needed to debug windows is included in these extension dlls extension commands are always prefixed with while some extensions are used only inside microsoft most of them are part of the public debugging tools for windows package the extension model is documented in the help file included with the debugging tools for windows windbg allows debugging microsoft windows kernel running on a vmware or vpc virtual machine using a named pipe this can be achieved by using a virtual com port in the case of vmware the kdvmware extension adds native support for vmware debugging to windows kernel the most commonly used extension is analyze v which analyzes the current state of the program being debugged and the machine process state at the moment of crash or hang this extension is often able to debug the current problem in a completely automated fashion when used without any switches analyze simply returns the results of its analysis the v and vv give further details about that analysis 
core war is a programming game in which two or more battle programs compete for the control of the memory array redcode simulator virtual computer these battle programs are written in an abstract assembly language called redcode the object of the game is to cause all processes of the opposing program to terminate leaving your program in sole possession of the machine core war was in part inspired by a game called darwin written by victor a vyssotsky robert morris sr and m douglas mcilroy at the bell labs in the 1960s the word core in the name comes from magnetic core memory an obsolete random access memory technology the same usage may be seen in other computer jargon terms such as core dump the first description of the redcode language was published in march 1984 in core war guidelines by d g jones and a k dewdney the game was introduced to the public in may 1984 in an article written by dewdney in scientific american dewdney revisited core war in his computer recreations column in march 1985 and again in january 1987 the international core wars society was founded in 1985 one year after dewdney s original article the icws published new standards for the redcode language in 1986 and 1988 and proposed an update in 1994 that was never formally set as the new standard nonetheless the 1994 draft was commonly adopted and extended and forms the basis for the de facto standard for redcode today the icws was directed by mark clarkson william r buckley and jon newman currently the icws is defunct assembled icws 94 style redcodeboth redcode and the mars environment are designed to provide a simple and abstract platform without the complexity of actual computers and processors although redcode is meant to resemble an ordinary cisc assembly language it differs in many ways from real assembly each program can also have several active processes each having its own instruction pointer each program starts with only one process but others can be created with the spl instruction the processes for each program execute alternately so that the execution speed of each process is inversely proportional to the number of active processes the program has a process dies when it executes a dat instruction or performs a division by zero a program is considered dead when it has no more processes left warriors are commonly divided into a number of broad categories although actual warriors may often combine the behavior of two or more of these three of the common strategies are also known as paper scissors and stone since their performance against each other approximates that of their namesakes in the well known playground game based on the understanding of core war strategies a programmer can create a warrior to achieve certain goals the warrior is saved in ascii format with a red extension revolutionary ideas come once in a while most of the time however programmers utilize the published warriors to get some ideas using optimizers such as optimax or core step optimizer tools a more compact and efficient warrior can be created since redcode is turing complete warriors can also be generated by genetic algorithms or genetic programming programs that integrate this evolutionary technique are also known as core war evolvers several small and fast evolvers were introduced by core war community but were more focused on tiny or nano core war settings the latest evolver with significant success was microgp which produced nano and tiny koths nevertheless evolotionary strategy still need to prove its effectiveness in bigger hills 
the unix philosophy is a set of cultural norms and philosophical approaches to developing software based on the experience of leading developers of the unix operating system doug mcilroy the inventor of unix pipes and one of the founders of the unix tradition summarized the philosophy as follows this is the unix philosophy write programs that do one thing and do it well write programs to work together write programs to handle text streams because that is a universal interface this is usually severely abridged to do one thing and do it well rob pike offers the following five maxims of complexity in programming in notes on programming in c though they can be easily viewed as points of a unix philosophy pike s rules 1 and 2 restate donald knuth s famous maxim premature optimization is the root of all evil ken thompson rephrased pike s rules 3 and 4 as when in doubt use brute force rules 3 and 4 are instances of the design philosophy kiss rule 5 was previously stated by fred brooks in the mythical man month jon bentley s programming pearls also has a chapter on the same design principle rule 5 is often shortened to write stupid code that uses smart data and is an instance of the guideline if your data structures are good enough the algorithm to manipulate them should be trivial in 1994 mike gancarz drew on his own experience with unix as well as discussions with fellow programmers and people in other fields who depended on unix to produce the unix philosophy which sums it up in 9 paramount precepts the 10 lesser tenets are ones which are not universally agreed upon as part of the unix philosophy and in some cases are hotly debated richard p gabriel suggests that a key advantage of unix was that it embodied a design philosophy he termed worse is better in the worse is better design style simplicity of both the interface and the implementation is more important than any other attribute of the system including correctness consistency and completeness gabriel argues that this design style has key evolutionary advantages though he questions the quality of some results for example in the early days unix was a monolithic kernel if a signal was delivered to a process while it was blocked on a long term i o in the kernel then what should be done should the signal be delayed possibly for a long time while the i o completed the signal handler could not be executed when the process was in kernel mode with sensitive kernel data on the stack should the kernel back out the system call and store it for replay and restart later assuming that the signal handler completes successfully in these cases ken thompson and dennis ritchie favored simplicity over perfection the unix system would occasionally return early from a system call with an error stating that it had done nothing the interrupted system call an error number 4 in today s systems of course the call had been aborted in order to call the signal handler this could only happen for a handful of long running system calls i e read write open select etc on the plus side this made the i o system many times simpler to design and understand the vast majority of user programs were never affected because they didn t handle or experience signals other than sigint c and would die right away if one was raised for the few other programs things like shells or text editors that respond to job control keypresses small wrappers could be added to system calls so as to retry the call right away if this eintr error was raised problem solved in a simple way eric s raymond in his book the art of unix programming summarizes the unix philosophy as the widely used kiss principle of keep it simple stupid he also provides a series of design rules it is controversial as to whether the free software foundation s gnu work alikes of standard unix programs follow the unix philosophy or misunderstand it certainly at least some unix old timers claim the latter since gnu tools are often substantially larger and more featureful than their unix equivalents already in 1983 brian kernighan and rob pike wrote a paper titled program design in the unix environment and gave a presentation on unix style or cat v considered harmful criticizing bsd s expansion of the functionality of basic unix tools like cat this trend only became much more significant with the advent of gnu and commercial unix variants and it is common for a single program to provide numerous features based on how it is called 
donald ervin knuth is a renowned computer scientist and professor emeritus of the art of computer programming at stanford university author of the seminal multi volume work the art of computer programming knuth has been called the father of the analysis of algorithms contributing to the development of and systematizing formal mathematical techniques for the rigorous analysis of the computational complexity of algorithms and in the process popularizing asymptotic notation in addition to fundamental contributions in several branches of theoretical computer science knuth is the creator of the tex computer typesetting system the related metafont font definition language and rendering system and the computer modern family of typefaces a prolific writer and scholar knuth created the web cweb computer programming systems designed to encourage and facilitate literate programming and designed the mmix instruction set architecture knuth was born in milwaukee wisconsin where his father owned a small printing business and taught bookkeeping at milwaukee lutheran high school which he attended he was an excellent student earning achievement awards he applied his intelligence in unconventional ways winning a contest when he was in eighth grade by finding over 4 500 words that could be formed from the letters in ziegler s giant bar this won him a television set for his school and a candy bar for everyone in his class knuth had a difficult time choosing physics over music as his major at case institute of technology he then switched from physics to mathematics and in 1960 he received his bachelor of science degree simultaneously receiving his master of science degree by a special award of the faculty who considered his work outstanding at case he managed the basketball team and applied his talents by constructing a formula for the value of each player this novel approach was covered by newsweek and by walter cronkite on the cbs television network while doing graduate studies knuth worked as a consultant writing compilers for different computers in 1963 he earned a ph d in mathematics from the california institute of technology where he became a professor and began work on the art of computer programming originally planned to be a single book and then planned as a six and then seven volume series in 1968 he published the first volume that same year he joined the faculty of stanford university having turned down a job offer from the national security agency in 1971 knuth was the recipient of the first acm grace murray hopper award he has received various other awards including the turing award the national medal of science the john von neumann medal and the kyoto prize after producing the third volume of his series in 1976 he expressed such frustration with the nascent state of the then newly developed electronic publishing tools that he took time out to work on typesetting and created the tex and metafont tools in recognition of knuth s contributions to the field of computer science in 1990 he was awarded the one of a kind academic title of professor of the art of computer programming which has since been revised to professor emeritus of the art of computer programming in 1992 he became an associate of the french academy of sciences also that year he retired from regular research and teaching at stanford university in order to finish the art of computer programming in 2003 he was elected as a foreign member of the royal society as of 2004 the first three volumes of his series have been re issued and knuth is currently working on volume four excerpts of which are released periodically on his website meanwhile knuth gives informal lectures a few times a year at stanford university which he calls computer musings he is also a visiting professor at the oxford university computing laboratory in the united kingdom in addition to his writings on computer science knuth a devout lutheran is also the author of 3 16 bible texts illuminated isbn 0 89579 252 4 in which he attempts to examine the bible by a process of stratified sampling namely an analysis of chapter 3 verse 16 of each book each verse is accompanied by a rendering in calligraphic art contributed by a group of calligraphers under the leadership of hermann zapf he is also the author of surreal numbers isbn 0 201 03812 9 a mathematical novelette on john conway s set theory construction of an alternate system of numbers instead of simply explaining the subject the book seeks to show the development of the mathematics knuth wanted the book to prepare students for doing original creative research on january 1 1990 knuth announced to his colleagues that he would no longer have an e mail address so that he might concentrate on his work in 2006 knuth was diagnosed with prostate cancer he underwent surgery in december that year and started a little bit of radiation therapy as a precaution but the prognosis looks pretty good as he reported in his video autobiography knuth is known for his professional humor a short list of his works perlis  wilkes  hamming  minsky  wilkinson  mccarthy  dijkstra  bachman  knuth  newell simon  rabin scott  backus  floyd  iverson  hoare  codd  cook  thompson ritchie  wirth  karp  hopcroft tarjan  cocke  sutherland  kahan  corbat  milner  lampson  hartmanis stearns  feigenbaum reddy  blum  pnueli  engelbart  gray  brooks  yao  ole johan dahl kristen nygaard  ron rivest adi shamir leonard adleman  alan kay  vint cerf bob kahn  peter naur  frances e allen  edmund m clarke e allen emerson joseph sifakis  barbara liskov 
recursion in mathematics and computer science is a method of defining functions in which the function being defined is applied within its own definition the term is also used more generally to describe a process of repeating objects in a self similar way for instance when the surfaces of two mirrors are almost parallel with each other the nested images that occur are a form of infinite recursion in mathematics and computer science a class of objects or methods defined by a simple base case and rules to reduce all other cases toward the base case for example the following is a recursive definition of a person s ancestors the fibonacci sequence is a classic example of recursion although many mathematical functions can be expressed recursively the overhead of actually applying the recursive definition may be prohibitive for example a convenient mental model is that a recursive definition defines objects in terms of previously defined objects of the class to define for example how do you move a stack of 100 boxes answer you move one box remember where you put it and then solve the smaller problem how do you move a stack of 99 boxes eventually you re left with the problem of how to move a single box which you know how to do definitions such as these are often found in mathematics for example the formal definition of natural numbers in set theory is 1 is a natural number and each natural number has a successor which is also a natural number here is another perhaps simpler way to understand recursive processes a more humorous illustration goes in order to understand recursion one must first understand recursion or perhaps more accurate is the following from andrew plotkin if you already know what recursion is just remember the answer otherwise find someone who is standing closer to douglas hofstadter than you are then ask him or her what recursion is examples of mathematical objects often defined recursively are functions sets and especially fractals the use of recursion in linguistics and the use of recursion in general dates back to the ancient indian linguist pini in the 5th century bc who made use of recursion in his grammar rules of sanskrit linguist noam chomsky theorizes that unlimited extension of a language such as english is possible only by the recursive device of embedding sentences in sentences thus a chatty person may say dorothy who met the wicked witch of the west in munchkin land where her wicked witch sister was killed liquidated her with a pail of water clearly two simple sentences dorothy met the wicked witch of the west in munchkin land and her sister was killed in munchkin land can be embedded in a third sentence dorothy liquidated her with a pail of water to obtain a very verbose sentence however if dorothy met the wicked witch can be analyzed as a simple sentence then the recursive sentence she lived in the house jack built could be analyzed that way too if jack built is analyzed as an adjective jack built that applies to the house in the same way wicked applies to the witch she lived in the jack built house is unusual perhaps poetic sounding but it is not clearly wrong the idea that recursion is the essential property that enables language is challenged by linguist daniel everett in his work cultural constraints on grammar and cognition in pirah another look at the design features of human language in which he hypothesizes that cultural factors made recursion unnecessary in the development of the pirah language this concept challenges chomsky s idea that recursion is the only trait which differentiates human and animal communication and is currently under intense debate recursion in linguistics enables discrete infinity by embedding phrases within phrases of the same type in a hierarchical structure without recursion language does not have discrete infinity and cannot embed sentences into infinity everett contests that language must have discrete infinity and that the piraha language which he claims lacks recursion is in fact finite he likens it to the finite game of chess which has a finite number of moves but is nevertheless very productive with novel moves being discovered throughout history recursion is the process a procedure goes through when one of the steps of the procedure involves rerunning the procedure a procedure that goes through recursion is said to be recursive something is also said to be recursive when it is the result of a recursive procedure to understand recursion one must recognize the distinction between a procedure and the running of a procedure a procedure is a set of steps that are to be taken based on a set of rules the running of a procedure involves actually following the rules and performing the steps an analogy might be that a procedure is like a menu in that it is the possible steps while running a procedure is actually choosing the courses for the meal from the menu a procedure is recursive if one of the steps that makes up the procedure calls for a new running of the procedure therefore a recursive four course meal would be a meal in which one of the choices of appetizer salad entre or dessert was an entire meal unto itself so a recursive meal might be potato skins baby greens salad chicken parmesan and for dessert a four course meal consisting of crab cakes caesar salad for an entre a four course meal and chocolate cake for dessert so on until each of the meals within the meals is completed a recursive procedure must complete every one of its steps even if a new running is called in one of its steps each running must run through the remaining steps what this means is that even if the salad is an entire four course meal unto itself you still have to eat your entre and dessert a common joke is the following definition of recursion another example occurs in kernighan and ritchie s the c programming language the following index entry is found on page 269 this is a parody on references in dictionaries which in some cases may lead to circular definitions among related words jokes often have an element of wisdom and also an element of misunderstanding this one is also the second shortest possible example of an erroneous recursive definition of an object the error being the absence of the termination condition newcomers to recursion are often bewildered by its apparent circularity until they learn to appreciate that a termination condition is key a variation is which actually does terminate as soon as the reader gets it other examples are recursive acronyms such as gnu php or hurd the canonical example of a recursively defined set is given by the natural numbers another interesting example is the set of all true reachable propositions in an axiomatic system this set is called true reachable propositions because in non constructive approaches to the foundations of mathematics the set of true propositions is larger than the set recursively constructed from the axioms and rules of inference see also gdel s incompleteness theorems a function may be partly defined in terms of itself a familiar example is the fibonacci number sequence f f f for such a definition to be useful it must lead to values which are non recursively defined in this case f 0 and f 1 a famous recursive function is the ackermann function which unlike the fibonacci sequence cannot be expressed without recursion the standard way to define new systems of mathematics or logic is to define objects then define operations on these these are the base cases after this all valid computations in the system are defined with rules for assembling these in this way if the base cases and rules are all proven to be calculable then any formula in the mathematical system will also be calculable this sounds unexciting but this type of proof is the normal way to prove that a calculation is impossible this can often save a lot of time for example this type of proof was used to prove that the area of a circle is not a simple ratio of its diameter and that no angle can be trisected with compass and straightedge both puzzles that fascinated the ancients dynamic programming is an approach to optimization which restates a multiperiod or multistep optimization problem in recursive form the key result in dynamic programming is the bellman equation which writes the value of the optimization problem at an earlier time in terms of its value at a later time a common method of simplification is to divide a problem into subproblems of the same type as a computer programming technique this is called divide and conquer and is key to the design of many important algorithms as well as being a fundamental part of dynamic programming recursion in computer programming is exemplified when a function is defined in terms of itself one example application of recursion is in parsers for programming languages the great advantage of recursion is that an infinite set of possible sentences designs or other data can be defined parsed or produced by a finite computer program recurrence relations are equations to define one or more sequences recursively some specific kinds of recurrence relation can be solved to obtain a non recursive definition a classic example of recursion is the definition of the factorial function given here in c code the function calls itself recursively on a smaller version of the input and multiplies the result of the recursive call by n until reaching the base case analogously to the mathematical definition of factorial use of recursion in an algorithm has both advantages and disadvantages the main advantage is usually simplicity the main disadvantage is often that the algorithm may require large amounts of memory if the depth of the recursion is very large it has been claimed that recursive algorithms are easier to understand because the code is shorter and is closer to a mathematical definition as seen in these factorial examples it is often possible to replace a recursive call with a simple loop as the following example of factorial shows it should be noted that on most cpus the above examples give correct results only for small values of n due to arithmetic overflow an example of a recursive algorithm is a procedure that processes all the nodes of a tree data structure to process the whole tree the procedure is called with a root node representing the tree as an initial parameter the procedure calls itself recursively on all child nodes of the given node until reaching the base case that is a node with no child nodes a tree data structure itself can be defined recursively like this in set theory this is a theorem guaranteeing that recursively defined functions exist given a set x an element a of x and a function the theorem states that there is a unique function such thatfor any natural number n take two functions f and g of domain n and codomain a such that where a is an element of a we want to prove that f g two functions are equal if they some common recurrence relations are 
in computer science statistical computing and bioinformatics the baum welch algorithm is used to find the unknown parameters of a hidden markov model it makes use of the forward backward algorithm and is named for leonard e baum and lloyd r welch the baum welch algorithm is a generalized expectation maximization algorithm it can compute maximum likelihood estimates and posterior mode estimates for the parameters of an hmm when given only emissions as training data the algorithm has two steps the algorithm was introduced in the paper the shannon lecture by welch which speaks to how the algorithm can be implemented efficiently the path counting algorithm an alternative to the baum welch algorithm 
an example plotting bessel functions and finding their local maxima scipy is an open source library of algorithms and mathematical tools for the python programming language scipy contains modules for optimization linear algebra integration interpolation special functions fft signal and image processing ode solvers and other tasks common in science and engineering it has a similar audience to applications as matlab and scilab scipy is currently distributed under the bsd license and its development is sponsored by enthought the basic data structure in scipy is a multidimensional array provided by the numpy module older versions of scipy used numeric as an array type which is now deprecated in favor of the newer numpy array code available subpackages scipy s core feature set is extended by many other dedicated software tools for example 
