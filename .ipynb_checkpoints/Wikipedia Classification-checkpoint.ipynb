{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Binary Articles Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : Antoine Mougeot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a project I did which goal was to perform binary classification on wikipedia articles. The purpose of this project was to learn more about classic Machine Learning classifiers and text classification.\n",
    "\n",
    "If you want to execute the following code, you should make sure to unzip the 2 files that can be download here : http://nlp.uned.es/social-tagging/wiki10+/ in the folder where this notebook is located. One folder contains more than 20,000 wikipedia articles and is a bit heavy (amost 300 Mo).\n",
    "\n",
    "I included the dataset I created and used with the notebook in the `/data/` folder but feel free to create another one. The execution of this notebook will create a new dataset in `/new_data/` and do the classifying. As articles are randomly chosen, result may change a bit from what I got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree \n",
    "import collections\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from shutil import copyfile\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Create the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, I chose to use the Wiki10+ dataset, accessible here : http://nlp.uned.es/social-tagging/wiki10+/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20762\n"
     ]
    }
   ],
   "source": [
    "tree = etree.parse('./tag-data.xml')\n",
    "titles = []\n",
    "for article in tree.xpath(\"/articles/article/title\"):\n",
    "    titles.append(article.text)\n",
    "    \n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `tag-data.xml` file provided with the dataset, we observe that there are 20,764 articles in it.\n",
    "\n",
    "As it is explained on UNED's webpage, the wikipedia articles from the dataset were tagged by users of the social bookmarking site Delicious. I decided to use those tags to find the two categories I would use to create my own dataset.\n",
    "\n",
    "### Choosing 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "for elem in tree.findall('article'):\n",
    "    for elem1 in elem.findall('tags'):\n",
    "        for elem2 in elem1.findall('tag'):\n",
    "            tags.append(elem2.find('name').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457708\n",
      "99162\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))\n",
    "print(len(set(tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found out that users used almiost 100,000 different tags to classify those articles.\n",
    "\n",
    "I went on to see which were associated to more than 1,000 articles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia 16715\n",
      "wiki 8681\n",
      "reference 5914\n",
      "history 3829\n",
      "research 2980\n",
      "science 2610\n",
      "interesting 2085\n",
      "programming 2062\n",
      "article 1944\n",
      "people 1901\n",
      "philosophy 1856\n",
      "culture 1803\n",
      "art 1627\n",
      "politics 1554\n",
      "software 1550\n",
      "design 1492\n",
      "language 1390\n",
      "books 1354\n",
      "technology 1338\n",
      "psychology 1293\n",
      "music 1286\n",
      "development 1241\n",
      "math 1238\n",
      "theory 1157\n",
      "religion 1149\n",
      "computer 1132\n",
      "literature 1089\n",
      "business 1058\n",
      "education 1037\n",
      "writing 1003\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(tags)\n",
    "\n",
    "for i,j in counter.most_common():\n",
    "    if (j>= 1000):\n",
    "        print (i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that point on, it would have been simple to generalize the code so that we could study any of those 2 tags. As asked, I chose 2 categories : **music** and **programming**.\n",
    "\n",
    "### Creating my dataset\n",
    "\n",
    "The next step is to get 2 sets of 1000 articles for the categories I chose..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "programming = []\n",
    "music = []\n",
    "for elem in tree.findall('article'):\n",
    "    for elem1 in elem.findall('tags'):\n",
    "        for elem2 in elem1.findall('tag'):\n",
    "            if elem2.find('name').text == 'programming':\n",
    "                programming.append((elem.find('hash').text, elem.find('title').text))\n",
    "            if elem2.find('name').text == 'music':\n",
    "                music.append((elem.find('hash').text, elem.find('title').text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2062\n",
      "1286\n"
     ]
    }
   ],
   "source": [
    "print(len(programming))\n",
    "print(len(music))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took the precaution to take out articles that were tagged as both music and programming. Out of curiosity, I looked at which articles were concerned :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambisonics\n",
      "Digital rights management\n",
      "foobar\n",
      "MP3\n",
      "Real Time Streaming Protocol\n",
      "Pure Data\n",
      "Digital Audio Access Protocol\n",
      "Synchronized Multimedia Integration Language\n",
      "Markov chain\n",
      "Reaktor\n",
      "reactable\n",
      "Ableton Live\n",
      "OpenSound Control\n",
      "ChucK\n",
      "Justin Frankel\n",
      "Fast Fourier transform\n",
      "Nerdcore hip hop\n",
      "Demoscene\n",
      "Code page 437\n",
      "Player Piano\n",
      "Hidden Markov model\n",
      "Stochastic\n",
      "Csound\n",
      "Digital signal processing\n",
      "Parsons code\n",
      "Music and mathematics\n",
      "SuperCollider\n",
      "ID3\n",
      "Jonathan Coulton\n",
      "Max (software)\n",
      "Player piano\n",
      "WAV\n",
      "Media Transfer Protocol\n"
     ]
    }
   ],
   "source": [
    "for i in programming[:]:\n",
    "    if i in music:\n",
    "        print(i[1])\n",
    "        programming.remove(i)\n",
    "        music.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if it could be expected, it was very interesting to see that articles such as MP3, WAV, Ableton Live or Fast Fourier Transform were concerned !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029\n",
      "1253\n"
     ]
    }
   ],
   "source": [
    "programming_hash = [i[0] for i in programming]\n",
    "music_hash = [i[0] for i in music]\n",
    "\n",
    "print(len(programming_hash))\n",
    "print(len(music_hash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that even after removing the articles in common, both sets contained more than 1000 articles, I went on to create my dataset with 1000 articles for each category :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle(programming_hash)\n",
    "shuffle(music_hash)\n",
    "programming_1000 = []\n",
    "music_1000 = []\n",
    "for i in range(1000):\n",
    "    programming_1000.append(programming_hash[i])\n",
    "    music_1000.append(music_hash[i])\n",
    "    \n",
    "#print(len(programming_1000))\n",
    "#print(len(music_1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`programming_1000` and `music_1000` are **2 lists of 1000 hash of Wikipedia articles**, as the files given in the initial dataset were named thanks to those hash, I can now create the dataset that I will use for the rest of this exercise :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"new_data\"):\n",
    "    shutil.rmtree(\"new_data\")\n",
    "\n",
    "if not os.path.exists(\"new_data\"):\n",
    "    os.makedirs(\"new_data\")\n",
    "    \n",
    "if not os.path.exists('new_data/programming'):\n",
    "    os.makedirs('new_data/programming')\n",
    "    \n",
    "if not os.path.exists('new_data/music'):\n",
    "    os.makedirs('new_data/music')\n",
    "\n",
    "for hash_ in programming_1000:\n",
    "    c = ('./documents/%s' % hash_)\n",
    "    p = ('./new_data/programming/%s' % hash_)\n",
    "    copyfile(c, p)\n",
    "        \n",
    "for hash_ in music_1000:\n",
    "    c = ('./documents/%s' % hash_)\n",
    "    p = ('./new_data/music/%s' % hash_)\n",
    "    copyfile(c, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 . Processing the Data\n",
    "\n",
    "Wikipedia articles are encoded as HTML files, in order to get rid of the HTML structure and keep only the content of the article, I used `BeautifulSoup` to clean my dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_soup(file):\n",
    "    \"\"\"\n",
    "    Input : A wikipedia article from our dataset\n",
    "    Output : The cleaned wikipedia article as a string \n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(file), \"lxml\")\n",
    "    for m in soup.find_all('a'):\n",
    "        m.replaceWithChildren()\n",
    "    text =[''.join(s.findAll(text=True))for s in soup.findAll('p')]\n",
    "    flat = reduce(lambda x,y: x+y, text)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can uncomment the following lines to see how a clean text looks like :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test = './testfile'\n",
    "#test_cleaned = clean_soup(test)\n",
    "#test_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to construct our features, we want to have our corpus of 1000 clean articles together in one list. `getcorpus` does that for us :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getcorpus(path):\n",
    "    \"\"\"\n",
    "    Input : A folder where the articles of a category are located\n",
    "    Output : A list where every element is the cleaned text for an article\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for filename in os.listdir(path):\n",
    "        vect = clean_soup(path + '/' + filename)\n",
    "        corpus.append(vect)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our 2 corpus as 2 lists, it is convenient to save them for later use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_corpus(path1, path2):\n",
    "    music_corpus = getcorpus(path1)\n",
    "    pickle.dump(music_corpus, open( './new_data/music_corpus.p', 'wb' ))\n",
    "    \n",
    "    programming_corpus = getcorpus(path2)\n",
    "    pickle.dump(programming_corpus, open( './new_data/programming_corpus.p', 'wb' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_music = './new_data/music'\n",
    "dir_programming = './new_data/programming'\n",
    "\n",
    "save_corpus(dir_music, dir_programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`music_corpus.p` and `programming_corpus.p` contain our two lists, saved with pickle. We can now use them to create our features and do the classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 . Choosing the features, genearating train and test sets\n",
    "\n",
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have two lists that contain our two corpus. The text has been processed as well as possible. The next step is to get relevant features from those articles, to do so I chose to use **tf-idf** statistic thanks too the `sklearn` library available for python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "music = pickle.load(open( './new_data/music_corpus.p', 'rb' ))\n",
    "programming = pickle.load(open( './new_data/programming_corpus.p', 'rb' ))\n",
    "\n",
    "corpus = music + programming\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`corpus` is a list of 2000 articles, the first 1000 are about music, the other 1000 about programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = lambda doc: doc.split(\" \")\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn_representation` is a sparse matrix where the lines are the 2000 wikipedia articles  and the columns are the weights for each feature according to tf-idf.\n",
    "\n",
    "We can add a last column to this matrix that will be our labels to compare for testing. The last column will be made of 1000 zeroes (*label for music*) and 1000 ones (*label for programming*) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listofzeros = [0] * 1000\n",
    "listofones = [1] * 1000\n",
    "\n",
    "label = listofzeros + listofones\n",
    "\n",
    "matrix = hstack((sklearn_representation,np.array(label)[:,None])).A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test sets\n",
    "\n",
    "Now we can create the train and test set in the following way :\n",
    "- Train is made of the elements 100 to 1899 (included) and shuffled\n",
    "- Test is made of the elements 0 to 99 (included) and 1900 to 1999 (included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = matrix[100:1900]\n",
    "np.random.shuffle(train)\n",
    "\n",
    "test_music = matrix[0:100]\n",
    "test_programming = matrix[1900:2000]\n",
    "\n",
    "test = np.concatenate((test_music, test_programming), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From test and train, we can then get `X_train`, `X_test`, `y_train`, `y_test` that we will use for the classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unlabeled = train[:,:-1]\n",
    "train_labels = train[:, -1]\n",
    "\n",
    "test_unlabeled = test[:,:-1]\n",
    "test_labels = test[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_unlabeled, test_unlabeled, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 . Classifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(name, clf, X_train, X_test, y_train, y_test):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    acc = 100*score\n",
    "    good = int(score*200)\n",
    "\n",
    "    stop = datetime.datetime.now()\n",
    "    length = int((stop-start).total_seconds())\n",
    "\n",
    "    print(\"Using {} as a classifier, the accuracy was {}%, meaning that {} articles out of 200 were classified correctly. \\\n",
    "          \\nThe fitting with this classifier took {} seconds\".format(name, acc, good, length))\n",
    "\n",
    "    return score, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify my 200 articles, I decided to use the 5 different classifiers :\n",
    "- Nearest Neighbors\n",
    "- Naive Bayes\n",
    "- (Radial Basis Function) Support Vector Machine\n",
    "- Random Forest\n",
    "- A Neural Network using Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nearest Neighbors as a classifier, the accuracy was 97.5%, meaning that 195 articles out of 200 were classified correctly.           \n",
      "The fitting with this classifier took 407 seconds\n",
      "Using Naive Bayes as a classifier, the accuracy was 93.5%, meaning that 187 articles out of 200 were classified correctly.           \n",
      "The fitting with this classifier took 51 seconds\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\",\n",
    "        \"Naive Bayes\",\n",
    "        \"RBF SVM\",\n",
    "        \"Random Forest\",\n",
    "        \"Neural Network\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    GaussianNB(),\n",
    "    SVC(gamma=2, C=1),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=20, max_features=\"auto\"),\n",
    "    MLPClassifier(alpha=1)]\n",
    "    \n",
    "scores = []\n",
    "lengths = []\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    score, length = train(name, clf, X_train, X_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "    lengths.append(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Using Nearest Neighbors as a classifier, the accuracy was 98.5%, meaning that 197 articles out of 200 were classified correctly.      The fitting with this classifier took 195 seconds\n",
    "Using Naive Bayes as a classifier, the accuracy was 91.5%, meaning that 183 articles out of 200 were classified correctly.           \n",
    "The fitting with this classifier took 48 seconds\n",
    "Using RBF SVM as a classifier, the accuracy was 96.5%, meaning that 193 articles out of 200 were classified correctly.           \n",
    "The fitting with this classifier took 1765 seconds\n",
    "Using Random Forest as a classifier, the accuracy was 92.0%, meaning that 184 articles out of 200 were classified correctly.           \n",
    "The fitting with this classifier took 12 seconds\n",
    "Using Neural Network as a classifier, the accuracy was 95.0%, meaning that 190 articles out of 200 were classified correctly.      \n",
    "The fitting with this classifier took 2615 seconds`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be considered as the end of the project but I tried to go further in my analysis : \n",
    "\n",
    "## 5 . Interpretations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I asked myself the two following question after getting those results :\n",
    "- Why do some classifier perform better than the other ?\n",
    "- Where do the running time difference came from ?\n",
    "\n",
    "To answer the first question, I tried to see if certain algorithms were simply considered better than other to perform binary classification. Most answers I came accross mentionned Nearest Neighbor, SVM and Random Forest. This was rather consistent with my result, except that I found no mention of Multilayer Perceptron, while the results with it were better than with Random forest.\n",
    "\n",
    "Next, to get an explanation for the running time, I looked at the algorithmic complexity of the classifiers I used. I mainly found my answers on sklearn's documentation as well as on papers :\n",
    "- Nearest Neighbor has a complexity in $O[D* log(N)]$ or $O[D * N]$ depeding on the algorithms involved. By default, the algorithm was selected automaticaly in my implementation. $D$ is the number or features and $N$ is the number of articles in the training set.\n",
    "- Naive Bayes has a complexity of $O[ N ]$ where $N$ is the number of training examples. It is the simplest algorithm I used.\n",
    "- SVM with RBF has a complexity of $O [ N^{2} * D ]$ where $D$ is the number or features and $N$ is the number of articles in the training set.\n",
    "- Random Forest has a complexity of $O [ n_{tree} * N * log(N) ]$ where $N$ is the size of the training set and $n_{tree}$ is the number of tree in the forest (20 for me). \n",
    "- MLP Neural Network's back-propagation algorithm has a complexity in $O[N * D * h^{k} * o * i]$ with $h$ neurons, $k$ hidden layers, $o$ outputs neurons and $i$ iterations.\n",
    "\n",
    "I may have made mistakes but those results explain a bit the running times I got. One clear trend can be observed : Classifiers which complexity depends on the number of features have a much longer running time than the other. It could be expected as I had a huge number of features (almost 300,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 . Using words embedding instead of bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "Once again, I am still learning about all these notions but from what I got, my understanding is that Nearest Neighbor, Naive bayes and Random Forest worked well because they are simple solutions to what is a fairly simple problem (binary classification). SVM is more complex but is performing very well when it comes to binary classification, even if it takes longer.\n",
    "\n",
    "For the Neural Network, what I assumed going forward with my research that bag of words is a model that is too simple for it. My hypothesis is that it would perform better using words embedding (both against itself using bag of words and against other classifiers using words embedding too). The argument for that being that our Neural Network would prefer to work in a space that is more complex and \"less sparse\".\n",
    "\n",
    "I descided to redo the work I did using word embedding instead of bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Text cleaning\n",
    "\n",
    "As I had to create a new training matrix, I decided to use this opportunity to clean my articles a bit better. Indeed, there were still some noise that could have been cleaned sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "music = pickle.load(open( './data/music_corpus.p', 'rb' ))\n",
    "programming = pickle.load(open( './data/programming_corpus.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def better_cleaning(list):\n",
    "    clean_list = []\n",
    "    for i in list:\n",
    "        i = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", i)\n",
    "        i = re.sub(r'[^\\w\\s]',' ', i)\n",
    "        i = re.sub(\"   \", \" \", i)\n",
    "        i = re.sub(\"  \", \" \", i)\n",
    "        i = i.lower()\n",
    "        clean_list.append(i)\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_music = better_cleaning(music)\n",
    "clean_programming = better_cleaning(programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save for later :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(clean_music, open( './data/clean_music.p', 'wb' ))\n",
    "pickle.dump(clean_programming, open( './data/clean_programming.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reopen :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "music = pickle.load(open( './data/clean_music.p', 'rb' ))\n",
    "programming = pickle.load(open( './data/clean_programming.p', 'rb' ))\n",
    "\n",
    "corpus = music + programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words embedding using doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use `gensim`'s implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this : http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "# numpy\n",
    "import numpy\n",
    "# random\n",
    "from random import shuffle\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert our dataset so that each article is a single line in a text document :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_file = open(\"music_test.txt\", \"w\")\n",
    "for i in range(0,100):\n",
    "    text_file.write(corpus[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"music_train.txt\", \"w\")\n",
    "for i in range(100,1000):\n",
    "    text_file.write(corpus[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"programming_train.txt\", \"w\")\n",
    "for i in range(1000,1900):\n",
    "    text_file.write(corpus[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"programming_test.txt\", \"w\")\n",
    "for i in range(1900,2000):\n",
    "    text_file.write(corpus[i] + '\\n')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec agregates all the words in a sentence into a vector. \n",
    "\n",
    "So we have to format sentences into `[['word1', 'word2', 'word3', ..., 'lastword'], ['label1']]`\n",
    "\n",
    "To do so, we need to implement our own version of `LabeledLineSentence` to obtain a set of `LabeledSentence`s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can enter ou data as a `LabeledLineSentence` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sources = {'music_test.txt':'music_test', 'music_train.txt':'music_train', 'programming_train.txt':'programming_train', 'programming_test.txt':'programming_test'}\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to build the vocabulary to our model. In order to get the best performence possible, we tried a set of different hyperparameters for `min_count`, `size`, `sample` and `dm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our models are set, we test them using the same classifiers as before :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretations\n",
    "\n",
    "The best model on average is model 9. It has `min_count = 1`, `size = 100`, `sample = 0.0001` and `dm = 1`.\n",
    "However, results were pretty stable, mainly between 0.55 and 0.6, which is aweful for binary classification.\n",
    "\n",
    "Tuning the hyperparameters turned out to be not that useful but it seemed like a good thing to try.\n",
    "\n",
    "We choose to go on with model 9 to see what the model looks like and how it performs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=0.0001, negative=5, workers=8, dm=1)\n",
    "model.build_vocab(sentences.to_array())\n",
    "for epoch in range(10):\n",
    "    model.train(sentences.sentences_perm(), total_examples=model.corpus_count, epochs=model.iter)\n",
    "    print('.')\n",
    "model.save('./chosenmodel.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./chosenmodel.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the model has understood the words we fed him, I looked at the most similar words to a few words I thought about, about music, about programming and some more general words :\n",
    "\n",
    "It seems to have understood many words pretty well. The model is not perfect but some results are quite satisfying. Here are some of the most similar words for a few words I chose :\n",
    "- day : night, weekend, morning, wednesday, yeay\n",
    "- large : small, huge, considerable, immense\n",
    "- small : large, simple, smaller, limited\n",
    "- name : nickname, homonym, surname, title, names\n",
    "\n",
    "\n",
    "- rock : punk, rocker, pop, rocking, ... (here, every word in the top ten is really close to rock !)\n",
    "- intrument : instruments, accordion, stringed, violin, ... (here also !)\n",
    "- ...\n",
    "\n",
    "\n",
    "- internet : network, limewire, networked, networking, ...\n",
    "- programming : imperative,languages, language, procedural, ...\n",
    "- ...\n",
    "\n",
    "The number of words inside the top ten most similar that are actually similar to a word vary. **However, you can see from the results below that the performances are notably better for words related to music and programming**. \n",
    "My explanation for that is that those words appeared more often and/or in a well defined context, which allowed doc2vec to get a better understanding of what it meant.\n",
    "On the other hand, when you look at other words like man, that is surely used a lot but in many different context, doc2vec has a harder time getting its signification.\n",
    "\n",
    "You can find the complete top ten for a few words below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('night', 0.5434675216674805),\n",
       " ('weekend', 0.526370108127594),\n",
       " ('konya', 0.5025175213813782),\n",
       " ('morning', 0.4978428781032562),\n",
       " ('ozian', 0.4972427189350128),\n",
       " ('minimaldb', 0.4670683741569519),\n",
       " ('wednesday', 0.44702115654945374),\n",
       " ('year', 0.44633448123931885),\n",
       " ('jhcore', 0.44280844926834106),\n",
       " ('lambdacore', 0.44060632586479187)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('small', 0.7566866278648376),\n",
       " ('huge', 0.4905131459236145),\n",
       " ('conglomerative', 0.48618534207344055),\n",
       " ('unlimited', 0.45853424072265625),\n",
       " ('copious', 0.4576635956764221),\n",
       " ('considerable', 0.45141348242759705),\n",
       " ('immense', 0.4485107362270355),\n",
       " ('strong', 0.4456759989261627),\n",
       " ('wechsler', 0.44471046328544617),\n",
       " ('lettering', 0.443570077419281)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('large', 0.7566866278648376),\n",
       " ('comparatively', 0.47882896661758423),\n",
       " ('philanderer', 0.45251065492630005),\n",
       " ('huge', 0.4516172409057617),\n",
       " ('simple', 0.4473158121109009),\n",
       " ('smaller', 0.4468696117401123),\n",
       " ('limited', 0.4350641369819641),\n",
       " ('ce20', 0.4332190454006195),\n",
       " ('conglomerative', 0.43264198303222656),\n",
       " ('organistrum', 0.4276430010795593)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nickname', 0.6126840114593506),\n",
       " ('homonym', 0.5997042059898376),\n",
       " ('pun', 0.5949548482894897),\n",
       " ('surname', 0.5589694976806641),\n",
       " ('title', 0.5577302575111389),\n",
       " ('anagram', 0.5569340586662292),\n",
       " ('names', 0.5463294386863708),\n",
       " ('paramour', 0.5361846685409546),\n",
       " ('ucckpl', 0.5287989377975464),\n",
       " ('apalachen', 0.5209699273109436)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('punk', 0.5610576868057251),\n",
       " ('rocker', 0.5556276440620422),\n",
       " ('pop', 0.5382617712020874),\n",
       " ('rocking', 0.5321325659751892),\n",
       " ('rockers', 0.5309041738510132),\n",
       " ('indie', 0.5242467522621155),\n",
       " ('roll', 0.5126678943634033),\n",
       " ('uplifting', 0.49986982345581055),\n",
       " ('glam', 0.4986695945262909),\n",
       " ('psychedelic', 0.49628734588623047)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('instruments', 0.5968011617660522),\n",
       " ('accordion', 0.5297654271125793),\n",
       " ('stringed', 0.5279640555381775),\n",
       " ('plucked', 0.5249077677726746),\n",
       " ('flute', 0.5213930010795593),\n",
       " ('violin', 0.5204111337661743),\n",
       " ('keyboard', 0.5151673555374146),\n",
       " ('woodwind', 0.4959862530231476),\n",
       " ('bagpipe', 0.4935358166694641),\n",
       " ('hurdy', 0.4913617968559265)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('instrument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('classical', 0.5478748083114624),\n",
       " ('media', 0.4841245412826538),\n",
       " ('musical', 0.48160994052886963),\n",
       " ('electronica', 0.4563618302345276),\n",
       " ('soca', 0.4551032781600952),\n",
       " ('ethnomusicology', 0.44308948516845703),\n",
       " ('calypso', 0.44126778841018677),\n",
       " ('western', 0.4394005537033081),\n",
       " ('aboriginal', 0.43857288360595703),\n",
       " ('fairs', 0.4376494884490967)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('music')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('concerts', 0.667274534702301),\n",
       " ('live', 0.5954722762107849),\n",
       " ('performance', 0.5832898616790771),\n",
       " ('nukes', 0.5790479183197021),\n",
       " ('stadium', 0.5250352621078491),\n",
       " ('rally', 0.5248817205429077),\n",
       " ('carnegie', 0.5235518217086792),\n",
       " ('hall', 0.5186064839363098),\n",
       " ('wembley', 0.5090302228927612),\n",
       " ('recital', 0.5024113059043884)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('concert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programming', 0.6129789352416992),\n",
       " ('languages', 0.60996013879776),\n",
       " ('harel', 0.5303844213485718),\n",
       " ('gml', 0.5231924653053284),\n",
       " ('pypy', 0.5139837861061096),\n",
       " ('intercal', 0.5129168033599854),\n",
       " ('nemerle', 0.5101083517074585),\n",
       " ('dandiya', 0.5076435208320618),\n",
       " ('grammar', 0.49712035059928894),\n",
       " ('disassembler', 0.4962505102157593)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macros', 0.5671015977859497),\n",
       " ('condominium', 0.5474796891212463),\n",
       " ('compiler', 0.5386015772819519),\n",
       " ('assembler', 0.5069373250007629),\n",
       " ('compiling', 0.4959162175655365),\n",
       " ('refactoring', 0.4838564991950989),\n",
       " ('swig', 0.48335230350494385),\n",
       " ('doxygen', 0.4818555414676666),\n",
       " ('binaries', 0.48058247566223145),\n",
       " ('matlab', 0.48041006922721863)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 0.4897530972957611),\n",
       " ('limewire', 0.45124053955078125),\n",
       " ('networked', 0.44089382886886597),\n",
       " ('online', 0.4404374361038208),\n",
       " ('networking', 0.43555939197540283),\n",
       " ('rickrolling', 0.4336913228034973),\n",
       " ('xri', 0.42589959502220154),\n",
       " ('blogging', 0.4216543436050415),\n",
       " ('leaked', 0.4198608100414276),\n",
       " ('websites', 0.4194316565990448)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('internet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('imperative', 0.6476056575775146),\n",
       " ('languages', 0.6234399676322937),\n",
       " ('language', 0.6129789352416992),\n",
       " ('procedural', 0.567463755607605),\n",
       " ('rulifson', 0.5477401614189148),\n",
       " ('paradigms', 0.5457649230957031),\n",
       " ('derksen', 0.5412634015083313),\n",
       " ('waldinger', 0.534734308719635),\n",
       " ('lisp', 0.5342938303947449),\n",
       " ('simula', 0.5213291645050049)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soldier', 0.5621863603591919),\n",
       " ('woman', 0.5024310946464539),\n",
       " ('crazy', 0.4884963631629944),\n",
       " ('grumpy', 0.4863646924495697),\n",
       " ('mystery', 0.4764661490917206),\n",
       " ('albatross', 0.4459526240825653),\n",
       " ('calf', 0.4443340301513672),\n",
       " ('fatzer', 0.44184619188308716),\n",
       " ('brutus', 0.4408373236656189),\n",
       " ('maccoll', 0.43469879031181335)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance\n",
    "\n",
    "Now let's have a look at the performances for doc2vec with our initial 5 classifiers.\n",
    "\n",
    "To do so we create a numpy array. There are two parallel arrays, one containing the vectors (train_arrays) and the other containing the labels (train_labels).\n",
    "We do the same for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((1800, 100))\n",
    "train_labels = numpy.zeros(1800)\n",
    "for i in range(900):\n",
    "    prefix_train_music = 'music_train_' + str(i)\n",
    "    prefix_train_programming = 'programming_train_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_music]\n",
    "    train_arrays[900 + i] = model.docvecs[prefix_train_programming]\n",
    "    train_labels[i] = 0\n",
    "    train_labels[900 + i] = 1\n",
    "    \n",
    "test_arrays = numpy.zeros((200, 100))\n",
    "test_labels = numpy.zeros(200)\n",
    "for i in range(100):\n",
    "    prefix_test_music = 'music_test_' + str(i)\n",
    "    prefix_test_programming = 'programming_test_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_music]\n",
    "    test_arrays[100 + i] = model.docvecs[prefix_test_programming]\n",
    "    test_labels[i] = 0\n",
    "    test_labels[100 + i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nearest Neighbors as a classifier, the accuracy was 54.50000000000001%\n",
      "Using Naive Bayes as a classifier, the accuracy was 89.5%\n",
      "Using RBF SVM as a classifier, the accuracy was 87.0%\n",
      "Using Random Forest as a classifier, the accuracy was 60.5%\n",
      "Using Neural Network as a classifier, the accuracy was 58.5%\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\",\n",
    "        \"Naive Bayes\",\n",
    "        \"RBF SVM\",\n",
    "        \"Random Forest\",\n",
    "        \"Neural Network\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    GaussianNB(),\n",
    "    SVC(gamma=0.001, C=1),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=20, max_features=\"auto\"),\n",
    "    MLPClassifier(alpha=1)]\n",
    "    \n",
    "scores = []\n",
    "lengths = []\n",
    "\n",
    "for name, classifier in zip(names, classifiers):\n",
    "    classifier.fit(train_arrays, train_labels)\n",
    "    acc = classifier.score(test_arrays, test_labels)\n",
    "    print(\"Using {} as a classifier, the accuracy was {}%\".format(name, 100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results are awful considering that we are performing binary classification, except for Naive Bayes which is suprisingly good and for SVM (after choosing better parameters) which is supposed to be good with binary classification.\n",
    "\n",
    "## 7 . Next steps \n",
    "\n",
    "To try and interpret those results, I looked at a similar project done with a famous IMDB movie review database used for binary sentiment analysis.\n",
    "The dataset is available here : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "And the project : https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb\n",
    "\n",
    "The dataset available from IMDB contains 25,000 movie reviews that are \"highly polar\". The projects has a high success rate for the prediction and also for the proximity between words.\n",
    "The main differences between my dataset and this one is :\n",
    "- Mine has fewer documents.\n",
    "- My documents are much longer.\n",
    "\n",
    "I have the following hypothesis to explain the results I got :\n",
    "- (Something is wrong with my code)\n",
    "- I have too few documents to allow my model to perform well\n",
    "- The fact that my documents are so long, makes it difficult to contextualize words and explain them efficiently. Why ? My intuition is that when we look at IMDB reviews, they are rather short, that makes the occurences of most world relatively small and also, the context is eaiser to identify. Whereas in a wikipedia article, one word may appear a lot of time, in a lot of different context, which may make the contexualization of one word harder.\n",
    "- IMDB review are more polar than my wikipedia articles this makes them easier to classify\n",
    "\n",
    "When I used bag of words, it was expected that there would be two clusters of words frequency (as words used talking about music and programming are generaly different). For doc2vec, a similar phenomenon (for \"word contextualisation\") seems less likely to occur.\n",
    "\n",
    "To verify my hypothesis I would like to try the following things :\n",
    "- Train the IMDB models with less reviews (1000 and 1000) like my dataset. Having bad results would be an argument in favor of saying that my dataset is too small to get good results.\n",
    "- Train my datasat but using only the introduction of every wikipedia articles. Having better results would be an argument in favor of saying that longer articles makes it harder to contextualize words and, as a consequence, to classify articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB\n",
    "\n",
    "Let's try to verify our hypothesis by doing what we did with our wikipedia articles on IMDB reviews. They are much shorter and much polarized, we hope to get better results thanks too those two differences with our initial corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the corpus :\n",
    "\n",
    "(Code similar to what was done before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folders = ['ImdbSmall/test/pos', 'ImdbSmall/train/pos', 'ImdbSmall/train/neg', 'ImdbSmall/test/neg']\n",
    "\n",
    "corpus_imdb = []\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        #i = u''\n",
    "        if filename != '.DS_Store':\n",
    "            i = codecs.open(folder + '/'+ filename, \"r\",encoding='utf-8', errors='ignore')\n",
    "            i = i.read()\n",
    "            i = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", i)\n",
    "            i = re.sub(r'[^\\w\\s]',' ', i)\n",
    "            for char in ['.', '\"', ',', '(', ')', '!', '?', ';', '_', ':']:\n",
    "                i = i.replace(char, ' ' + char + ' ')\n",
    "            i = re.sub(\"   \", \" \", i)\n",
    "            i = re.sub(\"  \", \" \", i)\n",
    "            i = i.lower()\n",
    "\n",
    "            corpus_imdb.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = open(\"pos_test.txt\", \"w\")\n",
    "for i in range(0,100):\n",
    "    text_file.write(corpus_imdb[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"pos_train.txt\", \"w\")\n",
    "for i in range(100,1000):\n",
    "    text_file.write(corpus_imdb[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"neg_train.txt\", \"w\")\n",
    "for i in range(1000,1900):\n",
    "    text_file.write(corpus_imdb[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"neg_test.txt\", \"w\")\n",
    "for i in range(1900,2000):\n",
    "    text_file.write(corpus_imdb[i] + '\\n')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources_imdb = {'pos_test.txt':'pos_test', 'pos_train.txt':'pos_train', 'neg_train.txt':'neg_train', 'neg_test.txt':'neg_test'}\n",
    "sentences_imdb = LabeledLineSentence(sources_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model chosen for the wikipedias articles on our new corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "model_imdb = Doc2Vec(min_count=1, window=10, size=100, sample=0.0001, negative=5, workers=8, dm=1)\n",
    "model_imdb.build_vocab(sentences_imdb.to_array())\n",
    "for epoch in range(10):\n",
    "    model_imdb.train(sentences_imdb.sentences_perm(), total_examples=model_imdb.corpus_count, epochs=model_imdb.iter)\n",
    "    print('.')\n",
    "model_imdb.save('./chosenmodel_imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((1800, 100))\n",
    "train_labels = numpy.zeros(1800)\n",
    "for i in range(900):\n",
    "    prefix_train_pos = 'pos_train_' + str(i)\n",
    "    prefix_train_neg = 'neg_train_' + str(i)\n",
    "    train_arrays[i] = model_imdb.docvecs[prefix_train_pos]\n",
    "    train_arrays[900 + i] = model_imdb.docvecs[prefix_train_neg]\n",
    "    train_labels[i] = 0\n",
    "    train_labels[900 + i] = 1\n",
    "    \n",
    "test_arrays = numpy.zeros((200, 100))\n",
    "test_labels = numpy.zeros(200)\n",
    "for i in range(100):\n",
    "    prefix_test_pos = 'pos_test_' + str(i)\n",
    "    prefix_test_neg = 'neg_test_' + str(i)\n",
    "    test_arrays[i] = model_imdb.docvecs[prefix_test_pos]\n",
    "    test_arrays[100 + i] = model_imdb.docvecs[prefix_test_neg]\n",
    "    test_labels[i] = 0\n",
    "    test_labels[100 + i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying the IMDB reviews using the same classifiers as before :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nearest Neighbors as a classifier, the accuracy was 82.0%\n",
      "Using Naive Bayes as a classifier, the accuracy was 66.0%\n",
      "Using RBF SVM as a classifier, the accuracy was 86.5%\n",
      "Using Random Forest as a classifier, the accuracy was 80.5%\n",
      "Using Neural Network as a classifier, the accuracy was 87.5%\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\",\n",
    "        \"Naive Bayes\",\n",
    "        \"RBF SVM\",\n",
    "        \"Random Forest\",\n",
    "        \"Neural Network\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    GaussianNB(),\n",
    "    SVC(gamma=0.001, C=1),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=20, max_features=\"auto\"),\n",
    "    MLPClassifier(alpha=1)]\n",
    "    \n",
    "scores = []\n",
    "lengths = []\n",
    "\n",
    "for name, classifier in zip(names, classifiers):\n",
    "    classifier.fit(train_arrays, train_labels)\n",
    "    acc = classifier.score(test_arrays, test_labels)\n",
    "    print(\"Using {} as a classifier, the accuracy was {}%\".format(name, 100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretations\n",
    "\n",
    "\n",
    "`\n",
    "Using Nearest Neighbors as a classifier, the accuracy was 82.0%\n",
    "Using Naive Bayes as a classifier, the accuracy was 66.0%\n",
    "Using RBF SVM as a classifier, the accuracy was 86.5%\n",
    "Using Random Forest as a classifier, the accuracy was 80.5%\n",
    "Using Neural Network as a classifier, the accuracy was 87.5%\n",
    "`\n",
    "\n",
    "The results are much better than what we got for our wikipedias articles. They seem in accordance with our originals beliefs :\n",
    "- Neural Network should perform better using a more advanced training model\n",
    "- SVM should perform well, whatever the method because it is good at handling binary classification\n",
    "- Naive Bayes should be the less efficient classification method\n",
    "\n",
    "In the original project that used IMDB reviews, they used 25,000 movies reviews. Our results with 2000 are very close to what they got, from this we can conclude that **a dataset of size 2000 should be enough to get a correct classification. Hence, the number of articles chosen is not the reason why our results were bad with word embedding**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki with Intro\n",
    "\n",
    "To end our study, we try to classify our articles using "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_clean_intro` allows us to get the introduction of a wikipedia article :\n",
    "    \n",
    "*(It seems like one article had no introduction, it will be considered as an empty article)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clean_intro(file):\n",
    "    \"\"\"\n",
    "    Input : A wikipedia article from our dataset\n",
    "    Output : The cleaned wikipedia article as a string \n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(file), \"lxml\")\n",
    "    for m in soup.find_all('a'):\n",
    "        m.replaceWithChildren()\n",
    "    text =[''.join(s.findAll(text=True))for s in soup.findAll('p')]\n",
    "    text2 = []\n",
    "    \n",
    "    keep_running = True\n",
    "    while (keep_running):\n",
    "        for i in text:\n",
    "            if i == '':\n",
    "\n",
    "                keep_running = False\n",
    "                break\n",
    "            else :\n",
    "                text2.append(i)\n",
    "        keep_running = False\n",
    "        \n",
    "    #Handling empty introduction problem :    \n",
    "    if len(text2) != 0:\n",
    "        i = reduce(lambda x,y: x+y, text2)\n",
    "        i = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", i)\n",
    "        i = re.sub(r'[^\\w\\s]',' ', i)\n",
    "        i = re.sub(\"   \", \" \", i)\n",
    "        i = re.sub(\"  \", \" \", i)\n",
    "        flat = i.lower()\n",
    "    else:\n",
    "        flat = ''\n",
    "    \n",
    "    return flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting our corpus of introductions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir1 = 'data/music'\n",
    "dir2 = 'data/programming'\n",
    "folders = [dir1, dir2]\n",
    "\n",
    "corpus_wiki_intro = []\n",
    "\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        text = get_clean_intro(folder + '/' + filename)\n",
    "        corpus_wiki_intro.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = open(\"music_test_intro.txt\", \"w\")\n",
    "for i in range(0,100):\n",
    "    text_file.write(corpus_wiki_intro[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"music_train_intro.txt\", \"w\")\n",
    "for i in range(100,1000):\n",
    "    text_file.write(corpus_wiki_intro[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"programming_train_intro.txt\", \"w\")\n",
    "for i in range(1000,1900):\n",
    "    text_file.write(corpus_wiki_intro[i] + '\\n')\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"programming_test_intro.txt\", \"w\")\n",
    "for i in range(1900,2000):\n",
    "    text_file.write(corpus_wiki_intro[i] + '\\n')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sources_intro = {'music_test_intro.txt':'music_test', 'music_train_intro.txt':'music_train', 'programming_train_intro.txt':'programming_train', 'programming_test_intro.txt':'programming_test'}\n",
    "sentences_intro = LabeledLineSentence(sources_intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our new model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "model_intro = Doc2Vec(min_count=1, window=10, size=100, sample=0.0001, negative=5, workers=8, dm=1)\n",
    "model_intro.build_vocab(sentences_intro.to_array())\n",
    "for epoch in range(10):\n",
    "    model_intro.train(sentences_intro.sentences_perm(), total_examples=model_intro.corpus_count, epochs=model_intro.iter)\n",
    "    print('.')\n",
    "model_intro.save('./chosenmodel_intro.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((1800, 100))\n",
    "train_labels = numpy.zeros(1800)\n",
    "for i in range(900):\n",
    "    prefix_train_music = 'music_train_' + str(i)\n",
    "    prefix_train_prog = 'programming_train_' + str(i)\n",
    "    train_arrays[i] = model_intro.docvecs[prefix_train_music]\n",
    "    train_arrays[900 + i] = model_intro.docvecs[prefix_train_prog]\n",
    "    train_labels[i] = 0\n",
    "    train_labels[900 + i] = 1\n",
    "    \n",
    "test_arrays = numpy.zeros((200, 100))\n",
    "test_labels = numpy.zeros(200)\n",
    "for i in range(100):\n",
    "    prefix_test_music = 'music_test_' + str(i)\n",
    "    prefix_test_prog = 'programming_test_' + str(i)\n",
    "    test_arrays[i] = model_intro.docvecs[prefix_test_music]\n",
    "    test_arrays[100 + i] = model_intro.docvecs[prefix_test_prog]\n",
    "    test_labels[i] = 0\n",
    "    test_labels[100 + i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying the Wikipedia Introductions using the same classifiers as before :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nearest Neighbors as a classifier, the accuracy was 85.5%\n",
      "Using Naive Bayes as a classifier, the accuracy was 80.0%\n",
      "Using RBF SVM as a classifier, the accuracy was 82.5%\n",
      "Using Random Forest as a classifier, the accuracy was 86.0%\n",
      "Using Neural Network as a classifier, the accuracy was 87.5%\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\",\n",
    "        \"Naive Bayes\",\n",
    "        \"RBF SVM\",\n",
    "        \"Random Forest\",\n",
    "        \"Neural Network\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    GaussianNB(),\n",
    "    SVC(gamma=0.001, C=1),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=20, max_features=\"auto\"),\n",
    "    MLPClassifier(alpha=1)]\n",
    "    \n",
    "scores = []\n",
    "lengths = []\n",
    "\n",
    "for name, classifier in zip(names, classifiers):\n",
    "    classifier.fit(train_arrays, train_labels)\n",
    "    acc = classifier.score(test_arrays, test_labels)\n",
    "    print(\"Using {} as a classifier, the accuracy was {}%\".format(name, 100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretations\n",
    "\n",
    "`\n",
    "Using Nearest Neighbors as a classifier, the accuracy was 85.5%\n",
    "Using Naive Bayes as a classifier, the accuracy was 80.0%\n",
    "Using RBF SVM as a classifier, the accuracy was 83.5%\n",
    "Using Random Forest as a classifier, the accuracy was 86.5%\n",
    "Using Neural Network as a classifier, the accuracy was 88.0%\n",
    "`\n",
    "\n",
    "Once again, the results are much better than what we got for the full wikipedias articles and are in accordance with our originals beliefs.\n",
    "\n",
    "Neural Network gives once again the best results.\n",
    "\n",
    "Using only the introduction of the Wikipedia Articles, we get better results than using the full articles. This seems to confirm our hypothesis that \"longer articles makes it harder to contextualize words and, as a consequence, to classify articles.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "For my work on IMDB and Wikipedia Introductions, I used the same model as the one I chose after tuning the parameters for the full Wikipedia Articles. There may be a set of hyperparameters that give even better results for the two experiments but given that the results were better than for the full articles without changing the hyperparameters, I thought that it wasn't necessary to do it.\n",
    "\n",
    "Once again, I based my hypothesis and my work on my current understanding of the problems I faced and of the techniques I used. There may be mistakes but for my general conclusion, I'll speak as if there were no major mistakes in my reasoning, nor in my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 . Conclusion\n",
    "\n",
    "Using tf-idf to establish our models, we had good results. However, even if all techniques had successfully classified more than 90 % of the articles in our test set, some were much faster than other.\n",
    "\n",
    "The main explanation to this phenomenon is that simple techniques such as Naive Bayes, Closest Neighbor and Random Forest were good at solving the rather simple problem that binary classification is. SVM and Neural Network had good results but were very slow, the explanation for that being that their running time depended on the number of features we got thanks to tf-idf, which was huge.\n",
    "\n",
    "To verify our explanations, we tried to train the same model using word embedding.\n",
    "\n",
    "The results we got for the full articles were very disapointing, except for SVM (and Naive Bayes, but we'll not put too much importance on it). There were to explanations for that : \n",
    "- The length of the articles makes is harder to contextualize words efficiently, which word embedding needs to classify articles correctly.\n",
    "- Articles aren't polarized enough, causing more classification errors\n",
    "- 2000 articles is not enough to classify effectively using word embedding.\n",
    "\n",
    "To verify those hypothesis, we used the same model as before with word embedding for shorter IMDB review as well as for the introduction only of our chosen Wikipedia articles. For both experiments, the results were much better than before and we saw that this time, SVM and Neural Network were performing better than other algorithms and were as fast as them.\n",
    "\n",
    "This seems to confirm both our hypothesis following the results obtained with tf-idf and the bad classification results using word embedding on full articles.\n",
    "\n",
    "tf-idf seems to be very powerfull for long articles, however some classifiers will struggle to use it efficiently as it can generate an enormous amount of features. Word embedding on the other end, will work better with complex classifiers but it will struggle if articles are too long, as words will be harder to contextualize correctly."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
