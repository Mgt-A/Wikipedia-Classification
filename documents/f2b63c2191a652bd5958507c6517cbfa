<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Bayes&#039; theorem,Bayes&#039; theorem,Absolutely continuous,Athanasios Papoulis,Bayesian inference,Bayesian network,Bayesian probability,Bayesian spam filtering,Binomial,Binomial distribution,Biometrika" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Bayes%27_theorem&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Bayes%27_theorem&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Bayes' theorem - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Bayes\'_theorem";
		var wgTitle = "Bayes\' theorem";
		var wgAction = "view";
		var wgArticleId = "49569";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282374912;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Bayes_theorem skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Bayes' theorem</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>In <a href="/wiki/Probability_theory" title="Probability theory">probability theory</a>, <b><i>Bayes'</i> theorem</b> (often called <b>Bayes' law</b> after Rev <a href="/wiki/Thomas_Bayes" title="Thomas Bayes">Thomas Bayes</a>) relates the <a href="/wiki/Conditional_probability" title="Conditional probability">conditional and marginal probabilities</a> of two <a href="/wiki/Random_event" title="Random event" class="mw-redirect">random events</a>. It is often used to compute <a href="/wiki/Posterior_probabilities" title="Posterior probabilities" class="mw-redirect">posterior probabilities</a> given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. (See <a href="/wiki/Bayes%27_theorem#Further_examples" title="Bayes' theorem">example 2</a>)</p>
<p>As a formal <a href="/wiki/Theorem" title="Theorem">theorem</a>, Bayes' theorem is valid in all common <a href="/wiki/Probability_interpretations" title="Probability interpretations">interpretations of probability</a>. However, it plays a central role in the debate around the <a href="/wiki/Foundations_of_statistics" title="Foundations of statistics">foundations of statistics</a>: <a href="/wiki/Frequentist" title="Frequentist" class="mw-redirect">frequentist</a> and <a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian</a> interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on <a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian probability</a> and <a href="/wiki/Frequency_probability" title="Frequency probability">frequentist probability</a> discuss these debates in greater detail.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Statement_of_Bayes.27_theorem"><span class="tocnumber">1</span> <span class="toctext">Statement of Bayes' theorem</span></a></li>
<li class="toclevel-1"><a href="#An_example"><span class="tocnumber">2</span> <span class="toctext">An example</span></a></li>
<li class="toclevel-1"><a href="#Derivation_from_conditional_probabilities"><span class="tocnumber">3</span> <span class="toctext">Derivation from conditional probabilities</span></a></li>
<li class="toclevel-1"><a href="#Alternative_forms_of_Bayes.27_theorem"><span class="tocnumber">4</span> <span class="toctext">Alternative forms of Bayes' theorem</span></a>
<ul>
<li class="toclevel-2"><a href="#Bayes.27_theorem_in_terms_of_odds_and_likelihood_ratio"><span class="tocnumber">4.1</span> <span class="toctext">Bayes' theorem in terms of odds and likelihood ratio</span></a></li>
<li class="toclevel-2"><a href="#Bayes.27_theorem_for_probability_densities"><span class="tocnumber">4.2</span> <span class="toctext">Bayes' theorem for probability densities</span></a></li>
<li class="toclevel-2"><a href="#Abstract_Bayes.27_theorem"><span class="tocnumber">4.3</span> <span class="toctext">Abstract Bayes' theorem</span></a></li>
<li class="toclevel-2"><a href="#Extensions_of_Bayes.27_theorem"><span class="tocnumber">4.4</span> <span class="toctext">Extensions of Bayes' theorem</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Further_examples"><span class="tocnumber">5</span> <span class="toctext">Further examples</span></a>
<ul>
<li class="toclevel-2"><a href="#Example_1:_Drug_testing"><span class="tocnumber">5.1</span> <span class="toctext">Example 1: Drug testing</span></a></li>
<li class="toclevel-2"><a href="#Example_2:__Bayesian_inference"><span class="tocnumber">5.2</span> <span class="toctext">Example 2: Bayesian inference</span></a></li>
<li class="toclevel-2"><a href="#Example_3:_The_Monty_Hall_problem"><span class="tocnumber">5.3</span> <span class="toctext">Example 3: The Monty Hall problem</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Historical_remarks"><span class="tocnumber">6</span> <span class="toctext">Historical remarks</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a>
<ul>
<li class="toclevel-2"><a href="#Versions_of_the_essay"><span class="tocnumber">8.1</span> <span class="toctext">Versions of the essay</span></a></li>
<li class="toclevel-2"><a href="#Commentaries"><span class="tocnumber">8.2</span> <span class="toctext">Commentaries</span></a></li>
<li class="toclevel-2"><a href="#Additional_material"><span class="tocnumber">8.3</span> <span class="toctext">Additional material</span></a></li>
</ul>
</li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Statement_of_Bayes.27_theorem" id="Statement_of_Bayes.27_theorem"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=1" title="Edit section: Statement of Bayes' theorem">edit</a>]</span> <span class="mw-headline">Statement of Bayes' theorem</span></h2>
<p>Bayes' theorem relates the <a href="/wiki/Conditional_probability" title="Conditional probability">conditional</a> and <a href="/wiki/Marginal_probability" title="Marginal probability" class="mw-redirect">marginal</a> probabilities of events <i>A</i> and <i>B</i>, where <i>B</i> has a non-vanishing probability:</p>
<dl>
<dd><img class="tex" alt="P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}." src="http://upload.wikimedia.org/math/1/8/8/188019d193258f9ba310da979906d24f.png" /></dd>
</dl>
<p>Each term in Bayes' theorem has a conventional name:</p>
<ul>
<li>P(<i>A</i>) is the <a href="/wiki/Prior_probability" title="Prior probability">prior probability</a> or <a href="/wiki/Marginal_probability" title="Marginal probability" class="mw-redirect">marginal probability</a> of <i>A</i>. It is "prior" in the sense that it does not take into account any information about <i>B</i>.</li>
<li>P(<i>A</i>|<i>B</i>) is the <a href="/wiki/Conditional_probability" title="Conditional probability">conditional probability</a> of <i>A</i>, given <i>B</i>. It is also called the <a href="/wiki/Posterior_probability" title="Posterior probability">posterior probability</a> because it is derived from or depends upon the specified value of <i>B</i>.</li>
<li>P(<i>B</i>|<i>A</i>) is the conditional probability of <i>B</i> given <i>A</i>.</li>
<li>P(<i>B</i>) is the prior or marginal probability of <i>B</i>, and acts as a <a href="/wiki/Normalizing_constant" title="Normalizing constant">normalizing constant</a>.</li>
</ul>
<p>Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</p>
<p><a name="An_example" id="An_example"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=2" title="Edit section: An example">edit</a>]</span> <span class="mw-headline">An example</span></h2>
<p>Suppose there is a co-ed school having 60% boys and 40% girls as students. The girl students wear trousers or skirts in equal numbers; the boys all wear trousers. An observer sees a (random) student from a distance; all they can see is that this student is wearing trousers. What is the probability this student is a girl? The correct answer can be computed using Bayes' theorem.</p>
<p>The event <i>A</i> is that the student observed is a girl, and the event <i>B</i> is that the student observed is wearing trousers. To compute P(<i>A</i>|<i>B</i>), we first need to know:</p>
<ul>
<li>P(<i>A</i>), or the probability that the student is a girl regardless of any other information. Since the observers sees a random student, meaning that all students have the same probability of being observed, and the fraction of girls among the students is 40%, this probability equals 0.4.</li>
<li>P(<i>A</i>'), or the probability that the student is a boy regardless of any other information (<i>A</i>' is the complementary event to <i>A</i>). This is 60%, or 0.6.</li>
<li>P(<i>B</i>|<i>A</i>), or the probability of the student wearing trousers given that the student is a girl. As they are as likely to wear skirts as trousers, this is 0.5.</li>
<li>P(<i>B</i>|<i>A</i>'), or the probability of the student wearing trousers given that the student is a boy. This is given as 1.</li>
<li>P(<i>B</i>), or the probability of a (randomly selected) student wearing trousers regardless of any other information. Since <span style="white-space:nowrap;">P(<i>B</i>) = P(<i>B</i>|<i>A</i>)P(<i>A</i>) + P(<i>B</i>|<i>A</i>')P(<i>A</i>')</span>, this is <span style="white-space:nowrap;">0.5×0.4 + 1×0.6 = 0.8</span>.</li>
</ul>
<p>Given all this information, the probability of the observer having spotted a girl given that the observed student is wearing trousers can be computed by substituting these values in the formula:</p>
<dl>
<dd><img class="tex" alt="P(A|B) = \frac{P(B|A) P(A)}{P(B)} = \frac{0.5 \times 0.4}{0.8} = 0.25." src="http://upload.wikimedia.org/math/1/6/3/163eb3d6882fa3cbf5939ecd81f6e24b.png" /></dd>
</dl>
<p>Another, essentially equivalent way of obtaining the same result is as follows. Assume, for concreteness, that there are 100 students, 60 boys and 40 girls. Among these, 60 boys and 20 girls wear trousers. All together there are 80 trouser-wearers, of which 20 are girls. Therefore the chance that a random trouser-wearer is a girl equals <span style="white-space:nowrap;">20/80 = 0.25</span>.</p>
<p>It is often helpful when calculating conditional probabilities to create a simple table containing the number of occurrences of each outcome, or the <a href="/wiki/Relative_frequency" title="Relative frequency" class="mw-redirect">relative frequencies</a> of each outcome, for each of the independent variables. The table below illustrates the use of this method for the above girl-or-boy example</p>
<table class="wikitable">
<tr>
<th></th>
<th>Girls</th>
<th>Boys</th>
<th>Total</th>
</tr>
<tr>
<td><b>Trousers</b></td>
<td>
<center>20</center>
</td>
<td>
<center>60</center>
</td>
<td>
<center>&#160;80</center>
</td>
</tr>
<tr>
<td><b>Skirts</b></td>
<td>
<center>20</center>
</td>
<td>
<center>&#160;0</center>
</td>
<td>
<center>&#160;20</center>
</td>
</tr>
<tr>
<td><b>Total</b></td>
<td>
<center>40</center>
</td>
<td>
<center>60</center>
</td>
<td>
<center>100</center>
</td>
</tr>
</table>
<p><a name="Derivation_from_conditional_probabilities" id="Derivation_from_conditional_probabilities"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=3" title="Edit section: Derivation from conditional probabilities">edit</a>]</span> <span class="mw-headline">Derivation from conditional probabilities</span></h2>
<p>To derive the theorem, we start from the definition of <a href="/wiki/Conditional_probability" title="Conditional probability">conditional probability</a>. The probability of event <i>A</i> given event <i>B</i> is</p>
<dl>
<dd><img class="tex" alt="P(A|B)=\frac{P(A \cap B)}{P(B)}." src="http://upload.wikimedia.org/math/a/6/d/a6df7814a46fc4c9670cc510b72bb318.png" /></dd>
</dl>
<p>Equivalently, the probability of event <i>B</i> given event <i>A</i> is</p>
<dl>
<dd><img class="tex" alt="P(B|A) = \frac{P(A \cap B)}{P(A)}. \!" src="http://upload.wikimedia.org/math/8/b/6/8b6c81124815aad54c91c42b3261165d.png" /></dd>
</dl>
<p>Rearranging and combining these two equations, we find</p>
<dl>
<dd><img class="tex" alt="P(A|B)\, P(B) = P(A \cap B) = P(B|A)\, P(A). \!" src="http://upload.wikimedia.org/math/e/f/a/efaf8fda8a92eeb2d8cf70468c20ed5a.png" /></dd>
</dl>
<p>This <a href="/wiki/Lemma_(mathematics)" title="Lemma (mathematics)">lemma</a> is sometimes called the product rule for probabilities. Dividing both sides by P(<i>B</i>), provided that it is non-zero, we obtain Bayes' theorem:</p>
<dl>
<dd><img class="tex" alt="P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)\,P(A)}{P(B)}. \!" src="http://upload.wikimedia.org/math/7/9/d/79d27075240450c1d54cd0ac440aff0a.png" /></dd>
</dl>
<p><a name="Alternative_forms_of_Bayes.27_theorem" id="Alternative_forms_of_Bayes.27_theorem"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=4" title="Edit section: Alternative forms of Bayes' theorem">edit</a>]</span> <span class="mw-headline">Alternative forms of Bayes' theorem</span></h2>
<p>Bayes' theorem is often completed by noting that, according to the <a href="/wiki/Law_of_total_probability" title="Law of total probability">Law of total probability</a>,</p>
<dl>
<dd><img class="tex" alt="P(B) = P(A\cap B) + P(A^C\cap B) = P(B|A) P(A) + P(B|A^C) P(A^C)." src="http://upload.wikimedia.org/math/6/b/b/6bbb748ada0b8a36411cf49b9b46c80f.png" /></dd>
</dl>
<p>where <i>A</i><sup><i>C</i></sup> is the <a href="/wiki/Complement_(set_theory)#Absolute_complement" title="Complement (set theory)">complementary</a> event of <i>A</i> (often called "not A"). So the theorem can be restated as the following formula</p>
<dl>
<dd><img class="tex" alt="P(A|B) = \frac{P(B | A)\, P(A)}{P(B|A) P(A) + P(B|A^C) P(A^C)}.  \!" src="http://upload.wikimedia.org/math/b/3/3/b337da08b983c9e9c6f741d856b4b72c.png" /></dd>
</dl>
<p>More generally, where {<i>A</i><sub><i>i</i></sub>} forms a <a href="/wiki/Partition_of_a_set" title="Partition of a set">partition</a> of the event space,</p>
<dl>
<dd><img class="tex" alt="P(A_i|B) = \frac{P(B | A_i)\, P(A_i)}{\sum_j P(B|A_j)\,P(A_j)} , \!" src="http://upload.wikimedia.org/math/d/a/a/daa4b03e8ea857a6957e4a1d0b8ef6b1.png" /></dd>
</dl>
<p>for any <i>A</i><sub><i>i</i></sub> in the partition.</p>
<p><a name="Bayes.27_theorem_in_terms_of_odds_and_likelihood_ratio" id="Bayes.27_theorem_in_terms_of_odds_and_likelihood_ratio"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=5" title="Edit section: Bayes' theorem in terms of odds and likelihood ratio">edit</a>]</span> <span class="mw-headline">Bayes' theorem in terms of odds and likelihood ratio</span></h3>
<p>Bayes' theorem can also be written neatly in terms of a likelihood ratio Λ and <a href="/wiki/Odds" title="Odds">odds</a> <i>O</i> as</p>
<dl>
<dd><img class="tex" alt="O(A|B)=O(A) \cdot \Lambda (A|B) " src="http://upload.wikimedia.org/math/6/a/a/6aa65315e6ae2da1b417789e43b4eb72.png" /></dd>
</dl>
<p>where <i>O(A|B)</i> are the <i>odds</i> of <i>A</i> given <i>B</i>,</p>
<dl>
<dd><img class="tex" alt="O(A|B)=\frac{P(A|B)}{P(A^C|B)} \!" src="http://upload.wikimedia.org/math/8/1/b/81be09df6b27e9b3d3ee7c8e4e6ced85.png" /></dd>
</dl>
<p><i>O(A)</i> are the odds of <i>A</i> by itself</p>
<dl>
<dd><img class="tex" alt="O(A)=\frac{P(A)}{P(A^C)} \!" src="http://upload.wikimedia.org/math/c/4/4/c4451387126a4c6a51739382cacb9e99.png" /></dd>
</dl>
<p>and <i>Λ(A|B)</i> is the <i>likelihood ratio</i>.</p>
<dl>
<dd><img class="tex" alt="\Lambda (A|B) = \frac{P(B|A)}{P(B|A^C)} \!" src="http://upload.wikimedia.org/math/f/5/6/f56ec5ec006faf90075937d2eb82be5d.png" /></dd>
</dl>
<p><a name="Bayes.27_theorem_for_probability_densities" id="Bayes.27_theorem_for_probability_densities"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=6" title="Edit section: Bayes' theorem for probability densities">edit</a>]</span> <span class="mw-headline">Bayes' theorem for probability densities</span></h3>
<p>There is also a version of Bayes' theorem for <a href="/wiki/Continuous_distribution" title="Continuous distribution" class="mw-redirect">continuous distributions</a>. It is somewhat harder to derive, since <a href="/wiki/Probability_density_function" title="Probability density function">probability densities</a> are not probabilities, so Bayes' theorem has to be established by a limit process; see Papoulis (citation below), Section 7.3 for an elementary derivation.</p>
<p>Bayes originally used the proposition to find a continuous posterior distribution given discrete observations.</p>
<p>Bayes' theorem for probability densities is formally similar to the theorem for probabilities:</p>
<dl>
<dd><img class="tex" alt=" f_X(x|Y=y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{f_Y(y|X=x)\,f_X(x)}{f_Y(y)} = \frac{f_Y(y|X=x)\,f_X(x)}{\int_{-\infty}^{\infty} f_Y(y|X=\xi )\,f_X(\xi )\,d\xi }.\!" src="http://upload.wikimedia.org/math/a/d/7/ad7d0916024da86ebe8c527af0bf5c44.png" /></dd>
</dl>
<p>There is an analogous statement of the <a href="/wiki/Law_of_total_probability" title="Law of total probability">law of total probability</a>, which is used in the denominator:</p>
<dl>
<dd><img class="tex" alt=" f_Y(y) = \int_{-\infty}^{\infty} f_Y(y|X=x )\,f_X(x)\,dx .\!" src="http://upload.wikimedia.org/math/4/3/5/435008b933ceedbe559afad427386ecf.png" /></dd>
</dl>
<p>As in the discrete case, the terms have standard names.</p>
<dl>
<dd><img class="tex" alt=" f_{X,Y}(x,y)\," src="http://upload.wikimedia.org/math/d/3/4/d34c38cff2ee4276d74edbeb382c0b2b.png" /></dd>
</dl>
<p>is the joint density function of <i>X</i> and <i>Y</i>,</p>
<dl>
<dd><img class="tex" alt=" f_X(x|Y=y)\," src="http://upload.wikimedia.org/math/0/7/8/078043dbc5e6d4f8440febe156268b22.png" /></dd>
</dl>
<p>is the posterior probability density function of <i>X</i> given <i>Y</i>&#160;=&#160;<i>y</i>,</p>
<dl>
<dd><img class="tex" alt=" f_Y(y|X=x) = L(x|y)\," src="http://upload.wikimedia.org/math/a/7/f/a7ff013471e762bfaed2c7f368b1c5b7.png" /></dd>
</dl>
<p>is (as a function of <i>x</i>) the likelihood function of <i>X</i> given <i>Y</i>&#160;=&#160;<i>y</i>, and</p>
<dl>
<dd><img class="tex" alt=" f_X(x)\," src="http://upload.wikimedia.org/math/e/6/b/e6be788d0861d297ae4dc009e559fdc4.png" /></dd>
</dl>
<p>and</p>
<dl>
<dd><img class="tex" alt=" f_Y(y)\!" src="http://upload.wikimedia.org/math/0/8/1/081da152bc5b06c0dd22b4bad4bcc407.png" /></dd>
</dl>
<p>are the marginal probability density functions of <i>X</i> and <i>Y</i> respectively, where <i>ƒ</i><sub><i>X</i></sub>(<i>x</i>) is the prior probability density function of <i>X</i>.</p>
<p><a name="Abstract_Bayes.27_theorem" id="Abstract_Bayes.27_theorem"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=7" title="Edit section: Abstract Bayes' theorem">edit</a>]</span> <span class="mw-headline">Abstract Bayes' theorem</span></h3>
<p>Given two <a href="/wiki/Absolutely_continuous" title="Absolutely continuous" class="mw-redirect">absolutely continuous</a> probability measures <span class="texhtml"><i>P</i>˜<i>Q</i></span> on the <a href="/wiki/Probability_space" title="Probability space">probability space</a> <img class="tex" alt="(\Omega, \mathcal{F})" src="http://upload.wikimedia.org/math/b/9/4/b9474d45d82accf03434710c10871795.png" /> and a sigma-algebra <img class="tex" alt="\mathcal{G} \subset \mathcal{F}" src="http://upload.wikimedia.org/math/6/9/5/6956beed6709f29c47056603dd448e37.png" />, the abstract Bayes theorem for a <img class="tex" alt="\mathcal{F}" src="http://upload.wikimedia.org/math/2/6/a/26afd73f8c17f310707120691ccc4a35.png" />-measurable random variable <span class="texhtml"><i>X</i></span> becomes</p>
<dl>
<dd><img class="tex" alt="E_P \left[ X|\mathcal{G} \right]  = \frac{E_Q \left[ \left.\frac{dP}{dQ} X \right|\mathcal{G} \right] }{E_Q \left[ \frac{dP}{dQ}|\mathcal{G} \right] } ." src="http://upload.wikimedia.org/math/d/9/6/d9603dc1b637c97148bf3de8fdb4c12d.png" /></dd>
</dl>
<p>Proof&#160;:</p>
<p>by definition of conditional probability,</p>
<p><img class="tex" alt=" E_P \left[ X|\mathcal{G} \right]  = \frac{E_P \left[ X 1_\mathcal{G} \right] }{P \left[ \mathcal{G} \right] }  = \frac{E_P \left[ X 1_\mathcal{G} \right] }{E_P \left[ 1_\mathcal{G} \right] } " src="http://upload.wikimedia.org/math/8/b/d/8bd115cf68c6a4f0cf31b43f2633de84.png" /></p>
<p>We further have that</p>
<dl>
<dd><img class="tex" alt=" E_Q \left[ \frac{dP}{dQ} X\right]  = \int \frac{dP}{dQ} X \,dQ   = \int X \, dP = E_P \left[ X \right]. " src="http://upload.wikimedia.org/math/5/6/f/56f2ca3ef1449c546ceee790554113ce.png" /></dd>
</dl>
<p>Hence,</p>
<dl>
<dd><img class="tex" alt=" E_Q \left[ \left.\frac{dP}{dQ} X\right|\mathcal{G} \right]  = \frac{E_Q \left[ \frac{dP}{dQ} X 1_\mathcal{G} \right] }{E_Q \left[ 1_\mathcal{G} \right] } = \frac{E_P \left[ X 1_\mathcal{G} \right] }{E_Q \left[ 1_\mathcal{G} \right] }" src="http://upload.wikimedia.org/math/3/1/3/313315b19ea8d0b5567029960fbfe0cf.png" /></dd>
</dl>
<dl>
<dd><img class="tex" alt=" E_Q \left[ \left.\frac{dP}{dQ}\right|\mathcal{G} \right]  = \frac{E_Q \left[ \frac{dP}{dQ} 1_\mathcal{G} \right] }{E_Q \left[ 1_\mathcal{G} \right] } = \frac{E_P \left[ 1_\mathcal{G} \right] }{E_Q \left[ 1_\mathcal{G} \right] }." src="http://upload.wikimedia.org/math/f/d/b/fdb6432d15f8214df74fc84b5f709826.png" /></dd>
</dl>
<p>Summarizing, we obtain the desired result.</p>
<p>This formulation is used in <a href="/wiki/Kalman_filtering" title="Kalman filtering" class="mw-redirect">Kalman filtering</a> to find <a href="/wiki/Zakai_equation" title="Zakai equation">Zakai equations</a>. It is also used in <a href="/wiki/Financial_mathematics" title="Financial mathematics" class="mw-redirect">financial mathematics</a> for change of <a href="/wiki/Numeraire" title="Numeraire" class="mw-redirect">numeraire</a> techniques.</p>
<p><a name="Extensions_of_Bayes.27_theorem" id="Extensions_of_Bayes.27_theorem"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=8" title="Edit section: Extensions of Bayes' theorem">edit</a>]</span> <span class="mw-headline">Extensions of Bayes' theorem</span></h3>
<p>Theorems analogous to Bayes' theorem hold in problems with more than two variables. For example:</p>
<dl>
<dd><img class="tex" alt=" P(A|B \cap C) = \frac{P(A) \, P(B|A) \, P(C|A \cap B)}{P(B) \, P(C|B)}\,." src="http://upload.wikimedia.org/math/7/2/1/7218ee907af6cab104e27ab9e1da81c0.png" /></dd>
</dl>
<p>This can be derived in a few steps from Bayes' theorem and the definition of conditional probability:</p>
<dl>
<dd><img class="tex" alt=" P(A|B \cap C) = \frac{P(A \cap B \cap C)}{P(B \cap C)} = \frac{P(C|A \cap B) \, P(A \cap B)}{P(B) \, P(C|B)} = \frac{P(A) \, P(B|A) \, P(C|A \cap B)}{P(B) \, P(C|B)}\,." src="http://upload.wikimedia.org/math/8/b/d/8bd22556ddfa2d4b30701490d6dc482f.png" /></dd>
</dl>
<p>Similarly,</p>
<dl>
<dd><img class="tex" alt=" P(A|B \cap C) = \frac{P(B|A \cap C) \, P(A|C)}{P(B|C)}\,," src="http://upload.wikimedia.org/math/e/4/7/e471c434fd7e59c97597e2a475f02ede.png" /></dd>
</dl>
<p>which can be regarded as a conditional Bayes' Theorem, and can be derived by as follows:</p>
<dl>
<dd><img class="tex" alt=" P(A|B \cap C) = \frac{P(A \cap B \cap C)}{P(B \cap C)} = \frac{P(B|A \cap C) \, P(A|C) \, P(C)}{P(C) \, P(B|C)} = \frac{P(B|A \cap C) \, P(A|C)}{P(B|C)}\,." src="http://upload.wikimedia.org/math/6/0/8/608b4383ffdfc7291252ed9cc0d9dfb8.png" /></dd>
</dl>
<p>A general strategy is to work with a decomposition of the <a href="/wiki/Joint_probability" title="Joint probability" class="mw-redirect">joint probability</a>, and to <a href="/wiki/Marginal_distribution" title="Marginal distribution">marginalize</a> (integrate) over the variables that are not of interest. Depending on the form of the decomposition, it may be possible to prove that some integrals must be 1, and thus they fall out of the decomposition; exploiting this property can reduce the computations very substantially. A <a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian network</a>, for example, specifies a factorization of a <a href="/wiki/Joint_distribution" title="Joint distribution" class="mw-redirect">joint distribution</a> of several variables in which the conditional probability of any one variable given the remaining ones takes a particularly simple form (see <a href="/wiki/Markov_blanket" title="Markov blanket">Markov blanket</a>).</p>
<p><a name="Further_examples" id="Further_examples"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=9" title="Edit section: Further examples">edit</a>]</span> <span class="mw-headline">Further examples</span></h2>
<p><a name="Example_1:_Drug_testing" id="Example_1:_Drug_testing"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=10" title="Edit section: Example 1: Drug testing">edit</a>]</span> <span class="mw-headline">Example 1: Drug testing</span></h3>
<p>Bayes' theorem is useful in evaluating the result of <a href="/wiki/Drug_test" title="Drug test">drug tests</a>. Suppose a certain drug test is 99% <a href="/wiki/Sensitivity_(tests)" title="Sensitivity (tests)" class="mw-redirect">sensitive</a> and 99% <a href="/wiki/Specificity_(tests)" title="Specificity (tests)" class="mw-redirect">specific</a>, that is, the test will correctly identify a drug user as testing positive 99% of the time, and will correctly identify a non-user as testing negative 99% of the time. This would seem to be a relatively accurate test, but Bayes' theorem will reveal a potential flaw. Let's assume a corporation decides to test its employees for <a href="/wiki/Opium" title="Opium">opium</a> use, and 0.5% of the employees use the drug. We want to know the <a href="/wiki/Probability" title="Probability">probability</a> that, given a positive drug test, an employee is actually a drug user. Let "D" be the event of being a drug user and "N" indicate being a non-user. Let "+" be the event of a positive drug test. We need to know the following:</p>
<ul>
<li><i>P</i>(<i>D</i>), or the probability that the employee is a drug user, regardless of any other information. This is 0.005, since 0.5% of the employees are drug users. This is the <i>prior probability</i> of D.</li>
<li><i>P</i>(<i>N</i>), or the probability that the employee is not a drug user. This is <span style="white-space:nowrap;">1 − <i>P</i>(<i>D</i>)</span>, or 0.995.</li>
<li><i>P</i>(+|<i>D</i>), or the probability that the test is positive, given that the employee is a drug user. This is 0.99, since the test is 99% accurate.</li>
<li><i>P</i>(+|<i>N</i>), or the probability that the test is positive, given that the employee is not a drug user. This is 0.01, since the test will produce a <a href="/wiki/False_positive" title="False positive" class="mw-redirect">false positive</a> for 1% of non-users.</li>
<li><i>P</i>(+), or the probability of a positive test event, regardless of other information. This is 0.0149 or 1.49%, which is found by adding the probability that a true positive result will appear (= 99% x 0.5% = 0.495%) plus the probability that a false positive will appear (= 1% x 99.5% = 0.995%). This is the prior probability of +.</li>
</ul>
<p>Given this information, we can compute the posterior probability <i>P</i>(<i>D</i>|+) of an employee who tested positive actually being a drug user:</p>
<dl>
<dd><img class="tex" alt="\begin{align}P(D|+) &amp; = \frac{P(+ | D) P(D)}{P(+)} \\
&amp; = \frac{P(+ | D) P(D)}{P(+ | D) P(D) + P(+ | N) P(N)} \\
&amp; = \frac{0.99 \times 0.005}{0.99 \times 0.005 + 0.01 \times 0.995} \\
&amp; = 0.3322.\end{align}" src="http://upload.wikimedia.org/math/b/c/6/bc6a73ad747014f6aa0ee102c0bb4246.png" /></dd>
</dl>
<p>Despite the apparently high accuracy of the test, the probability that an employee who tested positive actually did use drugs is only about 33%, so it is <i>actually more likely</i> that the employee is not a drug user. The rarer the condition for which we are testing, the greater the percentage of positive tests that will be false positives.</p>
<p><a name="Example_2:__Bayesian_inference" id="Example_2:__Bayesian_inference"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=11" title="Edit section: Example 2:  Bayesian inference">edit</a>]</span> <span class="mw-headline">Example 2: Bayesian inference</span></h3>
<p>Applications of Bayes' theorem often assume the philosophy underlying <a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian probability</a> that uncertainty and degrees of belief can be measured as probabilities. One such example follows. For additional worked out examples, including simpler examples, please see the article on the examples of <a href="/wiki/Bayesian_inference#Simple_examples_of_Bayesian_inference" title="Bayesian inference">Bayesian inference</a>.</p>
<p>We describe the marginal probability distribution of a variable <i>A</i> as the <a href="/wiki/Prior_probability_distribution" title="Prior probability distribution" class="mw-redirect">prior probability distribution</a> or simply the 'prior'. The conditional distribution of <i>A</i> given the "data" <i>B</i> is the <a href="/wiki/Posterior_probability_distribution" title="Posterior probability distribution" class="mw-redirect">posterior probability distribution</a> or just the 'posterior'.</p>
<p>Suppose we wish to know about the proportion <b>r</b> of voters in a large population who will vote "yes" in a referendum. Let <b>n</b> be the number of voters in a random sample (chosen with replacement, so that we have <a href="/wiki/Statistical_independence" title="Statistical independence" class="mw-redirect">statistical independence</a>) and let <b>m</b> be the number of voters in that random sample who will vote "yes". Suppose that we observe <i>n</i>&#160;=&#160;10 voters and <i>m</i>&#160;=&#160;7 say they will vote yes. From Bayes' theorem we can calculate the probability distribution function for <i>r</i> using</p>
<dl>
<dd><img class="tex" alt=" f(r | n=10, m=7) = 
  \frac {f(m=7 | r, n=10) \, f(r)} {\int_0^1 f(m=7|r, n=10) \, f(r) \, dr}. \!" src="http://upload.wikimedia.org/math/8/1/d/81dbc60f7af3163c790e7891c072ed53.png" /></dd>
</dl>
<p>From this we see that from the prior probability density function <i>f</i>(<i>r</i>) and the likelihood function <i>L</i>(<i>r</i>)&#160;=&#160;<i>f</i>(<i>m</i>&#160;=&#160;7|<i>r</i>, <i>n</i>&#160;=&#160;10), we can compute the posterior probability density function <i>f</i>(<i>r</i>|<i>n</i>&#160;=&#160;10, <i>m</i>&#160;=&#160;7).</p>
<p>The prior probability density function <i>f</i>(<i>r</i>) summarizes what we know about the distribution of <i>r</i> in the absence of any observation. We provisionally assume in this case that the prior distribution of <i>r</i> is uniform over the interval [0, 1]. That is, <i>f</i>(<i>r</i>) = 1. If some additional background information is found, we should modify the prior accordingly. However before we have any observations, all outcomes are equally likely.</p>
<p>Under the assumption of random sampling, choosing voters is just like choosing balls from an urn. The likelihood function <i>L</i>(<i>r</i>)&#160;=&#160;<i>P</i>(<i>m</i>&#160;=&#160;7|<i>r</i>, <i>n</i>&#160;=&#160;10,) for such a problem is just the probability of 7 successes in 10 trials for a <a href="/wiki/Binomial_distribution" title="Binomial distribution">binomial distribution</a>.</p>
<dl>
<dd><img class="tex" alt=" P( m=7 | r, n=10) = {10 \choose 7} \, r^7 \, (1-r)^3. " src="http://upload.wikimedia.org/math/d/f/2/df2df78c50ec6f12e78abfc669dad68e.png" /></dd>
</dl>
<p>As with the prior, the likelihood is open to revision -- more complex assumptions will yield more complex likelihood functions. Maintaining the current assumptions, we compute the normalizing factor,</p>
<dl>
<dd><img class="tex" alt="
\begin{align}
\int_0^1 P( m=7|r, n=10) \, f(r) \, dr &amp; = \int_0^1 {10 \choose 7} \, r^7 \, (1-r)^3 \, 1 \, dr \\
&amp; = {10 \choose 7} \left / {11 \choose 3,7,1} \right . = {10 \choose 7} \, \frac{1}{1320}
\end{align}
" src="http://upload.wikimedia.org/math/b/c/7/bc7df52ca063297f56643d96d35c5a64.png" /></dd>
</dl>
<p>and the posterior distribution for <i>r</i> is then</p>
<dl>
<dd><img class="tex" alt=" f(r | n=10, m=7) = 
 \frac{{10 \choose 7} \, r^7 \, (1-r)^3 \, 1} {{10 \choose 7} \, \frac{1}{1320}} = 1320 \, r^7 \, (1-r)^3 " src="http://upload.wikimedia.org/math/c/5/9/c5932ce24d2e109050d0846a541a70b1.png" /></dd>
</dl>
<p>for <i>r</i> between 0 and 1, inclusive.</p>
<p>One may be interested in the probability that more than half the voters will vote "yes". The prior probability that more than half the voters will vote "yes" is 1/2, by the symmetry of the <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniform distribution</a>. In comparison, the posterior probability that more than half the voters will vote "yes", i.e., the conditional probability given the outcome of the opinion poll – that seven of the 10 voters questioned will vote "yes" – is</p>
<dl>
<dd><img class="tex" alt="1320\int_{1/2}^1 r^7(1-r)^3\,dr \approx 0.887, \!" src="http://upload.wikimedia.org/math/d/c/5/dc58f3b5ce30cd67b381fb124e19c384.png" /></dd>
</dl>
<p>which is about an "89% chance".</p>
<p><a name="Example_3:_The_Monty_Hall_problem" id="Example_3:_The_Monty_Hall_problem"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=12" title="Edit section: Example 3: The Monty Hall problem">edit</a>]</span> <span class="mw-headline">Example 3: The Monty Hall problem</span></h3>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/Monty_Hall_problem" title="Monty Hall problem">Monty Hall problem</a></div>
<p>We are presented with three doors to choose - red, green, and blue - one of which has a prize hidden behind it. We choose the <b>red</b> door. The presenter, who knows where the prize is, opens the <b>blue</b> door and reveals that there is no prize behind it. He then asks if we wish to change our mind about our initial selection of <b>red</b>. Will changing our mind at this point improve our chances of winning the prize?</p>
<p>You might think that, with two doors left unopened, you have a 50:50 chance with either door, and so there is no point in changing doors. However, this is not the case. Let us call the situation that the prize is behind a given door <b>A<sub>r</sub></b>, <b>A<sub>g</sub></b>, and <b>A<sub>b</sub></b>.</p>
<p>To start with, <img class="tex" alt="P(A_r) = P(A_g) = P(A_b) = \frac 1 3" src="http://upload.wikimedia.org/math/f/9/c/f9c94dd8c87f868043f2ff054a4444b3.png" />, and to make things simpler we shall assume that we have already picked the red door.</p>
<p>Let us call <b>B</b> "the presenter opens the blue door". Without any prior knowledge, we would assign this a probability of 50%.</p>
<ul>
<li>In the situation where the prize is behind the red door, the presenter is free to pick between the green or the blue door at random. Thus, <span class="texhtml"><i>P</i>(<i>B</i> | <i>A</i><sub><i>r</i></sub>) = 1 / 2</span></li>
<li>In the situation where the prize is behind the green door, the presenter must pick the blue door. Thus, <span class="texhtml"><i>P</i>(<i>B</i> | <i>A</i><sub><i>g</i></sub>) = 1</span></li>
<li>In the situation where the prize is behind the blue door, the presenter must pick the green door. Thus, <span class="texhtml"><i>P</i>(<i>B</i> | <i>A</i><sub><i>b</i></sub>) = 0</span></li>
</ul>
<p>Thus,</p>
<dl>
<dd><img class="tex" alt="
\begin{align}
P(A_r|B) &amp; =  \frac{P(B | A_r) P(A_r)}{P(B)} =   \frac{\frac 1 2 \cdot \frac 1 3}{\frac 1 2} = \frac 1 3
\\
P(A_g|B) &amp; =  \frac{P(B | A_g) P(A_g)}{P(B)} =   \frac{1 \cdot \frac 1 3}{\frac 1 2} = \frac 2 3
\\
P(A_b|B) &amp; =  \frac{P(B | A_b) P(A_b)}{P(B)} =   \frac{0 \cdot \frac 1 3}{\frac 1 2} = 0.
\end{align}
" src="http://upload.wikimedia.org/math/4/6/f/46f1bf76203c510184e4d447d6c1101c.png" /></dd>
</dl>
<p>So, we should always choose the green door.</p>
<p>Note how this depends on the value of P(B). Another way of looking at the apparent inconsistency is that, when you chose the first door, you had a 1/3 chance of being right. When the second door was removed from the list of possibilities, this left the last door with a 2/3 chance of being right.</p>
<p><a name="Historical_remarks" id="Historical_remarks"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=13" title="Edit section: Historical remarks">edit</a>]</span> <span class="mw-headline">Historical remarks</span></h2>
<p>An investigation by a statistics professor (Stigler 1983) suggests that Bayes' theorem was discovered by <a href="/wiki/Nicholas_Saunderson" title="Nicholas Saunderson">Nicholas Saunderson</a> some time before Bayes. However, this interpretation is argued against in (Edwards 1986).</p>
<p>Bayes' theorem is named after the Reverend <a href="/wiki/Thomas_Bayes" title="Thomas Bayes">Thomas Bayes</a> (1702–1761), who studied how to compute a distribution for the parameter of a <a href="/wiki/Binomial_distribution" title="Binomial distribution">binomial distribution</a> (to use modern terminology). His friend, <a href="/wiki/Richard_Price" title="Richard Price">Richard Price</a>, edited and presented the work in 1763, after Bayes' death, as <i>An Essay towards solving a Problem in the Doctrine of Chances</i>. <a href="/wiki/Pierre-Simon_Laplace" title="Pierre-Simon Laplace">Pierre-Simon Laplace</a> replicated and extended these results in an essay of 1774, apparently unaware of Bayes' work.</p>
<p>One of Bayes' results (Proposition 5) gives a simple description of <a href="/wiki/Conditional_probability" title="Conditional probability">conditional probability</a>, and shows that it can be expressed independently of the order in which things occur:</p>
<dl>
<dd>If there be two subsequent events, the probability of the second b/N and the probability of both together P/N, and it being first discovered that the second event has also happened, from hence I guess that the first event has also happened, the probability I am right [i.e., the conditional probability of the first event being true given that the second has also happened] is P/b.</dd>
</dl>
<p>Note that the expression says nothing about the <i>order</i> in which the events occurred; it measures correlation, not causation. His preliminary results, in particular Propositions 3, 4, and 5, imply the result now called Bayes' Theorem (as described above), but it does not appear that Bayes himself emphasized or focused on that result</p>
<p>Bayes' main result (Proposition 9 in the essay) is the following: assuming a <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniform distribution</a> for the <a href="/wiki/Prior_distribution" title="Prior distribution" class="mw-redirect">prior distribution</a> of the <a href="/wiki/Binomial" title="Binomial">binomial</a> parameter <i>p</i>, the probability that <i>p</i> is between two values <i>a</i> and <i>b</i> is</p>
<dl>
<dd><img class="tex" alt="
\frac {\int_a^b {n+m \choose m} p^m (1-p)^n\,dp}
 {\int_0^1 {n+m \choose m} p^m (1-p)^n\,dp}
\!" src="http://upload.wikimedia.org/math/2/b/d/2bd02825f98ac442e1819e108ab10de5.png" /></dd>
</dl>
<p>where <i>m</i> is the number of observed successes and <i>n</i> the number of observed failures.</p>
<p>What is "Bayesian" about Proposition 9 is that Bayes presented it as a probability for the parameter <i>p</i>. So, one can compute probability for an experimental outcome, but also for the parameter which governs it, and the same algebra is used to make inferences of either kind.</p>
<p>Bayes states his question in a way that might make the idea of assigning a probability distribution to a parameter palatable to a frequentist. He supposes that a billiard ball is thrown at random onto a billiard table, and that the probabilities <i>p</i> and <i>q</i> are the probabilities that subsequent billiard balls will fall above or below the first ball.</p>
<p><a href="/wiki/Stephen_Fienberg" title="Stephen Fienberg">Stephen Fienberg</a> <a href="http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf" class="external autonumber" title="http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf" rel="nofollow">[1]</a> describes the evolution of the field from "inverse probability" at the time of Bayes and Laplace, and even of <a href="/wiki/Harold_Jeffreys" title="Harold Jeffreys">Harold Jeffreys</a> (1939) to "Bayesian" in the 1950's. The irony is that this label was introduced by <a href="/wiki/R.A._Fisher" title="R.A. Fisher" class="mw-redirect">R.A. Fisher</a> in a derogatory sense. So, historically, Bayes was not a "Bayesian". It is actually unclear whether or not he was a Bayesian in the modern sense of the term, i.e. whether or not he was interested in inference or merely in probability: the 1763 essay is more of a probability paper.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=14" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<div class="references-small" style="-moz-column-count:2; column-count:2;">
<ul>
<li><a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a></li>
<li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian network</a></li>
<li><a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian probability</a></li>
<li><a href="/wiki/Bayesian_spam_filtering" title="Bayesian spam filtering">Bayesian spam filtering</a></li>
<li><a href="/wiki/Thomas_Bayes" title="Thomas Bayes">Thomas Bayes</a></li>
<li><a href="/wiki/Bogofilter" title="Bogofilter">Bogofilter</a></li>
<li><a href="/wiki/Conjugate_prior" title="Conjugate prior">Conjugate prior</a></li>
<li><a href="/wiki/Empirical_Bayes_method" title="Empirical Bayes method">Empirical Bayes method</a></li>
<li><a href="/wiki/Monty_Hall_problem" title="Monty Hall problem">Monty Hall problem</a></li>
<li><a href="/wiki/Occam%27s_razor" title="Occam's razor">Occam's razor</a></li>
<li><a href="/wiki/Prosecutor%27s_fallacy" title="Prosecutor's fallacy">Prosecutor's fallacy</a></li>
<li><a href="/wiki/Raven_paradox" title="Raven paradox">Raven paradox</a></li>
<li><a href="/wiki/Recursive_Bayesian_estimation" title="Recursive Bayesian estimation">Recursive Bayesian estimation</a></li>
<li><a href="/wiki/Revising_opinions_in_statistics" title="Revising opinions in statistics">Revising opinions in statistics</a></li>
<li><a href="/wiki/Sequential_bayesian_filtering" title="Sequential bayesian filtering" class="mw-redirect">Sequential bayesian filtering</a></li>
<li><a href="/wiki/Borel%27s_paradox" title="Borel's paradox" class="mw-redirect">Borel's paradox</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes classifier</a></li>
</ul>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=15" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<div class="references-small"></div>
<p><a name="Versions_of_the_essay" id="Versions_of_the_essay"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=16" title="Edit section: Versions of the essay">edit</a>]</span> <span class="mw-headline">Versions of the essay</span></h3>
<ul>
<li>Thomas Bayes (1763), "An Essay towards solving a Problem in the Doctrine of Chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S.", <i><a href="/wiki/Philosophical_Transactions" title="Philosophical Transactions" class="mw-redirect">Philosophical Transactions</a>, Giving Some Account of the Present Undertakings, Studies and Labours of the Ingenious in Many Considerable Parts of the World</i> 53:370–418.</li>
<li>Thomas Bayes (1763/1958) "Studies in the History of Probability and Statistics: IX. Thomas Bayes' Essay Towards Solving a Problem in the Doctrine of Chances", <i><a href="/wiki/Biometrika" title="Biometrika">Biometrika</a></i> 45:296–315. <i>(Bayes' essay in modernized notation)</i></li>
<li>Thomas Bayes <a href="http://www.stat.ucla.edu/history/essay.pdf" class="external text" title="http://www.stat.ucla.edu/history/essay.pdf" rel="nofollow">"An essay towards solving a Problem in the Doctrine of Chances"</a>. <i>(Bayes' essay in the original notation)</i></li>
</ul>
<p><a name="Commentaries" id="Commentaries"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=17" title="Edit section: Commentaries">edit</a>]</span> <span class="mw-headline">Commentaries</span></h3>
<ul>
<li><a href="/wiki/George_Alfred_Barnard" title="George Alfred Barnard">G. A. Barnard</a> (1958) "Studies in the History of Probability and Statistics: IX. Thomas Bayes' Essay Towards Solving a Problem in the Doctrine of Chances", <i>Biometrika</i> 45:293–295. <i>(biographical remarks)</i></li>
<li>Daniel Covarrubias. <a href="http://www.stat.rice.edu/~blairc/seminar/Files/danTalk.pdf" class="external text" title="http://www.stat.rice.edu/~blairc/seminar/Files/danTalk.pdf" rel="nofollow">"An Essay Towards Solving a Problem in the Doctrine of Chances"</a>. <i>(an outline and exposition of Bayes' essay)</i></li>
<li>Stephen M. Stigler (1982). "Thomas Bayes' Bayesian Inference," <i>Journal of the Royal Statistical Society</i>, Series A, 145:250–258. <i>(Stigler argues for a revised interpretation of the essay; recommended)</i></li>
<li><a href="/wiki/Isaac_Todhunter" title="Isaac Todhunter">Isaac Todhunter</a> (1865). <i>A History of the Mathematical Theory of Probability from the time of Pascal to that of Laplace</i>, Macmillan. Reprinted 1949, 1956 by Chelsea and 2001 by Thoemmes.</li>
<li><a href="http://yudkowsky.net/bayes/bayes.html" class="external text" title="http://yudkowsky.net/bayes/bayes.html" rel="nofollow">An Intuitive Explanation of Bayesian Reasoning</a> (includes biography)</li>
</ul>
<p><a name="Additional_material" id="Additional_material"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit&amp;section=18" title="Edit section: Additional material">edit</a>]</span> <span class="mw-headline">Additional material</span></h3>
<ul>
<li>Pierre-Simon Laplace (1774/1986), "Memoir on the Probability of the Causes of Events", <i>Statistical Science</i> 1(3):364–378.</li>
<li>Stephen M. Stigler (1986), "Laplace's 1774 memoir on inverse probability", <i>Statistical Science</i> 1(3):359–378.</li>
<li>Stephen M. Stigler (1983), "Who Discovered Bayes' Theorem?" <i>The American Statistician</i> 37(4):290–296.</li>
<li>A. W. F. Edwards (1986), "Is the Reference in Hartley (1749) to Bayesian Inference?", <i>The American Statistician</i> 40(2):109–110.</li>
<li>Jeff Miller, <i>et al.</i>, <a href="http://jeff560.tripod.com/b.html" class="external text" title="http://jeff560.tripod.com/b.html" rel="nofollow">Earliest Known Uses of Some of the Words of Mathematics (B)</a>. (<i>very informative; recommended</i>)</li>
<li><a href="/wiki/Athanasios_Papoulis" title="Athanasios Papoulis">Athanasios Papoulis</a> (1984), <i>Probability, Random Variables, and Stochastic Processes</i>, second edition. New York: McGraw-Hill.</li>
<li><a href="http://www.inference.phy.cam.ac.uk/mackay/itila/" class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itila/" rel="nofollow">The on-line textbook: Information Theory, Inference, and Learning Algorithms</a>, by <a href="/wiki/David_J._C._MacKay" title="David J. C. MacKay">David J. C. MacKay</a> provides an up to date overview of the use of Bayes' theorem in information theory and machine learning.</li>
<li><a href="http://plato.stanford.edu/entries/bayes-theorem" class="external text" title="http://plato.stanford.edu/entries/bayes-theorem" rel="nofollow">Bayes' Theorem</a> entry in the <i><a href="/wiki/Stanford_Encyclopedia_of_Philosophy" title="Stanford Encyclopedia of Philosophy">Stanford Encyclopedia of Philosophy</a></i> by James Joyce, provides a comprehensive introduction to Bayes' theorem.</li>
<li><a href="http://plato.stanford.edu/entries/logic-inductive/" class="external text" title="http://plato.stanford.edu/entries/logic-inductive/" rel="nofollow">Stanford Encyclopedia of Philosophy: Inductive Logic</a> provides a comprehensive Bayesian treatment of Inductive Logic and Confirmation Theory.</li>
<li><cite id="Reference-Mathworld-Bayes.27_Theorem"><a href="/wiki/Eric_W._Weisstein" title="Eric W. Weisstein">Eric W. Weisstein</a>, <i><a href="http://mathworld.wolfram.com/BayesTheorem.html" class="external text" title="http://mathworld.wolfram.com/BayesTheorem.html" rel="nofollow">Bayes' Theorem</a></i> at <a href="/wiki/MathWorld" title="MathWorld">MathWorld</a>.</cite></li>
<li><i><a href="http://planetmath.org/encyclopedia/BayesTheorem.html" class="external text" title="http://planetmath.org/encyclopedia/BayesTheorem.html" rel="nofollow">Bayes' theorem</a></i> at <a href="/wiki/PlanetMath" title="PlanetMath">PlanetMath</a>.</li>
<li><a href="/wiki/Eliezer_S._Yudkowsky" title="Eliezer S. Yudkowsky" class="mw-redirect">Eliezer S. Yudkowsky</a> (2003), "<a href="http://yudkowsky.net/bayes/bayes.html" class="external text" title="http://yudkowsky.net/bayes/bayes.html" rel="nofollow">An Intuitive Explanation of Bayesian Reasoning</a>"</li>
<li><a href="http://www.celiagreen.com/charlesmccreery/statistics/bayestutorial.pdf" class="external text" title="http://www.celiagreen.com/charlesmccreery/statistics/bayestutorial.pdf" rel="nofollow">A tutorial on probability and Bayes’ theorem devised for Oxford University psychology students</a></li>
<li><a href="http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne%20--%20Confirmation%20Theory.pdf" class="external text" title="http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne%20--%20Confirmation%20Theory.pdf" rel="nofollow">Confirmation Theory</a> An extensive presentation of Bayesian Confirmation Theory</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 288/1000000
Post-expand include size: 1186/2048000 bytes
Template argument size: 321/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:49569-0!1!0!default!!en!2 and timestamp 20090407170134 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">http://en.wikipedia.org/wiki/Bayes%27_theorem</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Probability_theory" title="Category:Probability theory">Probability theory</a></span> | <span dir='ltr'><a href="/wiki/Category:Mathematical_theorems" title="Category:Mathematical theorems">Mathematical theorems</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_theorems" title="Category:Statistical theorems">Statistical theorems</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Bayes%27_theorem" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Bayes%27_theorem" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Bayes%27_theorem&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Bayes%27_theorem&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Bayes%27_theorem" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Bayes%27_theorem" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Bayes%27_theorem" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Bayes%27_theorem&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Bayes%27_theorem&amp;oldid=282374912" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Bayes%27_theorem&amp;id=282374912">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D9%85%D8%A8%D8%B1%D9%87%D9%86%D8%A9_%D8%A8%D8%A7%D9%8A%D8%B2">العربية</a></li>
				<li class="interwiki-bg"><a href="http://bg.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%BD%D0%B0_%D0%91%D0%B5%D0%B9%D1%81">Български</a></li>
				<li class="interwiki-ca"><a href="http://ca.wikipedia.org/wiki/Teorema_de_Bayes">Català</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Bayestheorem">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Teorema_de_Bayes">Español</a></li>
				<li class="interwiki-eu"><a href="http://eu.wikipedia.org/wiki/Bayesen_teorema">Euskara</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_Bayes">Français</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%A0%95%EB%A6%AC">한국어</a></li>
				<li class="interwiki-is"><a href="http://is.wikipedia.org/wiki/Form%C3%BAla_Bayes">Íslenska</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Teorema_di_Bayes">Italiano</a></li>
				<li class="interwiki-he"><a href="http://he.wikipedia.org/wiki/%D7%97%D7%95%D7%A7_%D7%91%D7%99%D7%99%D7%A1">עברית</a></li>
				<li class="interwiki-lt"><a href="http://lt.wikipedia.org/wiki/Bajeso_teorema">Lietuvių</a></li>
				<li class="interwiki-mn"><a href="http://mn.wikipedia.org/wiki/%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D1%8B%D0%BD_%D1%82%D0%B5%D0%BE%D1%80%D0%B5%D0%BC">Монгол</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Theorema_van_Bayes">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E3%83%99%E3%82%A4%E3%82%BA%E3%81%AE%E5%AE%9A%E7%90%86">日本語</a></li>
				<li class="interwiki-no"><a href="http://no.wikipedia.org/wiki/Bayes%27_teorem">‪Norsk (bokmål)‬</a></li>
				<li class="interwiki-pms"><a href="http://pms.wikipedia.org/wiki/F%C3%B3rmola_%C3%ABd_Bayes">Piemontèis</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Twierdzenie_Bayesa">Polski</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/Teorema_de_Bayes">Português</a></li>
				<li class="interwiki-ro"><a href="http://ro.wikipedia.org/wiki/Teorema_lui_Bayes">Română</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0">Русский</a></li>
				<li class="interwiki-sr"><a href="http://sr.wikipedia.org/wiki/%D0%91%D0%B0%D1%98%D0%B5%D1%81%D0%BE%D0%B2%D0%B0_%D1%82%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0">Српски / Srpski</a></li>
				<li class="interwiki-su"><a href="http://su.wikipedia.org/wiki/T%C3%A9or%C3%A9ma_Bayes">Basa Sunda</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/Bayesin_teoreema">Suomi</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Bayes_sats">Svenska</a></li>
				<li class="interwiki-tr"><a href="http://tr.wikipedia.org/wiki/Bayes_teoremi">Türkçe</a></li>
				<li class="interwiki-uk"><a href="http://uk.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D1%94%D1%81%D0%B0">Українська</a></li>
				<li class="interwiki-ur"><a href="http://ur.wikipedia.org/wiki/%D8%A8%DB%92%D8%B2_%D9%85%D8%B3%D9%84%D8%A6%DB%81_%D8%A7%D8%AB%D8%A8%D8%A7%D8%AA%DB%8C">اردو</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_l%C3%BD_Bayes">Tiếng Việt</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 7 April 2009, at 17:01 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv180 in 0.054 secs. --></body></html>
