<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Naive Bayes classifier,Bayes&#039; theorem,Bayesian inference,Bayesian network,Bayesian probability,Bayesian spam filtering,Bayesian statistics,Bernoulli distribution,Boosting,Classifier,Classifier (mathematics)" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Naive Bayes classifier - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Naive_Bayes_classifier";
		var wgTitle = "Naive Bayes classifier";
		var wgAction = "view";
		var wgArticleId = "87339";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 280896463;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Naive_Bayes_classifier skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Naive Bayes classifier</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>A <b>naive Bayes classifier</b> is a term in <a href="/wiki/Bayesian_statistics" title="Bayesian statistics" class="mw-redirect">Bayesian</a> <a href="/wiki/Statistics" title="Statistics">statistics</a> dealing with a simple probabilistic <a href="/wiki/Classifier_(mathematics)" title="Classifier (mathematics)">classifier</a> based on applying <a href="/wiki/Bayes%27_theorem" title="Bayes' theorem">Bayes' theorem</a> with strong (naive) <a href="/wiki/Statistical_independence" title="Statistical independence" class="mw-redirect">independence</a> assumptions. A more descriptive term for the underlying probability model would be "<a href="/wiki/Statistical_independence" title="Statistical independence" class="mw-redirect">independent</a> feature model".</p>
<p>In simple terms, a naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 4" in diameter. Even though these features depend on the existence of the other features, a naive Bayes classifier considers all of these properties to independently contribute to the probability that this fruit is an apple.</p>
<p>Depending on the precise nature of the probability model, naive Bayes classifiers can be trained very efficiently in a <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> setting. In many practical applications, parameter estimation for naive Bayes models uses the method of <a href="/wiki/Maximum_likelihood" title="Maximum likelihood">maximum likelihood</a>; in other words, one can work with the naive Bayes model without believing in <a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian probability</a> or using any Bayesian methods.</p>
<p>In spite of their naive design and apparently over-simplified assumptions, naive Bayes classifiers often work much better in many complex real-world situations than one might expect. Recently, careful analysis of the Bayesian classification problem has shown that there are some theoretical reasons for the apparently unreasonable <a href="/wiki/Efficacy" title="Efficacy">efficacy</a> of naive Bayes classifiers (<a href="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf" class="external text" title="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf" rel="nofollow">Zhang04</a>). An advantage of the naive Bayes classifier is that it requires a small amount of training data to estimate the parameters (means and variances of the variables) necessary for classification. Because independent variables are assumed, only the variances of the variables for each class need to be determined and not the entire covariance matrix.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#The_naive_Bayes_probabilistic_model"><span class="tocnumber">1</span> <span class="toctext">The naive Bayes probabilistic model</span></a></li>
<li class="toclevel-1"><a href="#Parameter_estimation"><span class="tocnumber">2</span> <span class="toctext">Parameter estimation</span></a></li>
<li class="toclevel-1"><a href="#Constructing_a_classifier_from_the_probability_model"><span class="tocnumber">3</span> <span class="toctext">Constructing a classifier from the probability model</span></a></li>
<li class="toclevel-1"><a href="#Discussion"><span class="tocnumber">4</span> <span class="toctext">Discussion</span></a></li>
<li class="toclevel-1"><a href="#Example:_document_classification"><span class="tocnumber">5</span> <span class="toctext">Example: document classification</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">8</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="The_naive_Bayes_probabilistic_model" id="The_naive_Bayes_probabilistic_model"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=1" title="Edit section: The naive Bayes probabilistic model">edit</a>]</span> <span class="mw-headline">The naive Bayes probabilistic model</span></h2>
<p>Abstractly, the probability model for a classifier is a conditional model</p>
<dl>
<dd><img class="tex" alt="p(C \vert F_1,\dots,F_n)\," src="http://upload.wikimedia.org/math/4/7/a/47a22e381fe04c90c2115d4ac369f590.png" /></dd>
</dl>
<p>over a dependent class variable <span class="texhtml"><i>C</i></span> with a small number of outcomes or <i>classes</i>, conditional on several feature variables <span class="texhtml"><i>F</i><sub>1</sub></span> through <span class="texhtml"><i>F</i><sub><i>n</i></sub></span>. The problem is that if the number of features <span class="texhtml"><i>n</i></span> is large or when a feature can take on a large number of values, then basing such a model on probability tables is infeasible. We therefore reformulate the model to make it more tractable.</p>
<p>Using <a href="/wiki/Bayes%27_theorem" title="Bayes' theorem">Bayes' theorem</a>, we write</p>
<dl>
<dd><img class="tex" alt="p(C \vert F_1,\dots,F_n) = \frac{p(C) \ p(F_1,\dots,F_n\vert C)}{p(F_1,\dots,F_n)}. \," src="http://upload.wikimedia.org/math/3/1/7/3174021f44ba0d31f6ede772624c5523.png" /></dd>
</dl>
<p>In plain English the above equation can be written as</p>
<dl>
<dd><img class="tex" alt="\mbox{posterior} = \frac{\mbox{prior} \times \mbox{likelihood}}{\mbox{evidence}}. \," src="http://upload.wikimedia.org/math/b/7/e/b7ee688183182869d44cfaa3b22f5eb6.png" /></dd>
</dl>
<p>In practice we are only interested in the numerator of that fraction, since the denominator does not depend on <span class="texhtml"><i>C</i></span> and the values of the features <span class="texhtml"><i>F</i><sub><i>i</i></sub></span> are given, so that the denominator is effectively constant. The numerator is equivalent to the <a href="/wiki/Joint_probability" title="Joint probability" class="mw-redirect">joint probability</a> model</p>
<dl>
<dd><img class="tex" alt="p(C, F_1, \dots, F_n)\," src="http://upload.wikimedia.org/math/0/4/4/044dfd3e1b822193af59618fa30640b2.png" /></dd>
</dl>
<p>which can be rewritten as follows, using repeated applications of the definition of <a href="/wiki/Conditional_probability" title="Conditional probability">conditional probability</a>:</p>
<dl>
<dd><img class="tex" alt="p(C, F_1, \dots, F_n)\," src="http://upload.wikimedia.org/math/0/4/4/044dfd3e1b822193af59618fa30640b2.png" /></dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="= p(C) \ p(F_1,\dots,F_n\vert C)" src="http://upload.wikimedia.org/math/f/9/a/f9afa278bebc99dcf98d0c128c0bc4f6.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="= p(C) \ p(F_1\vert C) \ p(F_2,\dots,F_n\vert C, F_1)" src="http://upload.wikimedia.org/math/8/4/4/84456edba2adc600eba6a60630263fee.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="= p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3,\dots,F_n\vert C, F_1, F_2)" src="http://upload.wikimedia.org/math/7/c/6/7c65e02bf0e3ea028db57b2cd36fb47d.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="= p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3\vert C, F_1, F_2) \ p(F_4,\dots,F_n\vert C, F_1, F_2, F_3)" src="http://upload.wikimedia.org/math/4/d/0/4d040737ebbea0caace384f9d7c8f3b5.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="= p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3\vert C, F_1, F_2) \ \dots p(F_n\vert C, F_1, F_2, F_3,\dots,F_{n-1})" src="http://upload.wikimedia.org/math/0/a/b/0ab2627d6b56c1fd3a1e7d27ba6b4868.png" /></dd>
</dl>
</dd>
</dl>
<p>and so forth. Now the "naive" <a href="/wiki/Conditional_independence" title="Conditional independence">conditional independence</a> assumptions come into play: assume that each feature <span class="texhtml"><i>F</i><sub><i>i</i></sub></span> is conditionally <a href="/wiki/Statistical_independence" title="Statistical independence" class="mw-redirect">independent</a> of every other feature <span class="texhtml"><i>F</i><sub><i>j</i></sub></span> for <img class="tex" alt="j\neq i" src="http://upload.wikimedia.org/math/c/c/f/ccf5f455f5de788b5d5eeaf339059de1.png" />. This means that</p>
<dl>
<dd><img class="tex" alt="p(F_i \vert C, F_j) = p(F_i \vert C)\," src="http://upload.wikimedia.org/math/8/3/0/83005704f3e8c5a3a325b4ded09708b7.png" /></dd>
</dl>
<p>and so the joint model can be expressed as</p>
<dl>
<dd><img class="tex" alt="p(C, F_1, \dots, F_n)
= p(C) \ p(F_1\vert C) \ p(F_2\vert C) \ p(F_3\vert C) \ \cdots\," src="http://upload.wikimedia.org/math/a/b/e/abe40736c314a87f0cf2e470f11d838d.png" /></dd>
</dl>
<dl>
<dd><img class="tex" alt="= p(C) \prod_{i=1}^n p(F_i \vert C).\," src="http://upload.wikimedia.org/math/a/b/9/ab90205c26b3ab9caa56efdb3fd29d85.png" /></dd>
</dl>
<p>This means that under the above independence assumptions, the conditional distribution over the class variable <span class="texhtml"><i>C</i></span> can be expressed like this:</p>
<dl>
<dd><img class="tex" alt="p(C \vert F_1,\dots,F_n) = \frac{1}{Z}  p(C) \prod_{i=1}^n p(F_i \vert C)" src="http://upload.wikimedia.org/math/b/a/e/bae76a0ac40ee656990387fe8f79d0bf.png" /></dd>
</dl>
<p>where <span class="texhtml"><i>Z</i></span> is a scaling factor dependent only on <img class="tex" alt="F_1,\dots,F_n" src="http://upload.wikimedia.org/math/4/5/2/4529f5e95212fcbc7ffbb467bd575bcb.png" />, i.e., a constant if the values of the feature variables are known.</p>
<p>Models of this form are much more manageable, since they factor into a so-called <i>class prior</i> <span class="texhtml"><i>p</i>(<i>C</i>)</span> and independent probability distributions <img class="tex" alt="p(F_i\vert C)" src="http://upload.wikimedia.org/math/c/8/0/c80fd97680b395e2f338b9bcc5656bbe.png" />. If there are <span class="texhtml"><i>k</i></span> classes and if a model for <span class="texhtml"><i>p</i>(<i>F</i><sub><i>i</i></sub>)</span> can be expressed in terms of <span class="texhtml"><i>r</i></span> parameters, then the corresponding naive Bayes model has (<i>k</i> − 1) + <i>n</i> <i>r</i> <i>k</i> parameters. In practice, often <span class="texhtml"><i>k</i> = 2</span> (binary classification) and <span class="texhtml"><i>r</i> = 1</span> (<a href="/wiki/Bernoulli_distribution" title="Bernoulli distribution">Bernoulli variables</a> as features) are common, and so the total number of parameters of the naive Bayes model is <span class="texhtml">2<i>n</i> + 1</span>, where <span class="texhtml"><i>n</i></span> is the number of binary features used for prediction.</p>
<p><a name="Parameter_estimation" id="Parameter_estimation"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=2" title="Edit section: Parameter estimation">edit</a>]</span> <span class="mw-headline">Parameter estimation</span></h2>
<p>All model parameters (<i>i.e.</i>, class priors and feature probability distributions) can be approximated with relative frequencies from the training set. These are maximum likelihood estimates of the probabilities. Non-discrete features need to be <a href="/wiki/Discretization" title="Discretization">discretized</a> first. Discretization can be <a href="/w/index.php?title=Unsupervised_discretization&amp;action=edit&amp;redlink=1" class="new" title="Unsupervised discretization (page does not exist)">unsupervised</a> (ad-hoc selection of bins) or <a href="/w/index.php?title=Supervised_discretization&amp;action=edit&amp;redlink=1" class="new" title="Supervised discretization (page does not exist)">supervised</a> (binning guided by information in training data).</p>
<p>If a given class and feature value never occur together in the training set then the frequency-based probability estimate will be zero. This is problematic since it will wipe out all information in the other probabilities when they are multiplied. It is therefore often desirable to incorporate a small-sample correction in all probability estimates such that <a href="/wiki/Pseudocount" title="Pseudocount">no probability is ever set to be exactly zero</a>.</p>
<p><a name="Constructing_a_classifier_from_the_probability_model" id="Constructing_a_classifier_from_the_probability_model"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=3" title="Edit section: Constructing a classifier from the probability model">edit</a>]</span> <span class="mw-headline">Constructing a classifier from the probability model</span></h2>
<p>The discussion so far has derived the independent feature model, that is, the naive Bayes <a href="/w/index.php?title=Probability_model&amp;action=edit&amp;redlink=1" class="new" title="Probability model (page does not exist)">probability model</a>. The naive Bayes <a href="/wiki/Classifier" title="Classifier">classifier</a> combines this model with a <a href="/wiki/Decision_rule" title="Decision rule">decision rule</a>. One common rule is to pick the hypothesis that is most probable; this is known as the <i><a href="/wiki/Maximum_a_posteriori" title="Maximum a posteriori" class="mw-redirect">maximum a posteriori</a></i> or <i>MAP</i> decision rule. The corresponding classifier is the function <span class="texhtml">classify</span> defined as follows:</p>
<dl>
<dd><img class="tex" alt="\mathrm{classify}(f_1,\dots,f_n) = \mathop{\mathrm{argmax}}_c \ p(C=c) \prod_{i=1}^n p(F_i=f_i\vert C=c)." src="http://upload.wikimedia.org/math/0/d/6/0d618707facce1bea29e9ce8b03be20a.png" /></dd>
</dl>
<p><a name="Discussion" id="Discussion"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=4" title="Edit section: Discussion">edit</a>]</span> <span class="mw-headline">Discussion</span></h2>
<p>One should notice that the independence assumption may lead to some unexpected results in the calculation of the posterior probability. In some circumstances, when there is a dependency between observations, the value computed above may be greater than one thereby contradicting the second axiom of probability which requires all probability values to be less than or equal to one.</p>
<p>Despite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>, such as the need for data sets that scale exponentially with the number of features. Like all probabilistic classifiers under the MAP decision rule, it arrives at the correct classification as long as the correct class is more probable than any other class; hence class probabilities do not have to be estimated very well. In other words, the overall classifier is robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below.</p>
<p><a name="Example:_document_classification" id="Example:_document_classification"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=5" title="Edit section: Example: document classification">edit</a>]</span> <span class="mw-headline">Example: document classification</span></h2>
<p>Here is a worked example of naive Bayesian classification to the <a href="/wiki/Document_classification" title="Document classification">document classification</a> problem. Consider the problem of classifying documents by their content, for example into <a href="/wiki/Spamming" title="Spamming" class="mw-redirect">spam</a> and non-spam <a href="/wiki/E-mail" title="E-mail">E-mails</a>. Imagine that documents are drawn from a number of classes of documents which can be modelled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class <i>C</i> can be written as</p>
<dl>
<dd><img class="tex" alt="p(w_i \vert C)\," src="http://upload.wikimedia.org/math/5/4/f/54f5d05ffaddba35f01b580cff44352d.png" /></dd>
</dl>
<p>(For this treatment, we simplify things further by assuming that the probability of a word in a document is independent of the length of a document, or that all documents are of the same length.)</p>
<p>Then the probability of a given document <i>D</i>, given a class <i>C</i>, is</p>
<dl>
<dd><img class="tex" alt="p(D\vert C)=\prod_i p(w_i \vert C)\," src="http://upload.wikimedia.org/math/4/a/4/4a4ed759de3f17f91be5c2b6f900887d.png" /></dd>
</dl>
<p>The question that we desire to answer is: "what is the probability that a given document <i>D</i> belongs to a given class <i>C</i>?" In other words, what is <img class="tex" alt="p(C \vert D)\," src="http://upload.wikimedia.org/math/b/c/6/bc68476e4d625d6649dc15998b80de86.png" />?</p>
<p>Now, by their definition, (see <a href="/wiki/Probability_axiom" title="Probability axiom" class="mw-redirect">Probability axiom</a>)</p>
<dl>
<dd><img class="tex" alt="p(D\vert C)={p(D\cap C)\over p(C)}" src="http://upload.wikimedia.org/math/7/7/b/77b516d530adfad335295ed750946be8.png" /></dd>
</dl>
<p>and</p>
<dl>
<dd><img class="tex" alt="p(C\vert D)={p(D\cap C)\over p(D)}" src="http://upload.wikimedia.org/math/0/1/6/016d5b5fbf4ebe500da3b3b9cb4298d1.png" /></dd>
</dl>
<p>Bayes' theorem manipulates these into a statement of probability in terms of <a href="/wiki/Likelihood" title="Likelihood" class="mw-redirect">likelihood</a>.</p>
<dl>
<dd><img class="tex" alt="p(C\vert D)={p(C)\over p(D)}\,p(D\vert C)" src="http://upload.wikimedia.org/math/f/8/a/f8a721484a0aafbdf29c9f1fc357de29.png" /></dd>
</dl>
<p>Assume for the moment that there are only two classes, <i>S</i> and ¬<i>S</i> (e.g. spam and not spam).</p>
<dl>
<dd><img class="tex" alt="p(D\vert S)=\prod_i p(w_i \vert S)\," src="http://upload.wikimedia.org/math/5/7/e/57e17fd9122ba156997e3a53a41e38d8.png" /></dd>
</dl>
<p>and</p>
<dl>
<dd><img class="tex" alt="p(D\vert\neg S)=\prod_i p(w_i\vert\neg S)\," src="http://upload.wikimedia.org/math/d/7/6/d762b98242953612d4877a96e030269b.png" /></dd>
</dl>
<p>Using the Bayesian result above, we can write:</p>
<dl>
<dd><img class="tex" alt="p(S\vert D)={p(S)\over p(D)}\,\prod_i p(w_i \vert S)" src="http://upload.wikimedia.org/math/9/a/f/9aff3b4958fdb888e5391a8f710a4adc.png" /></dd>
</dl>
<dl>
<dd><img class="tex" alt="p(\neg S\vert D)={p(\neg S)\over p(D)}\,\prod_i p(w_i \vert\neg S)" src="http://upload.wikimedia.org/math/2/e/2/2e240ac0b44df0296d485c9ceb6ebe13.png" /></dd>
</dl>
<p>Dividing one by the other gives:</p>
<dl>
<dd><img class="tex" alt="{p(S\vert D)\over p(\neg S\vert D)}={p(S)\,\prod_i p(w_i \vert S)\over p(\neg S)\,\prod_i p(w_i \vert\neg S)}" src="http://upload.wikimedia.org/math/9/a/d/9ad1bf98e32748e481c8c8ddb6862eb0.png" /></dd>
</dl>
<p>Which can be re-factored as:</p>
<dl>
<dd><img class="tex" alt="{p(S\vert D)\over p(\neg S\vert D)}={p(S)\over p(\neg S)}\,\prod_i {p(w_i \vert S)\over p(w_i \vert\neg S)}" src="http://upload.wikimedia.org/math/2/f/e/2fe9c15070644fbb769ada49428ed346.png" /></dd>
</dl>
<p>Thus, the probability ratio p(<i>S</i> | <i>D</i>) / p(¬<i>S</i> | <i>D</i>) can be expressed in terms of a series of <a href="/wiki/Likelihood_ratio" title="Likelihood ratio" class="mw-redirect">likelihood ratios</a>. The actual probability p(<i>S</i> | <i>D</i>) can be easily computed from log (p(<i>S</i> | <i>D</i>) / p(¬<i>S</i> | <i>D</i>)) based on the observation that p(<i>S</i> | <i>D</i>) + p(¬<i>S</i> | <i>D</i>) = 1.</p>
<p>Taking the <a href="/wiki/Logarithm" title="Logarithm">logarithm</a> of all these ratios, we have:</p>
<dl>
<dd><img class="tex" alt="\ln{p(S\vert D)\over p(\neg S\vert D)}=\ln{p(S)\over p(\neg S)}+\sum_i \ln{p(w_i\vert S)\over p(w_i\vert\neg S)}" src="http://upload.wikimedia.org/math/d/0/a/d0a16f3fc2876536cd6f22a25bd2121d.png" /></dd>
</dl>
<p>(This technique of "<a href="/wiki/Log-likelihood_ratio" title="Log-likelihood ratio" class="mw-redirect">log-likelihood ratios</a>" is a common technique in statistics. In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a <a href="/wiki/Sigmoid_curve" title="Sigmoid curve" class="mw-redirect">sigmoid curve</a>: see <a href="/wiki/Logit" title="Logit">logit</a> for details.)</p>
<p>Finally, the document can be classified as follows. It is spam if <img class="tex" alt="\ln{p(S\vert D)\over p(\neg S\vert D)} &gt; 0" src="http://upload.wikimedia.org/math/a/b/5/ab5712728ef0b4d73a9ece3fcafd8224.png" />, otherwise it is not spam.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=6" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Bayesian_spam_filtering" title="Bayesian spam filtering">Bayesian spam filtering</a></li>
<li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian network</a></li>
<li><a href="/wiki/Random_naive_Bayes" title="Random naive Bayes">Random naive Bayes</a></li>
<li><a href="/wiki/Linear_classifier" title="Linear classifier">Linear classifier</a></li>
<li><a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a> (esp. as Bayesian techniques relate to <a href="/wiki/Spam_(e-mail)" title="Spam (e-mail)" class="mw-redirect">spam</a>)</li>
<li><a href="/wiki/Boosting" title="Boosting">Boosting</a></li>
<li><a href="/wiki/Fuzzy_logic" title="Fuzzy logic">Fuzzy logic</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Neural_network" title="Neural network">Neural networks</a></li>
<li><a href="/wiki/Predictive_analytics" title="Predictive analytics">Predictive analytics</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine</a></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=7" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li>Domingos, Pedro &amp; Michael Pazzani (1997) "On the optimality of the simple Bayesian classifier under zero-one loss". <i>Machine Learning</i>, 29:103–­137. <i>(also online at <a href="http://citeseer.ist.psu.edu/" class="external text" title="http://citeseer.ist.psu.edu/" rel="nofollow">CiteSeer</a>: <a href="http://citeseer.ist.psu.edu/domingos97optimality.html" class="external autonumber" title="http://citeseer.ist.psu.edu/domingos97optimality.html" rel="nofollow">[1]</a>)</i></li>
</ul>
<ul>
<li>Rish, Irina. (2001). "An empirical study of the naive Bayes classifier". IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence. <i>(available online: <a href="http://www.research.ibm.com/people/r/rish/papers/RC22230.pdf" class="external text" title="http://www.research.ibm.com/people/r/rish/papers/RC22230.pdf" rel="nofollow">PDF</a>, <a href="http://www.research.ibm.com/people/r/rish/papers/ijcai-ws.ps" class="external text" title="http://www.research.ibm.com/people/r/rish/papers/ijcai-ws.ps" rel="nofollow">PostScript</a>)</i></li>
</ul>
<ul>
<li>Hand, DJ, &amp; Yu, K. (2001). "Idiot's Bayes - not so stupid after all?" International Statistical Review. Vol 69 part 3, pages 385-399. ISSN 0306-7734.</li>
</ul>
<ul>
<li>Mozina M, Demsar J, Kattan M, &amp; Zupan B. (2004). "Nomograms for Visualization of Naive Bayesian Classifier". In Proc. of PKDD-2004, pages 337-348. <i>(available online: <a href="http://www.ailab.si/blaz/papers/2004-PKDD.pdf" class="external text" title="http://www.ailab.si/blaz/papers/2004-PKDD.pdf" rel="nofollow">PDF</a>)</i></li>
</ul>
<ul>
<li>Maron, M. E. (1961). "Automatic Indexing: An Experimental Inquiry." Journal of the ACM (JACM) 8(3):404–417. <i>(available online: <a href="http://delivery.acm.org/10.1145/330000/321084/p404-maron.pdf?key1=321084&amp;key2=9636178211&amp;coll=GUIDE&amp;dl=ACM&amp;CFID=56729577&amp;CFTOKEN=37855803" class="external text" title="http://delivery.acm.org/10.1145/330000/321084/p404-maron.pdf?key1=321084&amp;key2=9636178211&amp;coll=GUIDE&amp;dl=ACM&amp;CFID=56729577&amp;CFTOKEN=37855803" rel="nofollow">PDF</a>)</i></li>
</ul>
<ul>
<li>Minsky, M. (1961). "Steps toward Artificial Intelligence." Proceedings of the IRE 49(1):8-30.</li>
</ul>
<ul>
<li>McCallum, A. and Nigam K. "A Comparison of Event Models for Naive Bayes Text Classification". In AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48. Technical Report WS-98-05. AAAI Press. 1998. <i>(available online: <a href="http://www.kamalnigam.com/papers/multinomial-aaaiws98.pdf" class="external text" title="http://www.kamalnigam.com/papers/multinomial-aaaiws98.pdf" rel="nofollow">PDF</a>)</i></li>
</ul>
<ul>
<li>Harry Zhang "The Optimality of Naive Bayes". (available online: <a href="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf" class="external text" title="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf" rel="nofollow">PDF</a>)</li>
</ul>
<ul>
<li>S.Kotsiantis, P. Pintelas, Increasing the Classification Accuracy of Simple Bayesian Classifier, Lecture Notes in Artificial Intelligence, AIMSA 2004, Springer-Verlag Vol 3192, pp. 198-207, 2004 (<a href="http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/improvingNB.pdf" class="external free" title="http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/improvingNB.pdf" rel="nofollow">http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/improvingNB.pdf</a>)</li>
</ul>
<ul>
<li>S. Kotsiantis, P. Pintelas, Logitboost of Simple Bayesian Classifier, Computational Intelligence in Data mining Special Issue of the Informatica Journal, Vol 29 (1), pp. 53-59, 2005 (<a href="http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/05_Kotsiantis-Logitboost%20of%20simble%20bayesian..._No%205.pdf" class="external free" title="http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/05_Kotsiantis-Logitboost%20of%20simble%20bayesian..._No%205.pdf" rel="nofollow">http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/05_Kotsiantis-Logitboost%20of%20simble%20bayesian..._No%205.pdf</a>)</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit&amp;section=8" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://citeseer.ist.psu.edu/30545.html" class="external text" title="http://citeseer.ist.psu.edu/30545.html" rel="nofollow">Naive Bayesian learning</a></li>
<li><a href="http://www.biomedcentral.com/1471-2105/7/514" class="external text" title="http://www.biomedcentral.com/1471-2105/7/514" rel="nofollow">Hierarchical Naive Bayes Classifiers for uncertain data</a> (an extension of the Naive Bayes classifier).</li>
<li><a href="http://www.convo.co.uk/x02/" class="external text" title="http://www.convo.co.uk/x02/" rel="nofollow">Online application of a Naive Bayes classifier</a> (emotion modelling), with a full explanation.</li>
<li><a href="http://www.bayesnews.xpg.com.br/" class="external text" title="http://www.bayesnews.xpg.com.br/" rel="nofollow">BayesNews - Bayesian RSS Reader</a> (useful for personal news clipping).</li>
</ul>
<dl>
<dt>Software</dt>
</dl>
<ul>
<li><a href="http://paul.luminos.nl/documents/show_document.php?d=198" class="external text" title="http://paul.luminos.nl/documents/show_document.php?d=198" rel="nofollow">Naive Bayes implementation in Visual Basic</a> (includes executable and source code).</li>
<li>An interactive <a href="/wiki/Microsoft_Excel" title="Microsoft Excel">Microsoft Excel</a> spreadsheet <a href="http://downloads.sourceforge.net/naivebayesclass/NaiveBayesDemo.xls?use_mirror=osdn" class="external text" title="http://downloads.sourceforge.net/naivebayesclass/NaiveBayesDemo.xls?use_mirror=osdn" rel="nofollow">Naive Bayes implementation</a> using <a href="/wiki/Visual_Basic_for_Applications" title="Visual Basic for Applications">VBA</a> (requires enabled macros) with viewable source code.</li>
<li><a href="http://jbnc.sourceforge.net/" class="external text" title="http://jbnc.sourceforge.net/" rel="nofollow">jBNC - Bayesian Network Classifier Toolbox</a></li>
<li><a href="http://popfile.sourceforge.net/" class="external text" title="http://popfile.sourceforge.net/" rel="nofollow">POPFile</a> Perl-based email proxy system classifies email into user-defined "buckets", including spam.</li>
<li><a href="http://cmp.felk.cvut.cz/cmp/software/stprtool/" class="external text" title="http://cmp.felk.cvut.cz/cmp/software/stprtool/" rel="nofollow">Statistical Pattern Recognition Toolbox for Matlab</a>.</li>
<li><a href="http://www.lwebzem.com/cgi-bin/res/naive_bayes_tm_classifier.cgi" class="external text" title="http://www.lwebzem.com/cgi-bin/res/naive_bayes_tm_classifier.cgi" rel="nofollow">Document Classification Using Naive Bayes Classifier with Perl</a>.</li>
<li><a href="http://www.sux0r.org" class="external text" title="http://www.sux0r.org" rel="nofollow">sux0r</a> An <a href="/wiki/Open_Source" title="Open Source" class="mw-redirect">Open Source</a> <a href="/wiki/Content_management_system" title="Content management system">Content management system</a> with a focus on Naive Bayesian categorization and probabilistic content.</li>
<li><a href="http://people.csail.mit.edu/jrennie/ifile/" class="external text" title="http://people.csail.mit.edu/jrennie/ifile/" rel="nofollow">ifile</a> - the first freely available (Naive) Bayesian mail/spam filter</li>
<li><a href="http://www.vni.com/products/imsl/documentation/CNL700_Docs/html/cstat/default.htm" class="external text" title="http://www.vni.com/products/imsl/documentation/CNL700_Docs/html/cstat/default.htm" rel="nofollow">Naive Bayes algorithm in the Visual Numerics IMSL C Library</a></li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 162/1000000
Post-expand include size: 0/2048000 bytes
Template argument size: 0/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:87339-0!1!0!default!!en!2 and timestamp 20090413081714 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">http://en.wikipedia.org/wiki/Naive_Bayes_classifier</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></span> | <span dir='ltr'><a href="/wiki/Category:Bayesian_statistics" title="Category:Bayesian statistics">Bayesian statistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_classification" title="Category:Statistical classification">Statistical classification</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Naive_Bayes_classifier" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Naive_Bayes_classifier" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Naive_Bayes_classifier&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Naive_Bayes_classifier" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Naive_Bayes_classifier" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Naive_Bayes_classifier" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Naive_Bayes_classifier&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Naive_Bayes_classifier&amp;oldid=280896463" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Naive_Bayes_classifier&amp;id=280896463">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Bayes-Klassifikator">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Clasificador_bayesiano">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Classification_na%C3%AFve_bayesienne">Français</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Classificatore_bayesiano">Italiano</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E5%8D%98%E7%B4%94%E3%83%99%E3%82%A4%E3%82%BA%E5%88%86%E9%A1%9E%E5%99%A8">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Naiwny_klasyfikator_bayesowski">Polski</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9_%D0%B1%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B8%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80">Русский</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 31 March 2009, at 17:16 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv145 in 0.078 secs. --></body></html>
