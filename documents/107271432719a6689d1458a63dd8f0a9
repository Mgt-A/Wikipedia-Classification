<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Scale-invariant feature transform,Articles with unsourced statements since August 2008,FeatureDetectionCompVisNavbox,2008,3D modeling,3D object recognition,3D single object recognition,Affine shape adaptation,Affine transformations,Augmented reality,August 20" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Scale-invariant feature transform - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Scale-invariant_feature_transform";
		var wgTitle = "Scale-invariant feature transform";
		var wgAction = "view";
		var wgArticleId = "1208345";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 283792218;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Scale-invariant_feature_transform skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Scale-invariant feature transform</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="infobox" cellspacing="5" style="width: 22em; text-align: left; font-size: 88%; line-height: 1.5em; width:20em;">
<tr>
<td colspan="2" class="" style="text-align:center; font-size: 125%; font-weight: bold;"><a href="/wiki/Feature_detection_(computer_vision)" title="Feature detection (computer vision)">Feature detection</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/File:Corner.png" class="image" title="Corner.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/thumb/5/5f/Corner.png/200px-Corner.png" width="200" height="174" border="0" /></a><br />
<span style="">Output of a typical corner detection algorithm</span></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Edge_detection" title="Edge detection">Edge detection</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Canny_edge_detector" title="Canny edge detector">Canny</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Canny_edge_detector#Conclusion" title="Canny edge detector">Canny-Deriche</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Edge_detection#Differential_edge_detection" title="Edge detection">Differential</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Sobel_operator" title="Sobel operator">Sobel</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Interest_point_detection" title="Interest point detection">Interest point detection</a></th>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Corner_detection" title="Corner detection">Corner detection</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_Harris_.26_Stephens_.2F_Plessey_corner_detection_algorithm" title="Corner detection">Harris operator</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_Shi_and_Tomasi_corner_detection_algorithm" title="Corner detection">Shi and Tomasi</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_level_curve_curvature_approach" title="Corner detection">Level curve curvature</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_SUSAN_corner_detector" title="Corner detection">SUSAN</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_FAST_feature_detector" title="Corner detection">FAST</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Blob_detection" title="Blob detection">Blob detection</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Blob_detection#The_Laplacian_of_Gaussian" title="Blob detection">Laplacian of Gaussian (LoG)</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Difference_of_Gaussians" title="Difference of Gaussians">Difference of Gaussians (DoG)</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Blob_detection#The_determinant_of_the_Hessian" title="Blob detection">Determinant of Hessian (DoH)</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Maximally_stable_extremal_regions" title="Maximally stable extremal regions">Maximally stable extremal regions</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Ridge_detection" title="Ridge detection">Ridge detection</a></th>
</tr>
<tr>
<th colspan="2" style="text-align:center;">Affine invariant feature detection</th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Affine_shape_adaptation" title="Affine shape adaptation">Affine shape adaptation</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Harris_affine_region_detector" title="Harris affine region detector">Harris affine</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Hessian_Affine_region_detector" title="Hessian Affine region detector">Hessian affine</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;">Feature description</th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><strong class="selflink">SIFT</strong></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/SURF" title="SURF">SURF</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/GLOH" title="GLOH">GLOH</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/LESH" title="LESH">LESH</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Scale-space" title="Scale-space" class="mw-redirect">Scale-space</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Scale-space_axioms" title="Scale-space axioms">Scale-space axioms</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Scale-space_implementation" title="Scale-space implementation" class="mw-redirect">Implementation details</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Pyramid_(image_processing)" title="Pyramid (image processing)">Pyramids</a></td>
</tr>
<tr>
<td style="text-align:right;" colspan="2">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;; font-size:xx-small;">This box: <a href="/wiki/Template:FeatureDetectionCompVisNavbox" title="Template:FeatureDetectionCompVisNavbox"><span title="View this template" style="">view</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/w/index.php?title=Template_talk:FeatureDetectionCompVisNavbox&amp;action=edit&amp;redlink=1" class="new" title="Template talk:FeatureDetectionCompVisNavbox (page does not exist)"><span title="Discussion about this template" style="">talk</span></a></div>
</td>
</tr>
</table>
<p><b>Scale-invariant feature transform</b> (or <b>SIFT</b>) is an algorithm in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> to detect and describe local features in images. The algorithm was published by <a href="/wiki/David_Lowe_(computer_scientist)" title="David Lowe (computer scientist)" class="mw-redirect">David Lowe</a> in 1999.<sup id="cite_ref-lowe99_0-0" class="reference"><a href="#cite_note-lowe99-0" title=""><span>[</span>1<span>]</span></a></sup></p>
<p>Applications include <a href="/wiki/Object_recognition" title="Object recognition">object recognition</a>, <a href="/wiki/Robotic_mapping" title="Robotic mapping">robotic mapping</a> and navigation, <a href="/wiki/Image_stitching" title="Image stitching">image stitching</a>, <a href="/wiki/3D_modeling" title="3D modeling">3D modeling</a>, <a href="/wiki/Gesture_recognition" title="Gesture recognition">gesture recognition</a>, <a href="/wiki/Video_tracking" title="Video tracking">video tracking</a>, and <a href="/wiki/Match_moving" title="Match moving">match moving</a>.</p>
<p>The algorithm is patented; the owner is the <a href="/wiki/University_of_British_Columbia" title="University of British Columbia">University of British Columbia</a>.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Features"><span class="tocnumber">1</span> <span class="toctext">Features</span></a></li>
<li class="toclevel-1"><a href="#Algorithm"><span class="tocnumber">2</span> <span class="toctext">Algorithm</span></a>
<ul>
<li class="toclevel-2"><a href="#Scale-space_extrema_detection"><span class="tocnumber">2.1</span> <span class="toctext">Scale-space extrema detection</span></a></li>
<li class="toclevel-2"><a href="#Keypoint_localization"><span class="tocnumber">2.2</span> <span class="toctext">Keypoint localization</span></a>
<ul>
<li class="toclevel-3"><a href="#Interpolation_of_nearby_data_for_accurate_position"><span class="tocnumber">2.2.1</span> <span class="toctext">Interpolation of nearby data for accurate position</span></a></li>
<li class="toclevel-3"><a href="#Discarding_low-contrast_keypoints"><span class="tocnumber">2.2.2</span> <span class="toctext">Discarding low-contrast keypoints</span></a></li>
<li class="toclevel-3"><a href="#Eliminating_edge_responses"><span class="tocnumber">2.2.3</span> <span class="toctext">Eliminating edge responses</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="#Orientation_assignment"><span class="tocnumber">2.3</span> <span class="toctext">Orientation assignment</span></a></li>
<li class="toclevel-2"><a href="#Keypoint_descriptor"><span class="tocnumber">2.4</span> <span class="toctext">Keypoint descriptor</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Comparison_of_SIFT_features_with_other_local_features"><span class="tocnumber">3</span> <span class="toctext">Comparison of SIFT features with other local features</span></a></li>
<li class="toclevel-1"><a href="#Applications"><span class="tocnumber">4</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2"><a href="#Object_recognition_using_SIFT_features"><span class="tocnumber">4.1</span> <span class="toctext">Object recognition using SIFT features</span></a></li>
<li class="toclevel-2"><a href="#Robot_localization_and_mapping"><span class="tocnumber">4.2</span> <span class="toctext">Robot localization and mapping</span></a></li>
<li class="toclevel-2"><a href="#Panorama_stitching"><span class="tocnumber">4.3</span> <span class="toctext">Panorama stitching</span></a></li>
<li class="toclevel-2"><a href="#3D_scene_modeling.2C_recognition_and_tracking"><span class="tocnumber">4.4</span> <span class="toctext">3D scene modeling, recognition and tracking</span></a></li>
<li class="toclevel-2"><a href="#3D_SIFT_descriptors_for_human_action_recognition"><span class="tocnumber">4.5</span> <span class="toctext">3D SIFT descriptors for human action recognition</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#Implementations"><span class="tocnumber">7</span> <span class="toctext">Implementations</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Features" id="Features"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=1" title="Edit section: Features">edit</a>]</span> <span class="mw-headline">Features</span></h2>
<p>The detection and description of local image features can help in object recognition. The SIFT features are local and based on the appearance of the object at particular interest points, and are invariant to image scale and rotation. They are also robust to changes in illumination, noise, and minor changes in viewpoint. In addition to these properties, they are highly distinctive, relatively easy to extract, allow for correct object identification with low probability of mismatch and are easy to match against a (large) database of local features. Object description by set of SIFT features is also robust to partial occlusion; as few as 3 SIFT features from an object are enough to compute its location and pose. Recognition can be performed in close-to-real time, at least for small databases and on modern computer hardware.<sup class="noprint Inline-Template"><span title="These claims need references to reliable sources&#160;since August 2008" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">citations needed</a></i>]</span></sup></p>
<p><a name="Algorithm" id="Algorithm"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=2" title="Edit section: Algorithm">edit</a>]</span> <span class="mw-headline">Algorithm</span></h2>
<p><a name="Scale-space_extrema_detection" id="Scale-space_extrema_detection"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=3" title="Edit section: Scale-space extrema detection">edit</a>]</span> <span class="mw-headline">Scale-space extrema detection</span></h3>
<p>This is the stage where the interest points, which are called keypoints in the SIFT framework, are detected. For this, the image is convolved with Gaussian filters at different scales, and then the difference of successive Gaussian-blurred images are taken. Keypoints are then taken as maxima/minima of the <a href="/wiki/Difference_of_Gaussians" title="Difference of Gaussians">Difference of Gaussians</a> (DoG) that occur at multiple scales. Specifically, a DoG image <img class="tex" alt="D \left( x, y, \sigma \right)" src="http://upload.wikimedia.org/math/e/2/8/e2835717b44325ed3d38e2d795afa0d9.png" /> is given by</p>
<dl>
<dd><img class="tex" alt="D \left( x, y, \sigma \right) = L \left( x, y, k_i\sigma \right) - L \left( x, y, k_j\sigma \right)" src="http://upload.wikimedia.org/math/9/5/9/9597523db32aabad5806992066db99de.png" />,</dd>
<dd>where <img class="tex" alt="L \left( x, y, k\sigma \right)" src="http://upload.wikimedia.org/math/4/c/0/4c049f4d3ad14fbb4657109e1c08dd13.png" /> is the original image <img class="tex" alt="I \left( x, y \right)" src="http://upload.wikimedia.org/math/6/9/9/699a1e5afae7cb73361eeeaa898272db.png" /> convolved with the <a href="/wiki/Gaussian_blur" title="Gaussian blur">Gaussian blur</a> <img class="tex" alt="G \left( x, y, k\sigma \right)" src="http://upload.wikimedia.org/math/0/2/1/02149de0f7ef3f940a87b82cf731abff.png" /> at scale <span class="texhtml"><i>k</i>σ</span>, i.e.,</dd>
</dl>
<dl>
<dd><img class="tex" alt="L \left( x, y, k\sigma \right) = G \left( x, y, k\sigma \right) * I \left( x, y \right)" src="http://upload.wikimedia.org/math/2/3/6/236c8ca4832fc7472db0f6a922b86633.png" /></dd>
</dl>
<p>Hence a DoG image between scales <span class="texhtml"><i>k</i><sub><i>i</i></sub>σ</span> and <span class="texhtml"><i>k</i><sub><i>j</i></sub>σ</span> is just the difference of the Gaussian-blurred images at scales <span class="texhtml"><i>k</i><sub><i>i</i></sub>σ</span> and <span class="texhtml"><i>k</i><sub><i>j</i></sub>σ</span>. For <a href="/wiki/Scale-space" title="Scale-space" class="mw-redirect">scale-space</a> extrema detection in the SIFT algorithm, the image is first convolved with Gaussian-blurs at different scales. The convolved images are grouped by octave (an octave corresponds to doubling the value of <span class="texhtml">σ</span>), and the value of <span class="texhtml"><i>k</i><sub><i>i</i></sub></span> is selected so that we obtain a fixed number of convolved images per octave. Then the Difference-of-Gaussian images are taken from adjacent Gaussian-blurred images per octave.</p>
<p>Once DoG images have been obtained, keypoints are identified as local minima/maxima of the DoG images across scales. This is done by comparing each pixel in the DoG images to its eight neighbors at the same scale and nine corresponding neighboring pixels in each of the neighboring scales. If the pixel value is the maximum or minimum among all compared pixels, it is selected as a candidate keypoint.</p>
<p>This keypoint detection step is a variation of one of the <a href="/wiki/Blob_detection" title="Blob detection">blob detection</a> methods by detecting scale-space extrema of the scale normalized Laplacian,<sup id="cite_ref-Lindeberg1998_2-0" class="reference"><a href="#cite_note-Lindeberg1998-2" title=""><span>[</span>3<span>]</span></a></sup> that is detecting points that are local extrema with respect to both space and scale, in the discrete case by comparisons with the nearest 26 neighbours in a discretized scale-space volume. The difference of Gaussians operator can be seen as an approximation to the Laplacian, here expressed in a <a href="/wiki/Pyramid_(image_processing)" title="Pyramid (image processing)">pyramid</a> setting.</p>
<p><a name="Keypoint_localization" id="Keypoint_localization"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=4" title="Edit section: Keypoint localization">edit</a>]</span> <span class="mw-headline">Keypoint localization</span></h3>
<div class="thumb tright">
<div class="thumbinner" style="width:182px;"><a href="/wiki/File:Sift_keypoints_filtering.jpg" class="image" title="After scale space extrema are detected (their location being shown in the uppermost image) the SIFT algorithm discards low contrast keypoints (remaining points are shown in the middle image) and then filters out those located on edges. Resulting set of keypoints is shown on last image."><img alt="" src="http://upload.wikimedia.org/wikipedia/en/thumb/4/44/Sift_keypoints_filtering.jpg/180px-Sift_keypoints_filtering.jpg" width="180" height="540" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Sift_keypoints_filtering.jpg" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
After scale space extrema are detected (their location being shown in the uppermost image) the SIFT algorithm discards low contrast keypoints (remaining points are shown in the middle image) and then filters out those located on edges. Resulting set of keypoints is shown on last image.</div>
</div>
</div>
<p>Scale-space extrema detection produces too many keypoint candidates, some of which are unstable. The next step in the algorithm is to perform a detailed fit to the nearby data for accurate location, scale, and ratio of <a href="/wiki/Principal_curvatures" title="Principal curvatures" class="mw-redirect">principal curvatures</a>. This information allows points to be rejected that have low contrast (and are therefore sensitive to noise) or are poorly localized along an edge. <a name="Interpolation_of_nearby_data_for_accurate_position" id="Interpolation_of_nearby_data_for_accurate_position"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=5" title="Edit section: Interpolation of nearby data for accurate position">edit</a>]</span> <span class="mw-headline">Interpolation of nearby data for accurate position</span></h4>
<p>First, for each candidate keypoint, interpolation of nearby data is used to accurately determine its position. The initial approach was to just locate each keypoint at the location and scale of the candidate keypoint.<sup id="cite_ref-lowe99_0-1" class="reference"><a href="#cite_note-lowe99-0" title=""><span>[</span>1<span>]</span></a></sup> The new approach calculates the interpolated location of the maximum, which substantially improves matching and stability.<sup id="cite_ref-lowe04_3-0" class="reference"><a href="#cite_note-lowe04-3" title=""><span>[</span>4<span>]</span></a></sup> The interpolation is done using the quadratic <a href="/wiki/Taylor_expansion" title="Taylor expansion" class="mw-redirect">Taylor expansion</a> of the Difference-of-Gaussian scale-space function, <img class="tex" alt="D \left( x, y, \sigma \right)" src="http://upload.wikimedia.org/math/e/2/8/e2835717b44325ed3d38e2d795afa0d9.png" /> with the candidate keypoint as the origin. This Taylor expansion is given by:</p>
<dl>
<dd><img class="tex" alt="D(\textbf{x}) = D + \frac{\partial D^T}{\partial \textbf{x}}\textbf{x} + \frac{1}{2}\textbf{x}^T \frac{\partial^2 D}{\partial \textbf{x}^2} \textbf{x}" src="http://upload.wikimedia.org/math/d/7/4/d74e1771afe8ac0b9e1080d870bea49b.png" /></dd>
</dl>
<p>where D and its derivatives are evaluated at the candidate keypoint and <img class="tex" alt="\textbf{x} = \left( x, y, \sigma \right)" src="http://upload.wikimedia.org/math/e/8/2/e82b2cae087435997b2c191eb9391019.png" /> is the offset from this point. The location of the extremum, <img class="tex" alt="\hat{\textbf{x}}" src="http://upload.wikimedia.org/math/e/c/e/ece291d68793beb41e8e77da66ae6cec.png" />, is determined by taking the derivative of this function with respect to <img class="tex" alt="\textbf{x}" src="http://upload.wikimedia.org/math/f/2/a/f2a48e1cd2da440643ea07a3b2f60e6f.png" /> and setting it to zero. If the offset <img class="tex" alt="\hat{\textbf{x}}" src="http://upload.wikimedia.org/math/e/c/e/ece291d68793beb41e8e77da66ae6cec.png" /> is larger than <span class="texhtml">0.5</span> in any dimension, then that's an indication that the extremum lies closer to another candidate keypoint. In this case, the candidate keypoint is changed and the interpolation performed instead about that point. Otherwise the offset is added to its candidate keypoint to get the interpolated estimate for the location of the extremum.</p>
<p><a name="Discarding_low-contrast_keypoints" id="Discarding_low-contrast_keypoints"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=6" title="Edit section: Discarding low-contrast keypoints">edit</a>]</span> <span class="mw-headline">Discarding low-contrast keypoints</span></h4>
<p>To discard the keypoints with low contrast, the value of the second-order Taylor expansion <img class="tex" alt="D(\textbf{x})" src="http://upload.wikimedia.org/math/3/f/4/3f4a079a2e9a77c5d9aa3c4e4065b2de.png" /> is computed at the offset <img class="tex" alt="\hat{\textbf{x}}" src="http://upload.wikimedia.org/math/e/c/e/ece291d68793beb41e8e77da66ae6cec.png" />. If this value is less than <span class="texhtml">0.03</span>, the candidate keypoint is discarded. Otherwise it is kept, with final location <img class="tex" alt="\textbf{y} + \hat{\textbf{x}}" src="http://upload.wikimedia.org/math/f/a/0/fa0b5cf3642353e067a70f34570492d4.png" /> and scale <span class="texhtml">σ</span>, where <img class="tex" alt="\textbf{y}" src="http://upload.wikimedia.org/math/f/f/5/ff58c8e0e55b508d25fa7aff97d497b1.png" /> is the original location of the keypoint at scale <span class="texhtml">σ</span>.</p>
<p><a name="Eliminating_edge_responses" id="Eliminating_edge_responses"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=7" title="Edit section: Eliminating edge responses">edit</a>]</span> <span class="mw-headline">Eliminating edge responses</span></h4>
<p>The DoG function will have strong responses along edges, even if the candidate keypoint is unstable to small amounts of noise. Therefore, in order to increase stability, we need to eliminate the keypoints that have poorly determined locations but have high edge responses.</p>
<p>For poorly defined peaks in the DoG function, the <a href="/wiki/Principal_curvature" title="Principal curvature">principal curvature</a> across the edge would be much larger than the principal curvature along it. Finding these principal curvatures amounts to solving for the eigenvalues of the second-order <a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessian matrix</a>, <b>H</b>:</p>
<dl>
<dd><img class="tex" alt=" \textbf{H} =  \begin{bmatrix}
  D_{xx} &amp; D_{xy} \\
  D_{xy} &amp; D_{yy}
\end{bmatrix} " src="http://upload.wikimedia.org/math/5/5/8/5589f4d1eb9fa86bba3eec504c2d09fe.png" /></dd>
</dl>
<p>The eigenvalues of <b>H</b> are proportional to the principal curvatures of D. It turns out that the ratio of the two eigenvalues, say <span class="texhtml">α</span> is the larger one, and <span class="texhtml">β</span> the smaller one, with ratio <span class="texhtml"><i>r</i> = α / β</span>, is sufficient for SIFT's purposes. The trace of <b>H</b>, i.e., <span class="texhtml"><i>D</i><sub><i>x</i><i>x</i></sub> + <i>D</i><sub><i>y</i><i>y</i></sub></span>, gives us the sum of the two eigenvalues, while its determinant, i.e., <img class="tex" alt="D_{xx} D_{yy} - D_{xy}^2" src="http://upload.wikimedia.org/math/e/2/7/e27266b9fbbb085d7d6d7b9f7167feba.png" />, yields the product. The ratio <img class="tex" alt=" \text{R} = \operatorname{Tr} \left( \textbf{H} \right)^2/\operatorname{Det}  \left( \textbf{H} \right)" src="http://upload.wikimedia.org/math/7/5/5/7555dbe6438492e25c415c0f37a95fb3.png" /> can be shown to be equal to <img class="tex" alt="\left( r+1 \right)^2/r" src="http://upload.wikimedia.org/math/0/0/b/00b304fa11ae1d3a50593d9767bacb48.png" />, which depends only on the ratio of the eigenvalues rather than their individual values. R is minimum when the eigenvalues are equal to each other. Therefore the higher the absolute difference between the two eigenvalues, which is equivalent to a higher absolute difference between the two principal curvatures of D, the higher the value of R. It follows that, for some threshold eigenvalue ratio <span class="texhtml"><i>r</i><sub>th</sub></span>, if R for a candidate keypoint is larger than <img class="tex" alt="\left( r_{\text{th}} + 1 \right)^2/r_{\text{th}}" src="http://upload.wikimedia.org/math/8/6/4/8641bc2db100e79ded5b346f3a70452e.png" />, that keypoint is poorly localized and hence rejected. The new approach uses <span class="texhtml"><i>r</i><sub>th</sub> = 10</span>.<sup id="cite_ref-lowe04_3-1" class="reference"><a href="#cite_note-lowe04-3" title=""><span>[</span>4<span>]</span></a></sup></p>
<p>This processing step for suppressing responses at edges is a transfer of a corresponding approach in the Harris operator for <a href="/wiki/Corner_detection" title="Corner detection">corner detection</a>. The difference is that the measure for thresholding is computed from the Hessian matrix instead of a second-moment matrix (see <a href="/wiki/Structure_tensor" title="Structure tensor">structure tensor</a>).</p>
<p><a name="Orientation_assignment" id="Orientation_assignment"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=8" title="Edit section: Orientation assignment">edit</a>]</span> <span class="mw-headline">Orientation assignment</span></h3>
<p>In this step, each keypoint is assigned one or more orientations based on local image gradient directions. This is the key step in achieving <a href="/wiki/Rotational_invariance" title="Rotational invariance">invariance to rotation</a> as the keypoint descriptor can be represented relative to this orientation and therefore achieve invariance to image rotation.</p>
<p>First, the Gaussian-smoothed image <img class="tex" alt="L \left( x, y, \sigma \right)" src="http://upload.wikimedia.org/math/3/5/d/35d3d513a6d71e7d04f3a5b942d1eb6d.png" /> at the keypoint's scale <span class="texhtml">σ</span> is taken so that all computations are performed in a scale-invariant manner. For an image sample <img class="tex" alt="L \left( x, y \right)" src="http://upload.wikimedia.org/math/e/d/6/ed6d058b0fb933e5e0b558a3fae30d7f.png" /> at scale <span class="texhtml">σ</span>, the gradient magnitude, <img class="tex" alt="m \left( x, y \right)" src="http://upload.wikimedia.org/math/c/5/6/c56e58c9a61f7d03e16cb18f023277fe.png" />, and orientation, <img class="tex" alt="\theta \left( x, y \right)" src="http://upload.wikimedia.org/math/8/c/1/8c17c3c1b3b81f1573edcee8c94a6946.png" />, are precomputed using pixel differences:</p>
<dl>
<dd><img class="tex" alt="m \left( x, y \right) = \sqrt{\left( L \left( x+1, y \right) - L \left( x-1, y \right) \right)^2 + \left( L \left( x, y+1 \right) - L \left( x, y-1 \right) \right)^2}" src="http://upload.wikimedia.org/math/2/d/f/2dfb39afd286a4ac2e17c76fa48b264d.png" /></dd>
</dl>
<dl>
<dd><img class="tex" alt="\theta \left( x, y \right) = \tan^{-1}\left(\frac{L \left( x, y+1 \right) - L \left( x, y-1 \right)}{L \left( x+1, y \right) - L \left( x-1, y \right)} \right)" src="http://upload.wikimedia.org/math/0/9/7/097936130aa680c77fdf69299e54e52b.png" /></dd>
</dl>
<p>The magnitude and direction calculations for the gradient are done for every pixel in a neighboring region around the keypoint in the Gaussian-blurred image L. An orientation histogram with 36 bins is formed, with each bin covering 10 degrees. Each sample in the neighboring window added to a histogram bin is weighted by its gradient magnitude and by a Gaussian-weighted circular window with a <span class="texhtml">σ</span> that is 1.5 times that of the scale of the keypoint. The peaks in this histogram correspond to dominant orientations. Once the histogram is filled, the orientations corresponding to the highest peak and local peaks that are within 80% of the highest peaks are assigned to the keypoint. In the case of multiple orientations being assigned, an additional keypoint is created having the same location and scale as the original keypoint for each additional orientation.</p>
<p><a name="Keypoint_descriptor" id="Keypoint_descriptor"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=9" title="Edit section: Keypoint descriptor">edit</a>]</span> <span class="mw-headline">Keypoint descriptor</span></h3>
<p>Previous steps found keypoint locations at particular scales and assigned orientations to them. This ensured invariance to image location, scale and rotation. Now we want to compute descriptor vectors for these keypoints such that the descriptors are highly distinctive and partially invariant to the remaining variations, like illumination, 3D viewpoint, etc. This step is image closest in scale to the keypoint's scale. Just like before, the contribution of each pixel is weighted by the gradient magnitude, and by a Gaussian with <span class="texhtml">σ</span> 1.5 times the scale of the keypoint. Histograms contain 8 bins each, and each descriptor contains a 4x4 array of 16 histograms around the keypoint. This leads to a SIFT feature vector with (4 x 4 x 8 = 128 elements). This vector is normalized to enhance invariance to changes in illumination.</p>
<p>Although the dimension of the descriptor, i.e. 128, seems high, descriptors with lower dimension than this don't perform as well across the range of matching tasks,<sup id="cite_ref-lowe04_3-2" class="reference"><a href="#cite_note-lowe04-3" title=""><span>[</span>4<span>]</span></a></sup> and the computational cost remains low due to the approximate BBF (see below) method used for finding the nearest-neighbor. Longer descriptors continue to do better but not by much and there is an additional danger of increased sensitivity to distortion and occlusion. It is also shown that feature matching accuracy is above 50% for viewpoint changes of up to 50 degrees. Therefore SIFT descriptors are invariant to minor affine changes. To test the distinctiveness of the SIFT descriptors, matching accuracy is also measured against varying number of keypoints in the testing database, and it is shown that matching accuracy decreases only very slightly for very large database sizes, thus indicating that SIFT features are highly distinctive.</p>
<p><a name="Comparison_of_SIFT_features_with_other_local_features" id="Comparison_of_SIFT_features_with_other_local_features"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=10" title="Edit section: Comparison of SIFT features with other local features">edit</a>]</span> <span class="mw-headline">Comparison of SIFT features with other local features</span></h2>
<p>There has been an extensive study done on the performance evaluation of different local descriptors, including SIFT, using a range of detectors.<sup id="cite_ref-Mikolajczyk2005_4-0" class="reference"><a href="#cite_note-Mikolajczyk2005-4" title=""><span>[</span>5<span>]</span></a></sup> The main results are summarized below:</p>
<ul>
<li>SIFT and SIFT-like <a href="/wiki/GLOH" title="GLOH">GLOH</a> features exhibit the highest matching accuracies (recall rates) for an affine transformation of 50 degrees. After this transformation limit, results start getting unreliable.</li>
</ul>
<ul>
<li>Distinctiveness of descriptors is measured by summing the eigenvalues of the descriptors, obtained by the <a href="/wiki/Principal_components_analysis" title="Principal components analysis" class="mw-redirect">Principal components analysis</a> of the descriptors normalized by their variance. This corresponds to the amount of variance captured by different descriptors, therefore, to their distinctiveness. PCA-SIFT (Principal Components Analysis applied to SIFT descriptors), GLOH and SIFT features give the highest values.</li>
</ul>
<ul>
<li>SIFT-based descriptors outperform other local descriptors on both textured and structured scenes, with the difference in performance larger on the textured scene.</li>
</ul>
<ul>
<li>For scale changes in the range 2-2.5 and image rotations in the range 30 to 45 degrees, SIFT and SIFT-based descriptors again outperform other local descriptors with both textured and structured scene content.</li>
</ul>
<ul>
<li>Performance for all local descriptors degraded on images introduced with a significant amount of blur, with the descriptors that are based on edges, like <a href="/wiki/Shape_context" title="Shape context">shape context</a>, performing increasingly poorly with increasing amount blur. This is because edges disappear in the case of a strong blur. But GLOH, PCA-SIFT and SIFT still performed better than the others. This is also true for evaluation in the case of illumination changes.</li>
</ul>
<p>The evaluations carried out suggests strongly that SIFT-based descriptors, which are region-based, are the most robust and distinctive, and are therefore best suited for feature matching. However, most recent feature descriptors such as <a href="/wiki/SURF" title="SURF">SURF</a> have not been evaluated in this study.</p>
<p><a href="/wiki/SURF" title="SURF">SURF</a> has later been shown to have similar performance to <a href="/wiki/SIFT" title="SIFT">SIFT</a>, while at the same time being much faster.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup></p>
<p>Recently, a slight variation of the descriptor employing an irregular histogram grid has been proposed that significantly improves its performance<sup id="cite_ref-irrgrid_6-0" class="reference"><a href="#cite_note-irrgrid-6" title=""><span>[</span>7<span>]</span></a></sup>. Instead of using a 4x4 grid of histogram bins, all bins extend to the center of the feature. This improves the descriptor's robustness to scale changes.</p>
<p><a name="Applications" id="Applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=11" title="Edit section: Applications">edit</a>]</span> <span class="mw-headline">Applications</span></h2>
<p><a name="Object_recognition_using_SIFT_features" id="Object_recognition_using_SIFT_features"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=12" title="Edit section: Object recognition using SIFT features">edit</a>]</span> <span class="mw-headline">Object recognition using SIFT features</span></h3>
<p>Given SIFT's ability to find distinctive keypoints that are invariant to location, scale and rotation, and robust to <a href="/wiki/Affine_transformations" title="Affine transformations" class="mw-redirect">affine transformations</a> (changes in <a href="/wiki/Scale" title="Scale">scale</a>, <a href="/wiki/Rotation" title="Rotation">rotation</a>, <a href="/wiki/Shear_mapping" title="Shear mapping">shear</a>, and position) and changes in illumination, they are usable for object recognition. The steps are given below.</p>
<ul>
<li>First, SIFT features are obtained from the input image using the algorithm described above.</li>
</ul>
<ul>
<li>These features are matched to the SIFT feature database obtained from the training images. This feature matching is done through a Euclidean-distance based nearest neighbor approach. To increase robustness, matches are rejected for those keypoints for which the ratio of the nearest neighbor distance to the second nearest neighbor distance is greater than 0.8. This discards many of the false matches arising from background clutter. Finally, to avoid the expensive search required for finding the Euclidean distance based nearest neighbor, an approximate algorithm, called the Best-Bin-First (BBF) algorithm is used.<sup id="cite_ref-Beis1997_7-0" class="reference"><a href="#cite_note-Beis1997-7" title=""><span>[</span>8<span>]</span></a></sup> This is a fast method for returning the nearest neighbor with high probability, and can give speedup by factor of 1000 while finding nearest neighbor (of interest) 95% of the time.</li>
</ul>
<ul>
<li>Although the distance ratio test described above discards many of the false matches arising from background clutter, we still have matches that belong to different objects. Therefore to increase robustness to object identification, we want to cluster those features that belong to the same object and reject the matches that are left out in the clustering process. This is done using the <a href="/wiki/Hough_Transform" title="Hough Transform" class="mw-redirect">Hough Transform</a>. This will identify clusters of features that vote for the same object pose. When clusters of features are found to vote for the same pose of an object, the probability of the interpretation being correct is much higher than for any single feature. Each keypoint votes for the set of object poses that are consistent with the keypoint's location, scale, and orientation. <i>Bins</i> that accumulate at least 3 votes are identified as as candidate object/pose matches.</li>
</ul>
<ul>
<li>For each candidate cluster, a least-squares solution for the best estimated affine projection parameters relating the training image to the input image is obtained. If the projection of a keypoint through these parameters lies within half the error range that was used for the parameters in the Hough transform bins, the keypoint match is kept. If fewer than 3 points remain after discarding outliers for a bin, then the object match is rejected. The least-squares fitting is repeated until no more rejections take place. This works better for planar surface recognition than 3D object recognition since the affine model is no longer accurate for 3D objects.</li>
</ul>
<p>SIFT features can essentially be applied to any task that requires identification of matching locations between images. Work has been done on applications such as recognition of particular object categories in 2D images, 3D reconstruction, motion tracking and segmentation, robot localization, image panorama stitching and epipolar calibration. Some of these are discussed in more detail below.</p>
<p><a name="Robot_localization_and_mapping" id="Robot_localization_and_mapping"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=13" title="Edit section: Robot localization and mapping">edit</a>]</span> <span class="mw-headline">Robot localization and mapping</span></h3>
<p>In this application,<sup id="cite_ref-Se2001_8-0" class="reference"><a href="#cite_note-Se2001-8" title=""><span>[</span>9<span>]</span></a></sup> a trinocular stereo system is used to determine 3D estimates for keypoint locations. Keypoints are used only when they appear in all 3 images with consistent disparities, resulting in very few outliers. As the robot moves, it localizes itself using feature matches to the existing 3D map, and then incrementally adds features to the map while updating their 3D positions using a Kalman filter. This provides a robust and accurate solution to the problem of robot localization in unknown environments.</p>
<p><a name="Panorama_stitching" id="Panorama_stitching"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=14" title="Edit section: Panorama stitching">edit</a>]</span> <span class="mw-headline">Panorama stitching</span></h3>
<p>SIFT feature matching can be used in <a href="/wiki/Image_stitching" title="Image stitching">image stitching</a> for fully automated <a href="/wiki/Panorama" title="Panorama">panorama</a> reconstruction from non-panoramic images. The SIFT features extracted from the input images are matched against each other to find <i>k</i> nearest-neighbors for each feature. These correspondences are then used to find <i>m</i> candidate matching images for each image. <a href="/wiki/Homography" title="Homography">Homographies</a> between pairs of images are then computed using <a href="/wiki/RANSAC" title="RANSAC">RANSAC</a> and a probabilistic model is used for verification. Because there is no restriction on the input images, graph search is applied to find connected components of image matches such that each connected component will correspond to a panorama. Finally for each connected component <a href="/wiki/Bundle_adjustment" title="Bundle adjustment">Bundle adjustment</a> is performed to solve for joint camera parameters, and the panorama is rendered using <a href="/w/index.php?title=Multi-band_blending&amp;action=edit&amp;redlink=1" class="new" title="Multi-band blending (page does not exist)">multi-band blending</a>. Because of the SIFT-inspired object recognition approach to panorama stitching, the resulting system is insensitive to the ordering, orientation, scale and illumination of the images. The input images contain multiple panoramas and noise images, and panoramic sequences are recognized and rendered as output. As said earlier, the algorithm is insensitive to the ordering, scale and orientation of the images. It is also insensitive to noise images which are not part of a panorama.<sup id="cite_ref-Brown2003_9-0" class="reference"><a href="#cite_note-Brown2003-9" title=""><span>[</span>10<span>]</span></a></sup></p>
<p><a name="3D_scene_modeling.2C_recognition_and_tracking" id="3D_scene_modeling.2C_recognition_and_tracking"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=15" title="Edit section: 3D scene modeling, recognition and tracking">edit</a>]</span> <span class="mw-headline">3D scene modeling, recognition and tracking</span></h3>
<p>This application uses SIFT features for <a href="/wiki/3D_object_recognition" title="3D object recognition" class="mw-redirect">3D object recognition</a> and <a href="/wiki/3D_modeling" title="3D modeling">3D modeling</a> in context of <a href="/wiki/Augmented_reality" title="Augmented reality">augmented reality</a>, in which synthetic objects with accurate pose are superimposed on real images. SIFT matching is done for a number of 2D images of a scene or object taken from different angles. This is used with <a href="/wiki/Bundle_adjustment" title="Bundle adjustment">bundle adjustment</a> to build a sparse 3D model of the viewed scene and to simultaneously recover camera poses and calibration parameters. Then the position, orientation and size of the virtual object are defined relative to the coordinate frame of the recovered model. For online <a href="/wiki/Match_moving" title="Match moving">match moving</a>, SIFT features again are extracted from the current video frame and matched to the features already computed for the world mode, resulting in a set of 2D-to-3D correspondences. These correspondences are then used to compute the current camera pose for the virtual projection and final rendering. A regularization technique is used to reduce the jitter in the virtual projection.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10" title=""><span>[</span>11<span>]</span></a></sup></p>
<p><a name="3D_SIFT_descriptors_for_human_action_recognition" id="3D_SIFT_descriptors_for_human_action_recognition"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=16" title="Edit section: 3D SIFT descriptors for human action recognition">edit</a>]</span> <span class="mw-headline">3D SIFT descriptors for human action recognition</span></h3>
<p>This application introduces 3D SIFT descriptors for spatio-temporal data in context of human action recognition in video sequences.<sup id="cite_ref-Scovanner2007_11-0" class="reference"><a href="#cite_note-Scovanner2007-11" title=""><span>[</span>12<span>]</span></a></sup> The Orientation Assignment and Descriptor Representation stages of the 2D SIFT algorithm are extended to describe SIFT features in a spatio-temporal domain. For application to human action recognition in a video sequence, random sampling of the training videos is carried out at different locations, times and scales. The spatio-temporal regions around these interest points are then described using the 3D SIFT descriptor. These descriptors are then clustered to form a spatio-temporal <a href="/wiki/Bag_of_words_model" title="Bag of words model">Bag of words model</a>. 3D SIFT descriptors extracted from the test videos are then matched against these <i>words</i> for human action classification.</p>
<p>The authors report much better results with their 3D SIFT descriptor approach than with other approaches like simple 2D SIFT descriptors and Gradient Magnitude.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12" title=""><span>[</span>13<span>]</span></a></sup></p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=17" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/3D_single_object_recognition" title="3D single object recognition">3D single object recognition</a></li>
<li><a href="/wiki/Autostitch" title="Autostitch">autostitch</a></li>
<li><a href="/wiki/Feature_detection_(computer_vision)" title="Feature detection (computer vision)">Feature detection (computer vision)</a></li>
<li><a href="/wiki/SURF" title="SURF">SURF</a> (Speeded Up Robust Features)</li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=18" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ol class="references">
<li id="cite_note-lowe99-0">^ <a href="#cite_ref-lowe99_0-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lowe99_0-1" title=""><sup><i><b>b</b></i></sup></a> <cite style="font-style:normal">Lowe, David G. (1999). "<a href="http://doi.ieeecs.org/10.1109/ICCV.1999.790410" class="external text" title="http://doi.ieeecs.org/10.1109/ICCV.1999.790410" rel="nofollow">Object recognition from local scale-invariant features</a>". <i>Proceedings of the International Conference on Computer Vision</i> <b>2</b>: 1150–1157. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a href="http://dx.doi.org/10.1109/ICCV.1999.790410" class="external text" title="http://dx.doi.org/10.1109/ICCV.1999.790410" rel="nofollow">10.1109/ICCV.1999.790410</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Proceedings+of+the+International+Conference+on+Computer+Vision&amp;rft.atitle=Object+recognition+from+local+scale-invariant+features&amp;rft.au=Lowe%2C+David+G.&amp;rft.date=1999&amp;rft.pages=1150%E2%80%931157&amp;rft.series=2&amp;rft_id=info:doi/10.1109%2FICCV.1999.790410&amp;rft_id=http%3A%2F%2Fdoi.ieeecs.org%2F10.1109%2FICCV.1999.790410"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <cite style="font-style:normal" class="web" id="CITEREFNowozin.2C_Sebastian2005">Nowozin, Sebastian (2005). <a href="http://user.cs.tu-berlin.de/~nowozin/autopano-sift/" class="external text" title="http://user.cs.tu-berlin.de/~nowozin/autopano-sift/" rel="nofollow">"autopano-sift"</a><span class="printonly">. <a href="http://user.cs.tu-berlin.de/~nowozin/autopano-sift/" class="external free" title="http://user.cs.tu-berlin.de/~nowozin/autopano-sift/" rel="nofollow">http://user.cs.tu-berlin.de/~nowozin/autopano-sift/</a></span><span class="reference-accessdate">. Retrieved on 2008-08-20</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=autopano-sift&amp;rft.atitle=&amp;rft.aulast=Nowozin%2C+Sebastian&amp;rft.au=Nowozin%2C+Sebastian&amp;rft.date=2005&amp;rft_id=http%3A%2F%2Fuser.cs.tu-berlin.de%2F%7Enowozin%2Fautopano-sift%2F&amp;rfr_id=info:sid/en.wikipedia.org:Scale-invariant_feature_transform"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-Lindeberg1998-2"><b><a href="#cite_ref-Lindeberg1998_2-0" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFLindeberg.2C_Tony1998">Lindeberg, Tony (1998). "<a href="http://www.nada.kth.se/cvap/abstracts/cvap198.html" class="external text" title="http://www.nada.kth.se/cvap/abstracts/cvap198.html" rel="nofollow">Feature detection with automatic scale selection</a>". <i>International Journal of Computer Vision</i> <b>30</b> (2): 79–116. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1023%2FA%3A1008045108935" class="external text" title="http://dx.doi.org/10.1023%2FA%3A1008045108935" rel="nofollow">10.1023/A:1008045108935</a></span><span class="printonly">. <a href="http://www.nada.kth.se/cvap/abstracts/cvap198.html" class="external free" title="http://www.nada.kth.se/cvap/abstracts/cvap198.html" rel="nofollow">http://www.nada.kth.se/cvap/abstracts/cvap198.html</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Feature+detection+with+automatic+scale+selection&amp;rft.jtitle=International+Journal+of+Computer+Vision&amp;rft.aulast=Lindeberg%2C+Tony&amp;rft.au=Lindeberg%2C+Tony&amp;rft.date=1998&amp;rft.volume=30&amp;rft.issue=2&amp;rft.pages=79%E2%80%93116&amp;rft_id=info:doi/10.1023%2FA%3A1008045108935&amp;rft_id=http%3A%2F%2Fwww.nada.kth.se%2Fcvap%2Fabstracts%2Fcvap198.html&amp;rfr_id=info:sid/en.wikipedia.org:Scale-invariant_feature_transform"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-lowe04-3">^ <a href="#cite_ref-lowe04_3-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lowe04_3-1" title=""><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-lowe04_3-2" title=""><sup><i><b>c</b></i></sup></a> <cite style="font-style:normal" class="" id="CITEREFLowe.2C_David_G.2004">Lowe, David G. (2004). "<a href="http://citeseer.ist.psu.edu/lowe04distinctive.html" class="external text" title="http://citeseer.ist.psu.edu/lowe04distinctive.html" rel="nofollow">Distinctive Image Features from Scale-Invariant Keypoints</a>". <i>International Journal of Computer Vision</i> <b>60</b> (2): 91–110. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" class="external text" title="http://dx.doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" rel="nofollow">10.1023/B:VISI.0000029664.99615.94</a></span><span class="printonly">. <a href="http://citeseer.ist.psu.edu/lowe04distinctive.html" class="external free" title="http://citeseer.ist.psu.edu/lowe04distinctive.html" rel="nofollow">http://citeseer.ist.psu.edu/lowe04distinctive.html</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Distinctive+Image+Features+from+Scale-Invariant+Keypoints&amp;rft.jtitle=International+Journal+of+Computer+Vision&amp;rft.aulast=Lowe%2C+David+G.&amp;rft.au=Lowe%2C+David+G.&amp;rft.date=2004&amp;rft.volume=60&amp;rft.issue=2&amp;rft.pages=91%E2%80%93110&amp;rft_id=info:doi/10.1023%2FB%3AVISI.0000029664.99615.94&amp;rft_id=http%3A%2F%2Fciteseer.ist.psu.edu%2Flowe04distinctive.html&amp;rfr_id=info:sid/en.wikipedia.org:Scale-invariant_feature_transform"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-Mikolajczyk2005-4"><b><a href="#cite_ref-Mikolajczyk2005_4-0" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFMikolajczyk.2C_K..3B_Schmid.2C_C.2005">Mikolajczyk, K.; Schmid, C. (2005). "<a href="http://research.microsoft.com/users/manik/projects/trade-off/papers/MikolajczykPAMI05.pdf" class="external text" title="http://research.microsoft.com/users/manik/projects/trade-off/papers/MikolajczykPAMI05.pdf" rel="nofollow">A performance evaluation of local descriptors</a>". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i> <b>27</b>: 1615–1630. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1109%2FTPAMI.2005.188" class="external text" title="http://dx.doi.org/10.1109%2FTPAMI.2005.188" rel="nofollow">10.1109/TPAMI.2005.188</a></span><span class="printonly">. <a href="http://research.microsoft.com/users/manik/projects/trade-off/papers/MikolajczykPAMI05.pdf" class="external free" title="http://research.microsoft.com/users/manik/projects/trade-off/papers/MikolajczykPAMI05.pdf" rel="nofollow">http://research.microsoft.com/users/manik/projects/trade-off/papers/MikolajczykPAMI05.pdf</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A+performance+evaluation+of+local+descriptors&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.aulast=Mikolajczyk%2C+K.%3B+Schmid%2C+C.&amp;rft.au=Mikolajczyk%2C+K.%3B+Schmid%2C+C.&amp;rft.date=2005&amp;rft.volume=27&amp;rft.pages=1615%E2%80%931630&amp;rft_id=info:doi/10.1109%2FTPAMI.2005.188&amp;rft_id=http%3A%2F%2Fresearch.microsoft.com%2Fusers%2Fmanik%2Fprojects%2Ftrade-off%2Fpapers%2FMikolajczykPAMI05.pdf&amp;rfr_id=info:sid/en.wikipedia.org:Scale-invariant_feature_transform"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> <a href="http://www.tu-chemnitz.de/etit/proaut/rsrc/iav07-surf.pdf" class="external free" title="http://www.tu-chemnitz.de/etit/proaut/rsrc/iav07-surf.pdf" rel="nofollow">http://www.tu-chemnitz.de/etit/proaut/rsrc/iav07-surf.pdf</a></li>
<li id="cite_note-irrgrid-6"><b><a href="#cite_ref-irrgrid_6-0" title="">^</a></b> <cite style="font-style:normal">Cui, Y.; Hasler, N.; Thormaehlen, T.; Seidel, H.-P. (July 2009). "<a href="http://www.mpi-inf.mpg.de/~hasler/download/CuiHasThoSei09igSIFT.pdf" class="external text" title="http://www.mpi-inf.mpg.de/~hasler/download/CuiHasThoSei09igSIFT.pdf" rel="nofollow">Scale Invariant Feature Transform with Irregular Orientation Histogram Binning</a>". <i>Proceedings of the International Conference on Image Analysis and Recognition (ICIAR 2009)</i>, Halifax, Canada: Springer.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Proceedings+of+the+International+Conference+on+Image+Analysis+and+Recognition+%28ICIAR+2009%29&amp;rft.atitle=Scale+Invariant+Feature+Transform+with+Irregular+Orientation+Histogram+Binning&amp;rft.aulast=Cui&amp;rft.aufirst=Y.&amp;rft.date=July+2009&amp;rft.pub=Springer&amp;rft.place=Halifax%2C+Canada&amp;rft_id=http%3A%2F%2Fwww.mpi-inf.mpg.de%2F%7Ehasler%2Fdownload%2FCuiHasThoSei09igSIFT.pdf"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-Beis1997-7"><b><a href="#cite_ref-Beis1997_7-0" title="">^</a></b> <cite style="font-style:normal">Beis, J.; Lowe, David G. (1997). "<a href="http://www.cs.ubc.ca/~lowe/papers/cvpr97.pdf" class="external text" title="http://www.cs.ubc.ca/~lowe/papers/cvpr97.pdf" rel="nofollow">Shape indexing using approximate nearest-neighbour search in high-dimensional spaces</a>". <i>Conference on Computer Vision and Pattern Recognition, Puerto Rico: sn</i>: 1000–1006. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a href="http://dx.doi.org/10.1109/CVPR.1997.609451" class="external text" title="http://dx.doi.org/10.1109/CVPR.1997.609451" rel="nofollow">10.1109/CVPR.1997.609451</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Conference+on+Computer+Vision+and+Pattern+Recognition%2C+Puerto+Rico%3A+sn&amp;rft.atitle=Shape+indexing+using+approximate+nearest-neighbour+search+in+high-dimensional+spaces&amp;rft.au=Beis%2C+J.&amp;rft.date=1997&amp;rft.pages=1000%E2%80%931006&amp;rft_id=info:doi/10.1109%2FCVPR.1997.609451&amp;rft_id=http%3A%2F%2Fwww.cs.ubc.ca%2F%7Elowe%2Fpapers%2Fcvpr97.pdf"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-Se2001-8"><b><a href="#cite_ref-Se2001_8-0" title="">^</a></b> <cite style="font-style:normal">Se, S.; Lowe, David G.; Little, J. (2001). "<a href="http://citeseer.ist.psu.edu/425735.html" class="external text" title="http://citeseer.ist.psu.edu/425735.html" rel="nofollow">Vision-based mobile robot localization and mapping using scale-invariant features</a>". <i>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</i> <b>2</b>: 2051. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a href="http://dx.doi.org/10.1109/ROBOT.2001.932909" class="external text" title="http://dx.doi.org/10.1109/ROBOT.2001.932909" rel="nofollow">10.1109/ROBOT.2001.932909</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Proceedings+of+the+IEEE+International+Conference+on+Robotics+and+Automation+%28ICRA%29&amp;rft.atitle=Vision-based+mobile+robot+localization+and+mapping+using+scale-invariant+features&amp;rft.au=Se%2C+S.%3B+Lowe%2C+David+G.%3B+Little%2C+J.&amp;rft.date=2001&amp;rft.pages=2051&amp;rft.series=2&amp;rft_id=info:doi/10.1109%2FROBOT.2001.932909&amp;rft_id=http%3A%2F%2Fciteseer.ist.psu.edu%2F425735.html"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-Brown2003-9"><b><a href="#cite_ref-Brown2003_9-0" title="">^</a></b> <cite style="font-style:normal">Brown, M.; Lowe, David G. (2003). "<a href="http://graphics.cs.cmu.edu/courses/15-463/2005_fall/www/Papers/BrownLowe.pdf" class="external text" title="http://graphics.cs.cmu.edu/courses/15-463/2005_fall/www/Papers/BrownLowe.pdf" rel="nofollow">Recognising Panoramas</a>". <i>Proceedings of the ninth IEEE International Conference on Computer Vision</i> <b>2</b>: 1218–1225. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a href="http://dx.doi.org/10.1109/ICCV.2003.1238630" class="external text" title="http://dx.doi.org/10.1109/ICCV.2003.1238630" rel="nofollow">10.1109/ICCV.2003.1238630</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Proceedings+of+the+ninth+IEEE+International+Conference+on+Computer+Vision&amp;rft.atitle=Recognising+Panoramas&amp;rft.au=Brown%2C+M.%3B+Lowe%2C+David+G.&amp;rft.date=2003&amp;rft.pages=1218%E2%80%931225&amp;rft.series=2&amp;rft_id=info:doi/10.1109%2FICCV.2003.1238630&amp;rft_id=http%3A%2F%2Fgraphics.cs.cmu.edu%2Fcourses%2F15-463%2F2005_fall%2Fwww%2FPapers%2FBrownLowe.pdf"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-10"><b><a href="#cite_ref-10" title="">^</a></b> Iryna Gordon and David G. Lowe, "<a href="http://www.cs.ubc.ca/labs/lci/papers/docs2006/lowe_gordon.pdf" class="external text" title="http://www.cs.ubc.ca/labs/lci/papers/docs2006/lowe_gordon.pdf" rel="nofollow">What and where: 3D object recognition with accurate pose</a>," in Toward Category-Level Object Recognition, (Springer-Verlag, 2006), pp. 67-82</li>
<li id="cite_note-Scovanner2007-11"><b><a href="#cite_ref-Scovanner2007_11-0" title="">^</a></b> <cite style="font-style:normal">Scovanner, Paul; Ali, S; Shah, M (2007). "A 3-dimensional sift descriptor and its application to action recognition". <i>Proceedings of the 15th International Conference on Multimedia</i>: 357-360. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a href="http://dx.doi.org/10.1145/1291233.1291311" class="external text" title="http://dx.doi.org/10.1145/1291233.1291311" rel="nofollow">10.1145/1291233.1291311</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Proceedings+of+the+15th+International+Conference+on+Multimedia&amp;rft.atitle=A+3-dimensional+sift+descriptor+and+its+application+to+action+recognition&amp;rft.au=Scovanner%2C+Paul&amp;rft.date=2007&amp;rft.pages=357-360&amp;rft_id=info:doi/10.1145%2F1291233.1291311"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-12"><b><a href="#cite_ref-12" title="">^</a></b> <cite style="font-style:normal">Niebles, J. C. Wang, H. and Li, Fei-Fei (2006). "<a href="http://vision.cs.princeton.edu/niebles/humanactions.htm" class="external text" title="http://vision.cs.princeton.edu/niebles/humanactions.htm" rel="nofollow">Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words</a>". <i>Proceedings of the British Machine Vision Conference (BMVC)</i>. <span class="reference-accessdate">Retrieved on <span class="mw-formatted-date" title="2008-08-20"><a href="/wiki/2008" title="2008">2008</a>-<a href="/wiki/August_20" title="August 20">08-20</a></span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Proceedings+of+the+British+Machine+Vision+Conference+%28BMVC%29&amp;rft.atitle=Unsupervised+Learning+of+Human+Action+Categories+Using+Spatial-Temporal+Words&amp;rft.au=Niebles%2C+J.+C.+Wang%2C+H.+and+Li%2C+Fei-Fei&amp;rft.date=2006&amp;rft.place=Edinburgh&amp;rft_id=http%3A%2F%2Fvision.cs.princeton.edu%2Fniebles%2Fhumanactions.htm"><span style="display: none;">&#160;</span></span></li>
</ol>
<p><a name="Implementations" id="Implementations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit&amp;section=19" title="Edit section: Implementations">edit</a>]</span> <span class="mw-headline">Implementations</span></h2>
<ul>
<li><a href="http://user.cs.tu-berlin.de/~nowozin/libsift/" class="external text" title="http://user.cs.tu-berlin.de/~nowozin/libsift/" rel="nofollow">Sebastian Nowozin (C#)</a></li>
<li><a href="http://vision.ucla.edu/~vedaldi/code/sift/sift.html" class="external text" title="http://vision.ucla.edu/~vedaldi/code/sift/sift.html" rel="nofollow">Andrea Vedaldi (Matlab/C)</a></li>
<li><a href="http://vision.ucla.edu/~vedaldi/code/siftpp/siftpp.html" class="external text" title="http://vision.ucla.edu/~vedaldi/code/siftpp/siftpp.html" rel="nofollow">Andrea Vedaldi (C++)</a></li>
<li><a href="http://www.cs.ubc.ca/~lowe/keypoints/" class="external text" title="http://www.cs.ubc.ca/~lowe/keypoints/" rel="nofollow">David Lowe (C/Matlab)</a></li>
<li><a href="http://web.engr.oregonstate.edu/~hess/index.html" class="external text" title="http://web.engr.oregonstate.edu/~hess/index.html" rel="nofollow">Rob Hess (C)</a></li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/research/affine/" class="external text" title="http://www.robots.ox.ac.uk/~vgg/research/affine/" rel="nofollow">Dr Krystian Mikolajczyk (C)</a></li>
<li><a href="http://fly.mpi-cbg.de/~saalfeld/javasift.html" class="external text" title="http://fly.mpi-cbg.de/~saalfeld/javasift.html" rel="nofollow">Stephan Saalfeld (Java)</a></li>
<li><a href="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=18441&amp;objectType=File" class="external text" title="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=18441&amp;objectType=File" rel="nofollow">Adam Chapman / ChangChang Wu (Matlab-GPU implementation)</a></li>
<li><a href="http://ivt.sourceforge.net" class="external text" title="http://ivt.sourceforge.net" rel="nofollow">Integrating Vision Toolkit (C++)</a> (folder IVT/src/Features/SIFTFeatures, by Pedram Azad and Lars Pätzold)</li>
<li><a href="http://sourceforge.net/projects/libsift" class="external text" title="http://sourceforge.net/projects/libsift" rel="nofollow">libsiftfast (Matlab/C++)</a> Uses x86 SSE optimizations and runs on multiple cores using OpenMP. (For Linux, Windows, and Macs). Under <a href="http://www.gnu.org/copyleft/lesser.html" class="external text" title="http://www.gnu.org/copyleft/lesser.html" rel="nofollow">LGPL</a></li>
<li><a href="http://www.cs.cityu.edu.hk/~wzhao2/lip-vireo.htm" class="external text" title="http://www.cs.cityu.edu.hk/~wzhao2/lip-vireo.htm" rel="nofollow">lip-vireo</a> (Linux and Windows versions)</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 4279/1000000
Post-expand include size: 50188/2048000 bytes
Template argument size: 15496/2048000 bytes
Expensive parser function count: 1/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1208345-0!1!0!default!!en!2 and timestamp 20090415131521 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Scale-invariant_feature_transform">http://en.wikipedia.org/wiki/Scale-invariant_feature_transform</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Computer_vision" title="Category:Computer vision">Computer vision</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_August_2008" title="Category:Articles with unsourced statements since August 2008">Articles with unsourced statements since August 2008</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Scale-invariant_feature_transform" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Scale-invariant_feature_transform" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Scale-invariant_feature_transform&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Scale-invariant_feature_transform" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Scale-invariant_feature_transform" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Scale-invariant_feature_transform" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Scale-invariant_feature_transform&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Scale-invariant_feature_transform&amp;oldid=283792218" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Scale-invariant_feature_transform&amp;id=283792218">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D8%AA%D8%AD%D9%88%D9%8A%D9%84_%D8%B5%D9%81%D8%A9_%D8%B5%D9%88%D8%B1%D8%A9_%D8%BA%D9%8A%D8%B1_%D9%85%D8%B1%D8%AA%D8%A8%D8%B7_%D8%A8%D9%85%D9%82%D9%8A%D8%A7%D8%B3">العربية</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Scale-invariant_feature_transform">Deutsch</a></li>
				<li class="interwiki-fa"><a href="http://fa.wikipedia.org/wiki/%D8%AA%D8%A8%D8%AF%DB%8C%D9%84_%D9%88%DB%8C%DA%98%DA%AF%DB%8C_%D9%85%D9%82%DB%8C%D8%A7%D8%B3%E2%80%8C%D9%86%D8%A7%D8%A8%D8%B3%D8%AA%D9%87">فارسی</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/SIFT">Svenska</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 14 April 2009, at 15:04 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv155 in 0.065 secs. --></body></html>
