<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Floating point,Accuracy and precision,Arbitrary-precision arithmetic,Archimedes,Arithmetic overflow,Arithmetic underflow,Association for Computing Machinery,Associative,Base (mathematics),Binary-coded decimal,Binary numeral system" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Floating_point&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Floating_point&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Floating point - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Floating_point";
		var wgTitle = "Floating point";
		var wgAction = "view";
		var wgArticleId = "11376";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 278442902;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Floating_point skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Floating point</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>In <a href="/wiki/Computing" title="Computing">computing</a>, <b>floating point</b> describes a system for numerical representation in which a <a href="/wiki/String_(computer_science)" title="String (computer science)">string</a> of digits (or <a href="/wiki/Bit" title="Bit">bits</a>) represents a <a href="/wiki/Rational_number" title="Rational number">rational number</a>.</p>
<p>The term <i>floating point</i> refers to the fact that the <a href="/wiki/Radix_point" title="Radix point">radix point</a> (decimal point, or, more commonly in computers, binary point) can "float": that is, it can be placed anywhere relative to the <a href="/wiki/Significant_figures" title="Significant figures">significant digits</a> of the number. This position is indicated separately in the internal representation, and floating-point representation can thus be thought of as a computer realization of <a href="/wiki/Scientific_notation" title="Scientific notation">scientific notation</a>. Over the years several different floating-point representations have been used in computers; however, for the last ten years the most commonly encountered representation is that defined by the <a href="/wiki/IEEE_754" title="IEEE 754" class="mw-redirect">IEEE 754</a> Standard.</p>
<p>The advantage of floating-point representation over <b><a href="/wiki/Fixed-point_arithmetic" title="Fixed-point arithmetic">fixed-point</a></b> (and <a href="/wiki/Integer_(computer_science)" title="Integer (computer science)">integer</a>) representation is that it can support a much wider range of values. For example, a fixed-point representation that has seven decimal digits, with the decimal point assumed to be positioned after the fifth digit, can represent the numbers 12345.67, 8765.43, 123.00, and so on, whereas a floating-point representation (such as the IEEE 754 decimal32 format) with seven decimal digits could in addition represent 1.234567, 123456.7, 0.00001234567, 1234567000000000, and so on. The floating-point format needs slightly more storage (to encode the position of the radix point), so when stored in the same space, floating-point numbers achieve their greater range at the expense of slightly less <a href="/wiki/Accuracy_and_precision" title="Accuracy and precision">precision</a>.</p>
<p>The speed of floating-point operations is an important measure of performance for computers in many application domains. It is measured in <a href="/wiki/FLOPS" title="FLOPS">FLOPS</a>.</p>
<p><a href="/wiki/TOP500" title="TOP500">World-class supercomputer installations</a> are generally rated in <a href="/wiki/Tera-" title="Tera-">teraflops</a>. In June 2008, the <a href="/wiki/IBM_Roadrunner" title="IBM Roadrunner">IBM Roadrunner</a> supercomputer achieved 1.026 <a href="/wiki/Peta-" title="Peta-">petaflops</a>, or 1.026 quadrillion floating-point operations per second. In November 2008, <a href="/wiki/Oak_Ridge_National_Laboratory" title="Oak Ridge National Laboratory">Oak Ridge National Laboratory</a>'s <a href="http://www.eurekalert.org/pub_releases/2008-11/ddoe-dor111008.php" class="external text" title="http://www.eurekalert.org/pub_releases/2008-11/ddoe-dor111008.php" rel="nofollow">Cray XT Jaguar</a> supercomputer was upgraded to hit a theoretical peak computing power of 1.64 petaflops making Jaguar the world's first petaflop system dedicated to open research.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Overview"><span class="tocnumber">1</span> <span class="toctext">Overview</span></a></li>
<li class="toclevel-1"><a href="#Range_of_floating-point_numbers"><span class="tocnumber">2</span> <span class="toctext">Range of floating-point numbers</span></a></li>
<li class="toclevel-1"><a href="#History"><span class="tocnumber">3</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1"><a href="#Implementation_in_actual_computers:_IEEE_floating-point"><span class="tocnumber">4</span> <span class="toctext">Implementation in actual computers: IEEE floating-point</span></a>
<ul>
<li class="toclevel-2"><a href="#Internal_representation"><span class="tocnumber">4.1</span> <span class="toctext">Internal representation</span></a></li>
<li class="toclevel-2"><a href="#Alternative_computer_representations_for_non-integral_numbers"><span class="tocnumber">4.2</span> <span class="toctext">Alternative computer representations for non-integral numbers</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Representable_numbers.2C_conversion_and_rounding"><span class="tocnumber">5</span> <span class="toctext">Representable numbers, conversion and rounding</span></a>
<ul>
<li class="toclevel-2"><a href="#Rounding_modes"><span class="tocnumber">5.1</span> <span class="toctext">Rounding modes</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Floating-point_arithmetic_operations"><span class="tocnumber">6</span> <span class="toctext">Floating-point arithmetic operations</span></a>
<ul>
<li class="toclevel-2"><a href="#Addition_and_subtraction"><span class="tocnumber">6.1</span> <span class="toctext">Addition and subtraction</span></a></li>
<li class="toclevel-2"><a href="#Multiplication"><span class="tocnumber">6.2</span> <span class="toctext">Multiplication</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Dealing_with_exceptional_cases"><span class="tocnumber">7</span> <span class="toctext">Dealing with exceptional cases</span></a></li>
<li class="toclevel-1"><a href="#Accuracy_problems"><span class="tocnumber">8</span> <span class="toctext">Accuracy problems</span></a>
<ul>
<li class="toclevel-2"><a href="#Machine_Precision"><span class="tocnumber">8.1</span> <span class="toctext">Machine Precision</span></a></li>
<li class="toclevel-2"><a href="#Minimizing_the_effect_of_accuracy_problems"><span class="tocnumber">8.2</span> <span class="toctext">Minimizing the effect of accuracy problems</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">9</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes_and_references"><span class="tocnumber">10</span> <span class="toctext">Notes and references</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">11</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Overview" id="Overview"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=1" title="Edit section: Overview">edit</a>]</span> <span class="mw-headline">Overview</span></h2>
<p>A number representation (called a <a href="/wiki/Numeral_system" title="Numeral system">numeral system</a> in mathematics) specifies some way of storing a number that may be encoded as a string of digits. The arithmetic is defined as a set of actions on the representation that simulate classical arithmetic operations.</p>
<p>There are several mechanisms by which strings of digits can represent numbers. In common mathematical notation, the digit string can be of any length, and the location of the <a href="/wiki/Radix_point" title="Radix point">radix point</a> is indicated by placing an explicit <a href="/wiki/Decimal_separator" title="Decimal separator">"point" character</a> (dot or comma) there. If the radix point is omitted then it is implicitly assumed to lie at the right (least significant) end of the string (that is, the number is an <a href="/wiki/Integer" title="Integer">integer</a>). In <a href="/wiki/Fixed-point_arithmetic" title="Fixed-point arithmetic">fixed-point</a> systems, some specific assumption is made about where the radix point is located in the string. For example, the convention could be that the string consists of 8 decimal digits with the decimal point in the middle, so that "00012345" has a value of 1.2345.</p>
<p>In <a href="/wiki/Scientific_notation" title="Scientific notation">scientific notation</a>, the given number is scaled by a power of 10 so that it lies within a certain range—typically between 1 and 10, with the radix point appearing immediately after the first digit. The scaling factor, as a power of ten, is then indicated separately at the end of the number. For example, the revolution period of <a href="/wiki/Jupiter_(planet)" title="Jupiter (planet)" class="mw-redirect">Jupiter's</a> moon <a href="/wiki/Io_(moon)" title="Io (moon)">Io</a> is 152853.5047 <a href="/wiki/Seconds" title="Seconds" class="mw-redirect">seconds</a>. This is represented in standard-form scientific notation as 1.528535047&#160;×&#160;10<sup>5</sup> seconds.</p>
<p>Floating-point representation is similar in concept to scientific notation. Logically, a floating-point number consists of:</p>
<ul>
<li>A signed digit string of a given length in a given <a href="/wiki/Base_(mathematics)" title="Base (mathematics)">base</a> (or <a href="/wiki/Radix" title="Radix">radix</a>). This is known as the <a href="/wiki/Significand" title="Significand">significand</a>, or sometimes the <a href="/wiki/Mantissa" title="Mantissa">mantissa</a> (see below) or coefficient. The radix point is not explicitly included, but is implicitly assumed to always lie in a certain position within the significand—often just after or just before the most significant digit, or to the right of the rightmost digit. This article will generally follow the convention that the radix point is just after the most significant (leftmost) digit. The length of the significand determines the precision to which numbers can be represented.</li>
<li>A signed integer <a href="/wiki/Exponent" title="Exponent" class="mw-redirect">exponent</a>, also referred to as the characteristic or scale, which modifies the magnitude of the number.</li>
</ul>
<p>The <i>significand</i> is multiplied by the <i>base</i> raised to the power of the <i>exponent</i>, equivalent to shifting the radix point from its implied position by a number of places equal to the value of the exponent—to the right if the exponent is positive or to the left if the exponent is negative.</p>
<p>Using base-10 (the familiar <a href="/wiki/Decimal_representation" title="Decimal representation">decimal</a> notation) as an example, the number 152853.5047, which has ten decimal digits of precision, is represented as the significand 1528535047 together with an exponent of 5 (if the implied position of the radix point is after the first most significant digit, here 1). To recover the actual value, a decimal point is placed after the first digit of the significand and the result is multiplied by 10<sup>5</sup> to give 1.528535047 × 10<sup>5</sup>, or 152853.5047.</p>
<p>Symbolically, this final value is</p>
<dl>
<dd><img class="tex" alt="s \times b^e" src="http://upload.wikimedia.org/math/6/a/0/6a06106cda85af68c47c98f1bceeb366.png" /></dd>
</dl>
<p>where <i>s</i> is the value of the significand (after taking into account the implied radix point), <i>b</i> is the base, and <i>e</i> is the exponent.</p>
<p>Equivalently, this is:</p>
<dl>
<dd><img class="tex" alt="\frac{s}{b^{p-1}} \times b^e" src="http://upload.wikimedia.org/math/1/b/c/1bc838e9f01209b00c6e771c2d84099f.png" /></dd>
</dl>
<p>where <i>s</i> here means the integer value of the entire significand, ignoring any implied decimal point, and <i>p</i> is the precision—the number of digits in the significand.</p>
<p>Historically, different bases have been used for representing floating-point numbers, with base 2 (<a href="/wiki/Binary_numeral_system" title="Binary numeral system">binary</a>) being the most common, followed by base 10 (decimal), and other less common varieties such as base 16.</p>
<p>The way in which the significand, exponent and sign bits are internally stored on a computer is implementation-dependent. The common IEEE formats are described in detail later and elsewhere, but as an example, in the binary single-precision (32-bit) floating-point representation <i>p</i>=24 and so the significand is a string of 24 <a href="/wiki/Bit" title="Bit">bits</a> (1s and 0s). For instance, the number <a href="/wiki/%CE%A0" title="Π" class="mw-redirect">π</a> rounded to 24 bits is 11.001001000011111101101. When this is stored using the IEEE 754 encoding, this is represented as <i>s</i>&#160;=&#160;110010010000111111011011 with <i>e</i>&#160;=&#160;1 (where <i>s</i> is assumed to have a binary point after the first bit), left-adjusted (or <i>normalized</i>) so the first first is the first non-zero bit (if there is one). Since this first bit of a non-zero binary significand is always 1 it need not be stored, giving an extra bit of precision. Normalization can therefore be thought of as a form of compression; it allows a binary significand to be compressed into a field one bit shorter than the maximum precision, at the expense of extra processing.</p>
<p>The word "mantissa" is often used as a synonym for significand. Many people do not consider this usage to be correct, because the mantissa is traditionally defined as the fractional part of a logarithm, while the <i>characteristic</i> is the integer part. This terminology comes from the way <a href="/wiki/Common_logarithm" title="Common logarithm">logarithm</a> tables were used before computers became commonplace. Log tables were actually tables of mantissas. Therefore, a mantissa is the logarithm of the significand.</p>
<p><a name="Range_of_floating-point_numbers" id="Range_of_floating-point_numbers"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=2" title="Edit section: Range of floating-point numbers">edit</a>]</span> <span class="mw-headline">Range of floating-point numbers</span></h2>
<p>By allowing the <a href="/wiki/Radix_point" title="Radix point">radix point</a> to be adjustable, floating-point notation allows calculations over a wide range of magnitudes, using a fixed number of digits, while maintaining good precision. For example, in a decimal floating-point system with three digits, the multiplication that humans would write as</p>
<dl>
<dd>0.12 × 0.12 = 0.0144</dd>
</dl>
<p>would be expressed as</p>
<dl>
<dd>(1.2&#160;×&#160;10<sup>−1</sup>) × (1.2&#160;×&#160;10<sup>−1</sup>) = (1.44&#160;×&#160;10<sup>−2</sup>)</dd>
</dl>
<p>In a fixed-point system with the decimal point at the left, it would be</p>
<dl>
<dd>0.120 × 0.120 = 0.014</dd>
</dl>
<p>A digit of the result was lost because of the inability of the digits and decimal point to 'float' relative to each other within the digit string.</p>
<p>The range of floating-point numbers depends on the number of bits or digits used for representation of the significand (the significant digits of the number) and for the exponent. On a typical computer system, a 'double precision' (64-bit) binary floating-point number has a coefficient of 53 bits (one of which is implied), an exponent of 11 bits, and one sign bit. Positive floating-point numbers in this format have an approximate range of 10<sup>−308</sup> to 10<sup>308</sup> (because 308 is approximately 1023 × log<sub>10</sub>(2), since the range of the exponent is [−1022,1023]). The complete range of the format is from about −10<sup>308</sup> through +10<sup>308</sup> (see <a href="/wiki/IEEE_754" title="IEEE 754" class="mw-redirect">IEEE 754</a>).</p>
<p>The number of normalized floating point numbers in a system F(B, P, L, U) (where B is the base of the system, P is the precision of the system to P numbers, L is the smallest exponent representable in the system, and U is the largest exponent used in the system) is: 2 * (B - 1) * B^(P-1) * (U - L + 1) + 1. The one is added because the number could be zero.</p>
<p>There is a smallest positive normalized floating-point number, Underflow level = UFL = B^L which has a 1 as the leading digit and 0 for the remaining digits of the mantissa, and the smallest possible value for the exponent.</p>
<p>There is a largest floating point number, Overflow level = OFL = B^(U + 1) * (1 - B^(-P)) which has B - 1 as the value for each digit of the mantissa and the largest possible value for the exponent.</p>
<p>Any number larger than OFL cannot be represented in the given floating-point system, and no number smaller than the UFL can be represented in the floating point system.</p>
<p><a name="History" id="History"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=3" title="Edit section: History">edit</a>]</span> <span class="mw-headline">History</span></h2>
<p>In 1938, <a href="/wiki/Konrad_Zuse" title="Konrad Zuse">Konrad Zuse</a> of Berlin, completed the "<a href="/wiki/Z1_(computer)" title="Z1 (computer)">Z1</a>", the first mechanical binary programmable computer. It was based on boolean algebra and had most of the basic ingredients of modern machines, using the binary system and today's standard separation of <a href="/wiki/Computer_storage" title="Computer storage" class="mw-redirect">storage</a> and control. Zuse's 1936 patent application (Z23139/GMD Nr. 005/021) also suggests a <a href="/wiki/Von_Neumann_architecture" title="Von Neumann architecture">von Neumann architecture</a> (re-invented in 1945) with program and data modifiable in storage. Originally the machine was called the "V1" but it was retroactively renamed after the war, to avoid confusion with the V1 missile. It worked with floating-point numbers having a 7-bit exponent, 16-bit significand, and a sign bit. The memory used sliding metal parts to store 16 such numbers, and worked well; but the arithmetic unit was less successful, occasionally suffering from certain mechanical engineering problems. The program was read from punched discarded 35 mm movie film. Data values could be entered from a numeric keyboard, and outputs were displayed on electric lamps. The machine was not a general purpose computer because it lacked looping capabilities. The <a href="/wiki/Z3_(computer)" title="Z3 (computer)">Z3</a> was completed in 1941 and was program-controlled.</p>
<p>Once electronic digital computers became a reality, the need to process data in this way was quickly recognized. The first commercial computer to be able to do this in hardware appears to be the <a href="/wiki/Z4_(computer)" title="Z4 (computer)">Z4</a> in 1950, followed by the <a href="/wiki/IBM_704" title="IBM 704">IBM 704</a> in 1954. For some time after that, floating-point hardware was an optional feature, and computers that had it were said to be "scientific computers", or to have "scientific computing" capability. All modern general-purpose computers have this ability. The <a href="/wiki/PDP-11/44" title="PDP-11/44">PDP-11/44</a> was an extension of the 11/34 that included the <a href="/wiki/Cache_memory" title="Cache memory" class="mw-redirect">cache memory</a> and floating-point units as a standard feature.</p>
<p>The <a href="/wiki/UNIVAC_1100/2200_series" title="UNIVAC 1100/2200 series">UNIVAC 1100/2200 series</a>, introduced in 1962, supported two floating-point formats. Single precision used 36 bits, organized into a 1-bit sign, 8-bit exponent, and a 27-bit significand. Double precision used 72 bits organized as a 1-bit sign, 11-bit exponent, and a 60-bit significand. The <a href="/wiki/IBM_7094" title="IBM 7094" class="mw-redirect">IBM 7094</a>, introduced the same year, also supported single and double precision, with slightly different formats.</p>
<p>Prior to the <a href="/wiki/IEEE-754" title="IEEE-754" class="mw-redirect">IEEE-754</a> standard, computers used many different forms of floating-point. These differed in the word-sizes, the format of the representations, and the rounding behavior of operations. These differing systems implemented different parts of the arithmetic in hardware and software, with varying accuracy.</p>
<p>The IEEE-754 standard was created in the early 1980s, after word sizes of 32 bits (or 16 or 64) had been generally settled upon. Among the innovations are these:</p>
<ul>
<li>A precisely specified encoding of the bits, so that all compliant computers would interpret bit patterns the same way. This made it possible to transfer floating-point numbers from one computer to another.</li>
<li>A precisely specified behavior of the arithmetic operations. This meant that a given program, with given data, would always produce the same result on any compliant computer. This helped reduce the almost mystical reputation that floating-point computation had for seemingly nondeterministic behavior.</li>
<li>The ability of exceptional conditions (overflow, divide by zero, etc.) to propagate through a computation in a benign manner and be handled by the software in a controlled way.</li>
</ul>
<p><a name="Implementation_in_actual_computers:_IEEE_floating-point" id="Implementation_in_actual_computers:_IEEE_floating-point"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=4" title="Edit section: Implementation in actual computers: IEEE floating-point">edit</a>]</span> <span class="mw-headline">Implementation in actual computers: IEEE floating-point</span></h2>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/IEEE_754-2008" title="IEEE 754-2008">IEEE 754-2008</a></div>
<p>The <a href="/wiki/IEEE" title="IEEE" class="mw-redirect">IEEE</a> has standardized the computer representation for binary floating-point numbers in <a href="/wiki/IEEE_754-2008" title="IEEE 754-2008">IEEE 754</a>. This standard is followed by almost all modern machines. Notable exceptions include IBM mainframes, which support <a href="/wiki/IBM_Floating_Point_Architecture" title="IBM Floating Point Architecture">IBM's own format</a> (in addition to the IEEE 754 binary and decimal formats), and Cray vector machines, where the T90 series had an IEEE version, but the SV1 still uses Cray floating-point format</p>
<p>The standard provides for many closely-related formats, differing in only a few details. Five of these formats are called <i>basic formats</i>, and two of these are especially widely used in computer hardware and languages:</p>
<ul>
<li><a href="/wiki/Single_precision" title="Single precision">Single precision</a>, called "float" in the <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> language family, and "real" or "real*4" in <a href="/wiki/Fortran" title="Fortran">Fortran</a>. This is a binary format that occupies 32 bits (4 bytes) and its significand has a precision of 24 bits (about 7 decimal digits).</li>
<li><a href="/wiki/Double_precision" title="Double precision">Double precision</a>, called "double" in the C language family, and "double precision" or "real*8" in Fortran. This is a binary format that occupies 64 bits (8 bytes) and its significand has a precision of 53 bits (about 16 decimal digits).</li>
</ul>
<p>The other basic formats are "quad" (128-bit) binary and decimal floating-point, and "double" (64-bit) decimal floating-point.</p>
<p>Less common formats include:</p>
<ul>
<li><a href="/wiki/Extended_precision" title="Extended precision">Extended precision</a> format, 80-bit floating point value. Usually "long double" in C language family.</li>
<li><a href="/wiki/Half_precision" title="Half precision">Half</a>, also called float16, a 16-bit floating point value.</li>
</ul>
<p>Any integer less than or equal to 2<sup>24</sup> can be exactly represented in the single precision format, and any integer less than or equal to 2<sup>53</sup> can be exactly represented in the double precision format. Furthermore, any reasonable power of 2 times such a number can be represented. This property is sometimes used in purely integer applications, to get 53-bit integers on platforms that have double precision floats but only 32-bit integers.</p>
<p>The bit representations of IEEE binary floating-point numbers are monotonic (increasing or decreasing in accordance with the numbers they represent), as long as exceptional values are avoided and the signs are handled properly. IEEE binary floating-point numbers are equal if and only if their integer bit representations are equal. Binary floating-point comparisons can therefore be done with simple integer comparisons on the bit patterns, as long as the signs match. However, the actual floating-point comparisons provided by hardware typically have much more sophistication in dealing with exceptional values.</p>
<p>To a rough approximation, the bit representation of an IEEE binary floating-point number is proportional to its base 2 logarithm, with an average error of about 3%. (This is because the exponent field is in the more significant part of the datum.) This can be exploited in some applications, such as volume ramping in digital sound processing.</p>
<p>Although the 32 bit ("single") and 64 bit ("double") formats are by far the most common, the standard actually allows for many different precision levels. Computer hardware (for example, the Intel Pentium series and the Motorola 68000 series) often provides an 80 bit <a href="/wiki/Extended_precision" title="Extended precision">extended precision</a> format, with a 15 bit exponent, a 64 bit significand, and no hidden bit.</p>
<p>There is controversy about the failure of most programming languages to make these extended precision formats available to programmers (although <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> and related programming languages usually provide these formats via the <a href="/wiki/Long_double" title="Long double">long double</a> type on such hardware). System vendors may also provide additional extended formats (e.g. 128 bits) emulated in software.</p>
<p>A project for revising the IEEE 754 standard was started in 2000 (see <a href="/wiki/IEEE_754_revision" title="IEEE 754 revision">IEEE 754 revision</a>); it was completed and approved in June 2008. It includes decimal floating-point formats and a 16 bit floating point format ("binary16"). binary16 has the same structure and rules as the older formats, with 1 sign bit, 5 exponent bits and 10 trailing significand bits. It is being used in the NVIDIA <a href="/wiki/Cg_(programming_language)" title="Cg (programming language)">Cg</a> graphics language, and in the openEXR standard.<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup></p>
<p><a name="Internal_representation" id="Internal_representation"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=5" title="Edit section: Internal representation">edit</a>]</span> <span class="mw-headline">Internal representation</span></h3>
<p>Floating-point numbers are typically packed into a computer datum as the sign bit, the exponent field, and the significand (mantissa), from left to right. For the IEEE 754 binary formats they are apportioned as follows:</p>
<table class="wikitable">
<tr>
<th>Type</th>
<th>Sign</th>
<th>Exponent</th>
<th>Exponent bias</th>
<th>significand</th>
<th>total</th>
</tr>
<tr>
<td><a href="/wiki/Half_precision" title="Half precision">Half</a> (<a href="/wiki/IEEE_754r" title="IEEE 754r" class="mw-redirect">IEEE 754r</a>)</td>
<td>1</td>
<td>5</td>
<td>15</td>
<td>10</td>
<td>16</td>
</tr>
<tr>
<td>Single</td>
<td>1</td>
<td>8</td>
<td>127</td>
<td>23</td>
<td>32</td>
</tr>
<tr>
<td>Double</td>
<td>1</td>
<td>11</td>
<td>1023</td>
<td>52</td>
<td>64</td>
</tr>
<tr>
<td>Quad</td>
<td>1</td>
<td>15</td>
<td>16383</td>
<td>112</td>
<td>128</td>
</tr>
</table>
<p>While the exponent can be positive or negative, in binary formats it is stored as an unsigned number that has a fixed "bias" added to it. Values of all 0s and all 1s in this field are reserved for special treatment (see <a href="#Dealing_with_exceptional_cases" title="">dealing with exceptional cases</a>). Therefore the legal exponent range for normalized numbers is [−126, 127] for single precision, [−1022, 1023] for double, or [−16382, 16383] for quad.</p>
<p>As described earlier, when a binary number is normalized the leftmost bit of the significand is known to be 1. In the IEEE binary interchange formats that bit is not actually stored in the computer datum. It is called the "hidden" or "implicit" bit. Because of this, single precision format actually has a significand with 24 bits of precision, double precision format has 53, and quad has 113.</p>
<p>For example, it was shown above that π, rounded to 24 bits of precision, has:</p>
<ul>
<li>sign = 0&#160;; <i>e</i> = 1&#160;; <i>s</i> = 110010010000111111011011 (including the hidden bit)</li>
</ul>
<p>The sum of the exponent bias (127) and the exponent (1) is 128, so this is represented in single precision format as</p>
<ul>
<li>0 10000000 10010010000111111011011 (excluding the hidden bit) = 40490FDB in <a href="/wiki/Hexadecimal" title="Hexadecimal">hexadecimal</a></li>
</ul>
<p><a name="Alternative_computer_representations_for_non-integral_numbers" id="Alternative_computer_representations_for_non-integral_numbers"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=6" title="Edit section: Alternative computer representations for non-integral numbers">edit</a>]</span> <span class="mw-headline">Alternative computer representations for non-integral numbers</span></h3>
<p>Floating-point representation, in particular the standard IEEE format, is by far the most common way of representing arbitrary real numbers in computers because it is efficiently handled in most large computer processors. However, there are alternatives:</p>
<ul>
<li><a href="/wiki/Fixed-point_arithmetic" title="Fixed-point arithmetic">Fixed-point</a> representation uses integer hardware operations controlled by a software implementation of a specific convention about the location of the binary or decimal point, for example, 6 bits or digits from the right. The hardware to manipulate these representations is less costly than floating-point and is also commonly used to perform integer operations. Binary fixed point is usually used in special-purpose applications on embedded processors that can only do integer arithmetic, but decimal fixed point is common in commercial applications.</li>
<li><a href="/wiki/Binary-coded_decimal" title="Binary-coded decimal">Binary-coded decimal</a></li>
<li>Where greater precision is desired, floating-point arithmetic can be <a href="/wiki/Emulate" title="Emulate" class="mw-redirect">emulated</a> in software with variable-sized significands which might grow and shrink as the program runs. This is called <a href="/wiki/Arbitrary-precision_arithmetic" title="Arbitrary-precision arithmetic">arbitrary-precision</a>, or "scaled bignum", arithmetic.</li>
<li>Some numbers (e.g., 1/3 and 0.1) cannot be represented exactly in binary floating-point no matter what the precision. Software packages that perform <a href="/wiki/Fraction_(mathematics)" title="Fraction (mathematics)">rational arithmetic</a> represent numbers as fractions with integral numerator and denominator, and can therefore represent any rational number exactly. Such packages generally need to use "bignum" arithmetic for the individual integers.</li>
<li><a href="/wiki/Computer_algebra_system" title="Computer algebra system">Computer algebra systems</a> such as <a href="/wiki/Mathematica" title="Mathematica">Mathematica</a>, <a href="/wiki/Maxima_(software)" title="Maxima (software)">Maxima</a> and <a href="/wiki/Maple_computer_algebra_system" title="Maple computer algebra system" class="mw-redirect">Maple</a> can often handle irrational numbers like <span class="texhtml">π</span> or <img class="tex" alt="\sqrt{3}" src="http://upload.wikimedia.org/math/7/d/2/7d2db2b2c90be143cb85c105105317da.png" /> in a completely "formal" way, without dealing with a specific encoding of the significand. Such programs can evaluate expressions like "<span class="texhtml">sin3π</span>" exactly, because they "know" the underlying mathematics.</li>
<li>A representation based on <a href="/wiki/Natural_logarithm" title="Natural logarithm">natural logarithms</a> is sometimes used in <a href="/wiki/FPGA" title="FPGA" class="mw-redirect">FPGA</a>-based applications where most arithmetic operations are multiplication or division.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup> Like floating-point representation, this solution has precision for smaller numbers, as well as a wide range.</li>
</ul>
<p><a name="Representable_numbers.2C_conversion_and_rounding" id="Representable_numbers.2C_conversion_and_rounding"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=7" title="Edit section: Representable numbers, conversion and rounding">edit</a>]</span> <span class="mw-headline">Representable numbers, conversion and rounding</span></h2>
<p>By their nature, all numbers expressed in floating-point format are <a href="/wiki/Rational_number" title="Rational number">rational numbers</a> with a terminating expansion in the relevant base (for example, a terminating decimal expansion in base-10, or a terminating binary expansion in base-2). Irrational numbers, such as <a href="/wiki/%CE%A0" title="Π" class="mw-redirect">π</a> or √2, or non-terminating rational numbers, must be approximated. The number of digits (or bits) of precision also limits the set of rational numbers that can be represented exactly. For example, the number 123456789 clearly cannot be exactly represented if only eight decimal digits of precision are available.</p>
<p>When a number is represented in some format (such as a character string) which is not a native floating-point representation supported in a computer implementation, then it will require a conversion before it can be used in that implementation. If the number can be represented exactly in the floating-point format then the conversion is exact. If there is not an exact representation then the conversion requires a choice of which floating-point number to use to represent the original value. The representation chosen will have a different value to the original, and the value thus adjusted is called the <i>rounded value</i>.</p>
<p>Whether or not a rational number has a terminating expansion depends on the base. For example, in base-10 the number 1/2 has a terminating expansion (0.5) while the number 1/3 does not (0.333...). In base-2 only rationals with denominators that are powers of 2 (such as 1/2 or 3/16) are terminating. Any rational with a denominator that has a prime factor other than 2 will have an infinite binary expansion. This means that numbers which appear to be short and exact when written in decimal format may need to be approximated when converted to binary floating-point. For example, the decimal number 0.1 is not representable in binary floating-point of any finite precision; the exact binary representation would have a "1100" sequence continuing endlessly:</p>
<dl>
<dd><i>e</i> = −??; <i>s</i> = 1100110011001100110011001100110011...,</dd>
</dl>
<p>where, as previously, <i>s</i> is the significand and <i>e</i> is the exponent.</p>
<p>When rounded to 24 bits this becomes</p>
<dl>
<dd><i>e</i> = −27; <i>s</i> = 110011001100110011001101,</dd>
</dl>
<p>which is actually 0.100000001490116119384765625 in decimal.</p>
<p>As a further example, the real number <a href="/wiki/%CE%A0" title="Π" class="mw-redirect">π</a>, represented in binary as an infinite series of bits is</p>
<dl>
<dd>11.0010010000111111011010101000100010000101101000110000100011010011...</dd>
</dl>
<p>but is</p>
<dl>
<dd>11.0010010000111111011011</dd>
</dl>
<p>when approximated by <a href="/wiki/Rounding" title="Rounding">rounding</a> to a precision of 24 bits.</p>
<p>In binary single-precision floating-point, this is represented as <i>s</i>&#160;=&#160;110010010000111111011011 with <i>e</i>&#160;=&#160;−22. This has a decimal value of</p>
<dl>
<dd><b>3.141592</b>7410125732421875,</dd>
</dl>
<p>whereas the more accurate approximation of the true value of π is</p>
<dl>
<dd><b>3.1415926535897932384626433832795</b>...</dd>
</dl>
<p>The result of rounding differs from the true value by about 0.03 parts per million, and matches the decimal representation of π in the first 7 digits. The difference is the <a href="/wiki/Discretization_error" title="Discretization error">discretization error</a> and is limited by the <a href="/wiki/Machine_epsilon" title="Machine epsilon">machine epsilon</a>.</p>
<p>The arithmetical difference between two consecutive representable floating-point numbers which have the same exponent is called an <a href="/wiki/Unit_in_the_Last_Place" title="Unit in the Last Place" class="mw-redirect">Unit in the Last Place</a> (ULP). For example, the numbers represented by 45670123 and 45670124 hexadecimal is one ULP. For numbers with an exponent of 0, an ULP is exactly 2<sup>−23</sup> or about 10<sup>−7</sup> in single precision, and about 10<sup>−16</sup> in double precision. The mandated behavior of IEEE-compliant hardware is that the result be within one-half of an ULP.</p>
<p><a name="Rounding_modes" id="Rounding_modes"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=8" title="Edit section: Rounding modes">edit</a>]</span> <span class="mw-headline">Rounding modes</span></h3>
<p>Rounding modes are used when the exact result of a floating-point operation (or a conversion to floating-point format) would need more significant digits than there are digits in the significand. There are several different <a href="/wiki/Rounding" title="Rounding">rounding schemes</a> (or <i>rounding modes</i>). Historically, <a href="/wiki/Truncation" title="Truncation">truncation</a> was the typical approach. Since the introduction of IEEE 754, the default method (<a href="/wiki/Rounding" title="Rounding"><i>round to nearest, ties to even</i></a>, sometimes called Banker's Rounding) is more commonly used. This method rounds the ideal (infinitely precise) result of an arithmetic operation to the nearest representable value, and give that representation as the result.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2" title=""><span>[</span>3<span>]</span></a></sup> In the case of a tie, the value that would make the significand end in a 0 bit is chosen. This IEEE standard applies to all fundamental algebraic operations, including square root, in the absence of exceptional conditions. It means that IEEE-compliant hardware behavior is completely determined in all 32 or 64 bits. ("Library" functions such as cosine and log are not mandated.)</p>
<p>Alternative rounding options are also available. IEEE-754-compliant hardware offers the following rounding modes:</p>
<ul>
<li>round to nearest, where ties round to the nearest even digit in the required position (the default and by far the most common mode)</li>
<li>round to nearest, where ties round away from zero (optional for binary floating-point and commonly used in decimal)</li>
<li>round up (toward +∞; negative results round toward zero)</li>
<li>round down (toward −∞; negative results round away from zero)</li>
<li>round toward zero (sometimes called "chop" mode; it is similar to the common behavior of float-to-integer conversions, which convert −3.9 to −3)</li>
</ul>
<p>Alternative modes are useful when the amount of error being introduced must be bounded. Applications that require a bounded error are multi-precision floating-point, and <a href="/wiki/Interval_arithmetic" title="Interval arithmetic">interval arithmetic</a>.</p>
<p>A further use of rounding modes is when a number is explicitly rounded to a certain number of decimal (or binary) places, as when rounding a result to euros and cents (two decimal places). In this case a common rounding mode is again "round to nearest, ties away from zero", in which a tie is rounded up for positive values.</p>
<p><a name="Floating-point_arithmetic_operations" id="Floating-point_arithmetic_operations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=9" title="Edit section: Floating-point arithmetic operations">edit</a>]</span> <span class="mw-headline">Floating-point arithmetic operations</span></h2>
<p>For ease of presentation and understanding, decimal <a href="/wiki/Radix" title="Radix">radix</a> with 7 digit precision will be used in the examples, as in the IEEE 754 <i>decimal32</i> format. The fundamental principles are the same in any <a href="/wiki/Radix" title="Radix">radix</a> or precision, except that normalization is optional (it does not affect the numerical value of the result). Here, <i>s</i> denotes the significand and <i>e</i> denotes the exponent.</p>
<p><a name="Addition_and_subtraction" id="Addition_and_subtraction"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=10" title="Edit section: Addition and subtraction">edit</a>]</span> <span class="mw-headline">Addition and subtraction</span></h3>
<p>A simple method to add floating-point numbers is to first represent them with the same exponent. In the example below, the second number is shifted right by three digits, and we then proceed with the usual addition method:</p>
<pre>
  123456.7 = 1.234567 * 10^5
  101.7654 = 1.017654 * 10^2 = 0.001017654 * 10^5
  
  Hence:
  123456.7 + 101.7654 = (1.234567 * 10^5) + (1.017654 * 10^2)
                      = (1.234567 * 10^5) + (0.001017654 * 10^5)
                      = (1.234567 + 0.001017654) * 10^5
                      =  1.235584654 * 10^5
</pre>
<p>In detail:</p>
<pre>
  e=5;  s=1.234567     (123456.7)
+ e=2;  s=1.017654     (101.7654)

  e=5;  s=1.234567
+ e=5;  s=0.001017654  (after shifting)
--------------------
  e=5;  s=1.235584654  (true sum: 123558.4654)
</pre>
<p>This is the true result, the exact sum of the operands. It will be rounded to seven digits and then normalized if necessary. The final result is</p>
<pre>
  e=5;  s=1.235585    (final sum: 123558.5)
</pre>
<p>Note that the low 3 digits of the second operand (654) are essentially lost. This is <a href="/wiki/Round-off_error" title="Round-off error">round-off error</a>. In extreme cases, the sum of two non-zero numbers may be equal to one of them:</p>
<pre>
  e=5;  s=1.234567
+ e=-3; s=9.876543

  e=5;  s=1.234567
+ e=5;  s=0.00000009876543 (after shifting)
----------------------
  e=5;  s=1.23456709876543 (true sum)
  e=5;  s=1.234567         (after rounding/normalization)
</pre>
<p>Another problem of loss of significance occurs when two close numbers are subtracted. In the following example <i>e</i>&#160;=&#160;5; <i>s</i>&#160;=&#160;1.234571 and <i>e</i>&#160;=&#160;5; <i>s</i>&#160;=&#160;1.234567 are representations of the rationals 123457.1467 and 123456.659.</p>
<pre>
  e=5;  s=1.234571
- e=5;  s=1.234567
----------------
  e=5;  s=0.000004
  e=-1; s=4.000000 (after rounding/normalization)
</pre>
<p>The best representation of this difference is <i>e</i>&#160;=&#160;−1; <i>s</i>&#160;=&#160;4.877000, which differs more than 20% from <i>e</i>&#160;=&#160;−1; <i>s</i>&#160;=&#160;4.000000. In extreme cases, the final result may be zero even though an exact calculation may be several million. This <i><a href="/wiki/Loss_of_significance" title="Loss of significance">cancellation</a></i> illustrates the danger in assuming that all of the digits of a computed result are meaningful. Dealing with the consequences of these errors is a topic in <a href="/wiki/Numerical_analysis" title="Numerical analysis">numerical analysis</a>; see also <a href="#Accuracy_problems" title="">Accuracy problems</a>.</p>
<p><a name="Multiplication" id="Multiplication"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=11" title="Edit section: Multiplication">edit</a>]</span> <span class="mw-headline">Multiplication</span></h3>
<p>To multiply, the significands are multiplied while the exponents are added, and the result is rounded and normalized.</p>
<pre>
  e=3;  s=4.734612
× e=5;  s=5.417242
-----------------------
  e=8;  s=25.648538980104 (true product)
  e=8;  s=25.64854        (after rounding)
  e=9;  s=2.564854        (after normalization)
</pre>
<p>Division is done similarly, but is more complicated.</p>
<p>There are no cancellation or absorption problems with multiplication or division, though small errors may accumulate as operations are performed repeatedly. In practice, the way these operations are carried out in digital logic can be quite complex (see <a href="/wiki/Booth%27s_multiplication_algorithm" title="Booth's multiplication algorithm">Booth's multiplication algorithm</a> and <a href="/wiki/Division_(digital)" title="Division (digital)">digital division</a>).<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup> For a fast, simple method, see the <a href="/wiki/Horner_scheme#Floating_point_multiplication_and_division" title="Horner scheme">Horner method</a>.</p>
<p><a name="Dealing_with_exceptional_cases" id="Dealing_with_exceptional_cases"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=12" title="Edit section: Dealing with exceptional cases">edit</a>]</span> <span class="mw-headline">Dealing with exceptional cases</span></h2>
<p>Floating-point computation in a computer can run into three kinds of problems:</p>
<ul>
<li>An operation can be mathematically illegal, such as division by zero.</li>
<li>An operation can be legal in principle, but not supported by the specific format, for example, calculating the square root of −1 or the inverse sine of 2 (both of which result in <a href="/wiki/Complex_number" title="Complex number">complex numbers</a>).</li>
<li>An operation can be legal in principle, but the result can be impossible to represent in the specified format, because the exponent is too large or too small to encode in the exponent field. Such an event is called an <a href="/wiki/Arithmetic_overflow" title="Arithmetic overflow">overflow</a> (exponent too large) or <a href="/wiki/Arithmetic_underflow" title="Arithmetic underflow">underflow</a> (exponent too small).</li>
</ul>
<p>Prior to the IEEE standard, such conditions usually caused the program to terminate, or triggered some kind of <a href="/wiki/Trap_(computing)" title="Trap (computing)">trap</a> that the programmer might be able to catch. How this worked was system-dependent, meaning that floating-point programs were not <a href="/wiki/Porting" title="Porting">portable</a>. Modern IEEE-compliant systems have a uniform way of handling these situations. An important part of the mechanism involves <i>error values</i> that result from a failing computation, and that can propagate silently through subsequent computation until they are detected at a point of the programmer's choosing.</p>
<p>The two error values are "infinity" (often denoted "INF"), and "<a href="/wiki/NaN" title="NaN">NaN</a>" ("not a number"), which covers all other errors. "Infinity" does not necessarily mean that the result is actually infinite. It simply means "too large to represent".</p>
<p>Both of these are encoded with the exponent field set to all 1s. (Recall that exponent fields of all 0s or all 1s are reserved for special meanings.) The significand field is set to something that can distinguish them—typically zero for INF and nonzero for NaN. The sign bit is meaningful for INF, that is, floating-point hardware distinguishes between +<img class="tex" alt="\infty" src="http://upload.wikimedia.org/math/d/2/4/d245777abca64ece2d5d7ca0d19fddb6.png" /> and −<img class="tex" alt="\infty" src="http://upload.wikimedia.org/math/d/2/4/d245777abca64ece2d5d7ca0d19fddb6.png" />.</p>
<p>When a nonzero number is divided by zero (the divisor must be <i>exactly</i> zero), a "zerodivide" event occurs, and the result is set to infinity of the appropriate sign. In other cases in which the result's exponent is too large to represent, such as division of an extremely large number by an extremely small number, an "overflow" event occurs, also producing infinity of the appropriate sign. This is different from a zerodivide, though both produce a result of infinity, and the distinction is usually unimportant in practice.</p>
<p>Floating-point hardware is generally designed to handle operands of infinity in a reasonable way, such as</p>
<ul>
<li>(+INF) + (+7) = (+INF)</li>
<li>(+INF) × (−2) = (−INF)</li>
<li>But: (+INF) × 0 = NaN—there is no meaningful thing to do</li>
</ul>
<p>When the result of an operation has an exponent too small to represent properly, an "underflow" event occurs. The hardware responds to this by changing to a format in which the significand is not normalized, and there is no "hidden" bit—that is, all bits of the significand are represented. The exponent field is set to the reserved value of zero. The significand is set to whatever it has to be in order to be consistent with the exponent. Such a number is said to be "<a href="/wiki/Denormal_number" title="Denormal number">subnormal</a>" (sometimes called a "denormalized number", or a "denorm" for short, referring to the older term for subnormals that was used in the first <a href="/wiki/IEEE_754-1985" title="IEEE 754-1985">first IEEE 754 standard</a> where subnormal numbers were always unnormalized). Subnormal numbers are valid operands to arithmetic operations.</p>
<p>If no significant bits are able to appear in the significand field, the number is zero. Note that, in this case, the exponent field and significand field are all zeros—floating-point zero is represented by all zeros.</p>
<p>The mandated behavior for dealing with overflow and underflow is that the appropriate result is computed, taking the rounding mode into consideration, as though the exponent range were infinitely large. If that resulting exponent can't be packed into its field correctly, the overflow/underflow action described above is taken.</p>
<p>Other errors, such as division of zero by zero, or taking the square root of −1, cause an "operand error" (known in IEEE 754 as an "invalid operation") event, and produce a NaN result. NaNs propagate aggressively through arithmetic operations—any NaN operand to almost any operation produces a NaN result.</p>
<p>In summary, there are five special "events" that may occur, though some of them are quite benign:</p>
<ul>
<li>An overflow occurs as described previously, producing an infinity</li>
<li>An underflow occurs as described previously, producing a subnormal number or zero</li>
<li>A zerodivide occurs as described previously, producing an infinity of the appropriate sign</li>
<li>An "operand error" ("invalid operation") occurs as described previously, producing a NaN</li>
<li>An "inexact" event occurs whenever the rounding of a result changed that result from the true mathematical value. This occurs almost all the time, and is usually ignored. It is looked at only in the most exacting applications.</li>
</ul>
<p>Computer hardware is typically able to raise <a href="/wiki/Exception_handling" title="Exception handling">exceptions</a> when these events occur. How this is done is system-dependent. Usually these exceptions are all <i>masked</i> (disabled) by software, and programs rely on the propagation of error values.</p>
<p><a name="Accuracy_problems" id="Accuracy_problems"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=13" title="Edit section: Accuracy problems">edit</a>]</span> <span class="mw-headline">Accuracy problems</span></h2>
<p>The fact that floating-point numbers cannot faithfully mimic the real numbers, and that floating-point operations cannot faithfully mimic true arithmetic operations, leads to many surprising situations. This is related to the finite <a href="/wiki/Precision_(computer_science)" title="Precision (computer science)">precision</a> with which computers generally represent numbers.</p>
<p>For example, the non-representability of 0.1 and 0.01 (in binary) means that the result of attempting to square 0.1 is neither 0.01 nor the representable number closest to it. In 24-bit (single precision) representation, 0.1 (decimal) was given previously as <i>e</i>&#160;=&#160;−4; <i>s</i>&#160;=&#160;110011001100110011001101, which is</p>
<dl>
<dd>0.100000001490116119384765625 exactly.</dd>
</dl>
<p>Squaring this number gives</p>
<dl>
<dd>0.010000000298023226097399174250313080847263336181640625 exactly.</dd>
</dl>
<p>Squaring it with single-precision floating-point hardware (with rounding) gives</p>
<dl>
<dd>0.010000000707805156707763671875 exactly.</dd>
</dl>
<p>But the representable number closest to 0.01 is</p>
<dl>
<dd>0.009999999776482582092285156250 exactly.</dd>
</dl>
<p>Also, the non-representability of π (and π/2) means that an attempted computation of tan(π/2) will not yield a result of infinity, nor will it even overflow. It is simply not possible for standard floating-point hardware to attempt to compute tan(π/2), because π/2 cannot be represented exactly. This computation in C:</p>
<pre>
  // Enough digits to be sure we get the correct approximation.
  double pi = 3.1415926535897932384626433832795;
  double z = tan(pi/2.0);
</pre>
<p>will give a result of 16331239353195370.0. In single precision (using the tanf function), the result will be −22877332.0.</p>
<p>By the same token, an attempted computation of sin(π) will not yield zero. The result will be (approximately) 0.1225&#160;×&#160;10<sup>-15</sup> in double precision, or −0.8742&#160;×&#160;10<sup>-7</sup> in single precision.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup></p>
<p>While floating-point addition and multiplication are both <a href="/wiki/Commutative" title="Commutative" class="mw-redirect">commutative</a> (<i>a</i> + <i>b</i> = <i>b</i> + <i>a</i> and <i>a</i>×<i>b</i> = <i>b</i>×<i>a</i>), they are not necessarily <a href="/wiki/Associative" title="Associative" class="mw-redirect">associative</a>. That is, (<i>a</i> + <i>b</i>) + <i>c</i> is not necessarily equal to <i>a</i> + (<i>b</i> + <i>c</i>). Using 7-digit decimal arithmetic:</p>
<pre>
 1234.567 + 45.67844 = 1280.245
                       1280.245 + 0.0004 = 1280.245
 but 
 45.67840 + 0.00004 = 45.67844
                      45.67844 + 1234.567 = 1280.246
</pre>
<p>They are also not necessarily <a href="/wiki/Distributive" title="Distributive" class="mw-redirect">distributive</a>. That is, (<i>a</i> + <i>b</i>) ×<i>c</i> may not be the same as <i>a</i>×<i>c</i> + <i>b</i>×<i>c</i>:</p>
<pre>
 1234.567 × 3.333333 = 4115.223
 1.234567 × 3.333333 = 4.115223
                       4115.223 + 4.115223 = 4119.338
 but 
 1234.567 + 1.234567 = 1235.802
                       1235.802 × 3.333333 = 4119.340
</pre>
<p>In addition to loss of significance, inability to represent numbers such as π and 0.1 exactly, and other slight inaccuracies, the following phenomena may occur:</p>
<ul>
<li>Cancellation: subtraction of nearly equal operands may cause extreme loss of accuracy. This is perhaps the most common and serious accuracy problem.</li>
<li>Conversions to integer are not intuitive: converting (63.0/9.0) to integer yields 7, but converting (0.63/0.09) may yield 6. This is because conversions generally truncate rather than round. <a href="/wiki/Floor_and_ceiling_functions" title="Floor and ceiling functions">Floor and ceiling functions</a> may produce answers which are off by one from the intuitively expected value.</li>
<li>Limited exponent range: results might overflow yielding infinity, or underflow yielding a <a href="/wiki/Denormal" title="Denormal" class="mw-redirect">denormal</a> value or zero. If a <a href="/wiki/Denormal_number" title="Denormal number">denormal number</a> results, precision will be lost.</li>
<li>Testing for safe division is problematic: Checking that the divisor is not zero does not guarantee that a division will not overflow and yield infinity.</li>
<li>Testing for equality is problematic. Two computational sequences that are mathematically equal may well produce different floating-point values. Programmers often perform comparisons within some tolerance (often a decimal constant, itself not accurately represented), but that doesn't necessarily make the problem go away.</li>
</ul>
<p><a name="Machine_Precision" id="Machine_Precision"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=14" title="Edit section: Machine Precision">edit</a>]</span> <span class="mw-headline">Machine Precision</span></h3>
<p>Machine Precision is a quantity that characterizes the accuracy of a floating point system. It is also known as unit roundoff or machine epsilon. Usually denoted E<sub>mach</sub>, its value depends on the particular rounding being used.</p>
<p>With rounding to zero,<br />
E<sub>mach</sub> = B^(1-P)<br />
whereas rounding to nearest,<br />
E<sub>mach</sub> = (1/2)*B^(1-P)</p>
<p>This is important since it bounds the <i>relative error</i> in representing any non-zero real number x within the normalized range of a floating point system:<br />
<span class="texhtml">| (<i>f</i><i>l</i>(<i>x</i>) − <i>x</i>) / <i>x</i> | &lt; =</span> E<sub>mach</sub></p>
<p><a name="Minimizing_the_effect_of_accuracy_problems" id="Minimizing_the_effect_of_accuracy_problems"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=15" title="Edit section: Minimizing the effect of accuracy problems">edit</a>]</span> <span class="mw-headline">Minimizing the effect of accuracy problems</span></h3>
<p>Because of the issues noted above, naive use of floating-point arithmetic can lead to many problems. The creation of thoroughly robust floating-point software is a complicated undertaking, and a good understanding of <a href="/wiki/Numerical_analysis" title="Numerical analysis">numerical analysis</a> is essential.</p>
<p>In addition to careful design of programs, careful handling by the <a href="/wiki/Compiler" title="Compiler">compiler</a> is required. Certain "optimizations" that compilers might make (for example, reordering operations) can work against the goals of well-behaved software. There is some controversy about the failings of compilers and language designs in this area. See the external references at the bottom of this article.</p>
<p>Floating-point arithmetic is at its best when it is simply being used to measure real-world quantities over a wide range of scales (such as the orbital period of <a href="/wiki/Io_(moon)" title="Io (moon)">Io</a> or the mass of the <a href="/wiki/Proton" title="Proton">proton</a>), and at its worst when it is expected to model the interactions of quantities expressed as decimal strings that are expected to be exact. An example of the latter case is financial calculations. For this reason, financial software tends not to use a binary floating-point number representation.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup> The "decimal" data type of the <a href="/wiki/C_Sharp_(programming_language)" title="C Sharp (programming language)">C# programming language</a>, and the IEEE 854 standard, are designed to avoid the problems of binary floating-point representation, and make the arithmetic always behave as expected when numbers are printed in decimal.</p>
<p>Small errors in floating-point arithmetic can grow when mathematical algorithms perform operations an enormous number of times. A few examples are matrix inversion, eigenvector computation, and differential equation solving. These algorithms must be very carefully designed if they are to work well.</p>
<p>Expectations from mathematics may not be realised in the field of floating-point computation. For example, it is known that <img class="tex" alt="(x+y)(x-y) = x^2-y^2\," src="http://upload.wikimedia.org/math/e/4/a/e4a8f9d1785121505817aec7f100ebeb.png" />, and that <img class="tex" alt="\sin^2{\theta}+\cos^2{\theta} = 1\," src="http://upload.wikimedia.org/math/0/0/0/0003793e1c0afcf8afb4c48f0c1e0c57.png" />. These facts cannot be counted on when the quantities involved are the result of floating-point computation.</p>
<p>A detailed treatment of the techniques for writing high-quality floating-point software is beyond the scope of this article, and the reader is referred to the references at the bottom of this article. Descriptions of a few simple techniques follow.</p>
<p>The use of the equality test (<tt>if (x==y) ...</tt>) is usually not recommended when expectations are based on results from pure mathematics. Such tests are sometimes replaced with "fuzzy" comparisons (<tt>if (abs(x-y) &lt; epsilon) ...</tt>), where epsilon is sufficiently small and tailored to the application, such as 1.0E-13 - see <a href="/wiki/Machine_epsilon" title="Machine epsilon">machine epsilon</a>). The wisdom of doing this varies greatly. It is often better to organize the code in such a way that such tests are unnecessary.</p>
<p>An awareness of when loss of significance can occur is useful. For example, if one is adding a very large number of numbers, the individual addends are very small compared with the sum. This can lead to loss of significance. Suppose, for example, that one needs to add many numbers, all approximately equal to 3. After 1000 of them have been added, the running sum is about 3000. A typical addition would then be something like</p>
<pre>
3253.671
+  3.141276
--------
3256.812
</pre>
<p>The low 3 digits of the addends are effectively lost. The <a href="/wiki/Kahan_summation_algorithm" title="Kahan summation algorithm">Kahan summation algorithm</a> may be used to reduce the errors.</p>
<p>Computations may be rearranged in a way that is mathematically equivalent but less prone to error. As an example, <a href="/wiki/Archimedes" title="Archimedes">Archimedes</a> approximated π by calculating the perimeters of polygons inscribing and circumscribing a circle, starting with hexagons, and successively doubling the number of sides. The recurrence formula for the circumscribed polygon is:</p>
<dl>
<dd><img class="tex" alt="t_0 = \frac{1}{\sqrt{3}}" src="http://upload.wikimedia.org/math/4/3/2/4329f82cd4a36a21e140157da12e5ceb.png" /></dd>
</dl>
<dl>
<dd><img class="tex" alt="t_{i+1} = \frac{\sqrt{t_i^2+1}-1}{t_i}\qquad\mathrm{second\ form:}\qquad t_{i+1} = \frac{t_i}{\sqrt{t_i^2+1}+1}" src="http://upload.wikimedia.org/math/a/c/6/ac65b3973561dd5b271eab2f4f051de2.png" /></dd>
</dl>
<dl>
<dd><img class="tex" alt="\pi \sim 6 \times 2^i \times t_i,\qquad\mathrm{converging\ as\ i \rightarrow \infty}\," src="http://upload.wikimedia.org/math/7/9/7/797fb30ed647a1f4983c9744b3feeb6d.png" /></dd>
</dl>
<p>Here is a computation using IEEE "double" (a significand with 53 bits of precision) arithmetic:</p>
<pre>
 i   6 × 2<sup>i</sup> × t<sub>i</sub>, first form    6 × 2<sup>i</sup> × t<sub>i</sub>, second form

 0   <b><span style="color:purple;">3</span></b>.4641016151377543863      <b><span style="color:purple;">3</span></b>.4641016151377543863
 1   <b><span style="color:purple;">3</span></b>.2153903091734710173      <b><span style="color:purple;">3</span></b>.2153903091734723496
 2   <b><span style="color:purple;">3.1</span></b>596599420974940120      <b><span style="color:purple;">3.1</span></b>596599420975006733
 3   <b><span style="color:purple;">3.14</span></b>60862151314012979      <b><span style="color:purple;">3.14</span></b>60862151314352708
 4   <b><span style="color:purple;">3.14</span></b>27145996453136334      <b><span style="color:purple;">3.14</span></b>27145996453689225
 5   <b><span style="color:purple;">3.141</span></b>8730499801259536      <b><span style="color:purple;">3.141</span></b>8730499798241950
 6   <b><span style="color:purple;">3.141</span></b>6627470548084133      <b><span style="color:purple;">3.141</span></b>6627470568494473
 7   <b><span style="color:purple;">3.141</span></b>6101765997805905      <b><span style="color:purple;">3.141</span></b>6101766046906629
 8   <b><span style="color:purple;">3.14159</span></b>70343230776862      <b><span style="color:purple;">3.14159</span></b>70343215275928
 9   <b><span style="color:purple;">3.14159</span></b>37488171150615      <b><span style="color:purple;">3.14159</span></b>37487713536668
10   <b><span style="color:purple;">3.141592</span></b>9278733740748      <b><span style="color:purple;">3.141592</span></b>9273850979885
11   <b><span style="color:purple;">3.141592</span></b>7256228504127      <b><span style="color:purple;">3.141592</span></b>7220386148377
12   <b><span style="color:purple;">3.1415926</span></b>717412858693      <b><span style="color:purple;">3.1415926</span></b>707019992125
13   <b><span style="color:purple;">3.1415926</span></b>189011456060      <b><span style="color:purple;">3.14159265</span></b>78678454728
14   <b><span style="color:purple;">3.1415926</span></b>717412858693      <b><span style="color:purple;">3.14159265</span></b>46593073709
15   <b><span style="color:purple;">3.14159</span></b>19358822321783      <b><span style="color:purple;">3.141592653</span></b>8571730119
16   <b><span style="color:purple;">3.1415926</span></b>717412858693      <b><span style="color:purple;">3.141592653</span></b>6566394222
17   <b><span style="color:purple;">3.1415</span></b>810075796233302      <b><span style="color:purple;">3.141592653</span></b>6065061913
18   <b><span style="color:purple;">3.1415926</span></b>717412858693      <b><span style="color:purple;">3.1415926535</span></b>939728836
19   <b><span style="color:purple;">3.141</span></b>4061547378810956      <b><span style="color:purple;">3.1415926535</span></b>908393901
20   <b><span style="color:purple;">3.14</span></b>05434924008406305      <b><span style="color:purple;">3.1415926535</span></b>900560168
21   <b><span style="color:purple;">3.14</span></b>00068646912273617      <b><span style="color:purple;">3.141592653589</span></b>8608396
22   <b><span style="color:purple;">3.1</span></b>349453756585929919      <b><span style="color:purple;">3.141592653589</span></b>8122118
23   <b><span style="color:purple;">3.14</span></b>00068646912273617      <b><span style="color:purple;">3.14159265358979</span></b>95552
24   <b><span style="color:purple;">3</span></b>.2245152435345525443      <b><span style="color:purple;">3.14159265358979</span></b>68907
25                              <b><span style="color:purple;">3.14159265358979</span></b>62246
26                              <b><span style="color:purple;">3.14159265358979</span></b>62246
27                              <b><span style="color:purple;">3.14159265358979</span></b>62246
28                              <b><span style="color:purple;">3.14159265358979</span></b>62246
              The true value is <b><span style="color:purple;">3.141592653589793238462643383...</span></b>
</pre>
<p>While the two forms of the recurrence formula are clearly equivalent, the first subtracts 1 from a number extremely close to 1, leading to huge cancellation errors. Note that, as the recurrence is applied repeatedly, the accuracy improves at first, but then it deteriorates. It never gets better than about 8 digits, even though 53-bit arithmetic should be capable of about 16 digits of precision. When the second form of the recurrence is used, the value converges to 15 digits of precision.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=16" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<div class="noprint tright portal" style="border:solid #aaa 1px;margin:0.5em 0 0.5em 0.5em;">
<table style="background:#f9f9f9; font-size:85%; line-height:110%;">
<tr>
<td><a href="/wiki/File:Internet_map_1024.jpg" class="image" title="Internet map 1024.jpg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/28px-Internet_map_1024.jpg" width="28" height="28" border="0" /></a></td>
<td style="padding:0 0.2em;"><i><b><a href="/wiki/Portal:Computer_Science" title="Portal:Computer Science" class="mw-redirect">Computer Science portal</a></b></i></td>
</tr>
</table>
</div>
<div style="column-count:3;-moz-column-count:3;-webkit-column-count:3">
<ul>
<li><a href="/wiki/Significant_digits" title="Significant digits" class="mw-redirect">Significant digits</a></li>
<li><a href="/wiki/Fixed-point_arithmetic" title="Fixed-point arithmetic">Fixed-point arithmetic</a></li>
<li><a href="/wiki/Computable_number" title="Computable number">Computable number</a></li>
<li><a href="/wiki/IEEE_754" title="IEEE 754" class="mw-redirect">IEEE 754</a> — Standard for Binary Floating-Point Arithmetic</li>
<li><a href="/wiki/IBM_Floating_Point_Architecture" title="IBM Floating Point Architecture">IBM Floating Point Architecture</a></li>
<li><a href="/wiki/FLOPS" title="FLOPS">FLOPS</a></li>
<li><a href="/wiki/%E2%88%920_(number)" title="−0 (number)">−0 (number)</a></li>
<li><a href="/wiki/Half_precision" title="Half precision">half precision</a></li>
<li><a href="/wiki/Single_precision" title="Single precision">single precision</a></li>
<li><a href="/wiki/Double_precision" title="Double precision">double precision</a></li>
<li><a href="/wiki/Quad_precision" title="Quad precision" class="mw-redirect">quad precision</a></li>
<li><a href="/wiki/Minifloat" title="Minifloat">minifloat</a></li>
<li><a href="/wiki/Numerical_Recipes" title="Numerical Recipes">Numerical Recipes</a></li>
<li><a href="/wiki/Q_(number_format)" title="Q (number format)">Q (number format)</a> for constant resolution</li>
</ul>
</div>
<p><a name="Notes_and_references" id="Notes_and_references"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=17" title="Edit section: Notes and references">edit</a>]</span> <span class="mw-headline">Notes and references</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> <a href="http://www.openexr.com/about.html" class="external text" title="http://www.openexr.com/about.html" rel="nofollow">openEXR</a></li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFHaohuan_Fu.2C_Oskar_Mencer.2C_Wayne_Luk2006">Haohuan Fu, Oskar Mencer, Wayne Luk (December 2006). "<a href="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=4042464" class="external text" title="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=4042464" rel="nofollow">Comparing Floating-point and Logarithmic Number Representations for Reconfigurable Acceleration</a>". <i>IEEE Conference on Field Programmable Technology</i><span class="printonly">. <a href="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=4042464" class="external free" title="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=4042464" rel="nofollow">http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=4042464</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Comparing+Floating-point+and+Logarithmic+Number+Representations+for+Reconfigurable+Acceleration&amp;rft.jtitle=IEEE+Conference+on+Field+Programmable+Technology&amp;rft.aulast=Haohuan+Fu%2C+Oskar+Mencer%2C+Wayne+Luk&amp;rft.au=Haohuan+Fu%2C+Oskar+Mencer%2C+Wayne+Luk&amp;rft.date=December+2006&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fsearch%2Fwrapper.jsp%3Farnumber%3D4042464&amp;rfr_id=info:sid/en.wikipedia.org:Floating_point"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-2"><b><a href="#cite_ref-2" title="">^</a></b> Computer hardware doesn't necessarily compute the exact value; it simply has to produce the equivalent rounded result as though it had computed the infinitely precise result.</li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> The enormous complexity of modern division algorithms once led to a famous error. An early version of the Intel Pentium chip was shipped with a division instruction that, on rare occasions, gave slightly incorrect results. Many computers had been shipped before the error was discovered. Until the defective computers were replaced, patched versions of compilers were developed that could avoid the failing cases. See <i><a href="/wiki/Pentium_FDIV_bug" title="Pentium FDIV bug">Pentium FDIV bug</a></i>.</li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> But an attempted computation of cos(π) yields −1 exactly. Since the derivative is nearly zero near π, the effect of the inaccuracy in the argument is far smaller than the spacing of the floating-point numbers around −1, and the rounded result is exact.</li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> <a href="http://speleotrove.com/decimal/" class="external text" title="http://speleotrove.com/decimal/" rel="nofollow">General Decimal Arithmetic</a></li>
</ol>
</div>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Floating_point&amp;action=edit&amp;section=18" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li>An edited reprint of the paper <i><a href="http://docs.sun.com/source/806-3568/ncg_goldberg.html" class="external text" title="http://docs.sun.com/source/806-3568/ncg_goldberg.html" rel="nofollow">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a></i>, by David Goldberg, published in the March, 1991 issue of Computing Surveys.</li>
<li><a href="/wiki/Donald_Knuth" title="Donald Knuth">Donald Knuth</a>. <i>The Art of Computer Programming</i>, Volume 2: <i>Seminumerical Algorithms</i>, Third Edition. Addison-Wesley, 1997. <a href="/wiki/Special:BookSources/0201896842" class="internal">ISBN 0-201-89684-2</a>. Section 4.2: Floating Point Arithmetic, pp.214–264.</li>
<li>Press et al. <i><a href="/wiki/Numerical_Recipes" title="Numerical Recipes">Numerical Recipes</a> in <a href="/wiki/C%2B%2B" title="C++">C++</a>. The Art of Scientific Computing,</i> <a href="/wiki/Special:BookSources/0521750334" class="internal">ISBN 0-521-75033-4</a>.</li>
<li>Kahan, William and Darcy, Joseph (2001). How Java’s floating-point hurts everyone everywhere. Retrieved September 5, 2003 from <a href="http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf" class="external free" title="http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf" rel="nofollow">http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf</a>.</li>
<li><a href="http://www.mrob.com/pub/math/floatformats.html" class="external text" title="http://www.mrob.com/pub/math/floatformats.html" rel="nofollow">Survey of Floating-Point Formats</a> This page gives a very brief summary of floating-point formats that have been used over the years.</li>
<li><i><a href="http://hal.archives-ouvertes.fr/hal-00128124/en/" class="external text" title="http://hal.archives-ouvertes.fr/hal-00128124/en/" rel="nofollow">The pitfalls of verifying floating-point computations</a></i>, by David Monniaux, also printed in <i><a href="/wiki/Association_for_Computing_Machinery" title="Association for Computing Machinery">ACM</a> Transactions on programming languages and systems (TOPLAS)</i>, May 2008: a compendium of non-intuitive behaviours of floating-point on popular architectures, with implications for program verification and testing</li>
<li><a href="http://www.opencores.org" class="external autonumber" title="http://www.opencores.org" rel="nofollow">[1]</a> The www.opencores.org website contains open source floating point IP cores for the implementation of floating point operators in FPGA or ASIC devices. The project, double_fpu, contains verilog source code of a double precision floating point unit. The project, fpuvhdl, contains vhdl source code of a single precision floating point unit.</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 1278/1000000
Post-expand include size: 10428/2048000 bytes
Template argument size: 3263/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:11376-0!1!0!default!!en!2 and timestamp 20090401072154 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Floating_point">http://en.wikipedia.org/wiki/Floating_point</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Data_types" title="Category:Data types">Data types</a></span> | <span dir='ltr'><a href="/wiki/Category:Computer_arithmetic" title="Category:Computer arithmetic">Computer arithmetic</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Floating_point" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Floating_point" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Floating_point&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Floating_point&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Floating_point" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Floating_point" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Floating_point" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Floating_point&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Floating_point&amp;oldid=278442902" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Floating_point&amp;id=278442902">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%B9%D9%85%D9%84%D9%8A%D8%A7%D8%AA_%D8%A7%D9%84%D8%AD%D8%B3%D8%A7%D8%A8%D9%8A%D8%A9_%D8%B9%D9%84%D9%89_%D8%A3%D8%B9%D8%AF%D8%A7%D8%AF_%D8%A7%D9%84%D9%81%D8%A7%D8%B5%D9%84%D8%A9_%D8%A7%D9%84%D8%B9%D8%A7%D8%A6%D9%85%D8%A9">العربية</a></li>
				<li class="interwiki-ca"><a href="http://ca.wikipedia.org/wiki/Coma_flotant">Català</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Gleitkommazahl">Deutsch</a></li>
				<li class="interwiki-et"><a href="http://et.wikipedia.org/wiki/Ujukomaarv">Eesti</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Coma_flotante">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Virgule_flottante">Français</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EB%B6%80%EB%8F%99%EC%86%8C%EC%88%98%EC%A0%90">한국어</a></li>
				<li class="interwiki-id"><a href="http://id.wikipedia.org/wiki/Floating-point">Bahasa Indonesia</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Numero_in_virgola_mobile">Italiano</a></li>
				<li class="interwiki-he"><a href="http://he.wikipedia.org/wiki/%D7%A0%D7%A7%D7%95%D7%93%D7%94_%D7%A6%D7%A4%D7%94">עברית</a></li>
				<li class="interwiki-hu"><a href="http://hu.wikipedia.org/wiki/Lebeg%C5%91pontos_sz%C3%A1m%C3%A1br%C3%A1zol%C3%A1s">Magyar</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Drijvendekommagetal">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E6%B5%AE%E5%8B%95%E5%B0%8F%E6%95%B0%E7%82%B9%E6%95%B0">日本語</a></li>
				<li class="interwiki-no"><a href="http://no.wikipedia.org/wiki/Flyttall">‪Norsk (bokmål)‬</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Liczba_zmiennoprzecinkowa">Polski</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/V%C3%ADrgula_flutuante">Português</a></li>
				<li class="interwiki-ro"><a href="http://ro.wikipedia.org/wiki/Virgul%C4%83_mobil%C4%83">Română</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%A7%D0%B8%D1%81%D0%BB%D0%B0_%D1%81_%D0%BF%D0%BB%D0%B0%D0%B2%D0%B0%D1%8E%D1%89%D0%B5%D0%B9_%D0%B7%D0%B0%D0%BF%D1%8F%D1%82%D0%BE%D0%B9">Русский</a></li>
				<li class="interwiki-sk"><a href="http://sk.wikipedia.org/wiki/Pohybliv%C3%A1_desatinn%C3%A1_%C4%8Diarka">Slovenčina</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/Liukuluku">Suomi</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Flyttal">Svenska</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/D%E1%BA%A5u_ch%E1%BA%A5m_%C4%91%E1%BB%99ng">Tiếng Việt</a></li>
				<li class="interwiki-tr"><a href="http://tr.wikipedia.org/wiki/Kayan_Nokta">Türkçe</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E6%B5%AE%E7%82%B9%E6%95%B0">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 20 March 2009, at 00:38.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv117 in 0.068 secs. --></body></html>
