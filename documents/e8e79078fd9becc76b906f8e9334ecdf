<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Principle of maximum entropy,Bayes theorem,Bayesian probability,Bounded interval,Closed form solution,Conserved quantities,Continuous distribution,Differential entropy,E.T. Jaynes,Entropy,Entropy maximization" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Principle of maximum entropy - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Principle_of_maximum_entropy";
		var wgTitle = "Principle of maximum entropy";
		var wgAction = "view";
		var wgArticleId = "201718";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 285189198;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Principle_of_maximum_entropy skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Principle of maximum entropy</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Text_document_with_red_question_mark.svg" class="image" title="Text document with red question mark.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article includes a <a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">list of references</a> or <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">external links</a>, but <b>its sources remain unclear because it lacks <a href="/wiki/Wikipedia:Citing_sources#Inline_citations" title="Wikipedia:Citing sources">inline citations</a>.</b> Please <a href="/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check" title="Wikipedia:WikiProject Fact and Reference Check">improve</a> this article by introducing more precise citations <a href="/wiki/Wikipedia:When_to_cite" title="Wikipedia:When to cite">where appropriate</a>. <small><i>(September 2008)</i></small></td>
</tr>
</table>
<div class="dablink">Not to be confused with the <a href="/wiki/Second_law_of_thermodynamics" title="Second law of thermodynamics">law of increasing entropy</a>.</div>
<p>The <b>principle of maximum entropy</b> is a postulate about a universal feature of any <a href="/w/index.php?title=Probability_assignment&amp;action=edit&amp;redlink=1" class="new" title="Probability assignment (page does not exist)">probability assignment</a> on a given set of <a href="/wiki/Proposition" title="Proposition">propositions</a> (<a href="/wiki/Event" title="Event">events</a>, <a href="/wiki/Hypothesis" title="Hypothesis">hypotheses</a>, <a href="/wiki/Index" title="Index">indices</a>, etc.). Let some testable information about a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> function be given. Consider the set of all trial <a href="/wiki/Probability_distribution" title="Probability distribution">probability distributions</a> that encode this <a href="/wiki/Information" title="Information">information</a>. Then, the probability distribution that maximizes the <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">information entropy</a> is the true probability distribution with respect to the testable information prescribed.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1"><a href="#Overview"><span class="tocnumber">2</span> <span class="toctext">Overview</span></a></li>
<li class="toclevel-1"><a href="#Testable_information"><span class="tocnumber">3</span> <span class="toctext">Testable information</span></a></li>
<li class="toclevel-1"><a href="#General_solution_for_the_maximum_entropy_distribution_with_linear_constraints"><span class="tocnumber">4</span> <span class="toctext">General solution for the maximum entropy distribution with linear constraints</span></a>
<ul>
<li class="toclevel-2"><a href="#Discrete_case"><span class="tocnumber">4.1</span> <span class="toctext">Discrete case</span></a></li>
<li class="toclevel-2"><a href="#Continuous_case"><span class="tocnumber">4.2</span> <span class="toctext">Continuous case</span></a></li>
<li class="toclevel-2"><a href="#Examples"><span class="tocnumber">4.3</span> <span class="toctext">Examples</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Justifications_for_the_principle_of_maximum_entropy"><span class="tocnumber">5</span> <span class="toctext">Justifications for the principle of maximum entropy</span></a>
<ul>
<li class="toclevel-2"><a href="#Information_entropy_as_a_measure_of_.27uninformativeness.27"><span class="tocnumber">5.1</span> <span class="toctext">Information entropy as a measure of 'uninformativeness'</span></a></li>
<li class="toclevel-2"><a href="#The_Wallis_derivation"><span class="tocnumber">5.2</span> <span class="toctext">The Wallis derivation</span></a></li>
<li class="toclevel-2"><a href="#Compatibility_with_Bayes_Rule"><span class="tocnumber">5.3</span> <span class="toctext">Compatibility with Bayes Rule</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">8</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="History" id="History"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=1" title="Edit section: History">edit</a>]</span> <span class="mw-headline">History</span></h2>
<p>The principle was first expounded by <a href="/wiki/E.T._Jaynes" title="E.T. Jaynes" class="mw-redirect">E.T. Jaynes</a> in his seminal papers of 1957 where he emphasized a natural correspondence between <a href="/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a> and <a href="/wiki/Information_theory" title="Information theory">information theory</a>. In particular, Jaynes offered a new and very general rationale why the Gibbsian method of statistical mechanics works. He suggested that the <a href="/wiki/Entropy" title="Entropy">entropy</a> in statistical mechanics, and the <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">information entropy</a> in information theory, are principally the same thing. Consequently, <a href="/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a> should be seen just as a particular application of a general tool of logical <a href="/wiki/Inference" title="Inference">inference</a> and <a href="/wiki/Information_theory" title="Information theory">information theory</a>.</p>
<p><a name="Overview" id="Overview"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=2" title="Edit section: Overview">edit</a>]</span> <span class="mw-headline">Overview</span></h2>
<p>In most practical cases, the <a href="/wiki/Testable_information" title="Testable information" class="mw-redirect">testable information</a> is given by a set of <a href="/wiki/Conserved_quantities" title="Conserved quantities" class="mw-redirect">conserved quantities</a> (average values of some moment functions), associated with the <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> in question. In this way the maximum entropy principle is most often used in <a href="/wiki/Statistical_thermodynamics" title="Statistical thermodynamics" class="mw-redirect">statistical thermodynamics</a>. Another possibility is to prescribe some <a href="/wiki/Symmetries" title="Symmetries" class="mw-redirect">symmetries</a> of the probability distribution. An equivalence between the <a href="/wiki/Conserved_quantities" title="Conserved quantities" class="mw-redirect">conserved quantities</a> and corresponding <a href="/wiki/Symmetry_groups" title="Symmetry groups" class="mw-redirect">symmetry groups</a> implies the same level of equivalence for both these two ways of specifying the testable information in the maximum entropy method.</p>
<p>The maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods, <a href="/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a> and <a href="/wiki/Logical_inference" title="Logical inference" class="mw-redirect">logical inference</a> in particular. Strictly speaking, the trial distributions, which do not maximize the entropy, are actually not <i>probability</i> distributions.</p>
<p>The maximum entropy principle makes explicit our freedom in using different forms of <a href="/wiki/Prior_information" title="Prior information" class="mw-redirect">prior information</a>. As a special case, a uniform <a href="/wiki/Prior_probability" title="Prior probability">prior probability</a> density (Laplace's <a href="/wiki/Principle_of_indifference" title="Principle of indifference">principle of indifference</a>) may be adopted. Thus, the maximum entropy principle is not just an <i>alternative</i> to the methods of inference of classical statistics, but its important conceptual generalization and correction.</p>
<p><a name="Testable_information" id="Testable_information"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=3" title="Edit section: Testable information">edit</a>]</span> <span class="mw-headline">Testable information</span></h2>
<p>The principle of maximum entropy is useful explicitly only when applied to <i>testable information</i>. A piece of information is testable if it can be determined whether a given distribution is consistent with it. For example, the statements</p>
<dl>
<dd>The expectation of the variable <i>x</i> is 2.87</dd>
</dl>
<p>and</p>
<dl>
<dd><i>p</i><sub>2</sub> + <i>p</i><sub>3</sub> &gt; 0.6</dd>
</dl>
<p>are statements of testable information.</p>
<p>Given testable information, the maximum entropy procedure consists of seeking the <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> which maximizes <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">information entropy</a>, subject to the constraints of the information. This constrained optimization problem is typically solved using the method of <a href="/wiki/Lagrange_multiplier" title="Lagrange multiplier" class="mw-redirect">Lagrange multipliers</a>.</p>
<p>Entropy maximization with no testable information takes place under a single constraint: the sum of the probabilities must be one. Under this constraint, the maximum entropy probability distribution is the <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniform distribution</a>,</p>
<dl>
<dd><img class="tex" alt="p_i=\frac{1}{n}\ {\rm for\ all}\ i\in\{\,1,\dots,n\,\}." src="http://upload.wikimedia.org/math/e/1/d/e1dff8534fc340440c782ff6b072ebf7.png" /></dd>
</dl>
<p>The principle of maximum entropy can thus be seen as a generalization of the classical <a href="/wiki/Principle_of_indifference" title="Principle of indifference">principle of indifference</a>, also known as the principle of insufficient reason</p>
<p><a name="General_solution_for_the_maximum_entropy_distribution_with_linear_constraints" id="General_solution_for_the_maximum_entropy_distribution_with_linear_constraints"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=4" title="Edit section: General solution for the maximum entropy distribution with linear constraints">edit</a>]</span> <span class="mw-headline">General solution for the maximum entropy distribution with linear constraints</span></h2>
<p><a name="Discrete_case" id="Discrete_case"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=5" title="Edit section: Discrete case">edit</a>]</span> <span class="mw-headline">Discrete case</span></h3>
<p>We have some testable information <i>I</i> about a quantity <i>x</i> taking values in {<i>x<sub>1</sub></i>, <i>x<sub>2</sub></i>,..., <i>x<sub>n</sub></i>}. We express this information as <i>m</i> constraints on the expectations of the functions <i>f<sub>k</sub></i>; that is, we require our <a href="/wiki/Epistemic_probability" title="Epistemic probability" class="mw-redirect">epistemic probability</a> distribution to satisfy</p>
<dl>
<dd><img class="tex" alt="\sum_{i=1}^n \Pr(x_i|I)f_k(x_i) = F_k \qquad k = 1, \cdots,m." src="http://upload.wikimedia.org/math/2/1/6/2167106ae1b1bea80992b3bb1fc67ed2.png" /></dd>
</dl>
<p>Furthermore, the probabilities must sum to one, giving the constraint</p>
<dl>
<dd><img class="tex" alt="\sum_{i=1}^n \Pr(x_i|I) = 1." src="http://upload.wikimedia.org/math/8/f/1/8f1c6cedf7c7d32b98d31f578c7b37f8.png" /></dd>
</dl>
<p>The probability distribution with maximum information entropy subject to these constraints is</p>
<dl>
<dd><img class="tex" alt="\Pr(x_i|I) = \frac{1}{Z(\lambda_1,\cdots, \lambda_m)} \exp\left[\lambda_1 f_1(x_i) + \cdots + \lambda_m f_m(x_i)\right]" src="http://upload.wikimedia.org/math/5/d/4/5d4ddd7832c285d097285e4efd4a9bf6.png" /></dd>
</dl>
<p>It is sometimes called the <a href="/wiki/Gibbs_distribution" title="Gibbs distribution" class="mw-redirect">Gibbs distribution</a>. The normalization constant is determined by</p>
<dl>
<dd><img class="tex" alt=" Z(\lambda_1,\cdots, \lambda_m) = \sum_{i=1}^n \exp\left[\lambda_1 f_1(x_i) + \cdots + \lambda_m f_m(x_i)\right]." src="http://upload.wikimedia.org/math/9/7/1/9718a9075d2e1029b4c8eb263d07db96.png" /></dd>
</dl>
<p>and is conventionally called the <a href="/wiki/Partition_function_(mathematics)" title="Partition function (mathematics)">partition function</a>. (Interestingly, the <a href="/wiki/Pitman-Koopman_theorem" title="Pitman-Koopman theorem" class="mw-redirect">Pitman-Koopman theorem</a> states that the necessary and sufficient condition for a sampling distribution to admit <a href="/wiki/Sufficiency_(statistics)" title="Sufficiency (statistics)">sufficient statistics</a> of bounded dimension is that it have the general form of a maximum entropy distribution.)</p>
<p>The λ<sub>k</sub> parameters are Lagrange multipliers whose particular values are determined by the constraints according to</p>
<dl>
<dd><img class="tex" alt="F_k = \frac{\partial}{\partial \lambda_k} \log Z(\lambda_1,\cdots, \lambda_m)." src="http://upload.wikimedia.org/math/5/f/d/5fd7c9f363afd1f609915e56e861bc73.png" /></dd>
</dl>
<p>These <i>m</i> simultaneous equations do not generally possess a <a href="/wiki/Closed_form_solution" title="Closed form solution" class="mw-redirect">closed form solution</a>, and are usually solved by <a href="/wiki/Numerical_analysis" title="Numerical analysis">numerical methods</a>.</p>
<p><a name="Continuous_case" id="Continuous_case"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=6" title="Edit section: Continuous case">edit</a>]</span> <span class="mw-headline">Continuous case</span></h3>
<p>For <a href="/wiki/Continuous_distribution" title="Continuous distribution" class="mw-redirect">continuous distributions</a>, the simple definition of Shannon entropy ceases to be so useful (see <i><a href="/wiki/Differential_entropy" title="Differential entropy">differential entropy</a></i>). Instead what is more useful is the <a href="/wiki/Relative_entropy" title="Relative entropy" class="mw-redirect">relative entropy</a> of the distribution, (<a href="/wiki/E.T._Jaynes" title="E.T. Jaynes" class="mw-redirect">Jaynes</a>, 1963, 1968, 2003),</p>
<dl>
<dd><img class="tex" alt="H_c=-\int p(x)\log\frac{p(x)}{m(x)}\,dx" src="http://upload.wikimedia.org/math/8/d/d/8dd3a097c6fbf48ed509159af3f8d177.png" /></dd>
</dl>
<p>where <i>m</i>(<i>x</i>), which Jaynes called the "invariant measure", is proportional to the <a href="/wiki/Limiting_density_of_discrete_points" title="Limiting density of discrete points">limiting density of discrete points</a>. For now, we shall assume that it is known; we will discuss it further after the solution equations are given.</p>
<p>Relative entropy is usually defined as the <a href="/wiki/Kullback-Leibler_divergence" title="Kullback-Leibler divergence" class="mw-redirect">Kullback-Leibler divergence</a> of <i>m</i> from <i>p</i> (although it is sometimes, confusingly, defined as the negative of this). The inference principle of minimizing this, due to Kullback, is known as the <a href="/wiki/Kullback-Leibler_divergence#Principle_of_minimum_discrimination_information" title="Kullback-Leibler divergence" class="mw-redirect">Principle of Minimum Discrimination Information</a>.</p>
<p>We have some testable information <i>I</i> about a quantity <i>x</i> which takes values in some <a href="/wiki/Interval_(mathematics)" title="Interval (mathematics)">interval</a> of the <a href="/wiki/Real_numbers" title="Real numbers" class="mw-redirect">real numbers</a> (all integrals below are over this interval). We express this information as <i>m</i> constraints on the expectations of the functions <i>f<sub>k</sub></i>, i.e. we require our epistemic probability density function to satisfy</p>
<dl>
<dd><img class="tex" alt="\int p(x|I)f_k(x)dx = F_k \qquad k = 1, \cdots,m" src="http://upload.wikimedia.org/math/0/f/c/0fc299476bf43d03dfeb16b8b744dafd.png" /></dd>
</dl>
<p>And of course, the probability density must integrate to one, giving the constraint</p>
<dl>
<dd><img class="tex" alt="\int p(x|I)dx = 1" src="http://upload.wikimedia.org/math/1/6/3/16394ab6395bbded089759d72fe0376f.png" /></dd>
</dl>
<p>The probability density function with maximum <i>H<sub>c</sub></i> subject to these constraints is</p>
<dl>
<dd><img class="tex" alt="p(x|I) = \frac{1}{Z(\lambda_1,\cdots, \lambda_m)} m(x)\exp\left[\lambda_1 f_1(x) + \cdots + \lambda_m f_m(x)\right]" src="http://upload.wikimedia.org/math/4/d/6/4d680e6dfae786099d35fbf9fb427074.png" /></dd>
</dl>
<p>with the <a href="/wiki/Partition_function_(mathematics)" title="Partition function (mathematics)">partition function</a> determined by</p>
<dl>
<dd><img class="tex" alt=" Z(\lambda_1,\cdots, \lambda_m) = \int m(x)\exp\left[\lambda_1 f_1(x) + \cdots + \lambda_m f_m(x)\right]dx" src="http://upload.wikimedia.org/math/4/f/7/4f736ae5513307a663246b8ff218d3fd.png" /></dd>
</dl>
<p>As in the discrete case, the values of the λ<sub>k</sub> parameters are determined by the constraints according to</p>
<dl>
<dd><img class="tex" alt="F_k = \frac{\partial}{\partial \lambda_k} \log Z(\lambda_1,\cdots, \lambda_m)" src="http://upload.wikimedia.org/math/a/2/c/a2c1cc6cc84c92bcbb12f1bc028a2055.png" /></dd>
</dl>
<p>The invariant measure function <i>m</i>(<i>x</i>) can be best understood by supposing that <i>x</i> is known to take values only in the <a href="/wiki/Bounded_interval" title="Bounded interval" class="mw-redirect">bounded interval</a> (<i>a</i>, <i>b</i>), and that no other information is given. Then the maximum entropy probability density function is</p>
<dl>
<dd><img class="tex" alt=" p(x|I) = A \cdot m(x), \qquad a &lt; x &lt; b" src="http://upload.wikimedia.org/math/b/2/3/b23b21aade410465228307bb8be14740.png" /></dd>
</dl>
<p>where <i>A</i> is a normalization constant. The invariant measure function is actually the prior density function encoding 'lack of relevant information'. It cannot be determined by the principle of maximum entropy, and must be determined by some other logical method, such as the <a href="/w/index.php?title=Principle_of_transformation_groups&amp;action=edit&amp;redlink=1" class="new" title="Principle of transformation groups (page does not exist)">principle of transformation groups</a> or <a href="/wiki/Marginalization_(probability)" title="Marginalization (probability)" class="mw-redirect">marginalization theory</a>.</p>
<p>Refer to Cover and Thomas for excellent explanation of the ideas .</p>
<p><a name="Examples" id="Examples"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=7" title="Edit section: Examples">edit</a>]</span> <span class="mw-headline">Examples</span></h3>
<p>For several examples of maximum entropy distributions, see the article on <a href="/wiki/Maximum_entropy_probability_distribution" title="Maximum entropy probability distribution">maximum entropy probability distributions</a>.</p>
<p><a name="Justifications_for_the_principle_of_maximum_entropy" id="Justifications_for_the_principle_of_maximum_entropy"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=8" title="Edit section: Justifications for the principle of maximum entropy">edit</a>]</span> <span class="mw-headline">Justifications for the principle of maximum entropy</span></h2>
<p>Proponents of the principle of maximum entropy justify its use in assigning epistemic probabilities in several ways, including the following two arguments. These arguments take the use of <a href="/wiki/Bayesian_probability" title="Bayesian probability">epistemic probability</a> as given, and thus have no force if the concept is itself under question.</p>
<p><a name="Information_entropy_as_a_measure_of_.27uninformativeness.27" id="Information_entropy_as_a_measure_of_.27uninformativeness.27"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=9" title="Edit section: Information entropy as a measure of 'uninformativeness'">edit</a>]</span> <span class="mw-headline">Information entropy as a measure of 'uninformativeness'</span></h3>
<p>Consider a <b>discrete epistemic probability distribution</b> among <i>m</i> mutually exclusive <a href="/wiki/Proposition" title="Proposition">propositions</a>. The most informative distribution would occur when one of the propositions was known to be true. In that case, the information entropy would be equal to zero. The least informative distribution would occur when there is no reason to favor any one of the propositions over the others. In that case, the only reasonable probability distribution would be uniform, and then the information entropy would be equal to its maximum possible value, log <i>m</i>. The information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is from zero (completely informative) to log <i>m</i> (completely uninformative).</p>
<p>By choosing to use the distribution with the maximum entropy allowed by our information, the argument goes, we are choosing the most uninformative distribution possible. To choose a distribution with lower entropy would be to assume information we do not possess; to choose one with a higher entropy would violate the constraints of the information we <i>do</i> possess. Thus the maximum entropy distribution is the only reasonable distribution.</p>
<p><a name="The_Wallis_derivation" id="The_Wallis_derivation"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=10" title="Edit section: The Wallis derivation">edit</a>]</span> <span class="mw-headline">The Wallis derivation</span></h3>
<p>The following argument is the result of a suggestion made by <a href="/w/index.php?title=Graham_Wallis&amp;action=edit&amp;redlink=1" class="new" title="Graham Wallis (page does not exist)">Graham Wallis</a> to E. T. Jaynes in 1962 (Jaynes, 2003). It is essentially the same mathematical argument used for the <a href="/wiki/Maxwell-Boltzmann_statistics" title="Maxwell-Boltzmann statistics" class="mw-redirect">Maxwell-Boltzmann statistics</a> in <a href="/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a>, although the conceptual emphasis is quite different. It has the advantage of being strictly combinatorial in nature, making no reference to information entropy as a measure of 'uncertainty', 'uninformativeness', or any other imprecisely defined concept. The information entropy function is not assumed <i>a priori</i>, but rather is found in the course of the argument; and the argument leads naturally to the procedure of maximizing the information entropy, rather than treating it in some other way.</p>
<p>Suppose an individual wishes to make an epistemic probability assignment among <i>m</i> <a href="/wiki/Mutually_exclusive" title="Mutually exclusive" class="mw-redirect">mutually exclusive</a> propositions. She has some testable information, but is not sure how to go about including this information in her probability assessment. She therefore conceives of the following random experiment. She will distribute <i>N</i> quanta of epistemic probability (each worth 1/<i>N</i>) at random among the <i>m</i> possibilities. (One might imagine that she will throw <i>N</i> balls into <i>m</i> buckets while blindfolded. In order to be as fair as possible, each throw is to be independent of any other, and every bucket is to be the same size.) Once the experiment is done, she will check if the probability assignment thus obtained is consistent with her information. If not, she will reject it and try again. Otherwise, her assessment will be</p>
<dl>
<dd><img class="tex" alt="p_i = \frac{n_i}{N}" src="http://upload.wikimedia.org/math/3/0/0/3002a17ff655aeb3186f1d6d0fb49204.png" /></dd>
</dl>
<p>where <i>n<sub>i</sub></i> is the number of quanta that were assigned to the <i>i</i><sup>th</sup> proposition.</p>
<p>Now, in order to reduce the 'graininess' of the epistemic probability assignment, it will be necessary to use quite a large number of quanta of epistemic probability. Rather than actually carry out, and possibly have to repeat, the rather long random experiment, our protagonist decides to simply calculate and use the most probable result. The probability of any particular result is the <a href="/wiki/Multinomial_distribution" title="Multinomial distribution">multinomial distribution</a>,</p>
<dl>
<dd><img class="tex" alt="Pr(\mathbf{p}) = W \cdot m^{-N}" src="http://upload.wikimedia.org/math/b/1/0/b104e29d7f0a49382871cde9e899cb92.png" /></dd>
</dl>
<p>where</p>
<dl>
<dd><img class="tex" alt="W = \frac{N!}{n_1&#160;!n_2&#160;!...n_m!}" src="http://upload.wikimedia.org/math/f/5/d/f5dedc9b38aeddbb914b9853da5bae91.png" /></dd>
</dl>
<p>is sometimes known as the multiplicity of the outcome.</p>
<p>The most probable result is the one which maximizes the multiplicity <i>W</i>. Rather than maximizing <i>W</i> directly, our protagonist could equivalently maximize any monotonic increasing function of <i>W</i>. She decides to maximize</p>
<dl>
<dd><img class="tex" alt="\begin{matrix}\frac{1}{N}\log W 
&amp;=&amp; \frac{1}{N}\log \frac{N!}{n_1&#160;!n_2&#160;!...n_m!}\qquad\qquad\qquad\qquad\qquad \\ \\ \ 
&amp;=&amp; \frac{1}{N}\log \frac{N!}{Np_1&#160;!Np_2&#160;!...Np_m!} \qquad\qquad\qquad\qquad\\ \\ \ 
&amp;=&amp; \frac{1}{N}\left( \log N! - \sum_{i=1}^m \log Np_i! \right) \qquad\qquad\end{matrix}" src="http://upload.wikimedia.org/math/e/c/0/ec09a82dad07397f4ad3ba62991d5bd4.png" /></dd>
</dl>
<p>At this point, in order simplify the expression, our protagonist takes the limit as <img class="tex" alt="N\to\infty" src="http://upload.wikimedia.org/math/f/f/6/ff60c79158503283e50aa6524a12db1e.png" />, i.e. as the epistemic probability levels go from grainy discrete values to smooth continuous values. Using <a href="/wiki/Stirling%27s_approximation" title="Stirling's approximation">Stirling's approximation</a>, she finds</p>
<dl>
<dd><img class="tex" alt="\begin{matrix}\lim_{N \to \infty}\left(\frac{1}{N}\log W\right) 
&amp;=&amp; \frac{1}{N}\left( N\log N - \sum_{i=1}^m Np_i\log Np_i \right)\qquad\qquad\qquad\qquad \\  \\  \ 
&amp;=&amp; \log N - \sum_{i=1}^m p_i\log Np_i \qquad\qquad\qquad\qquad\qquad\qquad \\  \\  \ 
&amp;=&amp; \log N - \log N \sum_{i=1}^m p_i - \sum_{i=1}^m p_i\log p_i \qquad\qquad\qquad \\  \\  \ 
&amp;=&amp; \left(1 - \sum_{i=1}^m p_i \right)\log N - \sum_{i=1}^m p_i\log p_i \qquad\qquad\qquad \\  \\  \ 
&amp;=&amp; - \sum_{i=1}^m p_i\log p_i  \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \\  \\  \ 
&amp;=&amp; H(\mathbf{p}) \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
\end{matrix}" src="http://upload.wikimedia.org/math/d/9/7/d97a6b8524f1044617e234e7f1a81921.png" /></dd>
</dl>
<p>All that remains for our protagonist to do is to maximize entropy under the constraints of her testable information. She has found that the maximum entropy distribution is the most probable of all "fair" random epistemic distributions, in the limit as the probability levels go from discrete to continuous.</p>
<p><a name="Compatibility_with_Bayes_Rule" id="Compatibility_with_Bayes_Rule"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=11" title="Edit section: Compatibility with Bayes Rule">edit</a>]</span> <span class="mw-headline">Compatibility with Bayes Rule</span></h3>
<p>Giffin et al. (2007) state that <a href="/wiki/Bayes_theorem" title="Bayes theorem" class="mw-redirect">Bayes' Rule</a> and the Principle of Maximum Entropy (MaxEnt) are completely compatible and can be seen as special cases of the Method of Maximum (relative) Entropy. They state that this method reproduces every aspect of orthodox Bayesian inference methods. In addition this new method opens the door to tackling problems that could not be addressed by either the MaxEnt or orthodox Bayesian methods individually. Moreover, recent contributions (Lazar 2003, and Schennach 2005) show that frequentist relative-entropy-based inference approaches (such as Empirical Likelihood and Exponentially Tilted Empirical Likelihood - see e.g. Owen 2001 and Kitamura 2006) can be combined with prior information to perform Bayesian posterior analysis.</p>
<p>On the other hand, Jaynes (1986, 1996) showed the Bayesian methods can lead to results differing from the maximum-entropy ones. Thus the relationship between the two methods, in particular the question whether one can be derived from the other, still needs clarification.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=12" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Entropy_maximization" title="Entropy maximization">Entropy maximization</a></li>
<li><a href="/wiki/Maximum_entropy_classifier" title="Maximum entropy classifier" class="mw-redirect">Maximum entropy classifier</a></li>
<li><a href="/wiki/Maximum_entropy_spectral_estimation" title="Maximum entropy spectral estimation">Maximum entropy spectral estimation</a></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=13" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li>Jaynes, E.T., 1957, <a href="http://prola.aps.org/pdf/PR/v106/i4/p620_1" class="external text" title="http://prola.aps.org/pdf/PR/v106/i4/p620_1" rel="nofollow">'Information Theory and Statistical Mechanics'</a>, in Physical Review May 1957 Volume 106, #4 (pages 620-630).</li>
<li>Jaynes, E. T., 1963, <a href="http://bayes.wustl.edu/etj/node1.html" class="external text" title="http://bayes.wustl.edu/etj/node1.html" rel="nofollow">'Information Theory and Statistical Mechanics'</a>, in Statistical Physics, K. Ford (ed.), Benjamin, New York, p. 181.</li>
<li>Jaynes, E. T., 1968, <a href="http://bayes.wustl.edu/etj/node1.html" class="external text" title="http://bayes.wustl.edu/etj/node1.html" rel="nofollow">'Prior Probabilities'</a>, IEEE Trans. on Systems Science and Cybernetics, SSC-4, 227.</li>
<li>Jaynes, E. T., 1986 (new version online 1996), <a href="http://bayes.wustl.edu/etj/node1.html" class="external text" title="http://bayes.wustl.edu/etj/node1.html" rel="nofollow">'Monkeys, kangaroos and <span class="texhtml"><i>N</i></span>'</a>, in <i>Maximum-Entropy and Bayesian Methods in Applied Statistics</i>, J. H. Justice (ed.), Cambridge University Press, Cambridge, p. 26.</li>
<li>Jaynes, E. T., 2003, <i>Probability Theory: The Logic of Science</i>, Cambridge University Press.</li>
<li>Giffin, A. and Caticha, A., 2007, <a href="http://arxiv.org/abs/0708.1593" class="external text" title="http://arxiv.org/abs/0708.1593" rel="nofollow"><i>Updating Probabilities with Data and Moments</i></a></li>
<li>Guiasu, S. and Shenitzer, A., 1985, 'The principle of maximum entropy', The Mathematical Intelligencer, <b>7</b>(1).</li>
<li>Harremoës P. and Topsøe F., 2001, <i>Maximum Entropy Fundamentals</i>, Entropy, 3(3), 191-226.</li>
<li>Kapur, J. N.; and Kesevan, H. K., 1992, <i>Entropy optimization principles with applications</i>, Boston: Academic Press. <a href="/wiki/Special:BookSources/0123976707" class="internal">ISBN 0-12-397670-7</a></li>
<li>Kitamura, Y., 2006, <a href="http://cowles.econ.yale.edu/P/cd/d15b/d1569.pdf" class="external text" title="http://cowles.econ.yale.edu/P/cd/d15b/d1569.pdf" rel="nofollow"><i>Empirical Likelihood Methods in Econometrics: Theory and Practice</i></a>,Cowles Foundation Discussion Papers 1569, Cowles Foundation, Yale University.</li>
<li>Lazar, N., 2003, "Bayesian Empirical Likelihood", Biometrika, 90, 319-326.</li>
<li>Owen, A. B., <i>Empirical Likelihood</i>, Chapman and Hall.</li>
<li>Schennach, S. M., 2005, "Bayesian Exponentially Tilted Empirical Likelihood", Biometrika, 92(1), 31-46.</li>
<li>Uffink, Jos, 1995, <a href="http://www.phys.uu.nl/~wwwgrnsl/jos/mepabst/mep.pdf" class="external text" title="http://www.phys.uu.nl/~wwwgrnsl/jos/mepabst/mep.pdf" rel="nofollow">'Can the Maximum Entropy Principle be explained as a consistency requirement?'</a>, Studies in History and Philosophy of Modern Physics <b>26B</b>, 223-261.</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit&amp;section=14" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li>An easy-to-read introduction to maximum entropy methods in the context of natural language processing is:</li>
</ul>
<dl>
<dd>Ratnaparkhi A. "A simple introduction to maximum entropy models for natural language processing" Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania, 1997. Available <a href="http://citeseer.ist.psu.edu/128751.html" class="external text" title="http://citeseer.ist.psu.edu/128751.html" rel="nofollow">here</a>.</dd>
</dl>
<ul>
<li><a href="http://homepages.inf.ed.ac.uk/s0450736/maxent.html" class="external text" title="http://homepages.inf.ed.ac.uk/s0450736/maxent.html" rel="nofollow">Maximum Entropy Modeling</a></li>
</ul>
<p>A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro.<a href="http://www.jneurosci.org/cgi/content/full/28/2/505" class="external autonumber" title="http://www.jneurosci.org/cgi/content/full/28/2/505" rel="nofollow">[1]</a></p>
<dl>
<dd>This page contains pointers to various papers and software implementations of Maximum Entropy Model on the net.</dd>
</dl>


<!-- 
NewPP limit report
Preprocessor node count: 205/1000000
Post-expand include size: 3374/2048000 bytes
Template argument size: 1382/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:201718-0!1!0!default!!en!2 and timestamp 20090421074140 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy">http://en.wikipedia.org/wiki/Principle_of_maximum_entropy</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Entropy_and_information" title="Category:Entropy and information">Entropy and information</a></span> | <span dir='ltr'><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_theory" title="Category:Statistical theory">Statistical theory</a></span> | <span dir='ltr'><a href="/wiki/Category:Bayesian_statistics" title="Category:Bayesian statistics">Bayesian statistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_principles" title="Category:Statistical principles">Statistical principles</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Articles_lacking_in-text_citations_from_September_2008" title="Category:Articles lacking in-text citations from September 2008">Articles lacking in-text citations from September 2008</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Principle_of_maximum_entropy" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Principle_of_maximum_entropy" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Principle_of_maximum_entropy&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Principle_of_maximum_entropy" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Principle_of_maximum_entropy" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Principle_of_maximum_entropy" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Principle_of_maximum_entropy&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Principle_of_maximum_entropy&amp;oldid=285189198" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Principle_of_maximum_entropy&amp;id=285189198">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Maximum-Entropie-Methode">Deutsch</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Principe_d%27entropie_maximale">Français</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC%E5%8E%9F%E7%90%86">日本語</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 21 April 2009, at 07:41 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv221 in 0.110 secs. --></body></html>
