<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Linear discriminant analysis,Articles with unsourced statements since December 2008,ANOVA,Affine transformation,Annals of Eugenics,Bankruptcy prediction,Canonical coordinates,Categorical,Categorical variable,Covariance,Cross-validation" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Linear discriminant analysis - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Linear_discriminant_analysis";
		var wgTitle = "Linear discriminant analysis";
		var wgAction = "view";
		var wgArticleId = "1470657";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 278488067;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Linear_discriminant_analysis skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Linear discriminant analysis</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Text_document_with_red_question_mark.svg" class="image" title="Text document with red question mark.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article includes a <a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">list of references</a> or <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">external links</a>, but <b>its sources remain unclear because it lacks <a href="/wiki/Wikipedia:Citing_sources#Inline_citations" title="Wikipedia:Citing sources">inline citations</a>.</b> Please <a href="/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check" title="Wikipedia:WikiProject Fact and Reference Check">improve</a> this article by introducing more precise citations <a href="/wiki/Wikipedia:When_to_cite" title="Wikipedia:When to cite">where appropriate</a>. <small><i>(December 2008)</i></small></td>
</tr>
</table>
<p><b>Linear discriminant analysis (LDA)</b> and the related <b>Fisher's linear discriminant</b> are methods used in <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> to find the <a href="/wiki/Linear_combination" title="Linear combination">linear combination</a> of <a href="/wiki/Features_(pattern_recognition)" title="Features (pattern recognition)">features</a> which best separate two or more classes of objects or events. The resulting combination may be used as a <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, or, more commonly, for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction" class="mw-redirect">dimensionality reduction</a> before later <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>.</p>
<p>LDA is closely related to <a href="/wiki/ANOVA" title="ANOVA" class="mw-redirect">ANOVA</a> (analysis of variance) and <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, which also attempt to express one <a href="/wiki/Dependent_variable" title="Dependent variable" class="mw-redirect">dependent variable</a> as a linear combination of other features or measurements. In the other two methods however, the dependent variable is a numerical quantity, while for LDA it is a <a href="/wiki/Categorical" title="Categorical">categorical</a> variable (<i>i.e.</i> the class label).</p>
<p>LDA is also closely related to <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA) and <a href="/wiki/Factor_analysis" title="Factor analysis">factor analysis</a> in that both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique&#160;: a distinction between independent variables and dependent variables (also called criterion variables) must be made.</p>
<p>LDA works when the measurements made on each observation are continuous quantities. When dealing with categorical variables, the equivalent technique is <a href="/w/index.php?title=Discriminant_correspondence_analysis&amp;action=edit&amp;redlink=1" class="new" title="Discriminant correspondence analysis (page does not exist)">discriminant correspondence analysis</a>.<sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since December 2008" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup></p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#LDA_for_two_classes"><span class="tocnumber">1</span> <span class="toctext">LDA for two classes</span></a></li>
<li class="toclevel-1"><a href="#Canonical_discriminant_analysis_for_k_classes"><span class="tocnumber">2</span> <span class="toctext">Canonical discriminant analysis for k classes</span></a></li>
<li class="toclevel-1"><a href="#Fisher.27s_linear_discriminant"><span class="tocnumber">3</span> <span class="toctext">Fisher's linear discriminant</span></a></li>
<li class="toclevel-1"><a href="#Multiclass_LDA"><span class="tocnumber">4</span> <span class="toctext">Multiclass LDA</span></a></li>
<li class="toclevel-1"><a href="#Practical_use"><span class="tocnumber">5</span> <span class="toctext">Practical use</span></a></li>
<li class="toclevel-1"><a href="#Applications"><span class="tocnumber">6</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2"><a href="#Bankruptcy_prediction"><span class="tocnumber">6.1</span> <span class="toctext">Bankruptcy prediction</span></a></li>
<li class="toclevel-2"><a href="#Face_recognition"><span class="tocnumber">6.2</span> <span class="toctext">Face recognition</span></a></li>
<li class="toclevel-2"><a href="#Marketing"><span class="tocnumber">6.3</span> <span class="toctext">Marketing</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="LDA_for_two_classes" id="LDA_for_two_classes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=1" title="Edit section: LDA for two classes">edit</a>]</span> <span class="mw-headline">LDA for two classes</span></h2>
<p>Consider a set of observations <b>x</b> (also called features, attributes, variables or measurements) for each sample of an object or event with known class <i>y</i>. This set of samples is called the <a href="/wiki/Training_set" title="Training set">training set</a>. The classification problem is then to find a good predictor for the class <i>y</i> of any sample of the same distribution (not necessarily from the training set) given only an observation <b>x</b>.</p>
<p>LDA approaches the problem by assuming that the conditional <a href="/wiki/Probability_density_function" title="Probability density function">probability density functions</a> <img class="tex" alt="p(\vec x|y=1)" src="http://upload.wikimedia.org/math/c/8/b/c8bb486e9693fd423fb3240b1ca228fc.png" /> and <img class="tex" alt="p(\vec x|y=0)" src="http://upload.wikimedia.org/math/6/1/b/61b4c1cab7a1e0ec1b59a896498b9316.png" /> are both <a href="/wiki/Normal_distribution" title="Normal distribution">normally distributed</a>. Under this assumption, the Bayes optimal solution is to predict points as being from the second class if the likelihood ratio is below some threshold T, so that</p>
<p><img class="tex" alt=" (\vec x- \vec \mu_0)^T \Sigma_{y=0}^{-1} ( \vec x- \vec \mu_0)\ +\ \mathrm{ln}|\Sigma_{y=0}|\ -\ (\vec x- \vec \mu_1)^T \Sigma_{y=1}^{-1} ( \vec x- \vec \mu_1)\ -\ \mathrm{ln}|\Sigma_{y=1}| \ &lt; \ T " src="http://upload.wikimedia.org/math/2/d/3/2d3c7f0d93de66e2a9fac0ba254fb01c.png" /></p>
<p>Without any further assumptions, the resulting classifier is referred to as QDA (<a href="/wiki/Quadratic_classifier" title="Quadratic classifier">quadratic discriminant analysis</a>). LDA also makes the simplifying <a href="/wiki/Homoscedastic" title="Homoscedastic" class="mw-redirect">homoscedastic</a> assumption (<i>i.e.</i> that the class <a href="/wiki/Covariance" title="Covariance">covariances</a> are identical, so <span class="texhtml">Σ<sub><i>y</i> = 0</sub> = Σ<sub><i>y</i> = 1</sub> = Σ</span>) and that the covariances have full rank. In this case, several terms cancel and the above decision criterion becomes a threshold on the <a href="/wiki/Dot_product" title="Dot product">dot product</a></p>
<dl>
<dd><img class="tex" alt=" \vec w \cdot \vec x &lt; c " src="http://upload.wikimedia.org/math/b/a/4/ba49e666b68560dbf377ea2d07d9654b.png" /></dd>
</dl>
<p>for some constant c, where</p>
<dl>
<dd><img class="tex" alt="\vec w = \Sigma^{-1} (\vec \mu_1 - \vec \mu_0)" src="http://upload.wikimedia.org/math/9/4/9/949911a79ba5bed61e73c139cbe6abe1.png" /></dd>
</dl>
<p>This means that the probability of an input <b>x</b> being in a class <i>y</i> is purely a function of this linear combination of the known observations.</p>
<p><a name="Canonical_discriminant_analysis_for_k_classes" id="Canonical_discriminant_analysis_for_k_classes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=2" title="Edit section: Canonical discriminant analysis for k classes">edit</a>]</span> <span class="mw-headline">Canonical discriminant analysis for k classes</span></h2>
<p>Canonical discriminant analysis finds axes (the number of categories -1 = k-1 <a href="/wiki/Canonical_coordinates" title="Canonical coordinates">canonical coordinates</a>) that best separate the categories. These linear functions are uncorrelated and define, in effect, an optimal k-1 space through the n-dimensional cloud of data that best separates (the projections in that space of) the k groups. See "<a href="#Multiclass_LDA" title="">Multiclass LDA</a>" below.</p>
<p><a name="Fisher.27s_linear_discriminant" id="Fisher.27s_linear_discriminant"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=3" title="Edit section: Fisher's linear discriminant">edit</a>]</span> <span class="mw-headline">Fisher's linear discriminant</span></h2>
<p>The terms <i>Fisher's linear discriminant</i> and <i>LDA</i> are often used interchangeably, although <a href="/wiki/Ronald_A._Fisher" title="Ronald A. Fisher" class="mw-redirect">Fisher's</a> original article <i>The Use of Multiple Measures in Taxonomic Problems</i> (1936) actually describes a slightly different discriminant, which does not make some of the assumptions of LDA such as normally distributed classes or equal class covariances.</p>
<p>Suppose two classes of observations have means <img class="tex" alt=" \vec \mu_{y=0}, \vec \mu_{y=1} " src="http://upload.wikimedia.org/math/8/1/b/81b299b099e9ee4ef6d242cd59c9e9a8.png" /> and covariances <span class="texhtml">Σ<sub><i>y</i> = 0</sub>,Σ<sub><i>y</i> = 1</sub></span>. Then the linear combination of features <img class="tex" alt=" \vec w \cdot \vec x " src="http://upload.wikimedia.org/math/1/d/1/1d1b94f0a533a6d46a0fc6ad9d0ee4b5.png" /> will have means <img class="tex" alt=" \vec w \cdot \vec \mu_{y=i} " src="http://upload.wikimedia.org/math/1/b/0/1b0c3eba903d3b86a550818a174d9e47.png" /> and variances <img class="tex" alt=" \vec w^T \Sigma_{y=i} \vec w " src="http://upload.wikimedia.org/math/d/3/9/d397afd368d1bf70c2bf97861404b0d0.png" /> for <span class="texhtml"><i>i</i> = 0,1</span>. Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:</p>
<dl>
<dd><img class="tex" alt="S=\frac{\sigma_{between}^2}{\sigma_{within}^2}= \frac{(\vec w \cdot \vec \mu_{y=1} - \vec w \cdot \vec \mu_{y=0})^2}{\vec w^T \Sigma_{y=1} \vec w + \vec w^T \Sigma_{y=0} \vec w} = \frac{(\vec w \cdot (\vec \mu_{y=1} - \vec \mu_{y=0}))^2}{\vec w^T (\Sigma_{y=0}+\Sigma_{y=1}) \vec w} " src="http://upload.wikimedia.org/math/b/a/5/ba5e080c677ceba3288a813398c307b7.png" /></dd>
</dl>
<p>This measure is, in some sense, a measure of the <a href="/wiki/Signal-to-noise_ratio" title="Signal-to-noise ratio">signal-to-noise ratio</a> for the class labelling. It can be shown that the maximum separation occurs when</p>
<dl>
<dd><img class="tex" alt=" \vec w = (\Sigma_{y=0}+\Sigma_{y=1})^{-1}(\vec \mu_{y=1} - \vec \mu_{y=0}) " src="http://upload.wikimedia.org/math/d/e/6/de682099cd136c757d4d026b8d238c8d.png" /></dd>
</dl>
<p>When the assumptions of LDA are satisfied, the above equation is equivalent to LDA.</p>
<p>Be sure to note that the vector <img class="tex" alt="\vec w" src="http://upload.wikimedia.org/math/f/4/f/f4fdaaba2256b79221853e1251248001.png" /> is the normal to the discriminant hyperplane. As an example, in a two dimensional problem, the line that best divides the two groups is perpendicular to <img class="tex" alt="\vec w" src="http://upload.wikimedia.org/math/f/4/f/f4fdaaba2256b79221853e1251248001.png" />.</p>
<p>Generally, the data points are projected onto <img class="tex" alt="\vec w" src="http://upload.wikimedia.org/math/f/4/f/f4fdaaba2256b79221853e1251248001.png" />. However, to find the actual plane that best separates the data, one must solve for the bias term <span class="texhtml"><i>b</i></span> in <span class="texhtml"><i>w</i><sup><i>T</i></sup>μ<sub>1</sub> + <i>b</i> = − (<i>w</i><sup><i>T</i></sup>μ<sub>2</sub> + <i>b</i>)</span>.</p>
<p><a name="Multiclass_LDA" id="Multiclass_LDA"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=4" title="Edit section: Multiclass LDA">edit</a>]</span> <span class="mw-headline">Multiclass LDA</span></h2>
<p>In the case where there are more than two classes, the analysis used in the derivation of the Fisher discriminant can be extended to find a <a href="/wiki/Subspace" title="Subspace">subspace</a> which appears to contain all of the class variability. Suppose that each of C classes has a mean <span class="texhtml">μ<sub><i>i</i></sub></span> and the same covariance <span class="texhtml">Σ</span>. Then the between class variability may be defined by the sample covariance of the class means</p>
<dl>
<dd><img class="tex" alt=" \Sigma_b = \frac{1}{C} \sum_{i=1}^C (\mu_i-\mu) (\mu_i-\mu)^T " src="http://upload.wikimedia.org/math/1/e/3/1e3e788668c652e0a299b3deca6e4d2e.png" /></dd>
</dl>
<p>where <span class="texhtml">μ</span> is the mean of the class means. The class separation in a direction <img class="tex" alt=" \vec w " src="http://upload.wikimedia.org/math/f/4/f/f4fdaaba2256b79221853e1251248001.png" /> in this case will be given by</p>
<dl>
<dd><img class="tex" alt=" S = \frac{\vec w^T \Sigma_b \vec w}{\vec w^T \Sigma \vec w} = \frac{\vec w^T (\Sigma_b \Sigma^{-1}) \Sigma \vec w}{\vec w ^T\Sigma \vec w} " src="http://upload.wikimedia.org/math/3/0/d/30ddf823e82966a05dfeb419c6ea5ddd.png" /></dd>
</dl>
<p>This means that when <img class="tex" alt=" \vec w " src="http://upload.wikimedia.org/math/f/4/f/f4fdaaba2256b79221853e1251248001.png" /> is an eigenvector of <span class="texhtml">Σ<sub><i>b</i></sub>Σ <sup>− 1</sup></span> the separation will be equal to the corresponding eigenvalue. Since <span class="texhtml">Σ<sub><i>b</i></sub></span> is of most rank C-1, then these non-zero eigenvectors identify a vector subspace containing the variability between features. These vectors are primarily used in feature reduction, as in PCA. The smaller eigenvectors will tend to be very sensitive to the exact choice of training data, and it is often necessary to use regularisation as described in the next section.</p>
<p>Other generalizations of LDA for multiple classes have been defined to address the more general problem of <a href="/wiki/Heteroscedastic" title="Heteroscedastic" class="mw-redirect">heteroscedastic</a> distributions (i.e., where the data distributions are not <a href="/wiki/Homoscedastic" title="Homoscedastic" class="mw-redirect">homoscedastic</a>). One such method is Heteroscedastic LDA (see e.g. <a href="http://www.clsp.jhu.edu/~kumar/thesis.ps" class="external text" title="http://www.clsp.jhu.edu/~kumar/thesis.ps" rel="nofollow">HLDA</a> among others).</p>
<p>If classification is required, instead of <a href="/wiki/Dimension_reduction" title="Dimension reduction">dimension reduction</a>, there are a number of alternative techniques available. For instance, the classes may be partitioned, and a standard Fisher discriminant or LDA used to classify each partition. A common example of this is "one against the rest" where the points from one class are put in one group, and everything else in the other, and then LDA applied. This will result in C classifiers, whose results are combined. Another common method is pairwise classification, where a new classifier is created for each pair of classes (giving C(C-1) classifiers in total), with the individual classifiers combined to produce a final classification.</p>
<p><a name="Practical_use" id="Practical_use"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=5" title="Edit section: Practical use">edit</a>]</span> <span class="mw-headline">Practical use</span></h2>
<p>In practice, the class means and covariances are not known. They can, however, be estimated from the training set. Either the <a href="/wiki/Maximum_likelihood_estimation" title="Maximum likelihood estimation" class="mw-redirect">maximum likelihood estimate</a> or the <a href="/wiki/Maximum_a_posteriori" title="Maximum a posteriori" class="mw-redirect">maximum a posteriori</a> estimate may be used in place of the exact value in the above equations. Although the estimates of the covariance may be considered optimal in some sense, this does not mean that the resulting discriminant obtained by substituting these values is optimal in any sense, even if the assumption of normally distributed classes is correct.</p>
<p>Another complication in applying LDA and Fisher's discriminant to real data occurs when the number of observations of each sample exceeds the number of samples. In this case, the covariance estimates do not have full rank, and so cannot be inverted. There are a number of ways to deal with this. One is to use a <a href="/wiki/Pseudo_inverse" title="Pseudo inverse" class="mw-redirect">pseudo inverse</a> instead of the usual matrix inverse in the above formulae. Another, called regularised discriminant analysis, is to artificially increase the number of available samples by adding white noise to the existing samples. These new samples do not actually have to be calculated, since their effect on the class covariances can be expressed mathematically as</p>
<dl>
<dd><img class="tex" alt=" C_{new} = C+\sigma^2 I\," src="http://upload.wikimedia.org/math/5/9/6/596d5bd258ab4aba1735f6b902251c06.png" /></dd>
</dl>
<p>where <span class="texhtml"><i>I</i></span> is the identity matrix, and <span class="texhtml">σ</span> is the amount of noise added, called in this context the <i>regularisation parameter</i>. The value of <span class="texhtml">σ</span> is usually chosen to give the best results on a <a href="/wiki/Cross-validation" title="Cross-validation">cross-validation</a> set. The new value of the covariance matrix is always invertible, and can be used in place of the original sample covariance in the above formulae.</p>
<p>Also, in many practical cases linear discriminants are not suitable. LDA and Fisher's discriminant can be extended for use in non-linear classification via the <a href="/wiki/Kernel_trick" title="Kernel trick">kernel trick</a>. Here, the original observations are effectively mapped into a higher dimensional non-linear space. Linear classification in this non-linear space is then equivalent to non-linear classification in the original space. The most commonly used example of this is the kernel Fisher discriminant.</p>
<p>LDA can be generalized to <a href="/wiki/Multiple_discriminant_analysis" title="Multiple discriminant analysis">multiple discriminant analysis</a>, where <i>c</i> becomes a <a href="/wiki/Categorical_variable" title="Categorical variable" class="mw-redirect">categorical variable</a> with <i>N</i> possible states, instead of only two. Analogously, if the class-conditional densities <img class="tex" alt="p(\vec x|c=i)" src="http://upload.wikimedia.org/math/b/3/b/b3b22f1819bfd2337460d288e57ac159.png" /> are normal with shared covariances, the <a href="/wiki/Sufficient_statistic" title="Sufficient statistic" class="mw-redirect">sufficient statistic</a> for <img class="tex" alt="P(c|\vec x)" src="http://upload.wikimedia.org/math/3/9/6/396a21cadcb9ebfa7a012b9d6ab8d1fd.png" /> are the values of <i>N</i> projections, which are the <a href="/wiki/Subspace" title="Subspace">subspace</a> spanned by the <i>N</i> means, <a href="/wiki/Affine_transformation" title="Affine transformation">affine projected</a> by the inverse covariance matrix. These projections can be found by solving a <a href="/wiki/Eigenvalue,_eigenvector_and_eigenspace#generalized_eigenvalue_problem" title="Eigenvalue, eigenvector and eigenspace">generalized eigenvalue problem</a>, where the numerator is the covariance matrix formed by treating the means as the samples, and the denominator is the shared covariance matrix.</p>
<p><a name="Applications" id="Applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=6" title="Edit section: Applications">edit</a>]</span> <span class="mw-headline">Applications</span></h2>
<p>In addition to the examples given below, LDA is applied in <a href="/wiki/Positioning_(marketing)" title="Positioning (marketing)">positioning</a>, <a href="/wiki/Product_management" title="Product management">product management</a>, and <a href="/wiki/Marketing_research" title="Marketing research">marketing research</a>.</p>
<p><a name="Bankruptcy_prediction" id="Bankruptcy_prediction"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=7" title="Edit section: Bankruptcy prediction">edit</a>]</span> <span class="mw-headline">Bankruptcy prediction</span></h3>
<p>In <a href="/wiki/Bankruptcy_prediction" title="Bankruptcy prediction">bankruptcy prediction</a> based on accounting ratios and other financial variables, linear discriminant analysis was the first statistical method applied to systematically explain which firms entered bankruptcy vs. survived. Despite limitations including known nonconformance of accounting ratios to the normal distribution assumptions of LDA, <a href="/wiki/Edward_Altman" title="Edward Altman">Edward Altman</a>'s 1968 model is still a leading model in practical applications.</p>
<p><a name="Face_recognition" id="Face_recognition"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=8" title="Edit section: Face recognition">edit</a>]</span> <span class="mw-headline">Face recognition</span></h3>
<p>In computerised <a href="/wiki/Facial_recognition_system" title="Facial recognition system">face recognition</a>, each face is represented by a large number of pixel values. Linear discriminant analysis is primarily used here to reduce the number of features to a more manageable number before classification. Each of the new dimensions is a linear combination of pixel values, which form a template. The linear combinations obtained using Fisher's linear discriminant are called <i>Fisher faces</i>, while those obtained using the related <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> are called <i><a href="/wiki/Eigenfaces" title="Eigenfaces" class="mw-redirect">eigenfaces</a></i>.</p>
<p><a name="Marketing" id="Marketing"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=9" title="Edit section: Marketing">edit</a>]</span> <span class="mw-headline">Marketing</span></h3>
<p>In <a href="/wiki/Marketing" title="Marketing">marketing</a>, discriminant analysis is often used to determine the factors which distinguish different types of customers and/or products on the basis of surveys or other forms of collected data. The use of discriminant analysis in marketing is usually described by the following steps:</p>
<ol>
<li>Formulate the problem and gather data - Identify the <a href="/wiki/Salience" title="Salience">salient</a> attributes consumers use to evaluate products in this category - Use <a href="/wiki/Quantitative_marketing_research" title="Quantitative marketing research">quantitative marketing research</a> techniques (such as <a href="/wiki/Statistical_survey" title="Statistical survey">surveys</a>) to collect data from a sample of potential customers concerning their ratings of all the product attributes. The data collection stage is usually done by marketing research professionals. Survey questions ask the respondent to rate a product from one to five (or 1 to 7, or 1 to 10) on a range of attributes chosen by the researcher. Anywhere from five to twenty attributes are chosen. They could include things like: ease of use, weight, accuracy, durability, colourfulness, price, or size. The attributes chosen will vary depending on the product being studied. The same question is asked about all the products in the study. The data for multiple products is codified and input into a statistical program such as <a href="/wiki/R_language" title="R language" class="mw-redirect">R</a>, <a href="/wiki/SPSS" title="SPSS">SPSS</a> or <a href="/wiki/SAS_programming_language" title="SAS programming language" class="mw-redirect">SAS</a>. (This step is the same as in Factor analysis).</li>
<li>Estimate the Discriminant Function Coefficients and determine the statistical significance and validity - Choose the appropriate discriminant analysis method. The direct method involves estimating the discriminant function so that all the predictors are assessed simultaneously. The stepwise method enters the predictors sequentially. The two-group method should be used when the dependent variable has two categories or states. The multiple discriminant method is used when the dependent variable has three or more categorical states. Use <a href="/wiki/Wilks%27_lambda_distribution" title="Wilks' lambda distribution">Wilks’s Lambda</a> to test for significance in SPSS or F stat in SAS. The most common method used to test validity is to split the sample into an estimation or analysis sample, and a validation or holdout sample. The estimation sample is used in constructing the discriminant function. The validation sample is used to construct a classification matrix which contains the number of correctly classified and incorrectly classified cases. The percentage of correctly classified cases is called the hit ratio.</li>
<li>Plot the results on a two dimensional map, define the dimensions, and interpret the results. The statistical program (or a related module) will map the results. The map will plot each product (usually in two dimensional space). The distance of products to each other indicate either how different they are. The dimensions must be labelled by the researcher. This requires subjective judgement and is often very challenging. See <a href="/wiki/Perceptual_mapping" title="Perceptual mapping">perceptual mapping</a>.</li>
</ol>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=10" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Data_mining" title="Data mining">Data mining</a></li>
<li><a href="/wiki/Decision_tree" title="Decision tree">Decision tree</a></li>
<li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Linear_classifier" title="Linear classifier">Linear classifier</a></li>
<li><a href="/wiki/Logit" title="Logit">Logit</a> (for <a href="/wiki/Logistic_regression" title="Logistic regression">logistic regression</a>)</li>
<li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
<li><a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">Multidimensional scaling</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Preference_regression" title="Preference regression" class="mw-redirect">Preference regression</a></li>
<li><a href="/wiki/Quadratic_classifier" title="Quadratic classifier">Quadratic classifier</a></li>
<li><a href="/wiki/Statistics" title="Statistics">Statistics</a></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=11" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li>Fisher, R.A. The Use of Multiple Measurements in Taxonomic Problems. <i><a href="/wiki/Annals_of_Eugenics" title="Annals of Eugenics" class="mw-redirect">Annals of Eugenics</a></i>, 7: 179-188 (1936) <a href="http://www.library.adelaide.edu.au/digitised/fisher/138.pdf" class="external text" title="http://www.library.adelaide.edu.au/digitised/fisher/138.pdf" rel="nofollow">pdf file</a></li>
<li><i>Discriminant Analysis and Statistical Pattern Recognition.</i> G.J. McLachlan. Wiley-Interscience; New Ed edition (August 4, 2004).</li>
<li><i>Pattern Classification</i> (2nd ed.), R.O. Duda, P.E. Hart, D.H. Stork, Wiley Interscience, (2000). <a href="/wiki/Special:BookSources/0471056693" class="internal">ISBN 0-471-05669-3</a></li>
<li>Friedman, J.H. Regularized Discriminant Analysis. <i>Journal of the American Statistical Association</i>, (1989)<a href="http://www.slac.stanford.edu/cgi-wrap/getdoc/slac-pub-4389.pdf" class="external text" title="http://www.slac.stanford.edu/cgi-wrap/getdoc/slac-pub-4389.pdf" rel="nofollow">pdf file</a></li>
<li>Martinez, A.M., Kak, A.C. PCA versus LDA. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 23, No. 2, pp. 228-233, 2001. <a href="http://www.ece.osu.edu/~aleix/pami01f.pdf" class="external text" title="http://www.ece.osu.edu/~aleix/pami01f.pdf" rel="nofollow">pdf file</a></i></li>
<li>Mika, S. et al. Fisher Discriminant Analysis with Kernels. <i>IEEE Conference on Neural Networks for Signal Processing IX</i>, (1999) <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.9904" class="external text" title="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.9904" rel="nofollow">On Citeseer archive</a></li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit&amp;section=12" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://people.revoledu.com/kardi/tutorial/LDA/index.html" class="external text" title="http://people.revoledu.com/kardi/tutorial/LDA/index.html" rel="nofollow">LDA tutorial using MS Excel</a></li>
<li><a href="http://www.vni.com/products/imsl/documentation/fort06/stat/NetHelp/default.htm?turl=dscrm.htm" class="external text" title="http://www.vni.com/products/imsl/documentation/fort06/stat/NetHelp/default.htm?turl=dscrm.htm" rel="nofollow">IMSL discriminant analysis function</a>, which has many useful mathematical definitions.</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 298/1000000
Post-expand include size: 4260/2048000 bytes
Template argument size: 1595/2048000 bytes
Expensive parser function count: 1/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1470657-0!1!0!default!!en!2 and timestamp 20090328020230 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">http://en.wikipedia.org/wiki/Linear_discriminant_analysis</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Multivariate_statistics" title="Category:Multivariate statistics">Multivariate statistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_classification" title="Category:Statistical classification">Statistical classification</a></span> | <span dir='ltr'><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></span> | <span dir='ltr'><a href="/wiki/Category:Psychometrics" title="Category:Psychometrics">Psychometrics</a></span> | <span dir='ltr'><a href="/wiki/Category:Market_research" title="Category:Market research">Market research</a></span> | <span dir='ltr'><a href="/wiki/Category:Marketing" title="Category:Marketing">Marketing</a></span> | <span dir='ltr'><a href="/wiki/Category:Consumer_behaviour" title="Category:Consumer behaviour">Consumer behaviour</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Articles_lacking_in-text_citations" title="Category:Articles lacking in-text citations">Articles lacking in-text citations</a></span> | <span dir='ltr'><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_December_2008" title="Category:Articles with unsourced statements since December 2008">Articles with unsourced statements since December 2008</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Linear_discriminant_analysis" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Linear_discriminant_analysis" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Linear_discriminant_analysis&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Linear_discriminant_analysis" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Linear_discriminant_analysis" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Linear_discriminant_analysis" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Linear_discriminant_analysis&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Linear_discriminant_analysis&amp;oldid=278488067" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Linear_discriminant_analysis&amp;id=278488067">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Diskriminanzanalyse">Deutsch</a></li>
				<li class="interwiki-eo"><a href="http://eo.wikipedia.org/wiki/Vikipedio:Projekto_matematiko/Lineara_diskriminanta_analitiko">Esperanto</a></li>
				<li class="interwiki-fa"><a href="http://fa.wikipedia.org/wiki/%D8%AA%D8%AD%D9%84%DB%8C%D9%84_%D8%AA%D9%81%DA%A9%DB%8C%DA%A9_%D8%AE%D8%B7%DB%8C">فارسی</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Analyse_discriminante_lin%C3%A9aire">Français</a></li>
				<li class="interwiki-hr"><a href="http://hr.wikipedia.org/wiki/Linearna_analiza_razli%C4%8Ditih">Hrvatski</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Analisi_discriminante">Italiano</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Discriminantanalyse">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Liniowa_analiza_dyskryminacyjna">Polski</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%81%D0%BA%D1%80%D0%B8%D0%BC%D0%B8%D0%BD%D0%B0%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7">Русский</a></li>
				<li class="interwiki-sl"><a href="http://sl.wikipedia.org/wiki/Diskriminantna_analiza">Slovenščina</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 20 March 2009, at 06:28.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv207 in 0.051 secs. --></body></html>
