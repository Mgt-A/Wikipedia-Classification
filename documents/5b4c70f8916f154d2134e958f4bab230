<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="K-means clustering,2009,April 7,Border detection,Cambridge University Press,Centroid,Centroids,Cluster analysis,Computer vision,Coreset,Data clustering" />
		<link rel="canonical" href="/wiki/K-means_clustering" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=K-means_clustering&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=K-means_clustering&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>k-means clustering - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "K-means_clustering";
		var wgTitle = "K-means clustering";
		var wgAction = "view";
		var wgArticleId = "1860407";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282733980;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-K-means_clustering skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">k-means clustering</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub">&nbsp;&nbsp;(Redirected from <a href="/w/index.php?title=K-means_algorithm&amp;redirect=no" title="K-means algorithm">K-means algorithm</a>)</div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><i>k</i>-means clustering</b> is a method of <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> which aims to <a href="/wiki/Partition_of_a_set" title="Partition of a set">partition</a> <i>n</i> objects into <i>k</i> clusters in which each object belongs to the cluster with the nearest <a href="/wiki/Mean" title="Mean">mean</a>. It is similar to the <a href="/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> for mixtures of <a href="/wiki/Gaussian_distribution" title="Gaussian distribution" class="mw-redirect">Gaussians</a> in that they both attempt to find the centers of natural clusters in the data.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Description"><span class="tocnumber">1</span> <span class="toctext">Description</span></a></li>
<li class="toclevel-1"><a href="#History"><span class="tocnumber">2</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1"><a href="#Demonstration_of_the_algorithm"><span class="tocnumber">3</span> <span class="toctext">Demonstration of the algorithm</span></a></li>
<li class="toclevel-1"><a href="#Pseudocode"><span class="tocnumber">4</span> <span class="toctext">Pseudocode</span></a></li>
<li class="toclevel-1"><a href="#Applications_of_the_algorithm"><span class="tocnumber">5</span> <span class="toctext">Applications of the algorithm</span></a>
<ul>
<li class="toclevel-2"><a href="#Image_segmentation"><span class="tocnumber">5.1</span> <span class="toctext">Image segmentation</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Relation_to_PCA"><span class="tocnumber">6</span> <span class="toctext">Relation to PCA</span></a></li>
<li class="toclevel-1"><a href="#Variations"><span class="tocnumber">7</span> <span class="toctext">Variations</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">9</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">11</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Description" id="Description"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=1" title="Edit section: Description">edit</a>]</span> <span class="mw-headline">Description</span></h2>
<p>Given a set of observations (<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>), where each observation is a <i>d</i>-dimensional real vector, then <i>k</i>-means clustering aims to partition this set into <i>k</i> partitions (<i>k</i>&lt;<i>n</i>) <b>S</b>={<i>S</i><sub>1</sub>,<i>S</i><sub>2</sub>,…,<i>S</i><sub><i>k</i></sub>} so as to minimize total intra-cluster variance, or, the squared error function</p>
<dl>
<dd><img class="tex" alt="\underset{\mathbf{S}} \operatorname{arg\,min} \sum_{i=1}^{k} \sum_{x_j \in S_i} (x_j - \mu_i)^2 " src="http://upload.wikimedia.org/math/2/2/b/22b2b8c1ead9a4cc92a34a8c87ba8f68.png" /></dd>
</dl>
<p>where <i>μ</i><sub><i>i</i></sub> is the mean of <i>S</i><sub><i>i</i></sub>.</p>
<p><a name="History" id="History"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=2" title="Edit section: History">edit</a>]</span> <span class="mw-headline">History</span></h2>
<p>The <span class="texhtml"><var>k</var></span>-means clustering was invented in 1956.<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup> The most common form of the algorithm uses an iterative refinement heuristic known as <a href="/wiki/Lloyd%27s_algorithm" title="Lloyd's algorithm">Lloyd's algorithm</a>.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup> Lloyd's algorithm starts by partitioning the input points into <span class="texhtml"><var>k</var></span> initial sets, either at random or using some heuristic data. It then calculates the mean point, or centroid, of each set. It constructs a new partition by associating each point with the closest centroid. Then the centroids are recalculated for the new clusters, and algorithm repeated by alternate application of these two steps until convergence, which is obtained when the points no longer switch clusters (or alternatively centroids are no longer changed).</p>
<p>Lloyd's algorithm and <span class="texhtml"><var>k</var></span>-means are often used synonymously, but in reality Lloyd's algorithm is a heuristic for solving the <span class="texhtml"><var>k</var></span>-means problem,<sup id="cite_ref-arthir_vassilvitskii_2-0" class="reference"><a href="#cite_note-arthir_vassilvitskii-2" title=""><span>[</span>3<span>]</span></a></sup> as with certain combinations of starting points and centroids, Lloyd's algorithm can in fact converge to the wrong answer (i.e. a different and locally optimal answer to the minimization function above exists.)</p>
<p>Other variations exist,<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup> but Lloyd's algorithm has remained popular because it converges extremely quickly in practice. In fact, many have observed that the number of iterations is typically much less than the number of points. Recently, however, David Arthur and Sergei Vassilvitskii showed that there exist certain point sets on which <span class="texhtml"><var>k</var></span>-means takes <a href="/wiki/Polynomial_time" title="Polynomial time">superpolynomial time</a>: <span class="texhtml">2<sup>Ω(√<var>n</var>)</sup></span> to converge.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup></p>
<p>Approximate <span class="texhtml"><var>k</var></span>-means algorithms have been designed that make use of <a href="/wiki/Coreset" title="Coreset">coresets</a>: small subsets of the original data.</p>
<p>In terms of performance the algorithm is not guaranteed to return a global optimum. The quality of the final solution depends largely on the initial set of clusters, and may, in practice, be much poorer than the global optimum<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup>. Since the algorithm is extremely fast, a common method is to run the algorithm several times and return the best clustering found.</p>
<p>A drawback of the <span class="texhtml"><var>k</var></span>-means algorithm is that the number of clusters <span class="texhtml"><var>k</var></span> is an input parameter. An inappropriate choice of <span class="texhtml"><var>k</var></span> may yield poor results. The algorithm also assumes that the <a href="/wiki/Variance" title="Variance">variance</a> is an appropriate measure of cluster scatter.</p>
<p><a name="Demonstration_of_the_algorithm" id="Demonstration_of_the_algorithm"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=3" title="Edit section: Demonstration of the algorithm">edit</a>]</span> <span class="mw-headline">Demonstration of the algorithm</span></h2>
<p>The following images demonstrate the <span class="texhtml"><var>k</var></span>-means clustering algorithm in action, for the two-dimensional case. The initial centres are generated randomly to demonstrate the stages in more detail. The background space partitions are only for illustration and are not generated by the <span class="texhtml"><var>k</var></span>-means algorithm.</p>
<table class="gallery" cellspacing="0" cellpadding="0">
<tr>
<td>
<div class="gallerybox" style="width: 155px;">
<div class="thumb" style="padding: 15px 0; width: 150px;">
<div style="margin-left: auto; margin-right: auto; width: 120px;"><a href="/wiki/File:K_Means_Example_Step_1.svg" class="image" title="K Means Example Step 1.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/120px-K_Means_Example_Step_1.svg.png" width="120" height="116" border="0" /></a></div>
</div>
<div class="gallerytext">
<p>First, <span class="texhtml"><var>k</var></span> initial points (in this case <span class="texhtml"><var>k</var></span>=3) are randomly selected from the data set (shown in color).</p>
</div>
</div>
</td>
<td>
<div class="gallerybox" style="width: 155px;">
<div class="thumb" style="padding: 21px 0; width: 150px;">
<div style="margin-left: auto; margin-right: auto; width: 120px;"><a href="/wiki/File:K_Means_Example_Step_2.svg" class="image" title="K Means Example Step 2.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/120px-K_Means_Example_Step_2.svg.png" width="120" height="104" border="0" /></a></div>
</div>
<div class="gallerytext">
<p><span class="texhtml"><var>k</var></span> groupings are created by associating every individual point with the nearest initial randomized point.</p>
</div>
</div>
</td>
<td>
<div class="gallerybox" style="width: 155px;">
<div class="thumb" style="padding: 21px 0; width: 150px;">
<div style="margin-left: auto; margin-right: auto; width: 120px;"><a href="/wiki/File:K_Means_Example_Step_3.svg" class="image" title="K Means Example Step 3.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/120px-K_Means_Example_Step_3.svg.png" width="120" height="104" border="0" /></a></div>
</div>
<div class="gallerytext">
<p>The <a href="/wiki/Centroid" title="Centroid">centroid</a> points of each of the <span class="texhtml"><var>k</var></span> clusters becomes the new initial randomized points.</p>
</div>
</div>
</td>
<td>
<div class="gallerybox" style="width: 155px;">
<div class="thumb" style="padding: 21px 0; width: 150px;">
<div style="margin-left: auto; margin-right: auto; width: 120px;"><a href="/wiki/File:K_Means_Example_Step_4.svg" class="image" title="K Means Example Step 4.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/120px-K_Means_Example_Step_4.svg.png" width="120" height="104" border="0" /></a></div>
</div>
<div class="gallerytext">
<p>Steps 2 &amp; 3 are repeated until a suitable level of convergence has been reached.</p>
</div>
</div>
</td>
</tr>
</table>
<p><a name="Pseudocode" id="Pseudocode"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=4" title="Edit section: Pseudocode">edit</a>]</span> <span class="mw-headline">Pseudocode</span></h2>
<p>Initialize the algorithm either heuristically or randomly. Iterate the following steps until convergence (stopping criteria met):</p>
<ol>
<li>For each data point <span class="texhtml"><var>x</var></span>, compute its membership in clusters by choosing the nearest centroid</li>
<li>For each centroid, recompute its location according to members</li>
</ol>
<pre>
<code> var m = initialCentroids(x, K);
 var N = x.length;
 while (!stoppingCriteria) {
   var w = [][];
   // calculate membership in clusters
   for (var n = 1; n &lt;= N; n++) {
     v = arg min (v0) dist(m[v0], x[n]);
     w[v].push(n);
   }
   // recompute the centroids
   for (var k = 1; k &lt;= K; k++) {
     m[k] = avg(x in w[k]);
   }
 }
 return m;
</code>
</pre>
<p><a name="Applications_of_the_algorithm" id="Applications_of_the_algorithm"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=5" title="Edit section: Applications of the algorithm">edit</a>]</span> <span class="mw-headline">Applications of the algorithm</span></h2>
<p><a name="Image_segmentation" id="Image_segmentation"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=6" title="Edit section: Image segmentation">edit</a>]</span> <span class="mw-headline">Image segmentation</span></h3>
<p>The <span class="texhtml"><var>k</var></span>-means clustering algorithm is commonly used in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> as a form of <a href="/wiki/Image_segmentation" title="Image segmentation" class="mw-redirect">image segmentation</a>. The results of the segmentation are used to aid <a href="/wiki/Border_detection" title="Border detection" class="mw-redirect">border detection</a> and <a href="/wiki/Object_recognition" title="Object recognition">object recognition</a>. In this context, the standard <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is usually insufficient in forming the clusters. Instead, a weighted distance measure utilizing <a href="/wiki/Pixel" title="Pixel">pixel</a> coordinates, <a href="/wiki/RGB" title="RGB" class="mw-redirect">RGB</a> pixel color and/or intensity, and image texture is commonly used.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup></p>
<p><a name="Relation_to_PCA" id="Relation_to_PCA"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=7" title="Edit section: Relation to PCA">edit</a>]</span> <span class="mw-headline">Relation to PCA</span></h2>
<p>It has been shown<sup id="cite_ref-7" class="reference"><a href="#cite_note-7" title=""><span>[</span>8<span>]</span></a></sup><sup id="cite_ref-8" class="reference"><a href="#cite_note-8" title=""><span>[</span>9<span>]</span></a></sup> that the relaxed solution of <span class="texhtml"><var>k</var></span>-means clustering, specified by the cluster indicators, is given by the PCA (<a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a>) principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace specified by the between-class scatter matrix.</p>
<p><a name="Variations" id="Variations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=8" title="Edit section: Variations">edit</a>]</span> <span class="mw-headline">Variations</span></h2>
<p>The set of squared error minimizing cluster functions also includes the <a href="/wiki/K-medoids" title="K-medoids"><span class="texhtml"><var>k</var></span>-medoids</a> algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e. it uses <a href="/wiki/Medoids" title="Medoids" class="mw-redirect">medoids</a> in place of <a href="/wiki/Centroids" title="Centroids" class="mw-redirect">centroids</a>.</p>
<p>An alternative method for choosing the initial centers was proposed in 2006 <sup id="cite_ref-arthir_vassilvitskii_2-1" class="reference"><a href="#cite_note-arthir_vassilvitskii-2" title=""><span>[</span>3<span>]</span></a></sup>, dubbed "<span class="texhtml"><var>k</var></span>-means++". The idea is to select centers in a way that they are already initially close to large quantities of points. The authors use <span class="texhtml"><i>L</i><sup>2</sup></span> norm in selecting the centers, but general <span class="texhtml"><i>L</i><sup><i>n</i></sup></span> may be used to tune the aggressiveness of the seeding.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=9" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Data_clustering" title="Data clustering" class="mw-redirect">Data clustering</a></li>
<li><a href="/wiki/Linde-Buzo-Gray_algorithm" title="Linde-Buzo-Gray algorithm">Linde-Buzo-Gray algorithm</a></li>
<li><a href="/wiki/K-medoid" title="K-medoid" class="mw-redirect">k-medoid</a></li>
<li><a href="/wiki/Silhouette_(clustering)" title="Silhouette (clustering)">Silhouette</a></li>
<li><a href="/wiki/K-means%2B%2B" title="K-means++">k-means++</a></li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=10" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> H. Steinhaus. Sur la division des corp materiels en parties. Bull. Acad. Polon. Sci., C1. III vol IV:801– 804, 1956.</li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> S. Lloyd, Last square quantization in PCM’s. Bell Telephone Laboratories Paper (1957). Published in journal much later: S. P. Lloyd. <a href="http://www.cs.toronto.edu/~roweis/csc2515-2006/readings/lloyd57.pdf" class="external text" title="http://www.cs.toronto.edu/~roweis/csc2515-2006/readings/lloyd57.pdf" rel="nofollow">Least squares quantization in PCM</a>. Special issue on quantization, IEEE Trans. Inform. Theory, 28:129–137, 1982.</li>
<li id="cite_note-arthir_vassilvitskii-2">^ <a href="#cite_ref-arthir_vassilvitskii_2-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-arthir_vassilvitskii_2-1" title=""><sup><i><b>b</b></i></sup></a> D. Arthur, S. Vassilvitskii: <a href="http://www.stanford.edu/~darthur/kMeansPlusPlus.pdf" class="external text" title="http://www.stanford.edu/~darthur/kMeansPlusPlus.pdf" rel="nofollow">"k-means++ The Advantages of Careful Seeding"</a> 2007 Symposium on Discrete Algorithms (SODA).</li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> T. Kanungo, D. M. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A. Y. Wu: <a href="http://dx.doi.org/10.1109/TPAMI.2002.1017616" class="external text" title="http://dx.doi.org/10.1109/TPAMI.2002.1017616" rel="nofollow">"An efficient k-means clustering algorithm: Analysis and implementation"</a> IEEE Trans. Pattern Analysis and Machine Intelligence, 24 (2002), 881-892.</li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> <cite style="font-style:normal"><a href="/w/index.php?title=David_Arthur&amp;action=edit&amp;redlink=1" class="new" title="David Arthur (page does not exist)">David Arthur</a> &amp; <a href="/w/index.php?title=Sergei_Vassilvitskii&amp;action=edit&amp;redlink=1" class="new" title="Sergei Vassilvitskii (page does not exist)">Sergei Vassilvitskii</a> (2006). "<a href="http://www.cs.duke.edu/courses/spring07/cps296.2/papers/kMeans-socg.pdf" class="external text" title="http://www.cs.duke.edu/courses/spring07/cps296.2/papers/kMeans-socg.pdf" rel="nofollow">How Slow is the k-means Method?</a>". <i>Proceedings of the 2006 Symposium on Computational Geometry (SoCG)</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=Proceedings+of+the+2006+Symposium+on+Computational+Geometry+%28SoCG%29&amp;rft.atitle=How+Slow+is+the+k-means+Method%3F&amp;rft.au=%5B%5BDavid+Arthur%5D%5D+%26+%5B%5BSergei+Vassilvitskii%5D%5D&amp;rft.date=2006&amp;rft_id=http%3A%2F%2Fwww.cs.duke.edu%2Fcourses%2Fspring07%2Fcps296.2%2Fpapers%2FkMeans-socg.pdf"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> Kryzysztof Cios, Witold Pedrycz, Roman Swiniarski - "Data Mining Methods for Knowledge Discover", 1998, Kluwer Acadmenic Publishers, page 383 to 384</li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> Shapiro, Linda G. &amp; Stockman, George C. (2001). <i>Computer Vision.</i> Upper Saddle River, NJ: Prentice Hall.</li>
<li id="cite_note-7"><b><a href="#cite_ref-7" title="">^</a></b> H. Zha, C. Ding, M. Gu, X. He and H.D. Simon. "Spectral Relaxation for K-means Clustering", Neural Information Processing Systems vol.14 (NIPS 2001). pp. 1057-1064, Vancouver, Canada. Dec. 2001.</li>
<li id="cite_note-8"><b><a href="#cite_ref-8" title="">^</a></b> Chris Ding and Xiaofeng He. "K-means Clustering via Principal Component Analysis". Proc. of Int'l Conf. Machine Learning (ICML 2004), pp 225-232. July 2004.</li>
</ol>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=11" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><cite style="font-style:normal">MacQueen, J. B. (1967). "<a href="http://projecteuclid.org/euclid.bsmsp/1200512992" class="external text" title="http://projecteuclid.org/euclid.bsmsp/1200512992" rel="nofollow">Some Methods for classification and Analysis of Multivariate Observations</a>" in <i>Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability</i>. <b>1</b>: 281–297, University of California Press. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a><a href="http://www.ams.org/mathscinet-getitem?mr=0214227" class="external text" title="http://www.ams.org/mathscinet-getitem?mr=0214227" rel="nofollow">0214227</a>. <span class="reference-accessdate">Retrieved on <span class="mw-formatted-date" title="2009-04-07"><a href="/wiki/2009" title="2009">2009</a>-<a href="/wiki/April_7" title="April 7">04-07</a></span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=&amp;rft.atitle=Some+Methods+for+classification+and+Analysis+of+Multivariate+Observations&amp;rft.aulast=MacQueen&amp;rft.aufirst=J.+B.&amp;rft.date=1967&amp;rft.pub=University+of+California+Press&amp;rft.pages=281%26ndash%3B297&amp;rft.series=1&amp;rft_id=http%3A%2F%2Fprojecteuclid.org%2Feuclid.bsmsp%2F1200512992"><span style="display: none;">&#160;</span></span></li>
<li>J. A. Hartigan (1975) "Clustering Algorithms". Wiley.</li>
</ul>
<ul>
<li><cite style="font-style:normal" class="" id="CITEREFHartiganWong1979">Hartigan, J. A.; Wong, M. A. (1979). "Algorithm AS 136: A K-Means Clustering Algorithm". <i>Applied Statistics</i> <b>28</b> (1): 100–108. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a>: <a href="http://www.jstor.org/stable/2346830" class="external text" title="http://www.jstor.org/stable/2346830" rel="nofollow">2346830</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Algorithm+AS+136%3A+A+K-Means+Clustering+Algorithm&amp;rft.jtitle=Applied+Statistics&amp;rft.aulast=Hartigan&amp;rft.aufirst=J.+A.&amp;rft.au=Hartigan%2C+J.+A.&amp;rft.au=Wong%2C+M.+A.&amp;rft.date=1979&amp;rft.volume=28&amp;rft.issue=1&amp;rft.pages=100%26ndash%3B108&amp;rfr_id=info:sid/en.wikipedia.org:K-means_clustering"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="book" id="CITEREFMacKay2003"><a href="/wiki/David_MacKay_(scientist)" title="David MacKay (scientist)" class="mw-redirect">MacKay, David</a> (2003). <a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/284.292.pdf" class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/284.292.pdf" rel="nofollow">"Chapter 20. An Example Inference Task: Clustering"</a>. <i><a href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html" class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html" rel="nofollow">Information Theory, Inference and Learning Algorithms</a></i>. <a href="/wiki/Cambridge_University_Press" title="Cambridge University Press">Cambridge University Press</a>. <a href="/wiki/Special:BookSources/0521642981" class="internal">ISBN 0-521-64298-1</a><span class="printonly">. <a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/284.292.pdf" class="external free" title="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/284.292.pdf" rel="nofollow">http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/284.292.pdf</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=Chapter+20.+An+Example+Inference+Task%3A+Clustering&amp;rft.atitle=Information+Theory%2C+Inference+and+Learning+Algorithms&amp;rft.aulast=MacKay&amp;rft.aufirst=David&amp;rft.au=MacKay%2C+David&amp;rft.date=2003&amp;rft.pub=%5B%5BCambridge+University+Press%5D%5D&amp;rft.isbn=0-521-64298-1&amp;rft_id=http%3A%2F%2Fwww.inference.phy.cam.ac.uk%2Fmackay%2Fitila%2Fbook.html&amp;rfr_id=info:sid/en.wikipedia.org:K-means_clustering"><span style="display: none;">&#160;</span></span></li>
<li>Data Mining (1998): "Methods for Knowledge Discovery" Kluwer Academic Publishers</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=12" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://www.phpandme.net/2009/03/clustering-web-pages-into-similar-topics/" class="external text" title="http://www.phpandme.net/2009/03/clustering-web-pages-into-similar-topics/" rel="nofollow">Application to Web Site clustering in PHP</a></li>
<li><a href="http://people.revoledu.com/kardi/tutorial/kMean/NumericalExample.htm" class="external text" title="http://people.revoledu.com/kardi/tutorial/kMean/NumericalExample.htm" rel="nofollow">Numerical Example of K-means clustering</a></li>
<li><a href="http://www.leet.it/home/lale/clustering/" class="external text" title="http://www.leet.it/home/lale/clustering/" rel="nofollow">Application example which uses K-means clustering to reduce the number of colors in images</a></li>
<li><a href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/AppletKM.html" class="external text" title="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/AppletKM.html" rel="nofollow">Interactive demo of the K-means-algorithm(Applet)</a></li>
<li><a href="http://www.javaworld.com/javaworld/jw-11-2006/jw-1121-thread.html" class="external text" title="http://www.javaworld.com/javaworld/jw-11-2006/jw-1121-thread.html" rel="nofollow">An example of multithreaded application which uses K-means in Java</a></li>
<li><a href="http://www25.brinkster.com/denshade/kmeans.php.htm" class="external text" title="http://www25.brinkster.com/denshade/kmeans.php.htm" rel="nofollow">K-means application in php</a></li>
<li><a href="http://informationandvisualization.de/blog/kmeans-and-voronoi-tesselation-built-processing" class="external text" title="http://informationandvisualization.de/blog/kmeans-and-voronoi-tesselation-built-processing" rel="nofollow">Another animation of the K-means-algorithm</a></li>
<li><a href="http://mloss.org/software/view/48/" class="external text" title="http://mloss.org/software/view/48/" rel="nofollow">A fast implementation of the K-means algorithm which uses the triangle inequality to speed up computation</a></li>
<li><a href="http://www.user.cifnet.com/~lwebzem/k_means/test3.cgi" class="external text" title="http://www.user.cifnet.com/~lwebzem/k_means/test3.cgi" rel="nofollow">K-means clustering using Perl. Online clustering.</a></li>
<li><a href="http://codingplayground.blogspot.com/2009/03/k-means-in-c.html" class="external text" title="http://codingplayground.blogspot.com/2009/03/k-means-in-c.html" rel="nofollow">K-means clustering using C++</a></li>
<li><a href="http://ai4r.rubyforge.org/index.html" class="external text" title="http://ai4r.rubyforge.org/index.html" rel="nofollow">K-means clustering implementation in Ruby (AI4R)</a></li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 1619/1000000
Post-expand include size: 12591/2048000 bytes
Template argument size: 5131/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1860407-0!1!0!default!!en!2 and timestamp 20090409091907 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/K-means_clustering">http://en.wikipedia.org/wiki/K-means_clustering</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Data_clustering_algorithms" title="Category:Data clustering algorithms">Data clustering algorithms</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_algorithms" title="Category:Statistical algorithms">Statistical algorithms</a></span> | <span dir='ltr'><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></span> | <span dir='ltr'><a href="/wiki/Category:Neural_networks" title="Category:Neural networks">Neural networks</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/K-means_clustering" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:K-means_clustering" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=K-means_clustering&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=K-means_clustering&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=K-means_clustering" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/K-means_clustering" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/K-means_clustering" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=K-means_clustering&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=K-means_clustering&amp;oldid=282733980" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=K-means_clustering&amp;id=282733980">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/K-Means-Algorithmus">Deutsch</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98">한국어</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/K-means">Italiano</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/K%E5%B9%B3%E5%9D%87%E6%B3%95">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Algorytm_centroid%C3%B3w">Polski</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/K-means">Русский</a></li>
				<li class="interwiki-uk"><a href="http://uk.wikipedia.org/wiki/%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D1%96%D1%8F_%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%BD%D1%8F_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%BC_%D0%BA%E2%80%93%D1%81%D0%B5%D1%80%D0%B5%D0%B4%D0%BD%D1%96%D1%85">Українська</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/K%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 9 April 2009, at 09:19 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv180 in 0.071 secs. --></body></html>
