<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Message Passing Interface,Special:Search/Message Passing Interface,.NET Framework,ANSI C,API,Actor model,Allinea Distributed Debugging Tool,Amdahl&#039;s law,Application checkpointing,Application programming interface,Asymmetric multiprocessing" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Message_Passing_Interface&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Message_Passing_Interface&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Message Passing Interface - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Message_Passing_Interface";
		var wgTitle = "Message Passing Interface";
		var wgAction = "view";
		var wgArticleId = "221466";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 283039158;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
<style type="text/css">/*<![CDATA[*/
.source-c {line-height: normal;}
.source-c li, .source-c pre {
	line-height: normal; border: 0px none white;
}
/**
 * GeSHi Dynamically Generated Stylesheet
 * --------------------------------------
 * Dynamically generated stylesheet for c
 * CSS class: source-c, CSS id: 
 * GeSHi (C) 2004 - 2007 Nigel McNie (http://qbnz.com/highlighter)
 */
.source-c .de1, .source-c .de2 {font-family: 'Courier New', Courier, monospace; font-weight: normal;}
.source-c  {}
.source-c .head {}
.source-c .foot {}
.source-c .imp {font-weight: bold; color: red;}
.source-c .ln-xtra {color: #cc0; background-color: #ffc;}
.source-c li {font-family: 'Courier New', Courier, monospace; color: black; font-weight: normal; font-style: normal;}
.source-c li.li2 {font-weight: bold;}
.source-c .kw1 {color: #b1b100;}
.source-c .kw2 {color: #000000; font-weight: bold;}
.source-c .kw3 {color: #000066;}
.source-c .kw4 {color: #993333;}
.source-c .co1 {color: #808080; font-style: italic;}
.source-c .co2 {color: #339933;}
.source-c .coMULTI {color: #808080; font-style: italic;}
.source-c .es0 {color: #000099; font-weight: bold;}
.source-c .br0 {color: #66cc66;}
.source-c .st0 {color: #ff0000;}
.source-c .nu0 {color: #cc66cc;}
.source-c .me1 {color: #202020;}
.source-c .me2 {color: #202020;}

/*]]>*/
</style>
<style type="text/css">/*<![CDATA[*/
@import "/w/index.php?title=MediaWiki:Geshi.css&usemsgcache=yes&action=raw&ctype=text/css&smaxage=2678400";
/*]]>*/
</style>		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Message_Passing_Interface skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Message Passing Interface</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_style.png" class="image" title="Ambox style.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/d/d6/Ambox_style.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article's <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">external links</a> <b>may not follow Wikipedia's <a href="/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_mirror_or_a_repository_of_links.2C_images.2C_or_media_files" title="Wikipedia:What Wikipedia is not">content policies</a> or <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">guidelines</a></b>. Please <a href="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" rel="nofollow">improve this article</a> by removing excessive or inappropriate external links.</td>
</tr>
</table>
<p><b>Message Passing Interface</b> (<b>MPI</b>) is a specification for an <a href="/wiki/API" title="API" class="mw-redirect">API</a> that allows many computers to communicate with one another. It is used in <a href="/wiki/Computer_clusters" title="Computer clusters" class="mw-redirect">computer clusters</a> and <a href="/wiki/Supercomputer" title="Supercomputer">supercomputers</a>. MPI was created by <a href="/wiki/William_Gropp" title="William Gropp" class="mw-redirect">William Gropp</a> and <a href="/w/index.php?title=Ewing_Lusk&amp;action=edit&amp;redlink=1" class="new" title="Ewing Lusk (page does not exist)">Ewing Lusk</a> and others.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Overview"><span class="tocnumber">1</span> <span class="toctext">Overview</span></a></li>
<li class="toclevel-1"><a href="#Functionality"><span class="tocnumber">2</span> <span class="toctext">Functionality</span></a></li>
<li class="toclevel-1"><a href="#Concepts"><span class="tocnumber">3</span> <span class="toctext">Concepts</span></a>
<ul>
<li class="toclevel-2"><a href="#Communicator"><span class="tocnumber">3.1</span> <span class="toctext">Communicator</span></a></li>
<li class="toclevel-2"><a href="#Point-to-point_basics"><span class="tocnumber">3.2</span> <span class="toctext">Point-to-point basics</span></a></li>
<li class="toclevel-2"><a href="#Collective_basics"><span class="tocnumber">3.3</span> <span class="toctext">Collective basics</span></a></li>
<li class="toclevel-2"><a href="#Derived_Datatypes"><span class="tocnumber">3.4</span> <span class="toctext">Derived Datatypes</span></a></li>
<li class="toclevel-2"><a href="#One-sided_communication_.28MPI-2.29"><span class="tocnumber">3.5</span> <span class="toctext">One-sided communication (MPI-2)</span></a></li>
<li class="toclevel-2"><a href="#Collective_extensions_.28MPI-2.29"><span class="tocnumber">3.6</span> <span class="toctext">Collective extensions (MPI-2)</span></a></li>
<li class="toclevel-2"><a href="#Dynamic_process_management_.28MPI-2.29"><span class="tocnumber">3.7</span> <span class="toctext">Dynamic process management (MPI-2)</span></a></li>
<li class="toclevel-2"><a href="#MPI_I.2FO_.28MPI-2.29"><span class="tocnumber">3.8</span> <span class="toctext">MPI I/O (MPI-2)</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Implementations"><span class="tocnumber">4</span> <span class="toctext">Implementations</span></a>
<ul>
<li class="toclevel-2"><a href="#.27Classical.27_cluster_and_supercomputer_implementations"><span class="tocnumber">4.1</span> <span class="toctext">'Classical' cluster and supercomputer implementations</span></a></li>
<li class="toclevel-2"><a href="#Python"><span class="tocnumber">4.2</span> <span class="toctext">Python</span></a></li>
<li class="toclevel-2"><a href="#OCaml"><span class="tocnumber">4.3</span> <span class="toctext">OCaml</span></a></li>
<li class="toclevel-2"><a href="#Java"><span class="tocnumber">4.4</span> <span class="toctext">Java</span></a></li>
<li class="toclevel-2"><a href="#Common_Language_Infrastucture"><span class="tocnumber">4.5</span> <span class="toctext">Common Language Infrastucture</span></a></li>
<li class="toclevel-2"><a href="#Hardware_implementations"><span class="tocnumber">4.6</span> <span class="toctext">Hardware implementations</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Example_program"><span class="tocnumber">5</span> <span class="toctext">Example program</span></a></li>
<li class="toclevel-1"><a href="#Adoption_of_MPI-2"><span class="tocnumber">6</span> <span class="toctext">Adoption of MPI-2</span></a></li>
<li class="toclevel-1"><a href="#The_future_of_MPI"><span class="tocnumber">7</span> <span class="toctext">The future of MPI</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">9</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">11</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Overview" id="Overview"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=1" title="Edit section: Overview">edit</a>]</span> <span class="mw-headline">Overview</span></h2>
<p>MPI is a language-independent <a href="/wiki/Communications_protocol" title="Communications protocol">communications protocol</a> used to program <a href="/wiki/Parallel_computers" title="Parallel computers" class="mw-redirect">parallel computers</a>. Both point-to-point and collective communication are supported. MPI "is a <a href="/wiki/Message-passing" title="Message-passing" class="mw-redirect">message-passing</a> application programmer interface, together with protocol and semantic specifications for how its features must behave in any implementation."<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup> MPI's goals are high performance, scalability, and portability. MPI remains the dominant model used in <a href="/wiki/High-performance_computing" title="High-performance computing">high-performance computing</a> today.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<p>MPI is not sanctioned by any major standards body; nevertheless, it has become a <i><a href="/wiki/De_facto" title="De facto">de facto</a></i> <a href="/wiki/Standardization" title="Standardization">standard</a> for <a href="/wiki/Communication" title="Communication">communication</a> among processes that model a <a href="/wiki/Parallel_programming" title="Parallel programming" class="mw-redirect">parallel program</a> running on a <a href="/wiki/Distributed_memory" title="Distributed memory">distributed memory system</a>. Actual distributed memory supercomputers such as computer clusters often run these programs. The principal MPI-1 model has no shared memory concept, and MPI-2 has only a limited <a href="/wiki/Distributed_shared_memory" title="Distributed shared memory">distributed shared memory</a> concept. Nonetheless, MPI programs are regularly run on shared memory computers. Designing programs around the MPI model (as opposed to explicit <a href="/wiki/Shared_memory" title="Shared memory">shared memory</a> models) has advantages on <a href="/wiki/Non-Uniform_Memory_Access" title="Non-Uniform Memory Access">NUMA</a> architectures since MPI encourages <a href="/wiki/Locality_of_reference#Memory_locality" title="Locality of reference">memory locality</a>.</p>
<p>Although MPI belongs in layers 5 and higher of the <a href="/wiki/OSI_Reference_Model" title="OSI Reference Model" class="mw-redirect">OSI Reference Model</a>, implementations may cover most layers of the reference model, with <a href="/wiki/Sockets" title="Sockets" class="mw-redirect">socket</a> and <a href="/wiki/Transmission_Control_Protocol" title="Transmission Control Protocol">TCP</a> being used in the transport layer.</p>
<p>Most MPI implementations consist of a specific set of routines (i.e., an API) callable from <a href="/wiki/Fortran" title="Fortran">Fortran</a>, <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a>, or <a href="/wiki/C%2B%2B" title="C++">C++</a> and from any language capable of interfacing with such routine libraries. The advantages of MPI over older message passing libraries are portability (because MPI has been implemented for almost every distributed memory architecture) and speed (because each <a href="/wiki/Implementation" title="Implementation">implementation</a> is in principle optimized for the <a href="/wiki/Hardware" title="Hardware">hardware</a> on which it runs).</p>
<p>MPI has Language Independent Specifications (LIS) for the function calls and language bindings. The first MPI standard specified <a href="/wiki/ANSI_C" title="ANSI C">ANSI C</a> and Fortran-77 language bindings together with the LIS. The draft of this standard was presented at Supercomputing 1994 (November 1994) and finalized soon thereafter. About 128 functions constitute the MPI-1.2 standard as it is now defined.</p>
<p>There are two versions of the standard that are currently popular: version 1.2 (shortly called MPI-1), which emphasizes message passing and has a static runtime environment, and MPI-2.1 (MPI-2), which includes new features such as parallel I/O, dynamic process management and remote memory operations.<sup id="cite_ref-Gropp99adv-pp4-5_2-0" class="reference"><a href="#cite_note-Gropp99adv-pp4-5-2" title=""><span>[</span>3<span>]</span></a></sup> MPI-2's LIS specifies over 500 functions and provides language bindings for ANSI C, ANSI Fortran (Fortran90), and ANSI C++. Interoperability of objects defined in MPI was also added to allow for easier mixed-language message passing programming. A side effect of MPI-2 standardization (completed in 1996) was clarification of the MPI-1 standard, creating the MPI-1.2 level.</p>
<p>It is important to note that MPI-2 is mostly a superset of MPI-1, although some functions have been deprecated. Thus MPI-1.2 programs still work under MPI implementations compliant with the MPI-2 standard.</p>
<p>MPI is often compared with <a href="/wiki/Parallel_Virtual_Machine" title="Parallel Virtual Machine">PVM</a>, which is a popular distributed environment and message passing system developed in 1989, and which was one of the systems that motivated the need for standard parallel message passing systems. Threaded shared memory programming models (such as <a href="/wiki/Pthreads" title="Pthreads" class="mw-redirect">Pthreads</a> and <a href="/wiki/OpenMP" title="OpenMP">OpenMP</a>) and message passing programming (MPI/<a href="/wiki/PVM" title="PVM" class="mw-redirect">PVM</a>) can be considered as complementary programming approaches, and can occasionally be seen used together in applications where this suits architecture, e.g. in servers with multiple large shared-memory nodes.</p>
<p><a name="Functionality" id="Functionality"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=2" title="Edit section: Functionality">edit</a>]</span> <span class="mw-headline">Functionality</span></h2>
<p>The MPI interface is meant to provide essential virtual topology, <a href="/wiki/Synchronization" title="Synchronization">synchronization</a>, and communication functionality between a set of processes (that have been mapped to nodes/servers/computer instances) in a language-independent way, with language-specific syntax (bindings), plus a few features that are language-specific. MPI programs always work with processes, but programmers commonly refer to the processes as processors. Typically, for maximum performance, each <a href="/wiki/CPU" title="CPU" class="mw-redirect">CPU</a> (or <a href="/wiki/Multi-core_(computing)" title="Multi-core (computing)" class="mw-redirect">core</a> in a multicore machine) will be assigned just a single process. This assignment happens at runtime through the agent that starts the MPI program, normally called mpirun or mpiexec.</p>
<p>The MPI library functions include, but are not limited to, point-to-point rendezvous-type send/receive operations, choosing between a <a href="/wiki/Cartesian" title="Cartesian">Cartesian</a> or <a href="/wiki/Graph_(data_structure)" title="Graph (data structure)">graph</a>-like logical process topology, exchanging data between process pairs (send/receive operations), combining partial results of computations (gathering and reduction operations), synchronizing nodes (barrier operation) as well as obtaining network-related information such as the number of processes in the computing session, current processor identity that a process is mapped to, neighboring processes accessible in a logical topology, and so on. Point-to-point operations come in synchronous, asynchronous, buffered, and <i>ready</i> forms, to allow both relatively stronger and weaker semantics for the synchronization aspects of a rendezvous-send. Many outstanding operations are possible in asynchronous mode, in most implementations.</p>
<p>MPI-1 and MPI-2 both enable implementations that do good work in overlapping communication and computation, but practice and theory differ. MPI also specifies <i>thread safe</i> interfaces, which have <a href="/wiki/Cohesion_(computer_science)" title="Cohesion (computer science)">cohesion</a> and <a href="/wiki/Coupling_(computer_science)" title="Coupling (computer science)">coupling</a> strategies that help avoid the manipulation of unsafe hidden state within the interface. It is relatively easy to write multithreaded point-to-point MPI code, and some implementations support such code. Multithreaded collective communication is best accomplished by using multiple copies of Communicators, as described below.</p>
<p><a name="Concepts" id="Concepts"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=3" title="Edit section: Concepts">edit</a>]</span> <span class="mw-headline">Concepts</span></h2>
<p>MPI provides a rich range of capabilities. The following concepts help in understanding and providing context for all of those capabilities and help the programmer to decide what functionality to use in their application programs.</p>
<p>There are eight basic concepts of MPI, four of which are unique to MPI-2.</p>
<p><a name="Communicator" id="Communicator"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=4" title="Edit section: Communicator">edit</a>]</span> <span class="mw-headline">Communicator</span></h3>
<p>Communicators are objects connecting groups of processes in the MPI session. Within each communicator each contained process has an independent identifier and the contained processes are arranged in an ordered topology. MPI also has explicit groups, but these are mainly good for organizing and reorganizing subsets of processes, before another communicator is made. MPI understands single group intracommunicator operations, and bi-partite (two-group) intercommunicator communication. In MPI-1, single group operations are most prevalent, with bi-partite operations finding their biggest role in MPI-2 where their usability is expanded to include collective communication and in dynamic process management.</p>
<p>Communicators can be partitioned using several commands in MPI, these commands include a graph-coloring-type algorithm called MPI_COMM_SPLIT, which is commonly used to derive topological and other logical subgroupings in an efficient way.</p>
<p><a name="Point-to-point_basics" id="Point-to-point_basics"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=5" title="Edit section: Point-to-point basics">edit</a>]</span> <span class="mw-headline">Point-to-point basics</span></h3>
<p>A number of important functions in the MPI API involve communication between two specific processes. A much used example is the MPI_Send interface, which allows one specified process to send a message to a second specified process. Point-to-point operations, as these are called, are particularly useful in patterned or irregular communication, for example, a <a href="/wiki/Data_parallelism" title="Data parallelism">data-parallel</a> architecture in which each processor routinely swaps regions of data with specific other processors between calculation steps, or a <a href="/wiki/Master-slave_(technology)" title="Master-slave (technology)">master-slave</a> architecture in which the master sends new task data to a slave whenever the previous task is completed.</p>
<p>MPI-1 specifies mechanisms for both <a href="/wiki/Blocking_(computing)" title="Blocking (computing)">blocking</a> and non-blocking point-to-point communication mechanisms, as well as the so-called 'ready-send' mechanism whereby a send request can be made only when the matching receive request has already been made.</p>
<p><a name="Collective_basics" id="Collective_basics"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=6" title="Edit section: Collective basics">edit</a>]</span> <span class="mw-headline">Collective basics</span></h3>
<p>Collective functions in the MPI API involve communication between all processes in a process group (which can mean the entire process pool or a program-defined subset). A typical function is the MPI_Bcast call (short for "broadcast"). This function takes data from one specially identified node and sends that message to all processes in the process group. A reverse operation is the MPI_Reduce call, which is a function designed to take data from all processes in a group, performs a user-chosen operation (like summing), and store the results on one individual node. These types of calls are often useful at the beginning or end of a large distributed calculation, where each processor operates on a part of the data and then combines it into a result.</p>
<p>There are also more complex operations such as MPI_Alltoall, which rearranges <i>n</i> items of data from each processor such that the <i>n</i>th node gets the <i>n</i>th item of data from each.</p>
<p><a name="Derived_Datatypes" id="Derived_Datatypes"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=7" title="Edit section: Derived Datatypes">edit</a>]</span> <span class="mw-headline">Derived Datatypes</span></h3>
<p>Many MPI functions require that you specify the type of the data which is sent between processors. This is because these arguments to MPI functions are variables, not defined types. If the data type is a standard one, such as, int, char, double, etc., you can use predefined MPI datatypes such as MPI_INT, MPI_CHAR, MPI_DOUBLE. Suppose your data is an array of ints and all the processors want to send their array to the root with MPI_Gather.</p>
<p>Here is a C example of how to do it:</p>
<div dir="ltr" style="text-align: left;">
<pre class="source-c">
<span class="kw4">int</span> array<span class="br0">[</span><span class="nu0">100</span><span class="br0">]</span>; 
<span class="kw4">int</span> root, total_p, *receive_array;
 
MPI_Comm_size<span class="br0">(</span>comm, &amp;total_p<span class="br0">)</span>;
receive_array=<span class="br0">(</span><span class="kw4">int</span> *<span class="br0">)</span> malloc<span class="br0">(</span>total_p*<span class="nu0">100</span>*<span class="kw4">sizeof</span><span class="br0">(</span><span class="kw4">int</span><span class="br0">)</span><span class="br0">)</span>;
MPI_Gather<span class="br0">(</span>array, <span class="nu0">100</span>, MPI_INT, receive_array, <span class="nu0">100</span>, MPI_INT, root, comm<span class="br0">)</span>;
</pre></div>
<p>However, you may instead wish to send your data as one block as opposed to 100 ints. You can do this by defining a continuous block derived data type.</p>
<div dir="ltr" style="text-align: left;">
<pre class="source-c">
MPI_Datatype newtype;
MPI_Type_contiguous<span class="br0">(</span><span class="nu0">100</span>, MPI_INT, &amp;newtype<span class="br0">)</span>;
MPI_Type_commit<span class="br0">(</span>&amp;newtype<span class="br0">)</span>;
MPI_Gather<span class="br0">(</span>array, <span class="nu0">1</span>, newtype, receive_array, <span class="nu0">1</span>, newtype, root, comm<span class="br0">)</span>;
</pre></div>
<p>Sometimes, your data might be a class or a data structure. In this case, there is not a predefined data type and you have to create one. You can make an MPI derived data type from MPI_predefined data types, by using MPI_Type_create_struct, which has the following format:</p>
<div dir="ltr" style="text-align: left;">
<pre class="source-c">
<span class="kw4">int</span> MPI_Type_create_struct<span class="br0">(</span><span class="kw4">int</span> count, <span class="kw4">int</span> blocklen<span class="br0">[</span><span class="br0">]</span>, MPI_Aint disp<span class="br0">[</span><span class="br0">]</span>, MPI_Datatype type<span class="br0">[</span><span class="br0">]</span>, MPI_Datatype *newtype<span class="br0">)</span>
</pre></div>
<p>where count is a number of blocks, also number of entries in types[], disp[] and blocklen[], blocklen[] — number of elements in each block (array of integer), disp[] — byte displacement of each block (array of integer), type[] — type of elements in each block (array of handles to datatype objects).</p>
<p>The disp[] array is needed because processors require the variables to be aligned a specific way on the memory. For example, Char is one byte and can go anywhere on the memory. Short is 2 bytes, so it goes to even memory addresses. Long is 4 bytes, it goes on locations divisible by 4 and so on. The compiler tries to accommodate this architecture in a class or data structure by putting padding between the variables. The safest way to find the distance between different variables in a data structure is by using their addresses by another MPI function, MPI_Get_address. You can use this function to calculate the displacement of all the elements of the data structure from the beginning of the data structure.</p>
<p>Suppose you have the following data structures:</p>
<div dir="ltr" style="text-align: left;">
<pre class="source-c">
  <span class="kw4">typedef</span> <span class="kw4">struct</span><span class="br0">{</span>
     <span class="kw4">int</span> f;
     <span class="kw4">short</span> p;
   <span class="br0">}</span> A
 
  <span class="kw4">typedef</span> <span class="kw4">struct</span><span class="br0">{</span>
    A a;
    <span class="kw4">int</span> pp,vp;
   <span class="br0">}</span> B
</pre></div>
<p>Here's the C code for building MPI-derived data type:</p>
<div dir="ltr" style="text-align: left;">
<pre class="source-c">
 
<span class="kw4">void</span> define_MPI_datatype<span class="br0">(</span><span class="br0">)</span><span class="br0">{</span>
 
  <span class="kw4">int</span> blocklen<span class="br0">[</span><span class="nu0">6</span><span class="br0">]</span>=<span class="br0">{</span><span class="nu0">1</span>,<span class="nu0">1</span>,<span class="nu0">1</span>,<span class="nu0">1</span>,<span class="nu0">1</span>,<span class="nu0">1</span><span class="br0">}</span>; <span class="co1">//The first and last elements mark the beg and end of data structure</span>
  MPI_Aint disp<span class="br0">[</span><span class="nu0">6</span><span class="br0">]</span>;
  MPI_Datatype newtype;
  MPI_Datatype type<span class="br0">[</span><span class="nu0">6</span><span class="br0">]</span>=<span class="br0">{</span>MPI_LB, MPI_INT, MPI_SHORT, MPI_INT, MPI_INT, MPI_UB<span class="br0">}</span>;
  B findsize<span class="br0">[</span><span class="nu0">2</span><span class="br0">]</span>; <span class="co1">//You need an array to establish the upper bound of the data structure</span>
  MPI_Aint findsize_addr, a_addr, f_addr, p_addr, pp_addr, vp_addr, UB_addr;
  <span class="kw4">int</span> error;
 
  MPI_Get_address<span class="br0">(</span>&amp;findsize<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span>, &amp;findsize_addr<span class="br0">)</span>;
  MPI_Get_address<span class="br0">(</span>&amp;<span class="br0">(</span>findsize<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class="br0">)</span>.<span class="me1">a</span>, &amp;a_addr<span class="br0">)</span>;
  MPI_Get_address<span class="br0">(</span>&amp;<span class="br0">(</span><span class="br0">(</span>findsize<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class="br0">)</span>.<span class="me1">a</span><span class="br0">)</span>.<span class="me1">f</span>, &amp;f_addr<span class="br0">)</span>;
  MPI_Get_address<span class="br0">(</span>&amp;<span class="br0">(</span><span class="br0">(</span>findsize<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class="br0">)</span>.<span class="me1">a</span><span class="br0">)</span>.<span class="me1">p</span>, &amp;p_addr<span class="br0">)</span>;
  MPI_Get_address<span class="br0">(</span>&amp;<span class="br0">(</span>findsize<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class="br0">)</span>.<span class="me1">pp</span>, &amp;pp_addr<span class="br0">)</span>;
  MPI_Get_address<span class="br0">(</span>&amp;<span class="br0">(</span>findsize<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span><span class="br0">)</span>.<span class="me1">vp</span>, &amp;vp_addr<span class="br0">)</span>;
  MPI_Get_address<span class="br0">(</span>&amp;findsize<span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>,&amp;UB_addr<span class="br0">)</span>;
 
  disp<span class="br0">[</span><span class="nu0">0</span><span class="br0">]</span>=a_addr-findsize_addr;
  disp<span class="br0">[</span><span class="nu0">1</span><span class="br0">]</span>=f_addr-findsize_addr;
  disp<span class="br0">[</span><span class="nu0">2</span><span class="br0">]</span>=p_addr-findsize_addr;
  disp<span class="br0">[</span><span class="nu0">3</span><span class="br0">]</span>=pp_addr-findsize_addr;
  disp<span class="br0">[</span><span class="nu0">4</span><span class="br0">]</span>=vp_addr-findsize_addr;
  disp<span class="br0">[</span><span class="nu0">5</span><span class="br0">]</span>=UB_addr-findsize_addr;
 
  error=MPI_Type_create_struct<span class="br0">(</span><span class="nu0">6</span>, blocklen, disp, type, &amp;newtype<span class="br0">)</span>;
  MPI_Type_commit<span class="br0">(</span>&amp;newtype<span class="br0">)</span>;
<span class="br0">}</span>
</pre></div>
<p><a name="One-sided_communication_.28MPI-2.29" id="One-sided_communication_.28MPI-2.29"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=8" title="Edit section: One-sided communication (MPI-2)">edit</a>]</span> <span class="mw-headline">One-sided communication (MPI-2)</span></h3>
<p>MPI-2 defines three one-sided communications operations, Put, Get, and Accumulate, being a write to remote memory, a read from remote memory, and a reduction operation on the same memory across a number of tasks. Also defined are three different methods for synchronising this communication - global, pairwise, and remote locks - as the specification does not guarantee that these operations have taken place until a synchronisation point.</p>
<p>These types of call can often be useful for algorithms in which synchronisation would be inconvenient (e.g. distributed matrix multiplication), or where it is desirable for tasks to be able to balance their load while other processors are operating on data.</p>
<p><a name="Collective_extensions_.28MPI-2.29" id="Collective_extensions_.28MPI-2.29"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=9" title="Edit section: Collective extensions (MPI-2)">edit</a>]</span> <span class="mw-headline">Collective extensions (MPI-2)</span></h3>
<p>This section needs to be developed.</p>
<table class="metadata plainlinks ambox mbox-small-left ambox-notice" style="margin: 4px 1em 4px 0; width: 238px; border-collapse: collapse; font-size: 88%; line-height: 1.25em;">
<tr>
<td class="mbox-image"><a href="/wiki/File:Wiki_letter_w.svg" class="image" title="Wiki letter w.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Wiki_letter_w.svg/20px-Wiki_letter_w.svg.png" width="20" height="20" border="0" /></a></td>
<td class="mbox-text" style="">This section requires <a href="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" rel="nofollow">expansion</a>.</td>
</tr>
</table>
<p><a name="Dynamic_process_management_.28MPI-2.29" id="Dynamic_process_management_.28MPI-2.29"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=10" title="Edit section: Dynamic process management (MPI-2)">edit</a>]</span> <span class="mw-headline">Dynamic process management (MPI-2)</span></h3>
<p>The key aspect of this MPI-2 feature is "the ability of an MPI process to participate in the creation of new MPI processes or to establish communication with MPI processes that have been started separately." The MPI-2 specification describes three main interfaces by which MPI processes can dynamically establish communications, MPI_Comm_spawn, MPI_Comm_accept/MPI_Comm_connect and MPI_Comm_join. The MPI_Comm_spawn interface allows an MPI process to spawn a number of instances of the named MPI process. The newly spawned set of MPI processes form a new MPI_COMM_WORLD intracommunicator but can communicate with the parent and the intercommunicator the function returns. MPI_Comm_spawn_multiple is an alternate interface that allows the different instances spawned to be different binaries with different arguments.<sup id="cite_ref-Gropp99adv-p7_3-0" class="reference"><a href="#cite_note-Gropp99adv-p7-3" title=""><span>[</span>4<span>]</span></a></sup></p>
<table class="metadata plainlinks ambox mbox-small-left ambox-notice" style="margin: 4px 1em 4px 0; width: 238px; border-collapse: collapse; font-size: 88%; line-height: 1.25em;">
<tr>
<td class="mbox-image"><a href="/wiki/File:Wiki_letter_w.svg" class="image" title="Wiki letter w.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Wiki_letter_w.svg/20px-Wiki_letter_w.svg.png" width="20" height="20" border="0" /></a></td>
<td class="mbox-text" style="">This section requires <a href="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" rel="nofollow">expansion</a>.</td>
</tr>
</table>
<p><a name="MPI_I.2FO_.28MPI-2.29" id="MPI_I.2FO_.28MPI-2.29"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=11" title="Edit section: MPI I/O (MPI-2)">edit</a>]</span> <span class="mw-headline">MPI I/O (MPI-2)</span></h3>
<p>The Parallel I/O feature introduced with MPI-2, is sometimes shortly called MPI-IO,<sup id="cite_ref-Gropp99adv-pp5-6_4-0" class="reference"><a href="#cite_note-Gropp99adv-pp5-6-4" title=""><span>[</span>5<span>]</span></a></sup> and refers to a collection of functions designed to allow the difficulties of managing I/O on distributed systems to be abstracted away to the MPI library, as well as allowing files to be easily accessed in a patterned fashion using the existing derived datatype functionality.</p>
<p>The little research that has been done on this feature indicates the difficulty for good performance. For example, some implementations of <a href="http://www.phys.uu.nl/~hulten/mod3a/report.ps" class="external text" title="http://www.phys.uu.nl/~hulten/mod3a/report.ps" rel="nofollow">sparse matrix-vector multiplications using the MPI I/O library</a> disastrously fail in efficient parallelization.</p>
<table class="metadata plainlinks ambox mbox-small-left ambox-notice" style="margin: 4px 1em 4px 0; width: 238px; border-collapse: collapse; font-size: 88%; line-height: 1.25em;">
<tr>
<td class="mbox-image"><a href="/wiki/File:Wiki_letter_w.svg" class="image" title="Wiki letter w.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Wiki_letter_w.svg/20px-Wiki_letter_w.svg.png" width="20" height="20" border="0" /></a></td>
<td class="mbox-text" style="">This section requires <a href="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" rel="nofollow">expansion</a>.</td>
</tr>
</table>
<p><a name="Implementations" id="Implementations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=12" title="Edit section: Implementations">edit</a>]</span> <span class="mw-headline">Implementations</span></h2>
<p><a name=".27Classical.27_cluster_and_supercomputer_implementations" id=".27Classical.27_cluster_and_supercomputer_implementations"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=13" title="Edit section: 'Classical' cluster and supercomputer implementations">edit</a>]</span> <span class="mw-headline">'Classical' cluster and supercomputer implementations</span></h3>
<p>The implementation language for MPI is different in general from the language or languages it seeks to support at runtime. Most MPI implementations are done in a combination of C, C++ and assembly language, and target C, C++, and Fortran programmers. However, the implementation language and the end-user language are in principle always decoupled.</p>
<p>The initial implementation of the MPI 1.x standard was <a href="/wiki/MPICH" title="MPICH">MPICH</a>, from Argonne National Laboratory (correctly pronounced M-P-I-C-H, not pronounced as a single syllable) and Mississippi State University. IBM also was an early implementor of the MPI standard, and most supercomputer companies of the early 1990s either commercialized MPICH, or built their own implementation of the MPI 1.x standard. <a href="/wiki/LAM/MPI" title="LAM/MPI">LAM/MPI</a> from Ohio Supercomputing Center was another early open implementation. Argonne National Laboratory has continued developing MPICH for over a decade, and now offers MPICH 2, which is an implementation of the MPI-2.1 standard. LAM/MPI and a number of other MPI efforts recently merged to form a combined project, <a href="/wiki/Open_MPI" title="Open MPI">Open MPI</a>. There are many other efforts that are derivatives of MPICH, LAM, and other works, too numerous to name here. Recently, Microsoft added an MPI effort to their Cluster Computing Kit (2005), based on MPICH 2.</p>
<p>Besides the mainstream of MPI programming for high performance, MPI has been used widely with Python, Perl, and Java. These communities are growing. MATLAB-based MPI appear in many forms, but no consensus on a single way of using MPI with MATLAB yet exists. The next sections detail some of these efforts.</p>
<p><a name="Python" id="Python"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=14" title="Edit section: Python">edit</a>]</span> <span class="mw-headline">Python</span></h3>
<p>There are at least five implementations of MPI for Python: <a href="http://www.cimec.org.ar/python/mpi4py.html" class="external text" title="http://www.cimec.org.ar/python/mpi4py.html" rel="nofollow">mpi4py</a>, <a href="http://datamining.anu.edu.au/~ole/pypar/" class="external text" title="http://datamining.anu.edu.au/~ole/pypar/" rel="nofollow">PyPar</a>, <a href="http://pympi.sourceforge.net/" class="external text" title="http://pympi.sourceforge.net/" rel="nofollow">PyMPI</a>, <a href="http://peloton.sdsc.edu/~tkaiser/mympi/" class="external text" title="http://peloton.sdsc.edu/~tkaiser/mympi/" rel="nofollow">MYMPI</a>, and the MPI submodule in <a href="/wiki/ScientificPython" title="ScientificPython">ScientificPython</a>. <a href="/wiki/PyMPI" title="PyMPI">PyMPI</a> is notable because it is <i>a variant python interpreter</i> making the multi-node application the interpreter itself, rather than the code the interpreter runs. <a href="/wiki/PyMPI" title="PyMPI">PyMPI</a> implements most of the MPI spec and automatically works with compiled code that needs to make MPI calls. PyPar, MYMPI, and ScientificPython's module all are designed to work like a typical module used with nothing but an import statement. They make it the coder's job to decide when and where the call to MPI_Init belongs. Recently the well known Boost C++ Libraries acquired Boost:MPI which included the <a href="http://www.boost.org/doc/libs/1_35_0/doc/html/mpi/python.html" class="external text" title="http://www.boost.org/doc/libs/1_35_0/doc/html/mpi/python.html" rel="nofollow">Boost:MPI Python Bindings</a>. This is of particular interest for those who want to mix C++ and Python.</p>
<p><a name="OCaml" id="OCaml"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=15" title="Edit section: OCaml">edit</a>]</span> <span class="mw-headline">OCaml</span></h3>
<p>The <a href="http://cristal.inria.fr/~xleroy/software.html#ocamlmpi" class="external text" title="http://cristal.inria.fr/~xleroy/software.html#ocamlmpi" rel="nofollow">OCamlMPI Module</a> implements a large subset of MPI functions and is in active use in scientific computing. To get a sense of its maturity: <a href="http://caml.inria.fr/pub/ml-archives/caml-list/2003/07/155910c4eeb09e684f02ea4ae342873b.en.html" class="external text" title="http://caml.inria.fr/pub/ml-archives/caml-list/2003/07/155910c4eeb09e684f02ea4ae342873b.en.html" rel="nofollow">it was reported on caml-list</a> that an eleven thousand line OCaml program was "MPI-ified", using the module, with an additional 500 lines of code and slight restructuring and has run with excellent results on up to 170 nodes in a supercomputer.</p>
<p><a name="Java" id="Java"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=16" title="Edit section: Java">edit</a>]</span> <span class="mw-headline">Java</span></h3>
<p>Although Java does not have an official MPI binding, there have been several attempts to bridge Java and MPI, with different degrees of success and compatibility. One of the first attempts was <a href="/w/index.php?title=Bryan_Carpenter&amp;action=edit&amp;redlink=1" class="new" title="Bryan Carpenter (page does not exist)">Bryan Carpenter</a>'s <a href="http://www.hpjava.org/mpiJava.html" class="external text" title="http://www.hpjava.org/mpiJava.html" rel="nofollow">mpiJava</a>, essentially a collection of <a href="/wiki/Java_Native_Interface" title="Java Native Interface">JNI</a> wrappers to a local C MPI library, resulting in a hybrid implementation with limited portability, which also has to be recompiled against the specific MPI library being used.</p>
<p>However, this original project also defined the <a href="http://www.hpjava.org/theses/shko/thesis_paper/node33.html" class="external text" title="http://www.hpjava.org/theses/shko/thesis_paper/node33.html" rel="nofollow">mpiJava API</a> (a <a href="/wiki/De-facto" title="De-facto" class="mw-redirect">de-facto</a> MPI <a href="/wiki/API" title="API" class="mw-redirect">API</a> for Java following the equivalent C++ bindings closely) which other subsequent Java MPI projects followed. An alternative although less used API is the <a href="http://www.hpjava.org/papers/MPJ-CPE/cpempi/node6.html" class="external text" title="http://www.hpjava.org/papers/MPJ-CPE/cpempi/node6.html" rel="nofollow">MPJ API</a>, designed to be more object-oriented and closer to <a href="/wiki/Sun_Microsystems" title="Sun Microsystems">Sun Microsystems</a>' coding conventions. Other than the API used, Java MPI libraries can be either dependent on a local MPI library, or implement the message passing functions in Java, while some like <a href="/wiki/P2P-MPI" title="P2P-MPI">P2P-MPI</a> also provide <a href="/wiki/Peer_to_peer" title="Peer to peer" class="mw-redirect">Peer to peer</a> functionality and allow mixed platform operation.</p>
<p>Some of the most challenging parts of any MPI implementation for Java arise from the language's own limitations and peculiarities, such as the lack of explicit <a href="/wiki/Data_pointer" title="Data pointer" class="mw-redirect">pointers</a> and linear memory address space for its objects , which make transferring multi-dimensional arrays and complex objects inefficient. The workarounds usually used involve transferring one line at a time and/or performing explicit de-<a href="/wiki/Serialization" title="Serialization">serialization</a> and <a href="/wiki/Cast_(computer_science)" title="Cast (computer science)" class="mw-redirect">casting</a> both at the sending and receiving end, simulating C or FORTRAN-like arrays by the use of a one-dimensional array, and pointers to primitive types by the use of single-element arrays, thus resulting in programming styles quite extraneous from Java's conventions.</p>
<p>One major improvement is MPJ Express by Dr. Aamir Shafi <a href="http://hpc.niit.edu.pk/~aamir" class="external autonumber" title="http://hpc.niit.edu.pk/~aamir" rel="nofollow">[1]</a>. This project was supervised by Bryan Carpenter and Mark Baker. On commodity platform like Fast Ethernet, advances in JVM technology now enable networking programs written in Java to rival their C counterparts. On the other hand, improvements in specialized networking hardware have continued, cutting down the communication costs to a couple of microseconds. Keeping both in mind, the key issue at present is not to debate the JNI approach versus the pure Java approach, but to provide a flexible mechanism for programs to swap communication protocols. The aim of this project is to provide a reference Java messaging system based on the MPI standard. The implementation follows a layered architecture based on an idea of device drivers. The idea is analogous to UNIX device drivers. For more info visit <a href="http://mpj-express.org" class="external autonumber" title="http://mpj-express.org" rel="nofollow">[2]</a></p>
<p><a name="Common_Language_Infrastucture" id="Common_Language_Infrastucture"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=17" title="Edit section: Common Language Infrastucture">edit</a>]</span> <span class="mw-headline">Common Language Infrastucture</span></h3>
<p>There are two managed <a href="/wiki/Common_Language_Infrastructure" title="Common Language Infrastructure">CLI</a> (<a href="/wiki/.NET_Framework" title=".NET Framework">.NET</a>) implementations of MPI. The first one is <a href="http://www.purempi.net" class="external text" title="http://www.purempi.net" rel="nofollow">Pure Mpi.NET</a> - its object-oriented API is powerful, yet easy to use for parallel programming. It is based on Windows Communication Foundation (<a href="/wiki/Windows_Communication_Foundation" title="Windows Communication Foundation">WCF</a>), allowing declarative specification of the binding and endpoint configuration for your environment and performance needs. The SDK takes advantage of .NET features, including generics, delegates, asynchronous results, exception handling, and extensibility points.</p>
<p>Another managed implementation is <a href="http://www.osl.iu.edu/research/mpi.net/" class="external text" title="http://www.osl.iu.edu/research/mpi.net/" rel="nofollow">MPI.NET</a>, a research effort at <a href="/wiki/Indiana_University" title="Indiana University">Indiana University</a> licensed under a BSD-style license. It is compatible with <a href="/wiki/Mono_(software)" title="Mono (software)">Mono</a>, and can make full use of underlying low-latency MPI network fabrics.</p>
<p><a name="Hardware_implementations" id="Hardware_implementations"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=18" title="Edit section: Hardware implementations">edit</a>]</span> <span class="mw-headline">Hardware implementations</span></h3>
<p>There has been research over time into implementing MPI directly into the hardware of the system, for example by means of <a href="/wiki/Processor-in-memory" title="Processor-in-memory">Processor-in-memory</a>, where the MPI operations are actually built into the microcircuitry of the <a href="/wiki/RAM" title="RAM" class="mw-redirect">RAM</a> chips in each node. By implication, this type of implementation would be independent of the language, OS or CPU on the system, but cannot be readily updated or unloaded.</p>
<p>Another approach has been to add hardware acceleration to one or more parts of the operation. This may include hardware processing of the MPI queues or the use of <a href="/wiki/RDMA" title="RDMA" class="mw-redirect">RDMA</a> to directly transfer data between memory and the network interface without needing CPU or kernel intervention.</p>
<p><a name="Example_program" id="Example_program"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=19" title="Edit section: Example program">edit</a>]</span> <span class="mw-headline">Example program</span></h2>
<p>Here is a "Hello World" program in MPI written in C. In this example, we send a "hello" message to each processor, manipulate it trivially, send the results back to the main process, and print the messages out.</p>
<div dir="ltr" style="text-align: left;">
<pre class="source-c">
 <span class="coMULTI">/*
  "Hello World" Type MPI Test Program
 */</span>
 <span class="co2">#include &lt;mpi.h&gt;</span>
 <span class="co2">#include &lt;stdio.h&gt;</span>
 <span class="co2">#include &lt;string.h&gt;</span>
 
 <span class="co2">#define BUFSIZE 128</span>
 <span class="co2">#define TAG 0</span>
 
 <span class="kw4">int</span> main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span> *argv<span class="br0">[</span><span class="br0">]</span><span class="br0">)</span>
 <span class="br0">{</span>
   <span class="kw4">char</span> idstr<span class="br0">[</span><span class="nu0">32</span><span class="br0">]</span>;
   <span class="kw4">char</span> buff<span class="br0">[</span>BUFSIZE<span class="br0">]</span>;
   <span class="kw4">int</span> numprocs;
   <span class="kw4">int</span> myid;
   <span class="kw4">int</span> i;
   MPI_Status stat; 
 
   MPI_Init<span class="br0">(</span>&amp;argc,&amp;argv<span class="br0">)</span>; <span class="coMULTI">/* all MPI programs start with MPI_Init; all 'N' processes exist thereafter */</span>
   MPI_Comm_size<span class="br0">(</span>MPI_COMM_WORLD,&amp;numprocs<span class="br0">)</span>; <span class="coMULTI">/* find out how big the SPMD world is */</span>
   MPI_Comm_rank<span class="br0">(</span>MPI_COMM_WORLD,&amp;myid<span class="br0">)</span>; <span class="coMULTI">/* and this processes' rank is */</span>
 
   <span class="coMULTI">/* At this point, all the programs are running equivalently, the rank is used to
      distinguish the roles of the programs in the SPMD model, with rank 0 often used
      specially... */</span>
   <span class="kw1">if</span><span class="br0">(</span>myid == <span class="nu0">0</span><span class="br0">)</span>
   <span class="br0">{</span>
     <span class="kw3">printf</span><span class="br0">(</span><span class="st0">"%d: We have %d processors<span class="es0">\n</span>"</span>, myid, numprocs<span class="br0">)</span>;
     <span class="kw1">for</span><span class="br0">(</span>i=<span class="nu0">1</span>;i&lt;numprocs;i++<span class="br0">)</span>
     <span class="br0">{</span>
       sprintf<span class="br0">(</span>buff, <span class="st0">"Hello %d! "</span>, i<span class="br0">)</span>;
       MPI_Send<span class="br0">(</span>buff, BUFSIZE, MPI_CHAR, i, TAG, MPI_COMM_WORLD<span class="br0">)</span>;
     <span class="br0">}</span>
     <span class="kw1">for</span><span class="br0">(</span>i=<span class="nu0">1</span>;i&lt;numprocs;i++<span class="br0">)</span>
     <span class="br0">{</span>
       MPI_Recv<span class="br0">(</span>buff, BUFSIZE, MPI_CHAR, i, TAG, MPI_COMM_WORLD, &amp;stat<span class="br0">)</span>;
       <span class="kw3">printf</span><span class="br0">(</span><span class="st0">"%d: %s<span class="es0">\n</span>"</span>, myid, buff<span class="br0">)</span>;
     <span class="br0">}</span>
   <span class="br0">}</span>
   <span class="kw1">else</span>
   <span class="br0">{</span>
     <span class="coMULTI">/* receive from rank 0: */</span>
     MPI_Recv<span class="br0">(</span>buff, BUFSIZE, MPI_CHAR, <span class="nu0">0</span>, TAG, MPI_COMM_WORLD, &amp;stat<span class="br0">)</span>;
     sprintf<span class="br0">(</span>idstr, <span class="st0">"Processor %d "</span>, myid<span class="br0">)</span>;
     strcat<span class="br0">(</span>buff, idstr<span class="br0">)</span>;
     strcat<span class="br0">(</span>buff, <span class="st0">"reporting for duty<span class="es0">\n</span>"</span><span class="br0">)</span>;
     <span class="coMULTI">/* send to rank 0: */</span>
     MPI_Send<span class="br0">(</span>buff, BUFSIZE, MPI_CHAR, <span class="nu0">0</span>, TAG, MPI_COMM_WORLD<span class="br0">)</span>;
   <span class="br0">}</span>
 
   MPI_Finalize<span class="br0">(</span><span class="br0">)</span>; <span class="coMULTI">/* MPI Programs end with MPI Finalize; this is a weak synchronization point */</span>
   <span class="kw1">return</span> <span class="nu0">0</span>;
 <span class="br0">}</span>
</pre></div>
<p>It is important to note that the runtime environment for the MPI implementation used (often called mpirun or mpiexec) spawns multiple copies of the program, with the total number of copies determining the number of process <i>ranks</i> in MPI_COMM_WORLD, which is an opaque descriptor for communication between the set of processes. A Single-Program-Multiple-Data (<a href="/wiki/SPMD" title="SPMD">SPMD</a>) programming model is thereby facilitated, but not required; many MPI implementations allow multiple, different, executables to be started in the same MPI job. Each process has its own rank, the total number of processes in the world, and the ability to communicate between them either with point-to-point (send/receive) communication, or by collective communication among the group. It is enough for MPI to provide an SPMD-style program with MPI_COMM_WORLD, its own rank, and the size of the world to allow for algorithms to decide what they do based on their rank. In more robust examples, I/O should be more carefully managed than in this example. MPI does not guarantee how POSIX I/O would actually work on a given system, but it commonly does work, at least from rank 0.</p>
<p>The notion of process and not processor is used in MPI. The copies of this program are <i>mapped</i> to processors by the runtime environment of MPI. In that sense, the parallel machine can map to 1 physical processor, or N where N is the total number of processors available, or something in between. For maximal potential for parallel speedup more physical processors are used. It should also be noted that this example adjusts its behavior to the size of the world N, so it also seeks to be scalable to the size given at runtime. There is no separate compilation for each size of the concurrency, although different decisions might be taken internally depending on that absolute amount of concurrency provided to the program.</p>
<p><a name="Adoption_of_MPI-2" id="Adoption_of_MPI-2"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=20" title="Edit section: Adoption of MPI-2">edit</a>]</span> <span class="mw-headline">Adoption of MPI-2</span></h2>
<table class="metadata plainlinks ambox ambox-content" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_content.png" class="image" title="Ambox content.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/f/f4/Ambox_content.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article <b>may contain <a href="/wiki/Wikipedia:No_original_research" title="Wikipedia:No original research">original research</a> or <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">unverified claims</a></b>. Please <a href="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&amp;action=edit" rel="nofollow">improve the article</a> by adding <a href="/wiki/Wikipedia:References" title="Wikipedia:References" class="mw-redirect">references</a>. See the <a href="/wiki/Talk:Message_Passing_Interface" title="Talk:Message Passing Interface">talk page</a> for details. <small><i>(May 2008)</i></small></td>
</tr>
</table>
<p>While the adoption of MPI-1.2 has been universal, including on almost all cluster computing, the acceptance of MPI-2.1 has been more limited. Here are some of the reasons.</p>
<ol>
<li>While MPI-1.2 emphasizes message passing and a minimal, static runtime environment, full MPI-2 implementations include I/O and dynamic process management, and the size of the middleware implementation is substantially larger. Furthermore, most sites that use batch scheduling systems cannot support dynamic process management. Parallel I/O is well accepted as a key value of MPI-2.</li>
<li>Many legacy MPI-1.2 programs were already developed by the time MPI-2 came out, and work fine. The threat of potentially lost portability by using MPI-2 functions kept people from using the enhanced standard for many years, though this is lessening in the mid 2000's, with wider support for MPI-2.</li>
<li>Many MPI-1.2 applications use only a subset of that standard (16-25 functions). This minimalism of use contrasts with the huge availability of functionality now afforded in MPI-2.</li>
</ol>
<p>Other inhibiting factors can be cited too, although these may amount more to perceptions and belief than fact. MPI-2 has been well supported in free and commercial implementations since at least the early 2000s, with some implementations coming earlier than that.</p>
<p><a name="The_future_of_MPI" id="The_future_of_MPI"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=21" title="Edit section: The future of MPI">edit</a>]</span> <span class="mw-headline">The future of MPI</span></h2>
<p>Some aspects of MPI's future appear solid; others less so. The <a href="/w/index.php?title=MPI_Forum&amp;action=edit&amp;redlink=1" class="new" title="MPI Forum (page does not exist)">MPI Forum</a> reconvened in 2007, to clarify some MPI-2 issues and explore developments for a possible MPI-3.</p>
<p>Irrespective of what the MPI Forum decides for MPI-3, MPI as a legacy interface will exist at the MPI-1.2 and MPI-2.1 levels for many years to come. Like Fortran, it is ubiquitous in technical computing, and it is taught and used widely. The body of free and commercial products that require MPI, combined with new ports of the existing free and commercial implementations to new target platforms, help ensure that MPI will go on indefinitely.</p>
<p>Architectures are changing, with greater internal concurrency (multi-core), better fine-grain concurrency control (threading, affinity), and more levels of memory hierarchy. <a href="/wiki/Multithreaded" title="Multithreaded" class="mw-redirect">Multithreaded</a> programs can take advantage of these developments more easily than single threaded applications. This has already yielded separate, complementary standards for <a href="/wiki/Symmetric_multiprocessing" title="Symmetric multiprocessing">symmetric multiprocessing</a>, namely <a href="/wiki/OpenMP" title="OpenMP">OpenMP</a>. The MPI-2 standard does define how standard-conforming implementations should deal with multithreaded issues, the standard does not require that implementations be multithreaded, or even thread safe. While multithreaded capable MPI implementations do exist, the number of multithreaded, message passing applications are few. The drive to achieve multi-level concurrency all within MPI is both a challenge and an opportunity for the standard in future.</p>
<p>Much of the discussion within the MPI Forum centres around <a href="/wiki/Fault_tolerance" title="Fault tolerance" class="mw-redirect">fault tolerance</a>. Improved fault tolerance within MPI would have clear benefits in the context of <a href="/wiki/Grid_computing" title="Grid computing">Grid computing</a>, a growing trend in large-scale computing.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=22" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<table class="metadata plainlinks mbox-small" style="border:1px solid #aaa; background-color:#f9f9f9;">
<tr>
<td class="mbox-image"><a href="http://en.wikibooks.org/wiki/Special:Search/Message_Passing_Interface" title="b:Special:Search/Message Passing Interface"><img alt="Sister project" src="http://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Wikibooks-logo-en-noslogan.svg/40px-Wikibooks-logo-en-noslogan.svg.png" width="40" height="40" border="0" /></a></td>
<td class="mbox-text" style=""><a href="/wiki/Wikibooks" title="Wikibooks">Wikibooks</a> has a book on the topic of
<div style="margin-left:10px;"><i><b><a href="http://en.wikibooks.org/wiki/Message-Passing_Interface" class="extiw" title="wikibooks:Message-Passing Interface">Message-Passing Interface</a></b></i></div>
</td>
</tr>
</table>
<ul>
<li><a href="/wiki/MPICH" title="MPICH">MPICH</a></li>
<li><a href="/wiki/LAM/MPI" title="LAM/MPI">LAM/MPI</a></li>
<li><a href="/wiki/Open_MPI" title="Open MPI">Open MPI</a></li>
<li><a href="/wiki/Microsoft_Messaging_Passing_Interface" title="Microsoft Messaging Passing Interface">Microsoft Messaging Passing Interface</a></li>
<li><a href="/wiki/OpenMP" title="OpenMP">OpenMP</a></li>
<li><a href="/w/index.php?title=SHMEM&amp;action=edit&amp;redlink=1" class="new" title="SHMEM (page does not exist)">SHMEM</a></li>
<li><a href="/wiki/Global_Arrays" title="Global Arrays">Global Arrays</a></li>
<li><a href="/wiki/Unified_Parallel_C" title="Unified Parallel C">Unified Parallel C</a></li>
<li><a href="/wiki/Occam_programming_language" title="Occam programming language" class="mw-redirect">Occam programming language</a></li>
<li><a href="/wiki/Linda_(coordination_language)" title="Linda (coordination language)">Linda (coordination language)</a></li>
<li><a href="/wiki/Parallel_Virtual_Machine" title="Parallel Virtual Machine">Parallel Virtual Machine</a></li>
<li><a href="/wiki/Calculus_of_communicating_systems" title="Calculus of communicating systems">Calculus of communicating systems</a></li>
<li><a href="/wiki/Calculus_of_Broadcasting_Systems" title="Calculus of Broadcasting Systems">Calculus of Broadcasting Systems</a></li>
<li><a href="/wiki/Actor_model" title="Actor model">Actor model</a></li>
<li><a href="/w/index.php?title=Interconnect_Driven_Server&amp;action=edit&amp;redlink=1" class="new" title="Interconnect Driven Server (page does not exist)">Interconnect Driven Server</a></li>
<li><a href="/wiki/Allinea_Distributed_Debugging_Tool" title="Allinea Distributed Debugging Tool">DDT</a> Debugging tool for MPI programs</li>
<li><a href="/wiki/Bulk_Synchronous_Parallel" title="Bulk Synchronous Parallel" class="mw-redirect">Bulk Synchronous Parallel</a> BSP Programming</li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=23" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> Gropp <i>et al</i> 96, p.3</li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <a href="http://portal.acm.org/citation.cfm?id=1188565" class="external text" title="http://portal.acm.org/citation.cfm?id=1188565" rel="nofollow">High-performance and scalable MPI over InfiniBand with reduced memory usage</a></li>
<li id="cite_note-Gropp99adv-pp4-5-2"><b><a href="#cite_ref-Gropp99adv-pp4-5_2-0" title="">^</a></b> Gropp <i>et al</i> 1999-advanced, pp.4-5</li>
<li id="cite_note-Gropp99adv-p7-3"><b><a href="#cite_ref-Gropp99adv-p7_3-0" title="">^</a></b> Gropp <i>et al</i> 1999-advanced, p.7</li>
<li id="cite_note-Gropp99adv-pp5-6-4"><b><a href="#cite_ref-Gropp99adv-pp5-6_4-0" title="">^</a></b> Gropp <i>et al</i> 1999-advanced, pp.5-6</li>
</ol>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=24" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><span class="boilerplate" id="foldoc"><i>This article was originally based on material from the <a href="/wiki/Free_On-line_Dictionary_of_Computing" title="Free On-line Dictionary of Computing">Free On-line Dictionary of Computing</a>, which is <a href="/wiki/Wikipedia:Foldoc_license" title="Wikipedia:Foldoc license">licensed</a> under the <a href="/wiki/GNU_Free_Documentation_License" title="GNU Free Documentation License">GFDL</a>.</i></span></li>
<li>Aoyama, Yukiya; Nakano, Jun (1999) <i><a href="http://www.redbooks.ibm.com/abstracts/sg245380.html" class="external text" title="http://www.redbooks.ibm.com/abstracts/sg245380.html" rel="nofollow">RS/6000 SP: Practical MPI Programming</a></i>, ITSO</li>
<li>Foster, Ian (1995) <i>Designing and Building Parallel Programs (Online)</i> Addison-Wesley <a href="/wiki/Special:BookSources/0201575949" class="internal">ISBN 0201575949</a>, chapter 8 <i><a href="http://www-unix.mcs.anl.gov/dbpp/text/node94.html#SECTION03500000000000000000" class="external text" title="http://www-unix.mcs.anl.gov/dbpp/text/node94.html#SECTION03500000000000000000" rel="nofollow">Message Passing Interface</a></i></li>
</ul>
<ul>
<li><i>Using MPI</i> series:
<ul>
<li>Gropp, William; Lusk, Ewing; Skjellum, Anthony. (1994) <i><a href="http://www-unix.mcs.anl.gov/mpi/usingmpi/usingmpi-1st/index.html" class="external text" title="http://www-unix.mcs.anl.gov/mpi/usingmpi/usingmpi-1st/index.html" rel="nofollow">Using MPI</a>: portable parallel programming with the message-passing interface.</i> <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a> In Scientific And Engineering Computation Series, Cambridge, MA, USA. 307 pp. <a href="/wiki/Special:BookSources/0262571048" class="internal">ISBN 0-262-57104-8</a></li>
<li>Gropp, William; Lusk, Ewing; Skjellum, Anthony. (1999) <i><a href="http://mitpress.mit.edu/book-home.tcl?isbn=0262571323" class="external text" title="http://mitpress.mit.edu/book-home.tcl?isbn=0262571323" rel="nofollow">Using MPI, 2nd Edition</a>: portable Parallel Programming with the Message Passing Interface.</i> MIT Press In Scientific And Engineering Computation Series, Cambridge, MA, USA. 395 pp. <a href="/wiki/Special:BookSources/9780262571326" class="internal">ISBN 978-0-262-57132-6</a></li>
<li>Gropp, William; R Thakur, E Lusk (1999) <i>Using MPI-2: Advanced Features of the Message Passing Interface</i> - MIT Press Cambridge, MA, USA <a href="/wiki/Special:BookSources/0262571331" class="internal">ISBN 0-262-57133-1</a></li>
</ul>
</li>
</ul>
<ul>
<li>Gropp, William; Lusk, Ewing; Skjellum, Anthony. (1996) <i><a href="http://www-unix.mcs.anl.gov/mpi/papers/archive/impls/mpich.ps" class="external text" title="http://www-unix.mcs.anl.gov/mpi/papers/archive/impls/mpich.ps" rel="nofollow">A High-Performance, Portable Implementation of the MPI Message Passing Interface</a></i> Parallel Computing</li>
<li>Pacheco, Peter S. (1997) <i><a href="http://books.google.it/books?&amp;id=tCVkM1z2aOoC" class="external text" title="http://books.google.it/books?&amp;id=tCVkM1z2aOoC" rel="nofollow">Parallel Programming with MPI</a></i>.<a href="http://www.cs.usfca.edu/mpi/" class="external autonumber" title="http://www.cs.usfca.edu/mpi/" rel="nofollow">[3]</a> 500 pp. Morgan Kaufmann <a href="/wiki/Special:BookSources/1558603395" class="internal">ISBN 1558603395</a>.</li>
</ul>
<ul>
<li><i>MPI—The Complete Reference</i> series:
<ul>
<li>Snir, Marc; Otto, Steve; Huss-Lederman, Steven; Walker, David; Dongarra, Jack (1995) <i><a href="http://www.netlib.org/utk/papers/mpi-book/mpi-book.html" class="external text" title="http://www.netlib.org/utk/papers/mpi-book/mpi-book.html" rel="nofollow">MPI: The Complete Reference</a></i>. MIT Press Cambridge, MA, USA. <a href="/wiki/Special:BookSources/0262692155" class="internal">ISBN 0-262-69215-5</a></li>
<li>M Snir, SW Otto, S Huss-Lederman, DW Walker, J (1998) <i>MPI—The Complete Reference: Volume 1, The MPI Core</i>. MIT Press, Cambridge, MA. <a href="/wiki/Special:BookSources/0262692155" class="internal">ISBN 0-262-69215-5</a></li>
<li>Gropp, William; Steven Huss-Lederman, Andrew Lumsdaine, Ewing Lusk, Bill Nitzberg, William Saphir, and Marc Snir (1998) <i><a href="http://mitpress.mit.edu/book-home.tcl?isbn=0262571234" class="external text" title="http://mitpress.mit.edu/book-home.tcl?isbn=0262571234" rel="nofollow">MPI—The Complete Reference: Volume 2, The MPI-2 Extensions</a></i>. MIT Press, Cambridge, MA <a href="/wiki/Special:BookSources/9780262571234" class="internal">ISBN 9780262571234</a></li>
</ul>
</li>
<li>Parallel Processing via MPI &amp; OpenMP, M. Firuziaan, O. Nommensen. Linux Enterprise, 10/2002</li>
<li>Vanneschi, Marco (1999) <i>Parallel paradigms for scientific computing</i> In Proc. of the European School on Computational Chemistry (1999, Perugia, Italy), number 75 in <i><a href="http://books.google.com/books?&amp;id=zMqVdFgVnrgC" class="external text" title="http://books.google.com/books?&amp;id=zMqVdFgVnrgC" rel="nofollow">Lecture Notes in Chemistry</a></i>, pages 170–183. Springer, 2000.</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit&amp;section=25" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://www.mpi-forum.org/" class="external text" title="http://www.mpi-forum.org/" rel="nofollow">MPI Forum</a> and <a href="http://www.mpi-forum.org/docs/" class="external text" title="http://www.mpi-forum.org/docs/" rel="nofollow">MPI specification</a></li>
<li><a href="http://www.dmoz.org/Computers/Parallel_Computing/Programming/Libraries/MPI/" class="external text" title="http://www.dmoz.org/Computers/Parallel_Computing/Programming/Libraries/MPI/" rel="nofollow">Message Passing Interface</a> at the <a href="/wiki/Open_Directory_Project" title="Open Directory Project">Open Directory Project</a></li>
<li><a href="http://www.open-mpi.org/" class="external text" title="http://www.open-mpi.org/" rel="nofollow">Open MPI web site</a></li>
<li><a href="http://www.lam-mpi.org/" class="external text" title="http://www.lam-mpi.org/" rel="nofollow">LAM/MPI web site</a></li>
<li><a href="http://www-unix.mcs.anl.gov/mpi/mpich/" class="external text" title="http://www-unix.mcs.anl.gov/mpi/mpich/" rel="nofollow">MPICH</a></li>
<li><a href="http://www.pccluster.org/" class="external text" title="http://www.pccluster.org/" rel="nofollow">SCore MPI</a></li>
<li><a href="http://www.scali.com/" class="external text" title="http://www.scali.com/" rel="nofollow">Scali MPI</a></li>
<li><a href="http://www.purempi.net" class="external text" title="http://www.purempi.net" rel="nofollow">MPI .NET</a></li>
<li><a href="http://nowlab.cse.ohio-state.edu/projects/mpi-iba/" class="external text" title="http://nowlab.cse.ohio-state.edu/projects/mpi-iba/" rel="nofollow">MVAPICH: MPI over InfiniBand</a></li>
<li><a href="http://www.parawiki.org/index.php/MPI" class="external text" title="http://www.parawiki.org/index.php/MPI" rel="nofollow">Parawiki page for MPI</a></li>
<li><a href="http://www.emsl.pnl.gov/docs/global/ga.html" class="external text" title="http://www.emsl.pnl.gov/docs/global/ga.html" rel="nofollow">Global Arrays</a></li>
<li><a href="http://www.pvmmpi06.org/" class="external text" title="http://www.pvmmpi06.org/" rel="nofollow">PVM/MPI Users' Group Meeting (2006 edition)</a></li>
<li>MPI tutorials and other documents: <a href="http://www-unix.mcs.anl.gov/mpi/learning.html" class="external autonumber" title="http://www-unix.mcs.anl.gov/mpi/learning.html" rel="nofollow">[4]</a>, <a href="http://www-unix.mcs.anl.gov/mpi/tutorial/mpiexmpl/src/hellow/C/main.html" class="external text" title="http://www-unix.mcs.anl.gov/mpi/tutorial/mpiexmpl/src/hellow/C/main.html" rel="nofollow">MPI Samples</a>, <a href="http://www.supercomputingsimplified.com" class="external text" title="http://www.supercomputingsimplified.com" rel="nofollow">Supercomputing Simplified</a>,<a href="http://www.personal.leeds.ac.uk/~bgy1mm/MPITutorial/MPIHome.html" class="external text" title="http://www.personal.leeds.ac.uk/~bgy1mm/MPITutorial/MPIHome.html" rel="nofollow">MPI Tutorial</a></li>
<li><a href="http://www.myri.com/scs/download-mpichgm.html" class="external text" title="http://www.myri.com/scs/download-mpichgm.html" rel="nofollow">MPICH over Myrinet (GM, classic driver)</a></li>
<li><a href="http://www.myri.com/scs/download-mpichmx.html" class="external text" title="http://www.myri.com/scs/download-mpichmx.html" rel="nofollow">MPICH over Myrinet (MX, next-gen driver)</a></li>
<li><a href="http://www.ll.mit.edu/MatlabMPI/" class="external text" title="http://www.ll.mit.edu/MatlabMPI/" rel="nofollow">Parallel Programming with MatlabMPI</a></li>
<li><a href="http://atc.ugr.es/javier-bin/mpitb" class="external text" title="http://atc.ugr.es/javier-bin/mpitb" rel="nofollow">MPI Toolbox for Octave (MPITB)</a> Parallel Computing using GNU Octave</li>
<li><a href="http://paradiseo.gforge.inria.fr/" class="external text" title="http://paradiseo.gforge.inria.fr/" rel="nofollow">Parallelize and distribute many optimization technics using MPI with ParadisEO</a></li>
<li><a href="http://exodus.physics.ucla.edu/appleseed/dev/developer.html" class="external text" title="http://exodus.physics.ucla.edu/appleseed/dev/developer.html" rel="nofollow">MacMPI</a></li>
<li><a href="http://www.cs.ubc.ca/labs/dsg/mpi-sctp/" class="external text" title="http://www.cs.ubc.ca/labs/dsg/mpi-sctp/" rel="nofollow">MPI over SCTP</a></li>
<li><a href="http://dps.epfl.ch/" class="external text" title="http://dps.epfl.ch/" rel="nofollow">Dynamic Parallel Schedules (DPS)</a> is an open source high-level framework in C++ on top of MPI</li>
<li><a href="http://ipython.scipy.org/moin/Parallel_Computing" class="external text" title="http://ipython.scipy.org/moin/Parallel_Computing" rel="nofollow">IPython</a> allows MPI applications to be steered interactively.</li>
<li><a href="http://www.udaparts.com/document/articles/snpisec.htm" class="external text" title="http://www.udaparts.com/document/articles/snpisec.htm" rel="nofollow">Sample code for parallel computing on windows with load balancing and disaster recovery</a></li>
<li><a href="http://mpj-express.org" class="external text" title="http://mpj-express.org" rel="nofollow">MPJ Express</a> An Implementation of MPI-like bindings in Java</li>
<li><a href="http://mpip.sourceforge.net/" class="external text" title="http://mpip.sourceforge.net/" rel="nofollow">mpiP</a> is an open-source, lightweight, scalable MPI profiling tool.</li>
<li><a href="http://www.redbooks.ibm.com/redbooks/pdfs/sg245380.pdf" class="external text" title="http://www.redbooks.ibm.com/redbooks/pdfs/sg245380.pdf" rel="nofollow">IBM Redbook: "RS/6000 SP: Practical MPI Programming"</a></li>
</ul>
<table class="navbox" cellspacing="0" style=";">
<tr>
<td style="padding:2px;">
<table cellspacing="0" class="nowraplinks collapsible autocollapse" style="width:100%;background:transparent;color:inherit;;">
<tr>
<th style=";" colspan="2" class="navbox-title">
<div style="float:left; width:6em;text-align:left;">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;;;border:none;; font-size:xx-small;"><a href="/wiki/Template:Parallel_computing" title="Template:Parallel computing"><span title="View this template" style=";;border:none;">v</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/wiki/Template_talk:Parallel_computing" title="Template talk:Parallel computing"><span title="Discussion about this template" style=";;border:none;">d</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="http://en.wikipedia.org/w/index.php?title=Template:Parallel_computing&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Template:Parallel_computing&amp;action=edit" rel="nofollow"><span title="Edit this template" style=";;border:none;;">e</span></a></div>
</div>
<span style="font-size:110%;"><a href="/wiki/Parallel_computing" title="Parallel computing">Parallel computing</a> topics</span></th>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">General</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/High-performance_computing" title="High-performance computing">High-performance computing</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Cluster_(computing)" title="Cluster (computing)">Cluster computing</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Distributed_computing" title="Distributed computing">Distributed computing</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Grid_computing" title="Grid computing">Grid computing</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Parallelism (levels)</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Bit-level_parallelism" title="Bit-level parallelism">Bit</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Instruction_level_parallelism" title="Instruction level parallelism">Instruction</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Data_parallelism" title="Data parallelism">Data</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Task_parallelism" title="Task parallelism">Task</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Threads</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Super-threading" title="Super-threading">Superthreading</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Hyper-threading" title="Hyper-threading">Hyperthreading</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Theory</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Amdahl%27s_law" title="Amdahl's law">Amdahl's law</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Gustafson%27s_law" title="Gustafson's law">Gustafson's law</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Cost_efficiency" title="Cost efficiency">Cost efficiency</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Karp-Flatt_metric" title="Karp-Flatt metric">Karp-Flatt metric</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Parallel_slowdown" title="Parallel slowdown">slowdown</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Speedup" title="Speedup">speedup</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Elements</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Process_(computing)" title="Process (computing)">Process</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Thread_(computer_science)" title="Thread (computer science)">Thread</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Fiber_(computer_science)" title="Fiber (computer science)">Fiber</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Parallel_Random_Access_Machine" title="Parallel Random Access Machine">PRAM</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Coordination</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Multiprocessing" title="Multiprocessing">Multiprocessing</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Multithreading_(computer_hardware)" title="Multithreading (computer hardware)">Multithreading</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Memory_coherence" title="Memory coherence">Memory coherency</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Cache_coherency" title="Cache coherency" class="mw-redirect">Cache coherency</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Barrier_(computer_science)" title="Barrier (computer science)">Barrier</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Synchronization_(computer_science)" title="Synchronization (computer science)">Synchronization</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Application_checkpointing" title="Application checkpointing">Application checkpointing</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Computer_programming" title="Computer programming">Programming</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Parallel_programming_model" title="Parallel programming model">Models</a> (<a href="/wiki/Implicit_parallelism" title="Implicit parallelism">Implicit parallelism</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Explicit_parallelism" title="Explicit parallelism">Explicit parallelism</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Concurrency_(computer_science)" title="Concurrency (computer science)">Concurrency</a>) <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Flynn%27s_taxonomy" title="Flynn's taxonomy">Flynn's taxonomy</a> <small>(<a href="/wiki/SISD" title="SISD">SISD</a>&#160;• <a href="/wiki/SIMD" title="SIMD">SIMD</a>&#160;• <a href="/wiki/MISD" title="MISD">MISD</a>&#160;• <a href="/wiki/MIMD" title="MIMD">MIMD</a>)</small></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Computer_hardware" title="Computer hardware" class="mw-redirect">Hardware</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em">Multiprocessing (<a href="/wiki/Symmetric_multiprocessing" title="Symmetric multiprocessing">Symmetric</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Asymmetric_multiprocessing" title="Asymmetric multiprocessing">Asymmetric</a>) <span style="font-weight:bold;">&#160;·</span> Memory (<a href="/wiki/Non-Uniform_Memory_Access" title="Non-Uniform Memory Access">NUMA</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Cache_only_memory_architecture" title="Cache only memory architecture">COMA</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Distributed_memory" title="Distributed memory">distributed</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Shared_memory" title="Shared memory">shared</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Distributed_shared_memory" title="Distributed shared memory">distributed shared</a>) <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Simultaneous_multithreading" title="Simultaneous multithreading">SMT</a><br />
<a href="/wiki/Massive_parallel_processing" title="Massive parallel processing">MPP</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Superscalar" title="Superscalar">Superscalar</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Vector_processor" title="Vector processor">Vector processor</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Supercomputer" title="Supercomputer">Supercomputer</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Beowulf_(computing)" title="Beowulf (computing)"><i>Beowulf</i></a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Application_programming_interface" title="Application programming interface">APIs</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/POSIX_Threads" title="POSIX Threads">POSIX Threads</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/OpenMP" title="OpenMP">OpenMP</a><span style="font-weight:bold;">&#160;·</span> <strong class="selflink">MPI</strong><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Unified_Parallel_C" title="Unified Parallel C">UPC</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Intel_Threading_Building_Blocks" title="Intel Threading Building Blocks">Intel Threading Building Blocks</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Boost_C%2B%2B_Libraries#Multithreading_.E2.80.93_Boost.Thread" title="Boost C++ Libraries">Boost.Thread</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Global_Arrays" title="Global Arrays">Global Arrays</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Charm%2B%2B" title="Charm++">Charm++</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Cilk" title="Cilk">Cilk</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Problems</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Embarrassingly_parallel" title="Embarrassingly parallel">Embarrassingly parallel</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Grand_Challenge_problem" title="Grand Challenge problem">Grand Challenge</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Software_lockout" title="Software lockout">Software lockout</a></div>
</td>
</tr>
</table>
</td>
</tr>
</table>


<!-- 
NewPP limit report
Preprocessor node count: 1622/1000000
Post-expand include size: 44753/2048000 bytes
Template argument size: 13929/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:221466-0!1!0!default!!en!2 and timestamp 20090415080753 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Message_Passing_Interface">http://en.wikipedia.org/wiki/Message_Passing_Interface</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Parallel_computing" title="Category:Parallel computing">Parallel computing</a></span> | <span dir='ltr'><a href="/wiki/Category:Application_programming_interfaces" title="Category:Application programming interfaces">Application programming interfaces</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Wikipedia_external_links_cleanup" title="Category:Wikipedia external links cleanup">Wikipedia external links cleanup</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_to_be_expanded_since_June_2008" title="Category:Articles to be expanded since June 2008">Articles to be expanded since June 2008</a></span> | <span dir='ltr'><a href="/wiki/Category:All_articles_to_be_expanded" title="Category:All articles to be expanded">All articles to be expanded</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_that_may_contain_original_research_since_May_2008" title="Category:Articles that may contain original research since May 2008">Articles that may contain original research since May 2008</a></span> | <span dir='ltr'><a href="/wiki/Category:All_articles_that_may_contain_original_research" title="Category:All articles that may contain original research">All articles that may contain original research</a></span> | <span dir='ltr'><a href="/wiki/Category:Wikipedia_articles_incorporating_text_from_FOLDOC" title="Category:Wikipedia articles incorporating text from FOLDOC">Wikipedia articles incorporating text from FOLDOC</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Message_Passing_Interface" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Message_Passing_Interface" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Message_Passing_Interface&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Message_Passing_Interface&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Message_Passing_Interface" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Message_Passing_Interface" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Message_Passing_Interface" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Message_Passing_Interface&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Message_Passing_Interface&amp;oldid=283039158" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Message_Passing_Interface&amp;id=283039158">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D8%A5%D9%85_%D8%A8%D9%8A_%D8%A2%D9%8A">العربية</a></li>
				<li class="interwiki-cs"><a href="http://cs.wikipedia.org/wiki/Message_Passing_Interface">Česky</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Message_Passing_Interface">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Interfaz_de_Paso_de_Mensajes">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Message_Passing_Interface">Français</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Message_Passing_Interface">Italiano</a></li>
				<li class="interwiki-lt"><a href="http://lt.wikipedia.org/wiki/Message_Passing_Interface">Lietuvių</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/Message_Passing_Interface">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Message_Passing_Interface">Polski</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/Message_Passing_Interface">Português</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/Message_Passing_Interface">Русский</a></li>
				<li class="interwiki-sk"><a href="http://sk.wikipedia.org/wiki/Message_Passing_Interface">Slovenčina</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/MPI">Tiếng Việt</a></li>
				<li class="interwiki-tr"><a href="http://tr.wikipedia.org/wiki/MPI">Türkçe</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 10 April 2009, at 20:04 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv198 in 0.046 secs. --></body></html>
