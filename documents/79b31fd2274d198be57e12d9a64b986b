<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Perceptron,AI Winter,Artificial neural network,Bernard Widrow,Binary,Binary Space Partition,Cornell Aeronautical Laboratory,Data set,Decision boundary,Dot product,Eigenvalue" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Perceptron&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Perceptron&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Perceptron - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Perceptron";
		var wgTitle = "Perceptron";
		var wgAction = "view";
		var wgArticleId = "172777";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282228681;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Perceptron skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Perceptron</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>The <b>perceptron</b> is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> invented in 1957 at the <a href="/wiki/Cornell_Aeronautical_Laboratory" title="Cornell Aeronautical Laboratory" class="mw-redirect">Cornell Aeronautical Laboratory</a> by <a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a>. It can be seen as the simplest kind of <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural network</a>: a <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Definition"><span class="tocnumber">1</span> <span class="toctext">Definition</span></a></li>
<li class="toclevel-1"><a href="#Learning_algorithm"><span class="tocnumber">2</span> <span class="toctext">Learning algorithm</span></a></li>
<li class="toclevel-1"><a href="#Variants"><span class="tocnumber">3</span> <span class="toctext">Variants</span></a></li>
<li class="toclevel-1"><a href="#Example"><span class="tocnumber">4</span> <span class="toctext">Example</span></a></li>
<li class="toclevel-1"><a href="#Multiclass_perceptron"><span class="tocnumber">5</span> <span class="toctext">Multiclass perceptron</span></a></li>
<li class="toclevel-1"><a href="#History"><span class="tocnumber">6</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">8</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Definition" id="Definition"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=1" title="Edit section: Definition">edit</a>]</span> <span class="mw-headline">Definition</span></h2>
<p>The Perceptron uses <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> <a href="/wiki/Eigenvalue" title="Eigenvalue" class="mw-redirect">eigenvalues</a> to represent feedforward neural networks and is a binary classifier that maps its input <span class="texhtml"><i>x</i></span> (a real-valued <a href="/wiki/Vector_space" title="Vector space">vector</a>) to an output value <span class="texhtml"><i>f</i>(<i>x</i>)</span> (a single <a href="/wiki/Binary" title="Binary">binary</a> value) across the matrix.</p>
<dl>
<dd><img class="tex" alt="
f(x) = \begin{cases}1 &amp; \text{if }w \cdot x + b &gt; 0\\0 &amp; \text{else}\end{cases}
" src="http://upload.wikimedia.org/math/5/0/e/50ef14a2714fdf22838ffd68154c5c90.png" /></dd>
</dl>
<p>where <span class="texhtml"><i>w</i></span> is a vector of real-valued weights and <img class="tex" alt="w \cdot x" src="http://upload.wikimedia.org/math/2/b/6/2b6f207a3abc53dc275356f5b7f67d12.png" /> is the <a href="/wiki/Dot_product" title="Dot product">dot product</a> (which computes a weighted sum). <span class="texhtml"><i>b</i></span> is the 'bias', a constant term that does not depend on any input value.</p>
<p>The value of <span class="texhtml"><i>f</i>(<i>x</i>)</span> (0 or 1) is used to classify <span class="texhtml"><i>x</i></span> as either a positive or a negative instance, in the case of a binary classification problem. The bias can be thought of as offsetting the activation function, or giving the output neuron a "base" level of activity. If <span class="texhtml"><i>b</i></span> is negative, then the weighted combination of inputs must produce a positive value greater than <span class="texhtml">− <i>b</i></span> in order to push the classifier neuron over the 0 threshold. Spatially, the bias alters the position (though not the orientation) of the <a href="/wiki/Decision_boundary" title="Decision boundary">decision boundary</a>.</p>
<p>Since the inputs are fed directly to the output unit via the weighted connections, the perceptron can be considered the simplest kind of feed-forward neural network.</p>
<p><a name="Learning_algorithm" id="Learning_algorithm"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=2" title="Edit section: Learning algorithm">edit</a>]</span> <span class="mw-headline">Learning algorithm</span></h2>
<p>The learning algorithm is the same across all neurons, therefore everything that follows is applied to a single neuron in isolation. We first define some variables:</p>
<ul>
<li><span class="texhtml"><i>x</i>(<i>j</i>)</span> denotes the j-th item in the n-dimensional input vector</li>
<li><span class="texhtml"><i>w</i>(<i>j</i>)</span> denotes the j-th item in the weight vector</li>
<li><span class="texhtml"><i>f</i>(<i>x</i>)</span> denotes the output from the neuron when presented with input <span class="texhtml"><i>x</i></span></li>
<li><span class="texhtml">α</span> is a constant where <img class="tex" alt="0 &lt; \alpha \leq 1" src="http://upload.wikimedia.org/math/0/4/f/04f919727c7c336763e9ec25d482b040.png" /> (learning rate)</li>
</ul>
<p>Further, assume for convenience that the bias term <span class="texhtml"><i>b</i></span> is zero. This is not a restriction since an extra dimension <span class="texhtml"><i>n</i> + 1</span> can be added to the input vectors x with <span class="texhtml"><i>x</i>(<i>n</i> + 1) = 1</span>, in which case <span class="texhtml"><i>w</i>(<i>n</i> + 1)</span> replaces the bias term.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:302px;"><a href="/wiki/File:Perceptron.svg" class="image" title="the appropriate weights are applied to the inputs, and the resulting weighted sum passed to a function which produces the output y"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/thumb/3/31/Perceptron.svg/300px-Perceptron.svg.png" width="300" height="199" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Perceptron.svg" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
the appropriate weights are applied to the inputs, and the resulting weighted sum passed to a function which produces the output y</div>
</div>
</div>
<p><br />
Learning is modeled as the weight vector being updated for multiple iterations over all training examples. Let <img class="tex" alt="D_m = \{(x_1,y_1),\dots,(x_m,y_m)\}" src="http://upload.wikimedia.org/math/e/3/e/e3ed2ee168a0e2768e1bfd1a2cd5d67e.png" /> denote a training set of <span class="texhtml"><i>m</i></span> training examples.</p>
<p>Each iteration the weight vector is updated as follows:</p>
<p>For each <span class="texhtml">(<i>x</i>,<i>y</i>)</span> pair in <img class="tex" alt="D_m = \{(x_1,y_1),\dots,(x_m,y_m)\}" src="http://upload.wikimedia.org/math/e/3/e/e3ed2ee168a0e2768e1bfd1a2cd5d67e.png" /></p>
<dl>
<dd><img class="tex" alt="w(j)&#160;:= w(j) + {\alpha(y-f(x))}{x(j)} \quad (j=1,\ldots,n)" src="http://upload.wikimedia.org/math/9/6/7/967001e06bfdb1cd41833d7140d870aa.png" /></dd>
</dl>
<p>Note that this means that a change in the weight vector will only take place for a given training example <span class="texhtml">(<i>x</i>,<i>y</i>)</span> if its output <span class="texhtml"><i>f</i>(<i>x</i>)</span> is different from the desired output <span class="texhtml"><i>y</i></span>.</p>
<p>The initialization of <span class="texhtml"><i>w</i></span> is usually performed simply by setting <span class="texhtml"><i>w</i>(<i>j</i>): = 0</span> for all elements <span class="texhtml"><i>w</i>(<i>j</i>)</span>.</p>
<p>The training set <span class="texhtml"><i>D</i><sub><i>m</i></sub></span> is said to be <a href="/wiki/Linearly_separable" title="Linearly separable" class="mw-redirect">linearly separable</a> if there exists a positive constant <span class="texhtml">γ</span> and a weight vector <span class="texhtml"><i>w</i></span> such that <img class="tex" alt="y_i \cdot\left( \langle w, x_i \rangle +b \right) &gt; \gamma " src="http://upload.wikimedia.org/math/b/1/e/b1e44610f4106097f2f068d61c547f54.png" /> for all <span class="texhtml"><i>i</i></span>. Novikoff (1962) proved that the perceptron algorithm converges after a finite number of iterations if the <a href="/wiki/Data_set" title="Data set">data set</a> is linearly separable and the number of mistakes is bounded by <img class="tex" alt="\left(\frac{2R}{\gamma}\right)^2" src="http://upload.wikimedia.org/math/e/6/b/e6bb53d8e7415a6c94b8084130a72c78.png" /> where <span class="texhtml"><i>R</i></span> the maximum norm of an input vector.</p>
<p>However, if the training set is not <a href="/wiki/Linearly_separable" title="Linearly separable" class="mw-redirect">linearly separable</a>, the above online algorithm is not guaranteed to converge.</p>
<p>Note that the decision boundary of a perceptron is invariant with respect to scaling of the weight vector, i.e. a perceptron trained with initial weight vector <span class="texhtml"><i>w</i></span> and learning rate <span class="texhtml">α</span> is an identical estimator to a perceptron trained with initial weight vector <span class="texhtml"><i>w</i> / α</span> and learning rate 1. Thus, since the initial weights become irrelevant with increasing number of iterations, the learning rate does not matter in the case of the perceptron and is usually just set to one.</p>
<p><a name="Variants" id="Variants"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=3" title="Edit section: Variants">edit</a>]</span> <span class="mw-headline">Variants</span></h2>
<p>The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far "in its pocket". The pocket algorithm then returns the solution in the pocket, rather than the last solution.</p>
<p>The <span class="texhtml">α</span>-perceptron further utilised a preprocessing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify <a href="http://en.wiktionary.org/wiki/analogue" class="extiw" title="wiktionary:analogue">analogue</a> patterns, by projecting them into a <a href="/wiki/Binary_Space_Partition" title="Binary Space Partition" class="mw-redirect">binary space</a>. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.</p>
<p>As an example, consider the case of having to classify data into two classes. Here is a small such data set, consisting of two points coming from two <a href="/wiki/Gaussian_distribution" title="Gaussian distribution" class="mw-redirect">Gaussian distributions</a>.</p>
<table class="gallery" cellspacing="0" cellpadding="0">
<tr>
<td>
<div class="gallerybox" style="width: 155px;">
<div class="thumb" style="padding: 31px 0; width: 150px;">
<div style="margin-left: auto; margin-right: auto; width: 120px;"><a href="/wiki/File:Two_class_Gaussian_data.png" class="image" title="Two class Gaussian data.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Two_class_Gaussian_data.png/120px-Two_class_Gaussian_data.png" width="120" height="84" border="0" /></a></div>
</div>
<div class="gallerytext">
<p>Two class gaussian data</p>
</div>
</div>
</td>
<td>
<div class="gallerybox" style="width: 155px;">
<div class="thumb" style="padding: 31px 0; width: 150px;">
<div style="margin-left: auto; margin-right: auto; width: 120px;"><a href="/wiki/File:Linear_classifier_on_Gaussian_data.png" class="image" title="Linear classifier on Gaussian data.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Linear_classifier_on_Gaussian_data.png/120px-Linear_classifier_on_Gaussian_data.png" width="120" height="84" border="0" /></a></div>
</div>
<div class="gallerytext">
<p>A linear classifier operating on the original space</p>
</div>
</div>
</td>
<td>
<div class="gallerybox" style="width: 155px;">
<div class="thumb" style="padding: 31px 0; width: 150px;">
<div style="margin-left: auto; margin-right: auto; width: 120px;"><a href="/wiki/File:Hidden_space_linear_classifier_on_Gaussian_data.png" class="image" title="Hidden space linear classifier on Gaussian data.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Hidden_space_linear_classifier_on_Gaussian_data.png/120px-Hidden_space_linear_classifier_on_Gaussian_data.png" width="120" height="84" border="0" /></a></div>
</div>
<div class="gallerytext">
<p>A linear classifier operating on a high-dimensional projection</p>
</div>
</div>
</td>
</tr>
</table>
<p>A linear classifier can only separate things with a <a href="/wiki/Hyperplane" title="Hyperplane">hyperplane</a>, so it's not possible to classify all the examples perfectly. On the other hand, we may project the data into a large number of dimensions. In this case a <a href="/wiki/Random_matrix" title="Random matrix">random matrix</a> was used to project the data linearly to a 1000-dimensional space; then each resulting data point was transformed through the <a href="/wiki/Hyperbolic_function" title="Hyperbolic function">hyperbolic tangent function</a>. A linear classifier can then separate the data, as shown in the third figure. However the data may still not be completely separable in this space, in which the perceptron algorithm would not converge. In the example shown, <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic steepest gradient descent</a> was used to adapt the parameters.</p>
<p>Furthermore, by adding nonlinear layers between the input and output, one can separate all data and indeed, with enough training data, model any well-defined function to arbitrary precision. This model is a generalization known as a <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptron</a>.</p>
<p>It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal.</p>
<p>Other training algorithms for linear classifiers are possible: see, e.g., <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a> and <a href="/wiki/Logistic_regression" title="Logistic regression">logistic regression</a>.</p>
<p><a name="Example" id="Example"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=4" title="Edit section: Example">edit</a>]</span> <span class="mw-headline">Example</span></h2>
<p>A perceptron (X<sub>1</sub>, X<sub>2</sub> input, X<sub>0</sub>*W<sub>0</sub>=b, TH=0.5) learns how to perform a <a href="/wiki/NAND" title="NAND" class="mw-redirect">NAND</a> function:</p>
<table class="wikitable">
<tr>
<td width="50" height="13" valign="bottom"></td>
<td width="50" valign="bottom"></td>
<td width="50" colspan="4" align="center" valign="bottom">Input</td>
<td width="40" colspan="3" align="center" valign="bottom">Initial</td>
<td width="40" colspan="3" align="center" valign="bottom">Output</td>
<td width="35" valign="bottom"></td>
<td width="70" valign="bottom"></td>
<td width="35" valign="bottom"></td>
<td width="70" valign="bottom"></td>
<td width="40" colspan="3" align="center" valign="bottom">Final</td>
</tr>
<tr>
<td height="40" valign="bottom">Threshold</td>
<td valign="bottom">Learning Rate</td>
<td colspan="3" align="center" valign="bottom">Sensor values</td>
<td valign="bottom">Desired output</td>
<td colspan="3" align="center" valign="bottom">Weights</td>
<td colspan="3" align="center" valign="bottom">Calculated</td>
<td align="center" valign="bottom">Sum</td>
<td align="center" valign="bottom">Network</td>
<td align="center" valign="bottom">Error</td>
<td align="center" valign="bottom">Correction</td>
<td colspan="3" align="center" valign="bottom">Weights</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">TH</td>
<td>LR</td>
<td>X0</td>
<td>X1</td>
<td>X2</td>
<td>Z</td>
<td>w0</td>
<td>w1</td>
<td>w2</td>
<td>C0</td>
<td>C1</td>
<td>C2</td>
<td>S</td>
<td>N</td>
<td>E</td>
<td>R</td>
<td>W0</td>
<td>W1</td>
<td>W2</td>
</tr>
<tr align="center" valign="bottom">
<td height="10"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>X0 x w0</td>
<td>X1 x w1</td>
<td>X2 x w2</td>
<td>C0+C1+C2</td>
<td>IF(S&gt;TH,1,0)</td>
<td>Z-N</td>
<td>LR x E</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.1</td>
<td>0</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.1</td>
<td>0</td>
<td>0</td>
<td>0.1</td>
<td>0</td>
<td>0</td>
<td>0.1</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.2</td>
<td>0</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0.2</td>
<td>0</td>
<td>0.1</td>
<td>0.2</td>
<td>0</td>
<td>0</td>
<td>0.2</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.3</td>
<td>0.1</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0.3</td>
<td>0.1</td>
<td>0.1</td>
<td>0.3</td>
<td>0.1</td>
<td>0.1</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.3</td>
<td>0.1</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0.3</td>
<td>0.1</td>
<td>0.1</td>
<td>0.3</td>
<td>0</td>
<td>0</td>
<td>0.3</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.4</td>
<td>0.1</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.4</td>
<td>0.1</td>
<td>0.1</td>
<td>0.4</td>
<td>0</td>
<td>0.1</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.5</td>
<td>0.1</td>
<td>0.2</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.1</td>
<td>0.2</td>
<td>0.5</td>
<td>0.1</td>
<td>0</td>
<td>0.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0.1</td>
<td>0.2</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0.5</td>
<td>0.1</td>
<td>0.2</td>
<td>0.5</td>
<td>0.1</td>
<td>0.2</td>
<td>0.8</td>
<td>1</td>
<td>-1</td>
<td>-0.1</td>
<td>0.4</td>
<td>0</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0.4</td>
<td>0</td>
<td>0.1</td>
<td>0.4</td>
<td>0</td>
<td>0</td>
<td>0.4</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.5</td>
<td>0</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.5</td>
<td>0</td>
<td>0.1</td>
<td>0.5</td>
<td>0</td>
<td>0.1</td>
<td>0.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0</td>
<td>0.1</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.6</td>
<td>0.1</td>
<td>0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0.6</td>
<td>0.1</td>
<td>0.1</td>
<td>0.6</td>
<td>0.1</td>
<td>0.1</td>
<td>0.8</td>
<td>1</td>
<td>-1</td>
<td>-0.1</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>1</td>
<td>-1</td>
<td>-0.1</td>
<td>0.5</td>
<td>-0.1</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>-0.1</td>
<td>-0.1</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.6</td>
<td>-0.1</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.6</td>
<td>-0.1</td>
<td>-0.1</td>
<td>0.6</td>
<td>0</td>
<td>-0.1</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.7</td>
<td>-0.1</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0.7</td>
<td>-0.1</td>
<td>0</td>
<td>0.7</td>
<td>-0.1</td>
<td>0</td>
<td>0.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.7</td>
<td>-0.1</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0.7</td>
<td>-0.1</td>
<td>0</td>
<td>0.7</td>
<td>-0.1</td>
<td>0</td>
<td>0.6</td>
<td>1</td>
<td>-1</td>
<td>-0.1</td>
<td>0.6</td>
<td>-0.2</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0.6</td>
<td>-0.2</td>
<td>-0.1</td>
<td>0.6</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.6</td>
<td>-0.2</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.6</td>
<td>-0.2</td>
<td>-0.1</td>
<td>0.6</td>
<td>0</td>
<td>-0.1</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.7</td>
<td>-0.2</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0.7</td>
<td>-0.2</td>
<td>0</td>
<td>0.7</td>
<td>-0.2</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.8</td>
<td>-0.1</td>
<td>0</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0.8</td>
<td>-0.1</td>
<td>0</td>
<td>0.8</td>
<td>-0.1</td>
<td>0</td>
<td>0.7</td>
<td>1</td>
<td>-1</td>
<td>-0.1</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.1</td>
<td>0.7</td>
<td>0</td>
<td>0</td>
<td>0.7</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.1</td>
<td>0.7</td>
<td>0</td>
<td>-0.1</td>
<td>0.6</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.1</td>
<td>0.7</td>
<td>-0.2</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.8</td>
<td>-0.1</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0.8</td>
<td>-0.1</td>
<td>-0.1</td>
<td>0.8</td>
<td>-0.1</td>
<td>-0.1</td>
<td>0.6</td>
<td>1</td>
<td>-1</td>
<td>-0.1</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.2</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.2</td>
<td>0.7</td>
<td>0</td>
<td>0</td>
<td>0.7</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.2</td>
</tr>
<tr align="center" valign="bottom">
<td height="13">0.5</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.7</td>
<td>-0.2</td>
<td>-0.2</td>
<td>0.7</td>
<td>0</td>
<td>-0.2</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
<td>+0.1</td>
<td>0.8</td>
<td>-0.2</td>
<td>-0.1</td>
</tr>
<tr align="center" valign="bottom">
<td height="13"><b>0.5</b></td>
<td><b>0.1</b></td>
<td><b>1</b></td>
<td><b>1</b></td>
<td><b>0</b></td>
<td><b>1</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>0</b></td>
<td><b>0.6</b></td>
<td><b>1</b></td>
<td><b>0</b></td>
<td><b>0</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
</tr>
<tr align="center" valign="bottom">
<td height="13"><b>0.5</b></td>
<td><b>0.1</b></td>
<td><b>1</b></td>
<td><b>1</b></td>
<td><b>1</b></td>
<td><b>0</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
<td><b>0.5</b></td>
<td><b>0</b></td>
<td><b>0</b></td>
<td><b>0</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
</tr>
<tr align="center" valign="bottom">
<td height="13"><b>0.5</b></td>
<td><b>0.1</b></td>
<td><b>1</b></td>
<td><b>0</b></td>
<td><b>0</b></td>
<td><b>1</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
<td><b>0.8</b></td>
<td><b>0</b></td>
<td><b>0</b></td>
<td><b>0.8</b></td>
<td><b>1</b></td>
<td><b>0</b></td>
<td><b>0</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
</tr>
<tr align="center" valign="bottom">
<td height="13"><b>0.5</b></td>
<td><b>0.1</b></td>
<td><b>1</b></td>
<td><b>0</b></td>
<td><b>1</b></td>
<td><b>1</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
<td><b>0.8</b></td>
<td><b>0</b></td>
<td><b>-0.1</b></td>
<td><b>0.7</b></td>
<td><b>1</b></td>
<td><b>0</b></td>
<td><b>0</b></td>
<td><b>0.8</b></td>
<td><b>-0.2</b></td>
<td><b>-0.1</b></td>
</tr>
</table>
<p>Note: Initial weight equals final weight of previous iteration. A too high learning rate makes the perceptron periodically oscillate around the solution. A possible enhancement is to use <span class="texhtml"><i>L</i><i>R</i><sup><i>n</i></sup></span> starting with n=1 and incrementing it by 1 when a loop in learning is found.</p>
<p><a name="Multiclass_perceptron" id="Multiclass_perceptron"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=5" title="Edit section: Multiclass perceptron">edit</a>]</span> <span class="mw-headline">Multiclass perceptron</span></h2>
<p>Like most other techniques for training linear classifiers, the perceptron generalizes naturally to multiclass classification. Here, the input <span class="texhtml"><i>x</i></span> and the output <span class="texhtml"><i>y</i></span> are drawn from arbitrary sets. A feature representation function <span class="texhtml"><i>f</i>(<i>x</i>,<i>y</i>)</span> maps each possible input/output pair to a finite-dimensional real-valued feature vector. As before, the feature vector is multiplied by a weight vector <span class="texhtml"><i>w</i></span>, but now the resulting score is used to choose among many possible outputs:</p>
<dl>
<dd><img class="tex" alt="\hat y = \mathrm{argmax}_y f(x,y) \cdot w" src="http://upload.wikimedia.org/math/8/e/7/8e792b2fd070f2d903bc060bb14d8f53.png" /></dd>
</dl>
<p>Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not. The update becomes:</p>
<dl>
<dd><img class="tex" alt=" w_{t+1} = w_t + f(x, y) - f(x,\hat y)" src="http://upload.wikimedia.org/math/3/6/b/36bb85f4bd6224de533386029bedf9fa.png" /></dd>
</dl>
<p>This multiclass formulation reduces to the original perceptron when <span class="texhtml"><i>x</i></span> is a real-valued vector, <span class="texhtml"><i>y</i></span> is chosen from <span class="texhtml">{0,1}</span>, and <span class="texhtml"><i>f</i>(<i>x</i>,<i>y</i>) = <i>y</i><i>x</i></span>.</p>
<p>For certain problems, input/output representations and features can be chosen so that <img class="tex" alt="\mathrm{argmax}_y f(x,y) \cdot w" src="http://upload.wikimedia.org/math/1/3/6/136d06bdba160d4e3893370bfcf947b4.png" /> can be found efficiently even though <span class="texhtml"><i>y</i></span> is chosen from a very large or even infinite set.</p>
<p>In recent years, perceptron training has become popular in the field of <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> for such tasks as <a href="/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">part-of-speech tagging</a> and <a href="/wiki/Syntactic_parsing" title="Syntactic parsing" class="mw-redirect">syntactic parsing</a> (Collins, 2002).</p>
<p><a name="History" id="History"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=6" title="Edit section: History">edit</a>]</span> <span class="mw-headline">History</span></h2>
<dl>
<dd><i>See also: <a href="/wiki/History_of_artificial_intelligence#Perceptrons_and_the_dark_age_of_connectionism" title="History of artificial intelligence">History of artificial intelligence</a>, <a href="/wiki/AI_Winter#The_abandonment_of_perceptrons_in_1969" title="AI Winter" class="mw-redirect">AI Winter</a> and <a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a></i></dd>
</dl>
<p>Although the perceptron initially seemed promising, it was eventually proved that perceptrons could not be trained to recognise many classes of patterns. This led to the field of neural network research stagnating for many years, before it was recognised that a feedforward neural network with two or more layers (also called a <a href="/wiki/Feedforward_neural_network#Multi-layer_perceptron" title="Feedforward neural network">multilayer perceptron</a>) had far greater processing power than perceptrons with one layer (also called a <a href="/wiki/Feedforward_neural_network#Single-layer_perceptron" title="Feedforward neural network">single layer perceptron</a>). Single layer perceptrons are only capable of learning <a href="/wiki/Linearly_separable" title="Linearly separable" class="mw-redirect">linearly separable</a> patterns; in 1969 a famous book entitled <i><b>Perceptrons</b></i> by <a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a> and <a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a> showed that it was impossible for these classes of network to learn an <a href="/wiki/XOR" title="XOR" class="mw-redirect">XOR</a> function. They conjectured (incorrectly) that a similar result would hold for a perceptron with three or more layers. Three years later <a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a> published a series of papers introducing networks capable of modelling differential, contrast-enhancing and XOR functions. (The papers were published in 1972 and 1973, see e.g.: Grossberg, Contour enhancement, short-term memory, and constancies in reverberating neural networks. Studies in Applied Mathematics, 52 (1973), 213-257, online <a href="http://cns.bu.edu/Profiles/Grossberg/Gro1973StudiesAppliedMath.pdf" class="external autonumber" title="http://cns.bu.edu/Profiles/Grossberg/Gro1973StudiesAppliedMath.pdf" rel="nofollow">[1]</a>). Nevertheless the often-cited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until <a href="/wiki/Neural_network" title="Neural network">neural network</a> research experienced a resurgence in the 1980s. This text was reprinted in 1987 as "Perceptrons - Expanded Edition" where some errors in the original text are shown and corrected.</p>
<p>More recently, interest in the perceptron learning algorithm has increased again after Freund and Schapire (1998) presented a voted formulation of the original algorithm (attaining large margin) and suggested that one can apply the <a href="/wiki/Kernel_trick" title="Kernel trick">kernel trick</a> to it.</p>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=7" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li>Freund, Y. and Schapire, R. E. 1998. Large margin classification using the perceptron algorithm. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT' 98). ACM Press.</li>
<li>Freund, Y. and Schapire, R. E. 1999. <a href="http://www.cs.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf" class="external text" title="http://www.cs.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf" rel="nofollow">Large margin classification using the perceptron algorithm.</a> In Machine Learning 37(3):277-296, 1999.</li>
<li>Gallant, S. I. (1990). <a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=80230" class="external text" title="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=80230" rel="nofollow">Perceptron-based learning algorithms.</a> IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179-191.</li>
<li>Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp. 386-408.</li>
<li>Minsky M L and Papert S A 1969 <i>Perceptrons</i> (Cambridge, MA: MIT Press)</li>
<li>Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615-622. Polytechnic Institute of Brooklyn.</li>
<li><a href="/wiki/Bernard_Widrow" title="Bernard Widrow">Widrow, B.</a>, Lehr, M.A., "30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation," <i>Proc. IEEE</i>, vol 78, no 9, pp. 1415-1442, (1990).</li>
<li><a href="/wiki/Michael_Collins_(computational_linguist)" title="Michael Collins (computational linguist)">Collins, M.</a> 2002. Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP '02)</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Perceptron&amp;action=edit&amp;section=8" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li>Chapter 3 <a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf" class="external text" title="http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf" rel="nofollow">Weighted networks - the perceptron</a> and chapter 4 <a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf" class="external text" title="http://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf" rel="nofollow">Perceptron learning</a> of <a href="http://page.mi.fu-berlin.de/rojas/neural/index.html.html" class="external text" title="http://page.mi.fu-berlin.de/rojas/neural/index.html.html" rel="nofollow"><i>Neural Networks - A Systematic Introduction</i></a> by <a href="/wiki/Ra%C3%BAl_Rojas" title="Raúl Rojas">Raúl Rojas</a> (<a href="/wiki/Special:BookSources/9783540605058" class="internal">ISBN 978-3540605058</a>)</li>
<li><a href="http://www-cse.ucsd.edu/users/elkan/250B/perceptron.pdf" class="external text" title="http://www-cse.ucsd.edu/users/elkan/250B/perceptron.pdf" rel="nofollow">Pithy explanation of the update rule</a> by Charles Elkan</li>
<li><a href="http://dynamicnotions.blogspot.com/2008/09/single-layer-perceptron.html" class="external text" title="http://dynamicnotions.blogspot.com/2008/09/single-layer-perceptron.html" rel="nofollow">C# implementation of a perceptron</a></li>
<li><a href="http://www.csulb.edu/~cwallis/artificialn/History.htm" class="external text" title="http://www.csulb.edu/~cwallis/artificialn/History.htm" rel="nofollow">History of perceptrons</a></li>
<li><a href="http://www.cis.hut.fi/ahonkela/dippa/node41.html" class="external text" title="http://www.cis.hut.fi/ahonkela/dippa/node41.html" rel="nofollow">Mathematics of perceptrons</a></li>
<li><a href="http://library.thinkquest.org/18242/perceptron.shtml" class="external text" title="http://library.thinkquest.org/18242/perceptron.shtml" rel="nofollow">Perceptron demo applet and an introduction by examples</a></li>
</ul>
<p><span id="interwiki-ru-fa"></span></p>


<!-- 
NewPP limit report
Preprocessor node count: 185/1000000
Post-expand include size: 34/2048000 bytes
Template argument size: 2/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:172777-0!1!0!default!!en!2 and timestamp 20090406235823 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Perceptron">http://en.wikipedia.org/wiki/Perceptron</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></span> | <span dir='ltr'><a href="/wiki/Category:Neural_networks" title="Category:Neural networks">Neural networks</a></span> | <span dir='ltr'><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Perceptron" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Perceptron" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Perceptron&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Perceptron&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Perceptron" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Perceptron" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Perceptron" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Perceptron&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Perceptron&amp;oldid=282228681" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Perceptron&amp;id=282228681">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D8%A8%D9%8A%D8%B1%D8%B3%D9%8A%D8%A8%D8%AA%D8%B1%D9%88%D9%86">العربية</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Perzeptron">Deutsch</a></li>
				<li class="interwiki-el"><a href="http://el.wikipedia.org/wiki/Perceptron">Ελληνικά</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Perceptr%C3%B3n">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Perceptron">Français</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0">한국어</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Percettrone">Italiano</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Perceptron">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Perceptron">Polski</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD">Русский</a></li>
				<li class="interwiki-sk"><a href="http://sk.wikipedia.org/wiki/Perceptr%C3%B3n">Slovenčina</a></li>
				<li class="interwiki-sl"><a href="http://sl.wikipedia.org/wiki/Perceptron">Slovenščina</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Perceptron">Svenska</a></li>
				<li class="interwiki-th"><a href="http://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%9E%E0%B8%AD%E0%B8%A3%E0%B9%8C%E0%B9%80%E0%B8%8B%E0%B8%9B%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%99">ไทย</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 6 April 2009, at 23:58.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv180 in 0.052 secs. --></body></html>
