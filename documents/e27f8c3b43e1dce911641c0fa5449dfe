<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Linear least squares,Least squares and regression analysis,Always,Arithmetic mean,B-spline,BHHH algorithm,BSD,BSD licenses,Bayes estimator,Beer-Lambert law,Block matrix" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Linear_least_squares&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Linear_least_squares&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Linear least squares - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Linear_least_squares";
		var wgTitle = "Linear least squares";
		var wgAction = "view";
		var wgArticleId = "484872";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282099812;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Linear_least_squares skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Linear least squares</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<div class="thumb tright">
<div class="thumbinner" style="width:182px;"><a href="/wiki/File:Linear_least_squares2.png" class="image" title="The result of fitting a quadratic function  (in blue) through a set of data points (xi,yi) (in red). In linear least squares the function need not be linear in the argument x, but only in the parameters βj that are determined to give the best fit."><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Linear_least_squares2.png/180px-Linear_least_squares2.png" width="180" height="216" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Linear_least_squares2.png" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
The result of fitting a quadratic function <img class="tex" alt="y=\beta_1+\beta_2x+\beta_3x^2\," src="http://upload.wikimedia.org/math/6/3/e/63e16a1474d14ce1e479e78a072e32b8.png" /> (in blue) through a set of data points <span class="texhtml">(<i>x</i><sub><i>i</i></sub>,<i>y</i><sub><i>i</i></sub>)</span> (in red). In linear least squares the function need not be linear in the argument <span class="texhtml"><i>x</i>,</span> but only in the parameters <span class="texhtml">β<sub><i>j</i></sub></span> that are determined to give the best fit.</div>
</div>
</div>
<p><b>Linear least squares</b> is an important <a href="/wiki/Computation" title="Computation">computational</a> problem, that arises primarily in applications when it is desired to fit a <a href="/wiki/Linear_function" title="Linear function">linear</a> <a href="/wiki/Mathematical_model" title="Mathematical model">mathematical model</a> to <a href="/wiki/Measurement" title="Measurement">measurements</a> obtained from <a href="/wiki/Experiment" title="Experiment">experiments</a>. The goals of linear least squares are to extract predictions from the measurements and to reduce the effect of measurement errors. Mathematically, it can be stated as the problem of finding an approximate solution to an <a href="/wiki/Overdetermined_system" title="Overdetermined system">overdetermined system</a> of linear equations. In <a href="/wiki/Statistics" title="Statistics">statistics</a>, it corresponds to the <a href="/wiki/Maximum_likelihood" title="Maximum likelihood">maximum likelihood</a> estimate for a <a href="/wiki/Linear_model" title="Linear model">linear model</a> with <a href="/wiki/Normal_distribution" title="Normal distribution">normally distributed</a> <a href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">error</a>.</p>
<p>Linear least square problems admit a <a href="/wiki/Closed-form_expression" title="Closed-form expression">closed-form solution</a>, in contrast to <a href="/wiki/Non-linear_least_squares" title="Non-linear least squares">non-linear least squares</a> problems, which often have to be solved by an <a href="/wiki/Iterative_method" title="Iterative method">iterative procedure</a>.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Motivational_example"><span class="tocnumber">1</span> <span class="toctext">Motivational example</span></a>
<ul>
<li class="toclevel-2"><a href="#Computation"><span class="tocnumber">1.1</span> <span class="toctext">Computation</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#The_general_problem"><span class="tocnumber">2</span> <span class="toctext">The general problem</span></a></li>
<li class="toclevel-1"><a href="#Uses_in_data_fitting"><span class="tocnumber">3</span> <span class="toctext">Uses in data fitting</span></a></li>
<li class="toclevel-1"><a href="#Derivation_of_the_normal_equations"><span class="tocnumber">4</span> <span class="toctext">Derivation of the normal equations</span></a></li>
<li class="toclevel-1"><a href="#Computation_2"><span class="tocnumber">5</span> <span class="toctext">Computation</span></a>
<ul>
<li class="toclevel-2"><a href="#Inverting_the_normal_equations"><span class="tocnumber">5.1</span> <span class="toctext">Inverting the normal equations</span></a></li>
<li class="toclevel-2"><a href="#Orthogonal_decomposition_methods"><span class="tocnumber">5.2</span> <span class="toctext">Orthogonal decomposition methods</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Properties_of_the_least-squares_estimators"><span class="tocnumber">6</span> <span class="toctext">Properties of the least-squares estimators</span></a>
<ul>
<li class="toclevel-2"><a href="#Limitations"><span class="tocnumber">6.1</span> <span class="toctext">Limitations</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Weighted_linear_least_squares"><span class="tocnumber">7</span> <span class="toctext">Weighted linear least squares</span></a></li>
<li class="toclevel-1"><a href="#Parameter_errors.2C_correlation_and_confidence_limits"><span class="tocnumber">8</span> <span class="toctext">Parameter errors, correlation and confidence limits</span></a></li>
<li class="toclevel-1"><a href="#Residual_values_and_correlation"><span class="tocnumber">9</span> <span class="toctext">Residual values and correlation</span></a></li>
<li class="toclevel-1"><a href="#Objective_function"><span class="tocnumber">10</span> <span class="toctext">Objective function</span></a></li>
<li class="toclevel-1"><a href="#Typical_uses_and_applications"><span class="tocnumber">11</span> <span class="toctext">Typical uses and applications</span></a></li>
<li class="toclevel-1"><a href="#Software_for_solving_LLSP"><span class="tocnumber">12</span> <span class="toctext">Software for solving LLSP</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">13</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">14</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">15</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Motivational_example" id="Motivational_example"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=1" title="Edit section: Motivational example">edit</a>]</span> <span class="mw-headline">Motivational example</span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:182px;"><a href="/wiki/File:Linear_least_squares_example2.png" class="image" title="A plot of the data points (in red), the least squares line of best fit (in blue), and the residuals (in green)."><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Linear_least_squares_example2.png/180px-Linear_least_squares_example2.png" width="180" height="227" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Linear_least_squares_example2.png" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
A plot of the data points (in red), the least squares line of best fit (in blue), and the residuals (in green).</div>
</div>
</div>
<p>As a result of an experiment, four <span class="texhtml">(<i>x</i>,<i>y</i>)</span> data points were obtained, <span class="texhtml">(1,6),</span> <span class="texhtml">(2,5),</span> <span class="texhtml">(3,7),</span> and <span class="texhtml">(4,10)</span> (shown in red in the picture on the right). It is desired to find a line <span class="texhtml"><i>y</i> = β<sub>1</sub> + β<sub>2</sub><i>x</i></span> that fits "best" these four points. In other words, we would like to find the numbers <span class="texhtml">β<sub>1</sub></span> and <span class="texhtml">β<sub>2</sub></span> that approximately solve the overdetermined linear system</p>
<dl>
<dd><img class="tex" alt="\begin{alignat}{3}
\beta_1  +  1\beta_2 &amp;&amp;\; = \;&amp;&amp; 6 &amp; \\
\beta_1  +  2\beta_2 &amp;&amp;\; = \;&amp;&amp; 5 &amp; \\
\beta_1  +  3\beta_2 &amp;&amp;\; = \;&amp;&amp; 7 &amp; \\
\beta_1  +  4\beta_2 &amp;&amp;\; = \;&amp;&amp; 10 &amp; \\
\end{alignat}" src="http://upload.wikimedia.org/math/5/b/a/5ba76cd5f940cdc4ae162cb7eb0ee55f.png" /></dd>
</dl>
<p>of four equations in two unknowns in some "best" sense.</p>
<p>The <a href="/wiki/Least_squares" title="Least squares">least squares</a> approach to solving this problem is to try to make as small as possible the sum of squares of "errors" between the right- and left-hand sides of these equations, that is, to find the <a href="/wiki/Maxima_and_minima" title="Maxima and minima">minimum</a> of the function</p>
<dl>
<dd><img class="tex" alt="S(\beta_1, \beta_2)=
 \left[6-(\beta_1+1\beta_2)\right]^2
+\left[5-(\beta_1+2\beta_2)   \right]^2
+\left[7-(\beta_1 +  3\beta_2)\right]^2
+\left[10-(\beta_1  +  4\beta_2)\right]^2." src="http://upload.wikimedia.org/math/f/e/6/fe62e7b8e90a5e57c59076fe9247087b.png" /></dd>
</dl>
<p>The minimum is determined by calculating the <a href="/wiki/Partial_derivative" title="Partial derivative">partial derivatives</a> of <span class="texhtml"><i>S</i>(β<sub>1</sub>,β<sub>2</sub>)</span> in respect to <span class="texhtml">β<sub>1</sub></span> and <span class="texhtml">β<sub>2</sub></span> and setting them to zero. This results in a system of two equations in two unknowns, called the normal equations, which, when solved, gives the solution</p>
<dl>
<dd><span class="texhtml">β<sub>1</sub> = 3.5</span></dd>
<dd><span class="texhtml">β<sub>2</sub> = 1.4</span></dd>
</dl>
<p>and the equation <span class="texhtml"><i>y</i> = 3.5 + 1.4<i>x</i></span> of the line of best fit. The <a href="/wiki/Residual_(statistics)" title="Residual (statistics)" class="mw-redirect">residuals</a>, that is, the discrepancies between the <span class="texhtml"><i>y</i></span> values from the experiment and the <span class="texhtml"><i>y</i></span> values calculated using the line of best fit are then found to be <span class="texhtml">1.1,</span> <span class="texhtml">− 1.3,</span> <span class="texhtml">− 0.7,</span> and <span class="texhtml">0.9</span> (see the picture on the right). The minimum value of the sum of squares is <span class="texhtml"><i>S</i>(3.5,1.4) = 1.1<sup>2</sup> + ( − 1.3)<sup>2</sup> + ( − 0.7)<sup>2</sup> + 0.9<sup>2</sup> = 4.2.</span></p>
<p><a name="Computation" id="Computation"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=2" title="Edit section: Computation">edit</a>]</span> <span class="mw-headline">Computation</span></h3>
<p>The common computational procedure to find a first-degree polynomial function approximation in a situation like this is as follows.</p>
<p>Use <img class="tex" alt="n \ " src="http://upload.wikimedia.org/math/e/4/8/e4861b707ff550ef16e31580a5206c7e.png" /> for the number of data points.</p>
<p>Find the four sums: <img class="tex" alt="\sum x" src="http://upload.wikimedia.org/math/3/5/0/350173c119b3d463b62e0b4652b236d4.png" />, <img class="tex" alt="\sum x^2" src="http://upload.wikimedia.org/math/0/3/3/0335620af4d47b988d52958bd251af73.png" />, <img class="tex" alt="\sum y" src="http://upload.wikimedia.org/math/1/3/2/1326ace6492777fed05702cfb3cdb4dd.png" />, and <img class="tex" alt="\sum xy" src="http://upload.wikimedia.org/math/b/9/f/b9fb2e62265f6acff7c1a60ee6c35e29.png" />.</p>
<p>The calculations for the slope, m, and the y-intercept, b, are as follows.</p>
<dl>
<dd><img class="tex" alt="m= \frac{(\sum y)(\sum x)-n(\sum xy)}{( \sum x)^2 - n(\sum x^2)}" src="http://upload.wikimedia.org/math/e/b/4/eb49c97d171bcd52e220911deac6aa5a.png" /></dd>
</dl>
<p>and</p>
<dl>
<dd><img class="tex" alt="b=\frac {(\sum x)(\sum xy)-(\sum y)(\sum x^2)}{( \sum x)^2 - n(\sum x^2)}" src="http://upload.wikimedia.org/math/b/d/e/bdea06a3287d543f035cac4012fc6feb.png" /></dd>
</dl>
<p><a name="The_general_problem" id="The_general_problem"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=3" title="Edit section: The general problem">edit</a>]</span> <span class="mw-headline">The general problem</span></h2>
<p>Consider an <a href="/wiki/Overdetermined_system" title="Overdetermined system">overdetermined system</a></p>
<dl>
<dd><img class="tex" alt="\sum_{j=1}^{n} X_{ij}\beta_j = y_i,\ (i=1, 2, \dots, m)," src="http://upload.wikimedia.org/math/4/6/6/466fe8c57876b308e65399fc52ede533.png" /></dd>
</dl>
<p>of <span class="texhtml"><i>m</i></span> <a href="/wiki/Linear_equation" title="Linear equation">linear equations</a> in <span class="texhtml"><i>n</i></span> unknowns, <img class="tex" alt="\beta_1, \beta_2, \dots, \beta_n," src="http://upload.wikimedia.org/math/b/1/3/b1363e8763632b45428945edb354578c.png" /> with <span class="texhtml"><i>m</i> &gt; <i>n</i>,</span> written in <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> form as</p>
<dl>
<dd><img class="tex" alt="\mathbf{X}\boldsymbol \beta = \mathbf y." src="http://upload.wikimedia.org/math/1/3/7/137bf0e2d724f556bfc202e6a89a16ec.png" /></dd>
</dl>
<p>Such a system usually has no solution, and the goal is then to find the numbers <span class="texhtml">β<sub><i>j</i></sub></span> which fit the equations "best", in the sense of solving the <a href="/wiki/Quadratic" title="Quadratic">quadratic</a> <a href="/wiki/Minimization" title="Minimization" class="mw-redirect">minimization</a> problem</p>
<dl>
<dd><img class="tex" alt="\underset{\beta} \operatorname{arg\,min} \sum_{i=1}^{m}\left|y_i - \sum_{j=1}^{n} X_{ij}\beta_j\right|^2 ." src="http://upload.wikimedia.org/math/4/d/5/4d5fbd9119fce80f63eb6539c026efc3.png" /></dd>
</dl>
<p>A justification for choosing this criterion is given in <a href="#Properties_of_the_least-squares_estimators" title="">properties</a> below. This minimization problem has a unique solution, provided that the <span class="texhtml"><i>n</i></span> columns of the matrix <span class="texhtml"><i>X</i></span> are <a href="/wiki/Linearly_independent" title="Linearly independent" class="mw-redirect">linearly independent</a>, given by solving the <b>normal equations</b></p>
<dl>
<dd><img class="tex" alt="\mathbf{\left(X^TX\right)\hat \boldsymbol \beta=X^Ty}." src="http://upload.wikimedia.org/math/e/9/5/e9544013f68c666c2584f65cff135fe1.png" /></dd>
</dl>
<p><a name="Uses_in_data_fitting" id="Uses_in_data_fitting"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=4" title="Edit section: Uses in data fitting">edit</a>]</span> <span class="mw-headline">Uses in data fitting</span></h2>
<p>The primary application of linear least squares is in data fitting. Given a set of <i>m</i> data points <img class="tex" alt="y_1, y_2,\dots, y_m," src="http://upload.wikimedia.org/math/4/7/9/4795873898b0cfdd4624be8a63571e7c.png" /> consisting of experimentally measured values taken at <i>m</i> values <img class="tex" alt="x_1, x_2,\dots, x_m" src="http://upload.wikimedia.org/math/f/3/3/f333fff88fc055911fb1665fdbfe7437.png" /> of an independent variable (<span class="texhtml"><i>x</i><sub><i>i</i></sub></span> may be scalar or vector quantities), and given a model function <img class="tex" alt="y=f(x, \boldsymbol \beta)," src="http://upload.wikimedia.org/math/1/c/d/1cddcbe99db604106598f9f490ce271e.png" /> with <img class="tex" alt="\boldsymbol \beta = (\beta_1, \beta_2, \dots, \beta_n)," src="http://upload.wikimedia.org/math/4/9/c/49cb733e01cb6d802c18696dbae7ad72.png" /> it is desired to find the parameters <span class="texhtml">β<sub><i>j</i></sub></span> such that the model function fits "best" the data. In linear least squares, linearity is meant to be with respect to parameters <span class="texhtml">β<sub><i>j</i></sub>,</span> so</p>
<dl>
<dd><img class="tex" alt="f(x, \boldsymbol \beta) = \sum_{j=1}^{n} \beta_j \phi_j(x)." src="http://upload.wikimedia.org/math/a/a/a/aaa7a7d4d7609885822e84a405ed718e.png" /></dd>
</dl>
<p>Here, the functions <span class="texhtml">φ<sub><i>j</i></sub></span> may be <b>nonlinear</b> with respect to the variable <b>x</b>.</p>
<p>Ideally, the model function fits the data exactly, so</p>
<dl>
<dd><img class="tex" alt="y_i = f(x_i, \boldsymbol \beta)" src="http://upload.wikimedia.org/math/6/4/5/645938adf739f612d5919574c07d6337.png" /></dd>
</dl>
<p>for all <img class="tex" alt="i=1, 2, \dots, m." src="http://upload.wikimedia.org/math/f/8/4/f84848d757cd257a791b19c971710ab0.png" /> This is usually not possible in practice, as there are more data points than there are parameters to be determined. The approach chosen then is to find the minimal possible value of the sum of squares of the <a href="/wiki/Residual_(statistics)" title="Residual (statistics)" class="mw-redirect">residuals</a></p>
<dl>
<dd><img class="tex" alt="r_i(\boldsymbol \beta)= y_i - f(x_i, \boldsymbol \beta),\  (i=1, 2, \dots, m) " src="http://upload.wikimedia.org/math/7/9/1/7919fdd4301bc842c33a9c3913897d77.png" /></dd>
</dl>
<p>so to minimize the function</p>
<dl>
<dd><img class="tex" alt="S(\boldsymbol \beta)=\sum_{i=1}^{m}r_i^2(\boldsymbol \beta)." src="http://upload.wikimedia.org/math/3/0/5/3058f73566b63bb55216e07cfd3d1187.png" /></dd>
</dl>
<p>After substituting for <span class="texhtml"><i>r</i><sub><i>i</i></sub></span> and then for <span class="texhtml"><i>f</i></span>, this minimization problem becomes the quadratic minimization problem above with <span class="texhtml"><i>X</i><sub><i>i</i><i>j</i></sub> = φ<sub><i>j</i></sub>(<i>x</i><sub><i>i</i></sub>)</span>, and the best fit can be found by solving the normal equations.</p>
<p><a name="Derivation_of_the_normal_equations" id="Derivation_of_the_normal_equations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=5" title="Edit section: Derivation of the normal equations">edit</a>]</span> <span class="mw-headline">Derivation of the normal equations</span></h2>
<p><i>S</i> is <a href="/wiki/Maxima_and_minima" title="Maxima and minima">minimized</a> when its gradient with respect to each parameter is equal to zero. The elements of the gradient vector are the partial derivatives of <i>S</i> with respect to the parameters:</p>
<dl>
<dd><img class="tex" alt="\frac{\partial S}{\partial \beta_j}=2\sum_i r_i\frac{\partial r_i}{\partial \beta_j}=0 \ (j=1,2,\dots, n)." src="http://upload.wikimedia.org/math/3/0/8/308af5e730def44abfeda95552e51a36.png" /></dd>
</dl>
<p>Since <img class="tex" alt="r_i= y_i - \sum_{j=1}^{n} X_{ij}\beta_j" src="http://upload.wikimedia.org/math/f/0/5/f053ce4d1bfc15a78a273b6607502955.png" />, the derivatives are</p>
<dl>
<dd><img class="tex" alt="\frac{\partial r_i}{\partial \beta_j}=-X_{ij}." src="http://upload.wikimedia.org/math/a/2/4/a24aefb1093d6b0796394b70d0194a2d.png" /></dd>
</dl>
<p>Substitution of the expressions for the residuals and the derivatives into the gradient equations gives</p>
<dl>
<dd><img class="tex" alt="\frac{\partial S}{\partial \beta_j}=-2\sum_{i=1}^{m}X_{ij} \left( y_i-\sum_{k=1}^{n} X_{ik}\beta_k \right)=0." src="http://upload.wikimedia.org/math/d/9/3/d93e01cda6a5f0ef84c8c856803e5fed.png" /></dd>
</dl>
<p>Upon rearrangement, the <b>normal equations</b></p>
<dl>
<dd><img class="tex" alt="\sum_{i=1}^{m}\sum_{k=1}^{n} X_{ij}X_{ik}\hat \beta_k=\sum_{i=1}^{m} X_{ij}y_i\ (j=1,2,\dots, n)\," src="http://upload.wikimedia.org/math/4/a/3/4a365188ec62fbd3250d83dee85b3768.png" /></dd>
</dl>
<p>are obtained. The normal equations are written in matrix notation as</p>
<dl>
<dd><img class="tex" alt="\mathbf{\left(X^TX\right)\hat \boldsymbol \beta=X^Ty}." src="http://upload.wikimedia.org/math/e/9/5/e9544013f68c666c2584f65cff135fe1.png" /></dd>
</dl>
<p>The solution of the normal equations yields the vector <img class="tex" alt="\hat \boldsymbol \beta" src="http://upload.wikimedia.org/math/5/4/9/549c14c2c1608f04c400f2a3c5e6471b.png" /> of the optimal parameter values.</p>
<p><a name="Computation_2" id="Computation_2"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=6" title="Edit section: Computation">edit</a>]</span> <span class="mw-headline">Computation</span></h2>
<p><a name="Inverting_the_normal_equations" id="Inverting_the_normal_equations"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=7" title="Edit section: Inverting the normal equations">edit</a>]</span> <span class="mw-headline">Inverting the normal equations</span></h3>
<p>Although the algebraic solution of the normal equations can be written as</p>
<dl>
<dd><img class="tex" alt="\mathbf{ \hat \boldsymbol\beta=\left(X^TX \right)^{-1}X^Ty}" src="http://upload.wikimedia.org/math/6/1/7/6175dc54794802efc09514e028381db0.png" /></dd>
</dl>
<p>it is not good practice to invert the normal equations matrix. An exception occurs in <a href="/wiki/Numerical_smoothing_and_differentiation" title="Numerical smoothing and differentiation">numerical smoothing and differentiation</a> where an analytical expression is required.</p>
<p>If the matrix <img class="tex" alt="\mathbf{X^TX}" src="http://upload.wikimedia.org/math/5/9/3/59309383fccbae1e8afbdfc1e14137bb.png" /> is well-conditioned and <a href="/wiki/Positive-definite_matrix" title="Positive-definite matrix">positive definite</a>, that is, it has full <a href="/wiki/Rank_(linear_algebra)" title="Rank (linear algebra)">rank</a>, the normal equations can be solved directly by using the <a href="/wiki/Cholesky_decomposition" title="Cholesky decomposition">Cholesky decomposition</a> <img class="tex" alt="\mathbf{X^TX=R^TR}" src="http://upload.wikimedia.org/math/e/1/d/e1d109889121c5aacc72d3f79a5fe4a2.png" />, where <b>R</b> is an upper <a href="/wiki/Triangular_matrix" title="Triangular matrix">triangular matrix</a>, giving</p>
<dl>
<dd><img class="tex" alt="\mathbf{ R^T R \hat \boldsymbol\beta = X^Ty}. " src="http://upload.wikimedia.org/math/a/e/0/ae0a1ecbc4fcade7bbc57f735a066a2f.png" /></dd>
</dl>
<p>The solution is obtained in two stages, a forward substitution, <img class="tex" alt="\mathbf{R^Tz=X^Ty}" src="http://upload.wikimedia.org/math/d/1/3/d13998a55729d2bc9ecfe1e9660a7236.png" />, followed by a backward substitution <img class="tex" alt="\mathbf{R\hat \boldsymbol\beta=z}" src="http://upload.wikimedia.org/math/c/9/c/c9c5d9d79b92737fbc9888f7e4bc4330.png" />. Both subtitutions are facilitated by the triangular nature of <b>R</b>.</p>
<p>See <a href="/wiki/Linear_regression#Example" title="Linear regression">example of linear regression</a> for a worked-out numerical example with three parameters.</p>
<p><a name="Orthogonal_decomposition_methods" id="Orthogonal_decomposition_methods"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=8" title="Edit section: Orthogonal decomposition methods">edit</a>]</span> <span class="mw-headline">Orthogonal decomposition methods</span></h3>
<p>Orthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more <a href="/wiki/Numerical_stability" title="Numerical stability">numerically stable</a>.</p>
<p>The extra stability results from not having to form the product <img class="tex" alt="\mathbf{X^TX}" src="http://upload.wikimedia.org/math/5/9/3/59309383fccbae1e8afbdfc1e14137bb.png" />. The residuals are written in matrix notation as</p>
<dl>
<dd><img class="tex" alt="\mathbf{r=y-X\boldsymbol\beta}." src="http://upload.wikimedia.org/math/4/4/0/4408bfd0bf07e5550adda39c59eb0c34.png" /></dd>
</dl>
<p>The matrix <b>X</b> is subjected to an orthogonal decomposition; the <a href="/wiki/QR_decomposition" title="QR decomposition">QR decomposition</a> will serve to illustrate the process.</p>
<dl>
<dd><img class="tex" alt="\mathbf{X=QR}" src="http://upload.wikimedia.org/math/7/4/9/7492b14299bde67793c045e939c01ba0.png" /></dd>
</dl>
<p>where <b>Q</b> is an <a href="/wiki/Orthogonal" title="Orthogonal" class="mw-redirect">orthogonal</a> <img class="tex" alt="m \times m" src="http://upload.wikimedia.org/math/a/3/2/a3287fa319bc6de5b5c69db370a6183b.png" /> matrix and <b>R</b> is an <img class="tex" alt="m \times n" src="http://upload.wikimedia.org/math/c/4/f/c4f729d02a67ac278165d81c624944ca.png" /> matrix which is <a href="/wiki/Block_matrix" title="Block matrix">partitioned</a> into a <img class="tex" alt="n \times n" src="http://upload.wikimedia.org/math/6/0/7/607acaa73c762411b20745149a11e90b.png" /> block, <img class="tex" alt="\mathbf\R_n" src="http://upload.wikimedia.org/math/5/b/8/5b87385524b24d1421ac49a8af306bdf.png" />, and a <img class="tex" alt="m-n \times n" src="http://upload.wikimedia.org/math/9/e/6/9e68af34b8bb5e96c0f9e167adb90ed1.png" /> zero block. <img class="tex" alt="\mathbf\R_n" src="http://upload.wikimedia.org/math/5/b/8/5b87385524b24d1421ac49a8af306bdf.png" /> is <a href="/wiki/Triangular_matrix" title="Triangular matrix">upper triangular</a>.</p>
<dl>
<dd><img class="tex" alt="\mathbf{R}= \begin{bmatrix}
\mathbf{R}_n \\
\mathbf{0}\end{bmatrix}." src="http://upload.wikimedia.org/math/7/0/9/709ca518dee4674d0f14ef61dff12842.png" /></dd>
</dl>
<p>The residual vector is left-multiplied by <img class="tex" alt="\mathbf {Q^T}" src="http://upload.wikimedia.org/math/0/f/8/0f888e630ee19f6a90e5b8b5f941950c.png" />.</p>
<dl>
<dd><img class="tex" alt="\mathbf{Q^Tr=Q^T y -\left(Q^TQ\right)R \boldsymbol\beta}= \begin{bmatrix}
\mathbf{\left(Q^T y\right)}_n -\mathbf{R}_n \boldsymbol\beta  \\
\mathbf{\left(Q^T y  \right)}_{m-n}\end{bmatrix}
= \begin{bmatrix}\mathbf{U}\\\mathbf{L}\end{bmatrix}
" src="http://upload.wikimedia.org/math/a/5/e/a5e9f94694137c42dfe1d8ff07776ce6.png" /></dd>
</dl>
<p>The sum of squares of the transformed residuals, <img class="tex" alt="S=\mathbf{r^T Q Q^Tr}" src="http://upload.wikimedia.org/math/b/1/4/b14ad25b87928895394be48a46f3ae28.png" />, is the same as before, <img class="tex" alt="S=\mathbf{r^Tr}" src="http://upload.wikimedia.org/math/3/b/d/3bd4924390e1d08a9b69ca6dae82d931.png" /> because <b>Q</b> is <a href="/wiki/Orthogonal" title="Orthogonal" class="mw-redirect">orthogonal</a>.</p>
<dl>
<dd><img class="tex" alt="S=\mathbf{U^TU+L^TL}" src="http://upload.wikimedia.org/math/2/c/b/2cb4af754bc3863c5d40e9fbd1519434.png" /></dd>
</dl>
<p>The minimum value of <i>S</i> is attained when the upper block, <b>U</b>, is zero. Therefore the parameters are found by solving</p>
<dl>
<dd><img class="tex" alt="\mathbf{R}_n \hat\boldsymbol\beta =\mathbf{\left(Q^T y \right)}_n." src="http://upload.wikimedia.org/math/b/7/5/b751203984c3104ab8e99fbabd11d763.png" /></dd>
</dl>
<p>These equations are easily solved as <img class="tex" alt="\mathbf{R}_n" src="http://upload.wikimedia.org/math/e/3/4/e34a9d167b8df7b5ec93ac655591446b.png" /> is upper triangular.</p>
<p>An alternative decomposition of <b>X</b> is the <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a> (SVD)<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup></p>
<dl>
<dd><img class="tex" alt="\mathbf{ X = U\Sigma V^*}." src="http://upload.wikimedia.org/math/9/7/4/974a3e39e246b57238ad1d34029f33d8.png" /></dd>
</dl>
<p>This is effectively another kind of orthogonal decomposition as both <b>U</b> and <b>V</b> are orthogonal. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, <img class="tex" alt="\mathbf{X^TX}" src="http://upload.wikimedia.org/math/5/9/3/59309383fccbae1e8afbdfc1e14137bb.png" />, is very ill-conditioned (i.e. if its <a href="/wiki/Condition_number" title="Condition number">condition number</a> multiplied by the machine's relative <a href="/wiki/Round-off_error" title="Round-off error">round-off error</a> is appreciably large). In that case, including the smallest <a href="/wiki/Singular_value" title="Singular value">singular values</a> in the inversion merely adds numerical noise to the solution. This can be cured using the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to <a href="/wiki/Factor_analysis" title="Factor analysis">factor analysis</a>.</p>
<p><a name="Properties_of_the_least-squares_estimators" id="Properties_of_the_least-squares_estimators"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=9" title="Edit section: Properties of the least-squares estimators">edit</a>]</span> <span class="mw-headline">Properties of the least-squares estimators</span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:182px;"><a href="/wiki/File:Linear_least_squares_geometric_interpretation.png" class="image" title="The residual vector, , which corresponds to the solution of a least squares system, , is orthogonal to the column space of the matrix X."><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Linear_least_squares_geometric_interpretation.png/180px-Linear_least_squares_geometric_interpretation.png" width="180" height="199" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Linear_least_squares_geometric_interpretation.png" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
The residual vector, <img class="tex" alt="y-X\boldsymbol \hat \beta" src="http://upload.wikimedia.org/math/2/5/e/25e24a2c1af14279b8e684cceae67e9c.png" />, which corresponds to the solution of a least squares system, <img class="tex" alt="y=X\boldsymbol \beta +\epsilon" src="http://upload.wikimedia.org/math/f/5/f/f5ffe56e3e7bab3d764ed5d73e8509ec.png" />, is orthogonal to the <a href="/wiki/Column_space" title="Column space">column space</a> of the matrix <span class="texhtml"><i>X</i></span>.</div>
</div>
</div>
<p>The gradient equations at the minimum can be written as</p>
<dl>
<dd><img class="tex" alt="\mathbf{(y-X\hat\boldsymbol\beta)X}=0." src="http://upload.wikimedia.org/math/1/a/7/1a76bad75ff534eb0a9d6b59d8019c94.png" /></dd>
</dl>
<p>A geometrical interpretation of these equations is that the vector of residuals, <img class="tex" alt="\mathbf{y-X\hat\boldsymbol\beta}" src="http://upload.wikimedia.org/math/2/9/4/294499dd3d999a28079391bc09179c33.png" /> is orthogonal to the <a href="/wiki/Column_space" title="Column space">column space</a> of <img class="tex" alt="\mathbf{X}" src="http://upload.wikimedia.org/math/5/9/8/598f6444904755dda4a859a1e377468e.png" />, since the dot product <img class="tex" alt="\mathbf{(y-X\hat\boldsymbol\beta)\cdot Xv}" src="http://upload.wikimedia.org/math/c/a/c/cac44c38be9cc9e1e8531187660efcd9.png" /> is equal to zero for <i>any</i> conformal vector, <img class="tex" alt="\mathbf{v}" src="http://upload.wikimedia.org/math/0/a/a/0aa3ec374bdc0d6a17aecbb6bcda6a89.png" />. This means that <img class="tex" alt="\mathbf{y}-\mathbf{X}\boldsymbol \hat \beta" src="http://upload.wikimedia.org/math/c/5/1/c515e90037b1240cb5dd37c5a6429198.png" /> is the shortest of all possible vectors <img class="tex" alt="\mathbf{y}-\mathbf{X}\boldsymbol \beta" src="http://upload.wikimedia.org/math/1/2/6/126a7d7f676dc27f9746ccf797979815.png" />, that is, the variance of the residuals is the minimum possible. This is illustrated at the right.</p>
<p>If the experimental errors, <img class="tex" alt="\epsilon \," src="http://upload.wikimedia.org/math/a/b/4/ab4a44edf45f22ea595d23054c8fc853.png" />, are uncorrelated, have a mean of zero and a constant variance, <span class="texhtml">σ</span>, the <a href="/wiki/Gauss-Markov_theorem" title="Gauss-Markov theorem" class="mw-redirect">Gauss-Markov theorem</a> states that the least-squares estimator, <img class="tex" alt="\hat \beta" src="http://upload.wikimedia.org/math/5/9/7/59744f666ff56f695bfe4d128a6784f4.png" />, has the minimum variance of all estimators that are linear combinations of the observations. In this sense it is the best, or optimal, estimator of the parameters. Note particularly that this property is independent of the statistical <a href="/wiki/Distribution_function" title="Distribution function">distribution function</a> of the errors. In other words, <i>the distribution function of the errors need not be a <a href="/wiki/Normal_distribution" title="Normal distribution">normal distribution</a></i>. However, for some probability distributions, there is no guarantee that the least-squares solution is even possible given the observations; still, in such cases it is the best estimator that is both linear and unbiased.</p>
<p>For example, it is easy to show that the <a href="/wiki/Arithmetic_mean" title="Arithmetic mean">arithmetic mean</a> of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the Gauss-Markov theorem apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.</p>
<p>However, in the case that the experimental errors do belong to a Normal distribution, the least-squares estimator is also a <a href="/wiki/Maximum_likelihood" title="Maximum likelihood">maximum likelihood</a> estimator.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<p>These properties underpin the use of the method of least squares for all types of data fitting, even when the assumptions are not strictly valid.</p>
<p><a name="Limitations" id="Limitations"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=10" title="Edit section: Limitations">edit</a>]</span> <span class="mw-headline">Limitations</span></h3>
<p>An assumption underlying the treatment given above is that the independent variable, <i>x</i>, is free of error. In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, <a href="/wiki/Total_least_squares" title="Total least squares">total least squares</a> also known as <i>Errors-in-variables model</i>, or <i>Rigorous least squares</i>, should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure.<sup id="cite_ref-pg_2-0" class="reference"><a href="#cite_note-pg-2" title=""><span>[</span>3<span>]</span></a></sup><sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup></p>
<p>In some cases the (weighted) normal equations matrix <img class="tex" alt="\mathbf{X^TX}" src="http://upload.wikimedia.org/math/5/9/3/59309383fccbae1e8afbdfc1e14137bb.png" /> is <a href="/wiki/Ill-conditioned" title="Ill-conditioned" class="mw-redirect">ill-conditioned</a>; this occurs when the measurements have only a marginal effect on one or more of the estimated parameters.<sup id="cite_ref-vm_4-0" class="reference"><a href="#cite_note-vm-4" title=""><span>[</span>5<span>]</span></a></sup> In these cases, the least squares estimate amplifies the measurement noise and may be grossly inaccurate. Various <a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">regularization</a> techniques can be applied in such cases, the most common of which is called <a href="/wiki/Tikhonov_regularization" title="Tikhonov regularization">Tikhonov regularization</a>. If further information about the parameters is known, for example, a range of possible values of <b>x</b>, then <a href="/wiki/Minimax" title="Minimax">minimax</a> techniques can also be used to increase the stability of the solution.</p>
<p>Another drawback of the least squares estimator is the fact that the norm of the residuals, <img class="tex" alt="\|\mathbf{y-X\boldsymbol\beta}\|" src="http://upload.wikimedia.org/math/f/1/8/f187bc1ff6077660128ac4589230b7a7.png" /> is minimized, whereas in some cases one is truly interested in obtaining small error in the parameter <img class="tex" alt="\mathbf{\boldsymbol\beta}" src="http://upload.wikimedia.org/math/c/8/a/c8a994ff514a1115084b4112794eff17.png" />, e.g., a small value of <img class="tex" alt="\|\boldsymbol\beta-\hat\boldsymbol\beta\|" src="http://upload.wikimedia.org/math/1/7/9/179b1b47ab448eabbc619f29a25a3223.png" />. However, since <img class="tex" alt="\boldsymbol\beta" src="http://upload.wikimedia.org/math/6/b/f/6bf37cfb9e3eeb9f68c0a32699ac20aa.png" /> is unknown, this quantity cannot be directly minimized. If a <a href="/wiki/Prior_probability" title="Prior probability">prior probability</a> on <img class="tex" alt="\boldsymbol\beta" src="http://upload.wikimedia.org/math/6/b/f/6bf37cfb9e3eeb9f68c0a32699ac20aa.png" /> is known, then a <a href="/wiki/Bayes_estimator" title="Bayes estimator">Bayes estimator</a> can be used to minimize the <a href="/wiki/Mean_squared_error" title="Mean squared error">mean squared error</a>, <img class="tex" alt="E \left\{ \| \boldsymbol\beta - \hat\boldsymbol\beta \|^2 \right\} " src="http://upload.wikimedia.org/math/2/c/6/2c60fc6378895b336319edf704383063.png" />. The least squares method is often applied when no prior is known. Surprisingly, however, better estimators can be constructed, an effect known as <a href="/wiki/Stein%27s_phenomenon" title="Stein's phenomenon" class="mw-redirect">Stein's phenomenon</a>. For example, if the measurement error is <a href="/wiki/Normal_distribution" title="Normal distribution">Gaussian</a>, several estimators are known which <a href="/wiki/Dominating_decision_rule" title="Dominating decision rule">dominate</a>, or outperform, the least squares technique; the best known of these is the <a href="/wiki/James-Stein_estimator" title="James-Stein estimator">James-Stein estimator</a>.</p>
<p><a name="Weighted_linear_least_squares" id="Weighted_linear_least_squares"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=11" title="Edit section: Weighted linear least squares">edit</a>]</span> <span class="mw-headline">Weighted linear least squares</span></h2>
<p>When the observations are not equally reliable, a weighted sum of squares</p>
<dl>
<dd><img class="tex" alt="S=\sum_{i=1}^{m}W_{ii}r_i^2" src="http://upload.wikimedia.org/math/0/3/d/03da7fb61e9d4ee920571f2f27a20110.png" /></dd>
</dl>
<p>may be minimized.</p>
<p>Each element of the <a href="/wiki/Diagonal_matrix" title="Diagonal matrix">diagonal</a> weight matrix, <b>W</b> should,ideally, be equal to the <a href="/wiki/Multiplicative_inverse" title="Multiplicative inverse">reciprocal</a> of the <a href="/wiki/Variance" title="Variance">variance</a> of the measurement.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup> The normal equations are then</p>
<dl>
<dd><img class="tex" alt="\mathbf{\left(X^TWX\right)\hat \boldsymbol \beta=X^TWy}." src="http://upload.wikimedia.org/math/8/a/d/8ad28fc18f3885e65ef9702b2bf02aeb.png" /></dd>
</dl>
<p><a name="Parameter_errors.2C_correlation_and_confidence_limits" id="Parameter_errors.2C_correlation_and_confidence_limits"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=12" title="Edit section: Parameter errors, correlation and confidence limits">edit</a>]</span> <span class="mw-headline">Parameter errors, correlation and confidence limits</span></h2>
<p>The parameter values are linear combinations of the observed values</p>
<dl>
<dd><img class="tex" alt="\mathbf{\hat \beta=(X^TWX)^{-1}X^TWy}." src="http://upload.wikimedia.org/math/2/9/c/29c1f1608a856ec4fbadc3f104778329.png" /></dd>
</dl>
<p>Therefore an expression for the errors on the parameter can be obtained by <a href="/wiki/Error_propagation" title="Error propagation" class="mw-redirect">error propagation</a> from the errors on the observations. Let the <a href="/wiki/Variance-covariance_matrix" title="Variance-covariance matrix" class="mw-redirect">variance-covariance matrix</a> for the observations be denoted by <b>M</b> and that of the parameters by <b>M<sup><span class="texhtml">β</span></sup></b>. Then,</p>
<dl>
<dd><img class="tex" alt="\mathbf{M^\beta=(X^TWX)^{-1}X^TW M W^TX(X^TWX)^{-1}}." src="http://upload.wikimedia.org/math/e/f/3/ef33222476a5292d866b71e4a85321dc.png" /></dd>
</dl>
<p>When <img class="tex" alt="\mathbf{W=M^{-1}}" src="http://upload.wikimedia.org/math/6/2/e/62eac3a51d555c17451c47f9bc3c2f98.png" />, this simplifies to</p>
<dl>
<dd><img class="tex" alt="\mathbf{M^\beta=(X^TWX)^{-1}}." src="http://upload.wikimedia.org/math/3/2/0/320ee4f3f97af49b5bfaa8f2b03e0569.png" /></dd>
</dl>
<p>When unit weights are used (<img class="tex" alt="\mathbf{W=I, \hat \beta=(X^TX)^{-1}X^Ty}" src="http://upload.wikimedia.org/math/9/e/4/9e42008295e21c54a2305df76c931d9d.png" />) it is implied that the experimental errors are uncorrelated and all equal: <img class="tex" alt="\mathbf{M}=\sigma^2 \mathbf{I}" src="http://upload.wikimedia.org/math/f/1/b/f1b026ca8d349eb202fd9ea380f00489.png" />, where <img class="tex" alt="\sigma^2\," src="http://upload.wikimedia.org/math/2/4/d/24dd4eca5f79ddd3740ac274afded971.png" /> is known as the variance of an observation of unit weight, and <img class="tex" alt="\mathbf{I}" src="http://upload.wikimedia.org/math/9/9/3/99390491be8757c2d00ba9ba45ee07d7.png" /> is an <a href="/wiki/Identity_matrix" title="Identity matrix">identity matrix</a>. In this case <img class="tex" alt="\sigma^2\," src="http://upload.wikimedia.org/math/2/4/d/24dd4eca5f79ddd3740ac274afded971.png" /> is approximated by <img class="tex" alt="\frac{S}{n-m}" src="http://upload.wikimedia.org/math/3/2/4/32408c07ced6906650d7074458ffcd10.png" />, where <i>S</i> is the minimum value of the objective function</p>
<dl>
<dd><img class="tex" alt="\mathbf{M^\beta=}\frac{S}{n-m}\mathbf{(X^TX)^{-1}}." src="http://upload.wikimedia.org/math/b/a/0/ba0072c107d0bf9936751b227863f9a4.png" /></dd>
</dl>
<p>In all cases, the <a href="/wiki/Variance" title="Variance">variance</a> of the parameter <span class="texhtml">β<sub><i>i</i></sub></span> is given by <img class="tex" alt="M^\beta_{ii}" src="http://upload.wikimedia.org/math/f/3/5/f352c14a7671b175a34cb37179c85d17.png" /> and the <a href="/wiki/Covariance" title="Covariance">covariance</a> between parameters <span class="texhtml">β<sub><i>i</i></sub></span> and <span class="texhtml">β<sub><i>j</i></sub></span> is given by <img class="tex" alt="M^\beta_{ij}" src="http://upload.wikimedia.org/math/6/0/2/6025cffecbcd4695157bd44940c226fe.png" />. <a href="/wiki/Standard_deviation" title="Standard deviation">Standard deviation</a> is the square root of variance and the correlation coefficient is given by <img class="tex" alt="\rho_{ij} = M^\beta_{ij}/\sigma_i/\sigma_j" src="http://upload.wikimedia.org/math/7/2/6/7260d80c42eaa5be491bfc4d94f78b3f.png" />. These error estimates reflect only <a href="/wiki/Random_errors" title="Random errors" class="mw-redirect">random errors</a> in the measurements. The true uncertainty in the parameters is larger due to the presence of <a href="/wiki/Systematic_errors" title="Systematic errors" class="mw-redirect">systematic errors</a> which, by definition, cannot be quantified. Note that even though the observations may be un-correlated, the parameters are always <a href="/wiki/Correlation_coefficient" title="Correlation coefficient" class="mw-redirect">correlated</a>.</p>
<p>It is often <i>assumed</i>, for want of any concrete evidence, that the error on a parameter belongs to a <a href="/wiki/Normal_distribution" title="Normal distribution">Normal distribution</a> with a mean of zero and standard deviation <span class="texhtml">σ</span>. Under that assumption the following <a href="/wiki/Confidence_limits" title="Confidence limits" class="mw-redirect">confidence limits</a> can be derived.</p>
<dl>
<dd>68% confidence limits, <img class="tex" alt="\hat \beta \pm \sigma" src="http://upload.wikimedia.org/math/4/4/0/4408462843dbef66e802f28a749fdf66.png" /></dd>
<dd>95% confidence limits, <img class="tex" alt="\hat \beta \pm 2\sigma" src="http://upload.wikimedia.org/math/9/b/7/9b79c39fb1d413308a3bfb1a386c0032.png" /></dd>
<dd>99% confidence limits, <img class="tex" alt="\hat \beta \pm 2.5\sigma" src="http://upload.wikimedia.org/math/c/f/9/cf97a6baec53960c3c86551b7e3b8c65.png" /></dd>
</dl>
<p>The assumption is not unreasonable when <i>m&gt;&gt;n</i>. If the experimental errors are normally distributed the parameters will belong to a <a href="/wiki/Student%27s_t-distribution" title="Student's t-distribution">Student's t-distribution</a> with <i>m-n</i> <a href="/wiki/Degrees_of_freedom" title="Degrees of freedom">degrees of freedom</a>. When <i>m&gt;&gt;n</i> Student's t-distribution approximates to a Normal distribution. Note, however, that these confidence limits cannot take systematic error into account. Also, parameter errors should be quoted to one significant figure only, as they are subject to <a href="/wiki/Sampling_error" title="Sampling error">sampling error</a>.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup></p>
<p>When the number of observations is relatively small, <a href="/wiki/Chebychev%27s_inequality" title="Chebychev's inequality" class="mw-redirect">Chebychev's inequality</a> can be used for an upper bound on probabilities, regardless of any assumptions about the distribution of experimental errors: the maximum probabilities that a parameter will be more than 1, 2 or 3 standard deviations away from its expectation value are 100%, 25% and 11% respectively.</p>
<p><a name="Residual_values_and_correlation" id="Residual_values_and_correlation"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=13" title="Edit section: Residual values and correlation">edit</a>]</span> <span class="mw-headline">Residual values and correlation</span></h2>
<p>The <a href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">residuals</a> are related to the observations by</p>
<dl>
<dd><img class="tex" alt="\mathbf{\hat r=y-X \hat \beta=y-X \left(X^TWX \right)^{-1}X^T W y}" src="http://upload.wikimedia.org/math/3/7/7/37737a6bd8b9da6d606da6032431e7c9.png" /></dd>
</dl>
<p>The <a href="/wiki/Symmetric_matrix" title="Symmetric matrix">symmetric</a>, <a href="/wiki/Idempotent" title="Idempotent" class="mw-redirect">idempotent</a> matrix <img class="tex" alt="\mathbf{X \left(X^TWX \right)^{-1}X^T W}" src="http://upload.wikimedia.org/math/2/2/5/225639b041067fdce77048a0f70a8682.png" /> is known in the statistics literature as the <a href="/wiki/Hat_matrix" title="Hat matrix">hat matrix</a>, <img class="tex" alt="\mathbf{H}" src="http://upload.wikimedia.org/math/d/a/3/da3623c683c463e210845c6d757af4dd.png" />. (<img class="tex" alt="\mathbf{H}" src="http://upload.wikimedia.org/math/d/a/3/da3623c683c463e210845c6d757af4dd.png" /> is idempotent because it is a <a href="/wiki/Projection_(linear_algebra)" title="Projection (linear algebra)">projection</a>.) Thus,</p>
<dl>
<dd><img class="tex" alt="\mathbf{\hat r=\left(I-H \right) y}" src="http://upload.wikimedia.org/math/6/3/b/63bf80bddf781710b34998eb70ad2b85.png" /></dd>
</dl>
<p>where <b>I</b> is an <a href="/wiki/Identity_matrix" title="Identity matrix">identity matrix</a>. The variance-covariance matrice of the residuals, <b>M<sup>r</sup></b> is given by</p>
<dl>
<dd><img class="tex" alt="\mathbf{M^r=\left(I-H \right) M \left(I-H \right)}." src="http://upload.wikimedia.org/math/f/3/6/f369c2802862ee97993f325b8bd73021.png" /></dd>
</dl>
<p>This shows that even though the observations may be uncorrelated, the residuals are <a href="/wiki/Always" title="Always">always</a> correlated.</p>
<p>The sum of residual values is equal to zero whenever the model function contains a constant term. Left-multiply the expression for the residuals by <img class="tex" alt="\mathbf{X^T}" src="http://upload.wikimedia.org/math/7/3/a/73a858fbf2e1e8ef3cc9dde075b0ea69.png" />.</p>
<dl>
<dd><img class="tex" alt="\mathbf{X^T\hat r=X^Ty-X^TX\boldsymbol\hat\beta=X^Ty-(X^TX)(X^TX)^{-1}X^Ty=0}" src="http://upload.wikimedia.org/math/5/1/d/51da5c4e7aba2369bbd0a4e077271122.png" /></dd>
</dl>
<p>Say, for example, that the first term of the model is a constant, so that <span class="texhtml"><i>X</i><sub><i>i</i>1</sub> = 1</span> for all <i>i</i>. In that case it follows that</p>
<dl>
<dd><img class="tex" alt="\sum_i^m X_{i1} \hat r_i=\sum_i^m \hat r_i=0." src="http://upload.wikimedia.org/math/a/f/c/afcf4cf6d011411797e3cb45ee08c1c2.png" /></dd>
</dl>
<p>Thus, in the <a href="#Motivational_example" title="">motivational example</a>, above, the fact that the sum of residual values is equal to zero it is not accidental but is a consequence of the presence of the constant term, α, in the model.</p>
<p>If experimental error follows a <a href="/wiki/Normal_distribution" title="Normal distribution">normal distribution</a>, then, because of the linear relationship between residuals and observations, so should residuals,<sup id="cite_ref-7" class="reference"><a href="#cite_note-7" title=""><span>[</span>8<span>]</span></a></sup> but since the observations are only a sample of the population of all possible observations, the residuals should belong to a <a href="/wiki/Student%27s_t-distribution" title="Student's t-distribution">Student's t-distribution</a>. <a href="/wiki/Studentized_residual" title="Studentized residual">Studentized residuals</a> are useful in making a statistical test for an <a href="/wiki/Outlier" title="Outlier">outlier</a> when a particular residual appears to be excessively large.</p>
<p><a name="Objective_function" id="Objective_function"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=14" title="Edit section: Objective function">edit</a>]</span> <span class="mw-headline">Objective function</span></h2>
<p>The objective function can be written as</p>
<dl>
<dd><img class="tex" alt="S=\mathbf{ y^T(I-H)^T(I-H)y=y^T(I-H)y}" src="http://upload.wikimedia.org/math/e/6/6/e661c809e521ba520c9de88f7a200645.png" /></dd>
</dl>
<p>since <img class="tex" alt="\mathbf{ (I-H)}" src="http://upload.wikimedia.org/math/e/d/f/edf48cb1a5b3b51f9b160220ced9fd86.png" /> is also symmetric and idempotent. It can be shown from this,<sup id="cite_ref-8" class="reference"><a href="#cite_note-8" title=""><span>[</span>9<span>]</span></a></sup> that the <a href="/wiki/Expected_value" title="Expected value">expected value</a> of <i>S</i> is <i>m-n</i>. Note, however, that this is true only if the weights have been assigned correctly. If unit weights are assumed, the expected value of <i>S</i> is <span class="texhtml">(<i>m</i> − <i>n</i>)σ<sup>2</sup></span>, where <span class="texhtml">σ<sup>2</sup></span> is the variance of an observation.</p>
<p>If it is assumed that the residuals belong to a Normal distribution, the objective function, being a sum of weighted squared residuals, will belong to a <a href="/wiki/Chi-square_distribution" title="Chi-square distribution">Chi-square (<span class="texhtml">χ<sup>2</sup></span>) distribution</a> with <i>m-n</i> <a href="/wiki/Degrees_of_freedom" title="Degrees of freedom">degrees of freedom</a>. Some illustrative percentile values of <span class="texhtml">χ<sup>2</sup></span> are given in the following table.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9" title=""><span>[</span>10<span>]</span></a></sup></p>
<dl>
<dd>
<table class="wikitable">
<tr>
<th>m-n</th>
<th><img class="tex" alt="\chi ^2 _{0.50}" src="http://upload.wikimedia.org/math/8/c/1/8c1cff476916c0b246b987ae563be612.png" /></th>
<th><img class="tex" alt="\chi ^2 _{0.95}" src="http://upload.wikimedia.org/math/b/4/a/b4a78477b2f5c08fee031fa150f9478a.png" /></th>
<th><img class="tex" alt="\chi ^2 _{0.99}" src="http://upload.wikimedia.org/math/6/a/f/6af326d6d522b745cf07bc23798de0fa.png" /></th>
</tr>
<tr>
<td>10</td>
<td>9.34</td>
<td>18.3</td>
<td>23.2</td>
</tr>
<tr>
<td>25</td>
<td>24.3</td>
<td>37.7</td>
<td>44.3</td>
</tr>
<tr>
<td>100</td>
<td>99.3</td>
<td>124</td>
<td>136</td>
</tr>
</table>
</dd>
</dl>
<p>These values can be used for a statistical criterion as to the <a href="/wiki/Goodness-of-fit" title="Goodness-of-fit" class="mw-redirect">goodness-of-fit</a>. When unit weights are used, the numbers should be divided by the variance of an observation.</p>
<p><a name="Typical_uses_and_applications" id="Typical_uses_and_applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=15" title="Edit section: Typical uses and applications">edit</a>]</span> <span class="mw-headline">Typical uses and applications</span></h2>
<ul>
<li>Polynomial fitting: models are <a href="/wiki/Polynomial" title="Polynomial">polynomials</a> in an independent variable, <i>x</i>:
<ul>
<li>Straight line: <img class="tex" alt="f(x, \boldsymbol \beta)=\beta_1 +\beta_2 x" src="http://upload.wikimedia.org/math/d/3/c/d3c31ead4b59a51cb5181cf2c3f01110.png" />.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10" title=""><span>[</span>11<span>]</span></a></sup></li>
<li>Quadratic: <img class="tex" alt="f(x, \boldsymbol \beta)=\beta_1  + \beta_2 x +\beta_3 x^2" src="http://upload.wikimedia.org/math/a/d/6/ad67c7ebcc9e7191c4c70d1d551ea9b0.png" />.</li>
<li>Cubic, quartic and higher polynomials. For high-order polynomials the use of <a href="/wiki/Orthogonal_polynomials" title="Orthogonal polynomials">orthogonal polynomials</a> is recommended.<sup id="cite_ref-vm_4-1" class="reference"><a href="#cite_note-vm-4" title=""><span>[</span>5<span>]</span></a></sup><sup id="cite_ref-11" class="reference"><a href="#cite_note-11" title=""><span>[</span>12<span>]</span></a></sup></li>
</ul>
</li>
<li><a href="/wiki/Numerical_smoothing_and_differentiation" title="Numerical smoothing and differentiation">Numerical smoothing and differentiation</a> — this is an application of polynomial fitting.</li>
<li>Multinomials in more than one independent variable, including surface fitting</li>
<li>Curve fitting with <a href="/wiki/B-spline" title="B-spline">B-splines</a> <sup id="cite_ref-pg_2-1" class="reference"><a href="#cite_note-pg-2" title=""><span>[</span>3<span>]</span></a></sup></li>
<li><a href="/wiki/Chemometrics" title="Chemometrics">Chemometrics</a>, <a href="/wiki/Calibration_curve" title="Calibration curve">Calibration curve</a>, <a href="/wiki/Standard_addition" title="Standard addition">Standard addition</a>, <a href="/wiki/Gran_plot" title="Gran plot">Gran plot</a>, <a href="/wiki/Beer-Lambert_law#chemical_analysis" title="Beer-Lambert law" class="mw-redirect">analysis of mixtures</a></li>
</ul>
<p><a name="Software_for_solving_LLSP" id="Software_for_solving_LLSP"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=16" title="Edit section: Software for solving LLSP">edit</a>]</span> <span class="mw-headline">Software for solving LLSP</span></h2>
<p><b>1. Free and opensource, with <a href="/wiki/List_of_OSI_approved_software_licences#OSI_approved_licenses" title="List of OSI approved software licences" class="mw-redirect">OSI-Approved licenses</a></b></p>
<table class="wikitable" border="1">
<tr>
<th>Name</th>
<th>License</th>
<th>Brief info</th>
</tr>
<tr>
<td><a href="http://lib.stat.cmu.edu/general/bvls" class="external text" title="http://lib.stat.cmu.edu/general/bvls" rel="nofollow">bvls</a></td>
<td><a href="/wiki/BSD" title="BSD" class="mw-redirect">BSD</a></td>
<td>Fortran code by Robert L. Parker &amp; Philip B. Stark</td>
</tr>
<tr>
<td><a href="http://www.netlib.org/lapack/double/dgelss.f" class="external text" title="http://www.netlib.org/lapack/double/dgelss.f" rel="nofollow">lapack dgelss</a></td>
<td><a href="/wiki/BSD" title="BSD" class="mw-redirect">BSD</a></td>
<td>made by Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,<br />
Courant Institute, Argonne National Lab, and Rice University</td>
</tr>
<tr>
<td><a href="/wiki/OpenOpt" title="OpenOpt">OpenOpt</a></td>
<td><a href="/wiki/BSD_licenses" title="BSD licenses">BSD</a></td>
<td>universal cross-platform Python-written numerical optimization framework;<br />
see its <a href="http://openopt.org/LLSP" class="external text" title="http://openopt.org/LLSP" rel="nofollow">LLSP</a> page and <a href="http://openopt.org/Problems" class="external text" title="http://openopt.org/Problems" rel="nofollow">full list of problems</a></td>
</tr>
</table>
<p><b>2. Commercial</b></p>
<ul>
<li>MATLAB <a href="http://www.mathworks.com/access/helpdesk/help/toolbox/optim/index.html?/access/helpdesk/help/toolbox/optim/ug/lsqlin.html" class="external text" title="http://www.mathworks.com/access/helpdesk/help/toolbox/optim/index.html?/access/helpdesk/help/toolbox/optim/ug/lsqlin.html" rel="nofollow">lsqlin</a></li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=17" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFLawsonHanson.2C_R._J.1974">Lawson, C. L.; Hanson, R. J. (1974). <i>Solving Least Squares Problems</i>. Englewood Cliffs, NJ: Prentice-Hall. <a href="/wiki/Special:BookSources/0138225850" class="internal">ISBN 0138225850</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Solving+Least+Squares+Problems&amp;rft.aulast=Lawson&amp;rft.aufirst=C.+L.&amp;rft.au=Lawson%2C+C.+L.&amp;rft.au=Hanson%2C+R.+J.&amp;rft.date=1974&amp;rft.place=Englewood+Cliffs%2C+NJ&amp;rft.pub=Prentice-Hall&amp;rft.isbn=0138225850&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFMargenauMurphy.2C_George_Moseley1956">Margenau, Henry; Murphy, George Moseley (1956). <i>The Mathematics of Physics and Chemistry</i>. Princeton: Van Nostrand.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Mathematics+of+Physics+and+Chemistry&amp;rft.aulast=Margenau&amp;rft.aufirst=Henry&amp;rft.au=Margenau%2C+Henry&amp;rft.au=Murphy%2C+George+Moseley&amp;rft.date=1956&amp;rft.place=Princeton&amp;rft.pub=Van+Nostrand&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-pg-2">^ <a href="#cite_ref-pg_2-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pg_2-1" title=""><sup><i><b>b</b></i></sup></a> <cite style="font-style:normal" class="book" id="CITEREFGans1992">Gans, Peter (1992). <i>Data fitting in the Chemical Sciences</i>. New York: Wiley. <a href="/wiki/Special:BookSources/0471934127" class="internal">ISBN 0471934127</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Data+fitting+in+the+Chemical+Sciences&amp;rft.aulast=Gans&amp;rft.aufirst=Peter&amp;rft.au=Gans%2C+Peter&amp;rft.date=1992&amp;rft.place=New+York&amp;rft.pub=Wiley&amp;rft.isbn=0471934127&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFDeming1943">Deming, W. E. (1943). <i>Statistical adjustment of Data</i>. New York: Wiley.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Statistical+adjustment+of+Data&amp;rft.aulast=Deming&amp;rft.aufirst=W.+E.&amp;rft.au=Deming%2C+W.+E.&amp;rft.date=1943&amp;rft.place=New+York&amp;rft.pub=Wiley&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-vm-4">^ <a href="#cite_ref-vm_4-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-vm_4-1" title=""><sup><i><b>b</b></i></sup></a> When fitting polynomials the normal equations matrix is a <a href="/wiki/Vandermonde_matrix" title="Vandermonde matrix">Vandermonde matrix</a>. Vandermode matrices become increasingly ill-conditioned as the order of the matrix increases.</li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> This implies that the observations are uncorrelated. If the observations are <a href="/wiki/Correlated" title="Correlated" class="mw-redirect">correlated</a>, the expression <img class="tex" alt="\textstyle S=\sum_k \sum_j r_k W_{kj} r_j\," src="http://upload.wikimedia.org/math/5/0/b/50b7134aa23d39670fb79f099b1e7c23.png" /> applies. In this case the weight matrix should ideally be equal to the inverse of the <a href="/wiki/Variance-covariance_matrix" title="Variance-covariance matrix" class="mw-redirect">variance-covariance matrix</a> of the observations.</li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFMandel1964">Mandel, John (1964). <i>The Statistical Analysis of Experimental Data</i>. New York: Interscience.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Statistical+Analysis+of+Experimental+Data&amp;rft.aulast=Mandel&amp;rft.aufirst=John&amp;rft.au=Mandel%2C+John&amp;rft.date=1964&amp;rft.place=New+York&amp;rft.pub=Interscience&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-7"><b><a href="#cite_ref-7" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFMardiaKent.2C_J._T..3B_Bibby.2C_J._M.1979">Mardia, K. V.; Kent, J. T.; Bibby, J. M. (1979). <i>Multivariate analysis</i>. New York: Academic Press. <a href="/wiki/Special:BookSources/0124712509" class="internal">ISBN 0124712509</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Multivariate+analysis&amp;rft.aulast=Mardia&amp;rft.aufirst=K.+V.&amp;rft.au=Mardia%2C+K.+V.&amp;rft.au=Kent%2C+J.+T.%3B+Bibby%2C+J.+M.&amp;rft.date=1979&amp;rft.place=New+York&amp;rft.pub=Academic+Press&amp;rft.isbn=0124712509&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-8"><b><a href="#cite_ref-8" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFHamilton1964">Hamilton, W. C. (1964). <i>Statistics in Physical Science</i>. New York: Ronald Press.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Statistics+in+Physical+Science&amp;rft.aulast=Hamilton&amp;rft.aufirst=W.+C.&amp;rft.au=Hamilton%2C+W.+C.&amp;rft.date=1964&amp;rft.place=New+York&amp;rft.pub=Ronald+Press&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-9"><b><a href="#cite_ref-9" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFSpiegel1975">Spiegel, Murray R. (1975). <i>Schaum's outline of theory and problems of probability and statistics</i>. New York: McGraw-Hill. <a href="/wiki/Special:BookSources/0585267391" class="internal">ISBN 0585267391</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Schaum%27s+outline+of+theory+and+problems+of+probability+and+statistics&amp;rft.aulast=Spiegel&amp;rft.aufirst=Murray+R.&amp;rft.au=Spiegel%2C+Murray+R.&amp;rft.date=1975&amp;rft.place=New+York&amp;rft.pub=McGraw-Hill&amp;rft.isbn=0585267391&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-10"><b><a href="#cite_ref-10" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFActon1959">Acton, F. S. (1959). <i>Analysis of Straight-Line Data</i>. New York: Wiley.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Analysis+of+Straight-Line+Data&amp;rft.aulast=Acton&amp;rft.aufirst=F.+S.&amp;rft.au=Acton%2C+F.+S.&amp;rft.date=1959&amp;rft.place=New+York&amp;rft.pub=Wiley&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-11"><b><a href="#cite_ref-11" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFGuest1961">Guest, P. G. (1961). <i>Numerical Methods of Curve Fitting</i>. Cambridge: Cambridge University Press.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Numerical+Methods+of+Curve+Fitting&amp;rft.aulast=Guest&amp;rft.aufirst=P.+G.&amp;rft.au=Guest%2C+P.+G.&amp;rft.date=1961&amp;rft.place=Cambridge&amp;rft.pub=Cambridge+University+Press&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
</ol>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=18" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFBj.C3.B6rck.2C_.C3.85ke1996">Björck, Åke (1996). <i>Numerical methods for least squares problems</i>. Philadelphia: SIAM. <a href="/wiki/Special:BookSources/0898713609" class="internal">ISBN 0-89871-360-9</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Numerical+methods+for+least+squares+problems&amp;rft.aulast=Bj%C3%B6rck%2C+%C3%85ke&amp;rft.au=Bj%C3%B6rck%2C+%C3%85ke&amp;rft.date=1996&amp;rft.place=Philadelphia&amp;rft.pub=SIAM&amp;rft.isbn=0-89871-360-9&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="book" id="CITEREFBevington.2C_Philip_RRobinson.2C_Keith_D2009">Bevington, Philip R; Robinson, Keith D (2003). <i>Data Reduction and Error Analysis for the Physical Sciences</i>. McGraw Hill. <a href="/wiki/Special:BookSources/0072472278" class="internal">ISBN 0072472278</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Data+Reduction+and+Error+Analysis+for+the+Physical+Sciences&amp;rft.aulast=Bevington%2C+Philip+R&amp;rft.au=Bevington%2C+Philip+R&amp;rft.au=Robinson%2C+Keith+D&amp;rft.date=2003&amp;rft.pub=McGraw+Hill&amp;rft.isbn=0072472278&amp;rfr_id=info:sid/en.wikipedia.org:Linear_least_squares"><span style="display: none;">&#160;</span></span></li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Linear_least_squares&amp;action=edit&amp;section=19" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<p><b>Theory</b></p>
<ul>
<li><a href="http://mathworld.wolfram.com/LeastSquaresFitting.html" class="external text" title="http://mathworld.wolfram.com/LeastSquaresFitting.html" rel="nofollow">Least Squares Fitting – From MathWorld</a></li>
<li><a href="http://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html" class="external text" title="http://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html" rel="nofollow">Least Squares Fitting-Polynomial – From MathWorld</a></li>
</ul>
<p><b>Online utilities</b></p>
<ul>
<li><a href="http://www.akiti.ca/LinLeastSqPoly4.html" class="external text" title="http://www.akiti.ca/LinLeastSqPoly4.html" rel="nofollow">Online Linear Least Squares Utility</a></li>
</ul>
<p><br /></p>
<table class="navbox" cellspacing="0" style=";">
<tr>
<td style="padding:2px;">
<table cellspacing="0" class="nowraplinks collapsible autocollapse" style="width:100%;background:transparent;color:inherit;;">
<tr>
<th style=";" colspan="2" class="navbox-title">
<div style="float:left; width:6em;text-align:left;">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;;;border:none;; font-size:xx-small;"><a href="/wiki/Template:Least_squares_and_regression_analysis" title="Template:Least squares and regression analysis"><span title="View this template" style=";;border:none;">v</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/w/index.php?title=Template_talk:Least_squares_and_regression_analysis&amp;action=edit&amp;redlink=1" class="new" title="Template talk:Least squares and regression analysis (page does not exist)"><span title="Discussion about this template" style=";;border:none;">d</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="http://en.wikipedia.org/w/index.php?title=Template:Least_squares_and_regression_analysis&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Template:Least_squares_and_regression_analysis&amp;action=edit" rel="nofollow"><span title="Edit this template" style=";;border:none;;">e</span></a></div>
</div>
<span style="font-size:110%;">Least squares and regression analysis</span></th>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Least_squares" title="Least squares">Least squares</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><strong class="selflink">Linear least squares</strong> - <a href="/wiki/Non-linear_least_squares" title="Non-linear least squares">Non-linear least squares</a> - <a href="/wiki/Partial_least_squares" title="Partial least squares" class="mw-redirect">Partial least squares</a> -<a href="/wiki/Total_least_squares" title="Total least squares">Total least squares</a> - <a href="/wiki/Gauss%E2%80%93Newton_algorithm" title="Gauss–Newton algorithm">Gauss–Newton algorithm</a> - <a href="/wiki/Levenberg%E2%80%93Marquardt_algorithm" title="Levenberg–Marquardt algorithm">Levenberg–Marquardt algorithm</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Regression_analysis" title="Regression analysis">Regression analysis</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a> - <a href="/wiki/Nonlinear_regression" title="Nonlinear regression">Nonlinear regression</a> - <a href="/wiki/Linear_model" title="Linear model">Linear model</a> - <a href="/wiki/Generalized_linear_model" title="Generalized linear model">Generalized linear model</a> - <a href="/wiki/Robust_regression" title="Robust regression">Robust regression</a> - <a href="/wiki/Least-squares_estimation_of_linear_regression_coefficients" title="Least-squares estimation of linear regression coefficients">Least-squares estimation of linear regression coefficients</a>- <a href="/wiki/Mean_and_predicted_response" title="Mean and predicted response">Mean and predicted response</a> - <a href="/wiki/Poisson_regression" title="Poisson regression">Poisson regression</a> - <a href="/wiki/Quantile_regression" title="Quantile regression">Quantile regression</a> - <a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a> - <a href="/wiki/Isotonic_regression" title="Isotonic regression">Isotonic regression</a> - <a href="/wiki/Ridge_regression" title="Ridge regression" class="mw-redirect">Ridge regression</a> - <a href="/wiki/Segmented_regression" title="Segmented regression">Segmented regression</a> - <a href="/wiki/Nonparametric_regression" title="Nonparametric regression">Nonparametric regression</a> - <a href="/wiki/Regression_discontinuity" title="Regression discontinuity">Regression discontinuity</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Statistics</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Gauss%E2%80%93Markov_theorem" title="Gauss–Markov theorem">Gauss–Markov theorem</a> - <a href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">Errors and residuals in statistics</a> - <a href="/wiki/Goodness_of_fit" title="Goodness of fit">Goodness of fit</a> - <a href="/wiki/Studentized_residual" title="Studentized residual">Studentized residual</a> - <a href="/wiki/Mean_squared_error" title="Mean squared error">Mean squared error</a> - <a href="/wiki/R-factor_(crystallography)" title="R-factor (crystallography)">R-factor (crystallography)</a> - <a href="/wiki/Mean_squared_prediction_error" title="Mean squared prediction error">Mean squared prediction error</a> - <a href="/wiki/Minimum_mean-square_error" title="Minimum mean-square error" class="mw-redirect">Minimum mean-square error</a> - <a href="/wiki/Root_mean_square_deviation" title="Root mean square deviation">Root mean square deviation</a> - <a href="/wiki/Squared_deviations" title="Squared deviations">Squared deviations</a> - <a href="/wiki/M-estimator" title="M-estimator">M-estimator</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Applications</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Curve_fitting" title="Curve fitting">Curve fitting</a> - <a href="/wiki/Calibration_curve" title="Calibration curve">Calibration curve</a> - <a href="/wiki/Numerical_smoothing_and_differentiation" title="Numerical smoothing and differentiation">Numerical smoothing and differentiation</a> - <a href="/wiki/Least_mean_squares_filter" title="Least mean squares filter">Least mean squares filter</a> - <a href="/wiki/Recursive_least_squares_filter" title="Recursive least squares filter">Recursive least squares filter</a> - <a href="/wiki/Moving_least_squares" title="Moving least squares">Moving least squares</a> - <a href="/wiki/BHHH_algorithm" title="BHHH algorithm">BHHH algorithm</a></div>
</td>
</tr>
</table>
</td>
</tr>
</table>


<!-- 
NewPP limit report
Preprocessor node count: 6992/1000000
Post-expand include size: 41004/2048000 bytes
Template argument size: 10861/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:484872-0!1!0!default!!en!2 and timestamp 20090406124640 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Linear_least_squares">http://en.wikipedia.org/wiki/Linear_least_squares</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Mathematical_optimization" title="Category:Mathematical optimization">Mathematical optimization</a></span> | <span dir='ltr'><a href="/wiki/Category:Linear_algebra" title="Category:Linear algebra">Linear algebra</a></span> | <span dir='ltr'><a href="/wiki/Category:Singular_value_decomposition" title="Category:Singular value decomposition">Singular value decomposition</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Linear_least_squares" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Linear_least_squares" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Linear_least_squares&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Linear_least_squares&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Linear_least_squares" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Linear_least_squares" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Linear_least_squares" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Linear_least_squares&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Linear_least_squares&amp;oldid=282099812" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Linear_least_squares&amp;id=282099812">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-af"><a href="http://af.wikipedia.org/wiki/Kleinste-kwadratemetode">Afrikaans</a></li>
				<li class="interwiki-cs"><a href="http://cs.wikipedia.org/wiki/Metoda_nejmen%C5%A1%C3%ADch_%C4%8Dtverc%C5%AF">Česky</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/M%C3%ADnimos_cuadrados">Español</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/Pienimm%C3%A4n_neli%C3%B6summan_menetelm%C3%A4">Suomi</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/M%C3%A9thode_des_moindres_carr%C3%A9s">Français</a></li>
				<li class="interwiki-gl"><a href="http://gl.wikipedia.org/wiki/M%C3%ADnimos_cadrados_lineais">Galego</a></li>
				<li class="interwiki-gl"><a href="http://gl.wikipedia.org/wiki/M%C3%ADnimos_cadrados">Galego</a></li>
				<li class="interwiki-he"><a href="http://he.wikipedia.org/wiki/%D7%A9%D7%99%D7%98%D7%AA_%D7%94%D7%A8%D7%99%D7%91%D7%95%D7%A2%D7%99%D7%9D_%D7%94%D7%A4%D7%97%D7%95%D7%AA%D7%99%D7%9D">עברית</a></li>
				<li class="interwiki-hu"><a href="http://hu.wikipedia.org/wiki/Legkisebb_n%C3%A9gyzetek_m%C3%B3dszere">Magyar</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Minimi_Quadrati">Italiano</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%97%E6%B3%95">日本語</a></li>
				<li class="interwiki-la"><a href="http://la.wikipedia.org/wiki/Methodus_quadratorum_minimorum">Latina</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Kleinste-kwadratenmethode">Nederlands</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Metoda_najmniejszych_kwadrat%C3%B3w">Polski</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/M%C3%A9todo_dos_m%C3%ADnimos_quadrados">Português</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BD%D0%B0%D0%B8%D0%BC%D0%B5%D0%BD%D1%8C%D1%88%D0%B8%D1%85_%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%BE%D0%B2">Русский</a></li>
				<li class="interwiki-su"><a href="http://su.wikipedia.org/wiki/Kuadrat_leutik">Basa Sunda</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Minstakvadratmetoden">Svenska</a></li>
				<li class="interwiki-tr"><a href="http://tr.wikipedia.org/wiki/En_k%C3%BC%C3%A7%C3%BCk_kareler_y%C3%B6ntemi">Türkçe</a></li>
				<li class="interwiki-ur"><a href="http://ur.wikipedia.org/wiki/%D9%84%DA%A9%DB%8C%D8%B1%DB%8C_%D8%A7%D9%82%D9%84_%D9%85%D8%B1%D8%A8%D8%B9%D8%A7%D8%AA">اردو</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/B%C3%ACnh_ph%C6%B0%C6%A1ng_t%E1%BB%91i_thi%E1%BB%83u_tuy%E1%BA%BFn_t%C3%ADnh">Tiếng Việt</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/B%C3%ACnh_ph%C6%B0%C6%A1ng_t%E1%BB%91i_thi%E1%BB%83u">Tiếng Việt</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 6 April 2009, at 12:46.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv54 in 0.176 secs. --></body></html>
