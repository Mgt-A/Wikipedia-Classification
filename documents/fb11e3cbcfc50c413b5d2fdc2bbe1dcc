<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Mutual information,Articles with unsourced statements since January 2009,Articles with unsourced statements since July 2008,ARACNE,Adjusted Mutual Information,Adjusted rand index,Alexander Strehl,Archaism,Athanasios Papoulis,Bit,Chain rule for Kolmogorov complexity" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Mutual_information&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Mutual_information&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Mutual information - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Mutual_information";
		var wgTitle = "Mutual information";
		var wgAction = "view";
		var wgArticleId = "427282";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 283164011;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Mutual_information skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Mutual information</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>In <a href="/wiki/Probability_theory" title="Probability theory">probability theory</a> and <a href="/wiki/Information_theory" title="Information theory">information theory</a>, the <b>mutual information</b> (sometimes known by the <a href="/wiki/Archaism" title="Archaism">archaic</a> term <b>transinformation</b>) of two <a href="/wiki/Random_variable" title="Random variable">random variables</a> is a quantity that measures the mutual dependence of the two variables. The most common <a href="/wiki/Unit_of_measurement" title="Unit of measurement" class="mw-redirect">unit of measurement</a> of mutual information is the <a href="/wiki/Bit" title="Bit">bit</a>, when logarithms to the base 2 are used.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Definition_of_mutual_information"><span class="tocnumber">1</span> <span class="toctext">Definition of mutual information</span></a></li>
<li class="toclevel-1"><a href="#Relation_to_other_quantities"><span class="tocnumber">2</span> <span class="toctext">Relation to other quantities</span></a></li>
<li class="toclevel-1"><a href="#Variations_of_the_mutual_information"><span class="tocnumber">3</span> <span class="toctext">Variations of the mutual information</span></a>
<ul>
<li class="toclevel-2"><a href="#Metric"><span class="tocnumber">3.1</span> <span class="toctext">Metric</span></a></li>
<li class="toclevel-2"><a href="#Conditional_mutual_information"><span class="tocnumber">3.2</span> <span class="toctext">Conditional mutual information</span></a></li>
<li class="toclevel-2"><a href="#Multivariate_mutual_information"><span class="tocnumber">3.3</span> <span class="toctext">Multivariate mutual information</span></a>
<ul>
<li class="toclevel-3"><a href="#Applications"><span class="tocnumber">3.3.1</span> <span class="toctext">Applications</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="#Normalized_variants"><span class="tocnumber">3.4</span> <span class="toctext">Normalized variants</span></a></li>
<li class="toclevel-2"><a href="#Weighted_variants"><span class="tocnumber">3.5</span> <span class="toctext">Weighted variants</span></a></li>
<li class="toclevel-2"><a href="#Absolute_mutual_information"><span class="tocnumber">3.6</span> <span class="toctext">Absolute mutual information</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Applications_of_mutual_information"><span class="tocnumber">4</span> <span class="toctext">Applications of mutual information</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">6</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Definition_of_mutual_information" id="Definition_of_mutual_information"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=1" title="Edit section: Definition of mutual information">edit</a>]</span> <span class="mw-headline">Definition of mutual information</span></h2>
<p>Formally, the mutual information of two discrete random variables <i>X</i> and <i>Y</i> can be defined as:</p>
<dl>
<dd><img class="tex" alt=" I(X;Y) = \sum_{y \in Y} \sum_{x \in X} 
                 p(x,y) \log{ \left( \frac{p(x,y)}{p_1(x)\,p_2(y)}
                              \right) }, \,\!
" src="http://upload.wikimedia.org/math/e/b/f/ebf36c4eb951e4273107124ff4c1f2ae.png" /></dd>
</dl>
<p>where <i>p</i>(<i>x</i>,<i>y</i>) is the <a href="/wiki/Joint_distribution" title="Joint distribution" class="mw-redirect">joint probability distribution function</a> of <i>X</i> and <i>Y</i>, and <span class="texhtml"><i>p</i><sub>1</sub>(<i>x</i>)</span> and <span class="texhtml"><i>p</i><sub>2</sub>(<i>y</i>)</span> are the <a href="/wiki/Marginal_probability" title="Marginal probability" class="mw-redirect">marginal probability</a> distribution functions of <i>X</i> and <i>Y</i> respectively.</p>
<p>In the <a href="/wiki/Continuum" title="Continuum">continuous</a> case, we replace summation by a definite <a href="/wiki/Double_integral" title="Double integral" class="mw-redirect">double integral</a>:</p>
<dl>
<dd><img class="tex" alt=" I(X;Y) = \int_Y \int_X 
                 p(x,y) \log{ \left( \frac{p(x,y)}{p_1(x)\,p_2(y)}
                              \right) } \; dx \,dy,

 " src="http://upload.wikimedia.org/math/f/7/d/f7db3e8920e04e35a92fbb25b0db2dc6.png" /></dd>
</dl>
<p>where <i>p</i>(<i>x</i>,<i>y</i>) is now the joint probability <i>density</i> function of <i>X</i> and <i>Y</i>, and <span class="texhtml"><i>p</i><sub>1</sub>(<i>x</i>)</span> and <span class="texhtml"><i>p</i><sub>2</sub>(<i>y</i>)</span> are the marginal probability density functions of <i>X</i> and <i>Y</i> respectively.</p>
<p>These definitions are ambiguous because the base of the log function is not specified. To disambiguate, the function <i>I</i> could be parameterized as <i>I</i>(<i>X</i>,<i>Y</i>,<i>b</i>) where <i>b</i> is the base. Alternatively, since the most common unit of measurement of mutual information is the bit, a base of 2 could be specified.</p>
<p>Intuitively, mutual information measures the information that <i>X</i> and <i>Y</i> share: it measures how much knowing one of these variables reduces our uncertainty about the other. For example, if <i>X</i> and <i>Y</i> are independent, then knowing <i>X</i> does not give any information about <i>Y</i> and vice versa, so their mutual information is zero. At the other extreme, if <i>X</i> and <i>Y</i> are identical then all information conveyed by <i>X</i> is shared with <i>Y</i>: knowing <i>X</i> determines the value of <i>Y</i> and vice versa. As a result, the mutual information is the same as the uncertainty contained in <i>Y</i> (or <i>X</i>) alone, namely the <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">entropy</a> of <i>Y</i> (or <i>X</i>: clearly if <i>X</i> and <i>Y</i> are identical they have equal entropy).</p>
<p>Mutual information quantifies the dependence between the <a href="/wiki/Joint_distribution" title="Joint distribution" class="mw-redirect">joint distribution</a> of <i>X</i> and <i>Y</i> and what the joint distribution would be if <i>X</i> and <i>Y</i> were independent. Mutual information is a measure of dependence in the following sense: <i>I</i>(<i>X</i>; <i>Y</i>) = 0 <a href="/wiki/If_and_only_if" title="If and only if">if and only if</a> <i>X</i> and <i>Y</i> are independent random variables. This is easy to see in one direction: if <i>X</i> and <i>Y</i> are independent, then <i>p</i>(<i>x</i>,<i>y</i>) = <i>p</i>(<i>x</i>) <i>p</i>(<i>y</i>), and therefore:</p>
<dl>
<dd><img class="tex" alt=" \log{ \left( \frac{p(x,y)}{p(x)\,p(y)} \right) } = \log 1 = 0. \,\!
 " src="http://upload.wikimedia.org/math/6/e/5/6e5087c7f138e9b0dd333686e86dc85e.png" /></dd>
</dl>
<p>Moreover, mutual information is nonnegative (i.e. <i>I</i>(<i>X</i>;<i>Y</i>)&#160;≥&#160;0; see below) and <a href="/wiki/Symmetric_function" title="Symmetric function">symmetric</a> (i.e. <i>I</i>(<i>X</i>;<i>Y</i>) = <i>I</i>(<i>Y</i>;<i>X</i>)).</p>
<p><a name="Relation_to_other_quantities" id="Relation_to_other_quantities"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=2" title="Edit section: Relation to other quantities">edit</a>]</span> <span class="mw-headline">Relation to other quantities</span></h2>
<p>Mutual information can be equivalently expressed as</p>
<dl>
<dd><img class="tex" alt="
\begin{align}
I(X;Y) &amp; {} = H(X) - H(X|Y) \\ 
&amp; {} = H(Y) - H(Y|X) \\ 
&amp; {} = H(X) + H(Y) - H(X,Y) \\
&amp; {} = H(X,Y) - H(X|Y) - H(Y|X)
\end{align}
" src="http://upload.wikimedia.org/math/f/0/5/f057d50bc7444abf29025acaffb4df40.png" /></dd>
</dl>
<p>where <i>H</i>(<i>X</i>) and <i>H</i>(<i>Y</i>) are the marginal <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">entropies</a>, <i>H</i>(<i>X</i>|<i>Y</i>) and <i>H</i>(<i>Y</i>|<i>X</i>) are the <a href="/wiki/Conditional_entropy" title="Conditional entropy">conditional entropies</a>, and <i>H</i>(<i>X</i>,<i>Y</i>) is the <a href="/wiki/Joint_entropy" title="Joint entropy">joint entropy</a> of <i>X</i> and <i>Y</i>. Since <i>H</i>(<i>X</i>) ≥ <i>H</i>(<i>X</i>|<i>Y</i>), this characterization is consistent with the nonnegativity property stated above.</p>
<p>Intuitively, if entropy <i>H</i>(<i>X</i>) is regarded as a measure of uncertainty about a random variable, then <i>H</i>(<i>X</i>|<i>Y</i>) is a measure of what <i>Y</i> does <em>not</em> say about <i>X</i>. This is "the amount of uncertainty remaining about <i>X</i> after <i>Y</i> is known", and thus the right side of the first of these equalities can be read as "the amount of uncertainty in <i>X</i>, minus the amount of uncertainty in <i>X</i> which remains after <i>Y</i> is known", which is equivalent to "the amount of uncertainty in <i>X</i> which is removed by knowing <i>Y</i>". This corroborates the intuitive meaning of mutual information as the amount of information (that is, reduction in uncertainty) that knowing either variable provides about the other.</p>
<p>Note that in the discrete case <i>H</i>(<i>X</i>|<i>X</i>) = 0 and therefore <i>H</i>(<i>X</i>) = <i>I</i>(<i>X</i>;<i>X</i>). Thus <i>I</i>(<i>X</i>;<i>X</i>) ≥ <i>I</i>(<i>X</i>;<i>Y</i>), and one can formulate the basic principle that a variable contains more information about itself than any other variable can provide.</p>
<p>Mutual information can also be expressed as a <a href="/wiki/Kullback-Leibler_divergence" title="Kullback-Leibler divergence" class="mw-redirect">Kullback-Leibler divergence</a>, of the product <i>p</i>(<i>x</i>) × <i>p</i>(<i>y</i>) of the <a href="/wiki/Marginal_distribution" title="Marginal distribution">marginal distributions</a> of the two random variables <i>X</i> and <i>Y</i>, from <i>p</i>(<i>x</i>,<i>y</i>) the random variables' <a href="/wiki/Joint_distribution" title="Joint distribution" class="mw-redirect">joint distribution</a>:</p>
<dl>
<dd><img class="tex" alt=" I(X;Y) = D_{\mathrm{KL}}(p(x,y)\|p(x)p(y)). " src="http://upload.wikimedia.org/math/6/4/2/642931f5e9ef0e5f29e3fb5ef24c87b8.png" /></dd>
</dl>
<p>Furthermore, let <i>p</i>(<i>x</i>|<i>y</i>) = <i>p</i>(<i>x</i>, <i>y</i>) / <i>p</i>(<i>y</i>). Then</p>
<dl>
<dd><img class="tex" alt="
\begin{align}
I(X;Y) &amp; {} = \sum_y p(y) \sum_x p(x|y) \log_2 \frac{p(x|y)}{p(x)} \\
&amp; {} =  \sum_y p(y) \; D_{\mathrm{KL}}(p(x|y)\|p(x)) \\
&amp; {} = \mathbb{E}_Y\{D_{\mathrm{KL}}(p(x|y)\|p(x))\}.
\end{align}
" src="http://upload.wikimedia.org/math/3/a/5/3a5f6bd614910767bdaf7dc6eb5aadd3.png" /></dd>
</dl>
<p>Thus mutual information can also be understood as the <a href="/wiki/Expected_value" title="Expected value">expectation</a> of the Kullback-Leibler divergence of the univariate distribution <i>p</i>(<i>x</i>) of <i>X</i> from the <a href="/wiki/Conditional_distribution" title="Conditional distribution" class="mw-redirect">conditional distribution</a> <i>p</i>(<i>x</i>|<i>y</i>) of <i>X</i> given <i>Y</i>: the more different the distributions <i>p</i>(<i>x</i>|<i>y</i>) and <i>p</i>(<i>x</i>), the greater the <a href="/wiki/Kullback-Leibler_divergence" title="Kullback-Leibler divergence" class="mw-redirect">information gain</a>.</p>
<p><a name="Variations_of_the_mutual_information" id="Variations_of_the_mutual_information"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=3" title="Edit section: Variations of the mutual information">edit</a>]</span> <span class="mw-headline">Variations of the mutual information</span></h2>
<p>Several variations on the mutual information have been proposed to suit various needs. Among these are normalized variants and generalizations to more than two variables.</p>
<p><a name="Metric" id="Metric"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=4" title="Edit section: Metric">edit</a>]</span> <span class="mw-headline">Metric</span></h3>
<p>Many applications require a <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a>, that is, a distance measure between points. The quantity</p>
<dl>
<dd><span class="texhtml"><i>d</i>(<i>X</i>,<i>Y</i>) = <i>H</i>(<i>X</i>,<i>Y</i>) − <i>I</i>(<i>X</i>;<i>Y</i>)</span></dd>
</dl>
<p>satisfies the basic properties of a metric; most importantly, the <a href="/wiki/Triangle_inequality" title="Triangle inequality">triangle inequality</a>, but also <a href="/wiki/Non-negative" title="Non-negative" class="mw-redirect">non-negativity</a>, <a href="/wiki/Identity_of_indiscernibles" title="Identity of indiscernibles">indiscernability</a> and symmetry. In addition, one also has <img class="tex" alt="d(X,Y) \le H(X,Y)" src="http://upload.wikimedia.org/math/b/9/9/b9990b12fbc78c0d97b810d1cbc7ed02.png" />, and so</p>
<dl>
<dd><img class="tex" alt="D(X,Y) = d(X,Y)/H(X,Y) \le 1" src="http://upload.wikimedia.org/math/6/5/9/659910602bfb8215a83cc17c83896a74.png" /></dd>
</dl>
<p>The metric <i>D</i> is a <a href="/w/index.php?title=Universal_metric&amp;action=edit&amp;redlink=1" class="new" title="Universal metric (page does not exist)">universal metric</a>, in that if any other distance measure places <i>X</i> and <i>Y</i> close-by, then the <i>D</i> will also judge them close.<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup></p>
<p><a name="Conditional_mutual_information" id="Conditional_mutual_information"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=5" title="Edit section: Conditional mutual information">edit</a>]</span> <span class="mw-headline">Conditional mutual information</span></h3>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/Conditional_mutual_information" title="Conditional mutual information">Conditional mutual information</a></div>
<p>Sometimes it is useful to express the mutual information of two random variables conditioned on a third.</p>
<dl>
<dd><img class="tex" alt="I(X;Y|Z) = \mathbb E_Z \big(I(X;Y)|Z\big)
    = \sum_{z\in Z} \sum_{y\in Y} \sum_{x\in X}
      p_Z(z) p_{X,Y|Z}(x,y|z) \log \frac{p_{X,Y|Z}(x,y|z)}{p_{X|Z}(x|z)p_{Y|Z}(y|z)}," src="http://upload.wikimedia.org/math/8/a/a/8aac43d316a5bca7276aa3c56d7230e9.png" /></dd>
</dl>
<p>which can be simplified as</p>
<dl>
<dd><img class="tex" alt="I(X;Y|Z) = \sum_{z\in Z} \sum_{y\in Y} \sum_{x\in X}
      p_{X,Y,Z}(x,y,z) \log \frac{p_Z(z)p_{X,Y,Z}(x,y,z)}{p_{X,Z}(x,z)p_{Y,Z}(y,z)}." src="http://upload.wikimedia.org/math/0/0/6/00639eb303e9806b96a64df823d892b7.png" /></dd>
</dl>
<p>Conditioning on a third random variable may either increase or decrease the mutual information, but it is always true that</p>
<dl>
<dd><img class="tex" alt="I(X;Y|Z) \ge 0" src="http://upload.wikimedia.org/math/5/7/6/576e6ff97c2b3a9e4e3cbe12cb4e5a5c.png" /></dd>
</dl>
<p>for discrete, jointly distributed random variables <i>X</i>, <i>Y</i>, <i>Z</i>. This result has been used as a basic building block for proving other <a href="/wiki/Inequalities_in_information_theory" title="Inequalities in information theory">inequalities in information theory</a>.</p>
<p><a name="Multivariate_mutual_information" id="Multivariate_mutual_information"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=6" title="Edit section: Multivariate mutual information">edit</a>]</span> <span class="mw-headline">Multivariate mutual information</span></h3>
<p>Several generalizations of mutual information to more than two random variables have been proposed, such as <a href="/wiki/Total_correlation" title="Total correlation">total correlation</a> and <a href="/wiki/Interaction_information" title="Interaction information">interaction information</a>. If Shannon entropy is viewed as a <a href="/wiki/Signed_measure" title="Signed measure">signed measure</a> in the context of <a href="/wiki/Information_diagram" title="Information diagram">information diagrams</a>, as explained in the article <i><a href="/wiki/Information_theory_and_measure_theory" title="Information theory and measure theory">Information theory and measure theory</a></i>, then the only definition of multivariate mutual information that makes sense<sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since January 2009" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup> is as follows:</p>
<dl>
<dd><span class="texhtml"><i>I</i>(<i>X</i><sub>1</sub>) = <i>H</i>(<i>X</i><sub>1</sub>)</span></dd>
</dl>
<p>and for <span class="texhtml"><i>n</i> &gt; 1,</span></p>
<dl>
<dd><img class="tex" alt="I(X_1;\,...\,;X_n) = I(X_1;\,...\,;X_{n-1}) - I(X_1;\,...\,;X_{n-1}|X_n)," src="http://upload.wikimedia.org/math/9/9/7/9977a815c26d7735d291d23688ae8c79.png" /></dd>
</dl>
<p>where (as above) we define</p>
<dl>
<dd><img class="tex" alt="I(X_1;\,...\,;X_{n-1}|X_n) = \mathbb E_{X_n} \big(I(X_1;\,...\,;X_{n-1})|X_n\big)." src="http://upload.wikimedia.org/math/1/7/0/170014196ff19977057475bb7a632ca4.png" /></dd>
</dl>
<p>(This definition of multivariate mutual information is identical to that of <a href="/wiki/Interaction_information" title="Interaction information">interaction information</a> except for a change in sign when the number of random variables is odd.)</p>
<p><a name="Applications" id="Applications"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=7" title="Edit section: Applications">edit</a>]</span> <span class="mw-headline">Applications</span></h4>
<p>Some have criticized the blind application of information diagrams used to derive the above definition, and indeed it has found rather limited practical application, since it is difficult to visualize or grasp the significance of this quantity for a large number of random variables. It can be zero, positive, or negative for any <img class="tex" alt="n \ge 3." src="http://upload.wikimedia.org/math/9/7/9/979394f8d46961b0db129c038e9577f5.png" /></p>
<p>One high-dimensional generalization scheme that maximizes the mutual information between the joint distribution and other target variables is found to be useful in <a href="/wiki/Feature_selection" title="Feature selection">feature selection</a>.</p>
<p><a name="Normalized_variants" id="Normalized_variants"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=8" title="Edit section: Normalized variants">edit</a>]</span> <span class="mw-headline">Normalized variants</span></h3>
<p>Normalized variants of the mutual information are provided by the <i>coefficients of constraint</i> (Coombs, Dawes &amp; Tversky 1970) or <i>uncertainty coefficient</i> (Press &amp; Flannery 1988)</p>
<dl>
<dd><img class="tex" alt="
C_{XY}=\frac{I(X;Y)}{H(Y)} ~~~~\mbox{and}~~~~ C_{YX}=\frac{I(X;Y)}{H(X)}.
" src="http://upload.wikimedia.org/math/3/2/b/32b46bcb115c89bf2fa29992343e7184.png" /></dd>
</dl>
<p>The two coefficients are not necessarily equal. A more useful and symmetric scaled information measure is the <i>redundancy</i><sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since July 2008" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup></p>
<dl>
<dd><img class="tex" alt="R= \frac{I(X;Y)}{H(X)+H(Y)}" src="http://upload.wikimedia.org/math/3/2/f/32f77ec76817db6fd8f8ed732c29d329.png" /></dd>
</dl>
<p>which attains a minimum of zero when the variables are independent and a maximum value of</p>
<dl>
<dd><img class="tex" alt="R_{\max }=\frac{\min (H(X),H(Y))}{H(X)+H(Y)} " src="http://upload.wikimedia.org/math/c/b/6/cb6ad439b4977703d4977d3b52a292ef.png" /></dd>
</dl>
<p>when one variable becomes completely redundant with the knowledge of the other. See also <i><a href="/wiki/Redundancy_(information_theory)" title="Redundancy (information theory)">Redundancy (information theory)</a></i>. Another symmetrical measure is the <i>symmetric uncertainty</i> (Witten &amp; Frank 2005), given by</p>
<dl>
<dd><img class="tex" alt="U(X,Y) = 2R = 2 \frac{I(X;Y)}{H(X)+H(Y)}" src="http://upload.wikimedia.org/math/c/9/0/c9014dd4b7b6aa460b3918649720548f.png" /></dd>
</dl>
<p>which represents a weighted average of the two uncertainty coefficients (Press &amp; Flannery 1988).</p>
<p>Other normalized versions are provided by the following expressions (Yao 2003, Strehl &amp; Ghosh 2002).</p>
<dl>
<dd><img class="tex" alt="
\frac{I(X;Y)}{\operatorname{min}(H(X),H(Y))}, ~~~~~~~ \frac{I(X;Y)}{H(X,Y)}, ~~~~~~~ \frac{I(X;Y)}{\sqrt{H(X)H(Y)}}
" src="http://upload.wikimedia.org/math/5/c/6/5c68aeac73252d6dff17b6c344812a91.png" /></dd>
</dl>
<p>The quantity</p>
<dl>
<dd><img class="tex" alt="D^\prime(X,Y)=1-\frac{I(X;Y)}{\operatorname{max}(H(X),H(Y))}" src="http://upload.wikimedia.org/math/b/9/8/b9835e9fe2a4dd0546033bc8d7912e81.png" /></dd>
</dl>
<p>is a <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a>, <i>i.e.</i> satisfies the triangle inequality, <i>etc.</i> The metric <img class="tex" alt="D^\prime" src="http://upload.wikimedia.org/math/6/2/6/626337bc7c457a62805ca451d539a6fd.png" /> is also a universal metric.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<p><a name="Weighted_variants" id="Weighted_variants"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=9" title="Edit section: Weighted variants">edit</a>]</span> <span class="mw-headline">Weighted variants</span></h3>
<p>In the traditional formulation of the mutual information,</p>
<dl>
<dd><img class="tex" alt=" I(X;Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)}, " src="http://upload.wikimedia.org/math/e/8/4/e84bdbeee4c83ba0aace93707c46bc69.png" /></dd>
</dl>
<p>each <i>event</i> or <i>object</i> specified by <span class="texhtml">(<i>x</i>,<i>y</i>)</span> is weighted by the corresponding probability <span class="texhtml"><i>p</i>(<i>x</i>,<i>y</i>)</span>. This assumes that all objects or events are equivalent <i>apart from</i> their probability of occurrence. However, in some applications it may be the case that certain objects or events are more <i>significant</i> than others, or that certain patterns of association are more semantically important than others.</p>
<p>For example, the deterministic mapping <span class="texhtml">{(1,1),(2,2),(3,3)}</span> may be viewed as stronger (by some standard) than the deterministic mapping <span class="texhtml">{(1,3),(2,1),(3,2)}</span>, although these relationships would yield the same mutual information. This is because the mutual information is not sensitive at all to any inherent ordering in the variable values (Cronbach 1954, Coombs &amp; Dawes 1970, Lockhead 1970), and is therefore not sensitive at all to the <b>form</b> of the relational mapping between the associated variables. If it is desired that the former relation — showing agreement on all variable values — be judged stronger than the later relation, then it is possible to use the following <i>weighted mutual information</i> (Guiasu 1977)</p>
<dl>
<dd><img class="tex" alt=" I(X;Y) = \sum_{y \in Y} \sum_{x \in X} w(x,y) p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)}, " src="http://upload.wikimedia.org/math/2/9/2/292ac40c37d95c424255335eb0534187.png" /></dd>
</dl>
<p>which places a weight <span class="texhtml"><i>w</i>(<i>x</i>,<i>y</i>)</span> on the probability of each variable value co-occurrence, <span class="texhtml"><i>p</i>(<i>x</i>,<i>y</i>)</span>. This allows that certain probabilities may carry more or less significance than others, thereby allowing the quantification of relevant <i>holistic</i> or <i>prägnanz</i> factors. In the above example, using larger relative weights for <span class="texhtml"><i>w</i>(1,1)</span>, <span class="texhtml"><i>w</i>(2,2)</span>, and <span class="texhtml"><i>w</i>(3,3)</span> would have the effect of assessing greater <i>informativeness</i> for the relation <span class="texhtml">{(1,1),(2,2),(3,3)}</span> than for the relation <span class="texhtml">{(1,3),(2,1),(3,2)}</span>, which may be desirable in some cases of pattern recognition, and the like. There has been little mathematical work done on the weighted mutual information and its properties, however.</p>
<p><a name="Absolute_mutual_information" id="Absolute_mutual_information"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=10" title="Edit section: Absolute mutual information">edit</a>]</span> <span class="mw-headline">Absolute mutual information</span></h3>
<p>Using the ideas of <a href="/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Kolmogorov complexity</a>, one can consider the mutual information of two sequences independent of any probability distribution:</p>
<dl>
<dd><span class="texhtml"><i>I</i><sub><i>K</i></sub>(<i>X</i>;<i>Y</i>) = <i>K</i>(<i>X</i>) − <i>K</i>(<i>X</i> | <i>Y</i>).</span></dd>
</dl>
<p>To establish that this quantity is symmetric up to a logarithmic factor (<img class="tex" alt="I_K(X;Y) \approx I_K(Y;X)" src="http://upload.wikimedia.org/math/4/1/a/41a613a59587a353a5d9712d1f0fbf8e.png" />) requires the <a href="/wiki/Chain_rule_for_Kolmogorov_complexity" title="Chain rule for Kolmogorov complexity">chain rule for Kolmogorov complexity</a> <cite class="inline">(<a href="#CITEREFLi1997" title="">Li 1997</a>)</cite>. Approximations of this quantity via <a href="/wiki/Data_compression" title="Data compression">compression</a> can be used to define a <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">distance measure</a> to perform a <a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering" class="mw-redirect">hierarchical clustering</a> of sequences without having any <a href="/wiki/Domain_knowledge" title="Domain knowledge">domain knowledge</a> of the sequences <cite class="inline">(<a href="#CITEREFCilibrasi2005" title="">Cilibrasi 2005</a>)</cite>.</p>
<p><a name="Applications_of_mutual_information" id="Applications_of_mutual_information"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=11" title="Edit section: Applications of mutual information">edit</a>]</span> <span class="mw-headline">Applications of mutual information</span></h2>
<p>In many applications, one wants to maximize mutual information (thus increasing dependencies), which is often equivalent to minimizing <a href="/wiki/Conditional_entropy" title="Conditional entropy">conditional entropy</a>. Examples include:</p>
<ul>
<li>The <a href="/wiki/Channel_capacity" title="Channel capacity">channel capacity</a> is equal to the mutual information, maximized over all input distributions.</li>
<li><a href="/w/index.php?title=Discriminative_training&amp;action=edit&amp;redlink=1" class="new" title="Discriminative training (page does not exist)">Discriminative training</a> procedures for <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">hidden Markov models</a> have been proposed based on the <a href="/w/index.php?title=Maximum_mutual_information&amp;action=edit&amp;redlink=1" class="new" title="Maximum mutual information (page does not exist)">maximum mutual information</a> (MMI) criterion.</li>
<li><a href="/wiki/RNA_structure" title="RNA structure">RNA secondary structure</a> prediction from a <a href="/wiki/Multiple_sequence_alignment" title="Multiple sequence alignment">multiple sequence alignment</a>.</li>
<li>Mutual information has been used as a criterion for <a href="/wiki/Feature_selection" title="Feature selection">feature selection</a> and feature transformations in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>. It can be used to characterize both the relevance and redundancy of variables, such as the <a href="/wiki/Minimum_redundancy_feature_selection" title="Minimum redundancy feature selection">minimum redundancy feature selection</a>.</li>
<li>Mutual information is often used as a significance function for the computation of <a href="/wiki/Collocation" title="Collocation">collocations</a> in <a href="/wiki/Corpus_linguistics" title="Corpus linguistics">corpus linguistics</a>.</li>
<li>Mutual information is used in <a href="/wiki/Medical_imaging" title="Medical imaging">medical imaging</a> for <a href="/wiki/Image_registration" title="Image registration">image registration</a>. Given a reference image (for example, a brain scan), and a second image which needs to be put into the same <a href="/wiki/Coordinate_system" title="Coordinate system">coordinate system</a> as the reference image, this image is deformed until the mutual information between it and the reference image is maximized.</li>
<li>Detection of <a href="/wiki/Phase_synchronization" title="Phase synchronization">phase synchronization</a> in <a href="/wiki/Time_series" title="Time series">time series</a> analysis</li>
<li>In the <a href="/wiki/Infomax" title="Infomax">infomax</a> method for neural-net and other machine learning, including the infomax-based <a href="/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a> algorithm</li>
<li>Average mutual information in <a href="/wiki/Delay_embedding_theorem" title="Delay embedding theorem" class="mw-redirect">delay embedding theorem</a> is used for determining the <i>embedding delay</i> parameter.</li>
<li>Mutual information between <a href="/wiki/Genes" title="Genes" class="mw-redirect">genes</a> in <a href="/wiki/Microarray" title="Microarray">expression microarray</a> data is used by the <a href="/wiki/ARACNE" title="ARACNE">ARACNE</a> algorithm for reconstruction of <a href="/wiki/Gene_regulatory_network" title="Gene regulatory network">gene networks</a>.</li>
<li>Mutual information is used as a clusterings comparing measure, provided some advantages over other classical measures such as the <a href="/wiki/Rand_index" title="Rand index">Rand index</a> and the <a href="/wiki/Adjusted_rand_index" title="Adjusted rand index">Adjusted rand index</a>.</li>
<li>The adjusted-for-chance version of the mutual information is the <a href="/wiki/Adjusted_Mutual_Information" title="Adjusted Mutual Information">Adjusted Mutual Information</a> (AMI). It is used for comparing clustering. It corrects the effect of agreement solely due to chance between clusterings, similar to the way the <a href="/wiki/Adjusted_rand_index" title="Adjusted rand index">Adjusted rand index</a> corrects the <a href="/wiki/Rand_index" title="Rand index">Rand index</a>. A Matlab program for calculating the Adjusted Mutual Information between two clusterings can be obtained from <a href="http://ee.unsw.edu.au/~nguyenv/Software.htm" class="external free" title="http://ee.unsw.edu.au/~nguyenv/Software.htm" rel="nofollow">http://ee.unsw.edu.au/~nguyenv/Software.htm</a></li>
</ul>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=12" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Pointwise_mutual_information" title="Pointwise mutual information">Pointwise mutual information</a></li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=13" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> Alexander Kraskov, Harald Stögbauer, Ralph G. Andrzejak, and Peter Grassberger, "Hierarchical Clustering Based on Mutual Information", (2003) <i><a href="http://arxiv.org/abs/q-bio/0311039" class="external text" title="http://arxiv.org/abs/q-bio/0311039" rel="nofollow">ArXiv q-bio/0311039</a></i></li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> Kraskov, <i>et al.</i> <i>ibid.</i></li>
</ol>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Mutual_information&amp;action=edit&amp;section=14" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><cite style="font-style:normal" class="" id="CITEREFCilibrasiPaul_Vit.26aacute.3Bnyi2009">Cilibrasi, R.; Paul Vitányi (2005). "<a href="http://www.cwi.nl/~paulv/papers/cluster.pdf" class="external text" title="http://www.cwi.nl/~paulv/papers/cluster.pdf" rel="nofollow">Clustering by compression</a>" (<a href="/wiki/PDF" title="PDF" class="mw-redirect">PDF</a>). <i>IEEE Transactions on Information Theory</i> <b>51</b> (4): 1523–1545. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1109%2FTIT.2005.844059" class="external text" title="http://dx.doi.org/10.1109%2FTIT.2005.844059" rel="nofollow">10.1109/TIT.2005.844059</a></span><span class="printonly">. <a href="http://www.cwi.nl/~paulv/papers/cluster.pdf" class="external free" title="http://www.cwi.nl/~paulv/papers/cluster.pdf" rel="nofollow">http://www.cwi.nl/~paulv/papers/cluster.pdf</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Clustering+by+compression&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.aulast=Cilibrasi&amp;rft.aufirst=R.&amp;rft.au=Cilibrasi%2C+R.&amp;rft.au=Paul+Vit%26aacute%3Bnyi&amp;rft.date=2005&amp;rft.volume=51&amp;rft.issue=4&amp;rft.pages=1523%E2%80%931545&amp;rft_id=info:doi/10.1109%2FTIT.2005.844059&amp;rft_id=http%3A%2F%2Fwww.cwi.nl%2F%7Epaulv%2Fpapers%2Fcluster.pdf&amp;rfr_id=info:sid/en.wikipedia.org:Mutual_information"><span style="display: none;">&#160;</span></span></li>
<li>Coombs, C. H., Dawes, R. M. &amp; Tversky, A. (1970), <i>Mathematical Psychology: An Elementary Introduction</i>, Prentice-Hall, Englewood Cliffs, NJ.</li>
<li>Cronbach L. J. (1954). On the non-rational application of information measures in psychology, in H Quastler, ed., <i>Information Theory in Psychology: Problems and Methods</i>, Free Press, Glencoe, Illinois, pp. 14—30.</li>
<li>Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography, <i>Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics</i>, 1989.</li>
<li>Guiasu, Silviu (1977), <i>Information Theory with Applications</i>, McGraw-Hill, New York.</li>
<li><cite style="font-style:normal" class="book" id="CITEREFLiPaul_Vit.26aacute.3Bnyi1997">Li, Ming; Paul Vitányi (1997). <i>An introduction to Kolmogorov complexity and its applications</i>. New York: <a href="/wiki/Springer-Verlag" title="Springer-Verlag" class="mw-redirect">Springer-Verlag</a>. <a href="/wiki/Special:BookSources/0387948686" class="internal">ISBN 0387948686</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=An+introduction+to+Kolmogorov+complexity+and+its+applications&amp;rft.aulast=Li&amp;rft.aufirst=Ming&amp;rft.au=Li%2C+Ming&amp;rft.au=Paul+Vit%26aacute%3Bnyi&amp;rft.date=1997&amp;rft.place=New+York&amp;rft.pub=%5B%5BSpringer-Verlag%5D%5D&amp;rft.isbn=0387948686&amp;rfr_id=info:sid/en.wikipedia.org:Mutual_information"><span style="display: none;">&#160;</span></span></li>
<li>Lockhead G. R. (1970). Identification and the form of multidimensional discrimination space, <i>Journal of Experimental Psychology</i> <b>85</b>(1), 1-10.</li>
<li><a href="/wiki/Athanasios_Papoulis" title="Athanasios Papoulis">Athanasios Papoulis</a>. <i>Probability, Random Variables, and Stochastic Processes</i>, second edition. New York: McGraw-Hill, 1984. <i>(See Chapter 15.)</i></li>
<li>Press, W. H., Flannery, B. P., Teukolsky, S. A. &amp; Vetterling, W. T. (1988), <i>Numerical Recipes in C: The Art of Scientific Computing</i>, Cambridge University Press, Cambridge, p. 634</li>
<li><cite style="font-style:normal" class="" id="CITEREF.5B.5BAlexander_Strehl.7CStrehl.5D.5DJoydeep_Ghosh2009"><a href="/wiki/Alexander_Strehl" title="Alexander Strehl">Strehl</a>, Alexander; Joydeep Ghosh (2002). "<a href="http://strehl.com/download/strehl-jmlr02.pdf" class="external text" title="http://strehl.com/download/strehl-jmlr02.pdf" rel="nofollow">Cluster ensembles -- a knowledge reuse framework for combining multiple partitions</a>" (<a href="/wiki/PDF" title="PDF" class="mw-redirect">PDF</a>). <i>Journal of Machine Learning Research</i> <b>3</b>: 583–617. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1162%2F153244303321897735" class="external text" title="http://dx.doi.org/10.1162%2F153244303321897735" rel="nofollow">10.1162/153244303321897735</a></span><span class="printonly">. <a href="http://strehl.com/download/strehl-jmlr02.pdf" class="external free" title="http://strehl.com/download/strehl-jmlr02.pdf" rel="nofollow">http://strehl.com/download/strehl-jmlr02.pdf</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Cluster+ensembles+--+a+knowledge+reuse+framework+for+combining+multiple+partitions&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.aulast=%5B%5BAlexander+Strehl%7CStrehl%5D%5D&amp;rft.aufirst=Alexander&amp;rft.au=%5B%5BAlexander+Strehl%7CStrehl%5D%5D%2C+Alexander&amp;rft.au=Joydeep+Ghosh&amp;rft.date=2002&amp;rft.volume=3&amp;rft.pages=583%E2%80%93617&amp;rft_id=info:doi/10.1162%2F153244303321897735&amp;rft_id=http%3A%2F%2Fstrehl.com%2Fdownload%2Fstrehl-jmlr02.pdf&amp;rfr_id=info:sid/en.wikipedia.org:Mutual_information"><span style="display: none;">&#160;</span></span></li>
<li>Witten, Ian H. &amp; Frank, Eibe (2005), <i>Data Mining: Practical Machine Learning Tools and Techniques</i>, Morgan Kaufmann, Amsterdam.</li>
<li>Yao, Y. Y. (2003) Information-theoretic measures for knowledge discovery and data mining, in <i>Entropy Measures, Maximum Entropy Principle and Emerging Applications</i> , Karmeshu (ed.), Springer, pp. 115-136.</li>
<li>Peng, H.C., Long, F., and Ding, C., "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy," IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 27, No. 8, pp.1226-1238, 2005. <a href="http://research.janelia.org/peng/proj/mRMR/index.htm" class="external text" title="http://research.janelia.org/peng/proj/mRMR/index.htm" rel="nofollow">Program</a></li>
<li>Andre S. Ribeiro, Stuart A. Kauffman, Jason Lloyd-Price, Bjorn Samuelsson, and Joshua Socolar, (2008) "Mutual Information in Random Boolean models of regulatory networks", Physical Review E, Vol.77, No.1. arXiv:0707.3642.</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 2055/1000000
Post-expand include size: 16383/2048000 bytes
Template argument size: 5115/2048000 bytes
Expensive parser function count: 2/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:427282-0!1!0!default!!en!2 and timestamp 20090411122935 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Mutual_information">http://en.wikipedia.org/wiki/Mutual_information</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Information_theory" title="Category:Information theory">Information theory</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_January_2009" title="Category:Articles with unsourced statements since January 2009">Articles with unsourced statements since January 2009</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_July_2008" title="Category:Articles with unsourced statements since July 2008">Articles with unsourced statements since July 2008</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Mutual_information" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Mutual_information" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Mutual_information&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Mutual_information&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Mutual_information" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Mutual_information" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Mutual_information" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Mutual_information&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Mutual_information&amp;oldid=283164011" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Mutual_information&amp;id=283164011">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Transinformation">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Informaci%C3%B3n_mutua">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Information_mutuelle">Français</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E7%9B%B8%E4%BA%92%E6%83%85%E5%A0%B1%E9%87%8F">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Informacja_wzajemna">Polski</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%92%D0%B7%D0%B0%D0%B8%D0%BC%D0%BD%D0%B0%D1%8F_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%8F">Русский</a></li>
				<li class="interwiki-simple"><a href="http://simple.wikipedia.org/wiki/Mutual_information">Simple English</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 11 April 2009, at 12:29 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv210 in 0.044 secs. --></body></html>
