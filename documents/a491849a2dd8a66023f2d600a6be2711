<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="N-gram,N-gram,BLAST,Base pairs,Bayesian inference,Bigram,Claude Shannon,Computational linguistics,Cosine similarity,Data compression,Dissociated press" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=N-gram&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=N-gram&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>N-gram - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "N-gram";
		var wgTitle = "N-gram";
		var wgAction = "view";
		var wgArticleId = "986182";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282707313;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-N-gram skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">N-gram</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_style.png" class="image" title="Ambox style.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/d/d6/Ambox_style.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">The introduction to this article provides <b>insufficient context</b> for those unfamiliar with the subject.<br />
<small>Please help <a href="http://en.wikipedia.org/w/index.php?title=N-gram&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=N-gram&amp;action=edit" rel="nofollow">improve the article</a> with a <a href="/wiki/Wikipedia:Writing_better_articles#Lead_section" title="Wikipedia:Writing better articles">good introductory style</a>.</small></td>
</tr>
</table>
<div class="dablink">Not to be confused with <a href="/wiki/Engram" title="Engram">engram</a>.</div>
<p>N-gram models are a type of probabilistic model for predicting the next item in a sequence. <i>n</i>-grams are used in various areas of statistical <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> and genetic sequence analysis.</p>
<p>An <i>n</i>-gram is a sub-sequence of <i>n</i> items from a given <a href="/wiki/Sequence" title="Sequence">sequence</a>. The items in question can be <a href="/wiki/Phoneme" title="Phoneme">phonemes</a>, syllables, letters, words or <a href="/wiki/Base_pairs" title="Base pairs" class="mw-redirect">base pairs</a> according to the application.</p>
<p>An <i>n</i>-gram of size 1 is referred to as a "<a href="/wiki/Unigram" title="Unigram" class="mw-redirect">unigram</a>"; size 2 is a "<a href="/wiki/Bigram" title="Bigram">bigram</a>" (or, less commonly, a "digram"); size 3 is a "<a href="/wiki/Trigram" title="Trigram">trigram</a>"; and size 4 or more is simply called an "<i>n</i>-gram". Some <a href="/wiki/Language_model" title="Language model">language models</a> built from n-grams are "(<i>n</i>&#160;âˆ’&#160;1)-order <a href="/wiki/Markov_chain" title="Markov chain">Markov models</a>".</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Examples"><span class="tocnumber">1</span> <span class="toctext">Examples</span></a></li>
<li class="toclevel-1"><a href="#n-gram_models"><span class="tocnumber">2</span> <span class="toctext">n-gram models</span></a></li>
<li class="toclevel-1"><a href="#n-grams_for_approximate_matching"><span class="tocnumber">3</span> <span class="toctext">n-grams for approximate matching</span></a></li>
<li class="toclevel-1"><a href="#Other_applications"><span class="tocnumber">4</span> <span class="toctext">Other applications</span></a></li>
<li class="toclevel-1"><a href="#Bias-versus-variance_trade-off"><span class="tocnumber">5</span> <span class="toctext">Bias-versus-variance trade-off</span></a>
<ul>
<li class="toclevel-2"><a href="#Smoothing_techniques"><span class="tocnumber">5.1</span> <span class="toctext">Smoothing techniques</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Google_use_of_N-gram"><span class="tocnumber">6</span> <span class="toctext">Google use of N-gram</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Bibliography"><span class="tocnumber">8</span> <span class="toctext">Bibliography</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Examples" id="Examples"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=1" title="Edit section: Examples">edit</a>]</span> <span class="mw-headline">Examples</span></h2>
<p>Here are examples of <i><b>word</b></i> level 3-grams and 4-grams (and counts of the number of times they appeared) from the <a href="/wiki/N-gram#Google_use_of_N-gram" title="N-gram">Google n-gram corpus</a>.</p>
<ul>
<li>ceramics collectables collectibles (55)</li>
<li>ceramics collectables fine (130)</li>
<li>ceramics collected by (52)</li>
<li>ceramics collectible pottery (50)</li>
<li>ceramics collectibles cooking (45)</li>
</ul>
<p>4-grams</p>
<ul>
<li>serve as the incoming (92)</li>
<li>serve as the incubator (99)</li>
<li>serve as the independent (794)</li>
<li>serve as the index (223)</li>
<li>serve as the indication (72)</li>
<li>serve as the indicator (120)</li>
</ul>
<p><a name="n-gram_models" id="n-gram_models"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=2" title="Edit section: n-gram models">edit</a>]</span> <span class="mw-headline"><i>n</i>-gram models</span></h2>
<p>An <b><i>n</i>-gram model</b> models sequences, notably natural languages, using the statistical properties of <i>n</i>-grams.</p>
<p>This idea can be traced to an experiment by <a href="/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon</a>'s work in <a href="/wiki/Information_theory" title="Information theory">information theory</a>. His question was, given a sequence of letters (for example, the sequence "for ex"), what is the <a href="/wiki/Likelihood" title="Likelihood" class="mw-redirect">likelihood</a> of the next letter? From training data, one can derive a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> for the next letter given a history of size <span class="texhtml"><i>n</i></span>: <i>a</i> = 0.4, <i>b</i> = 0.00001, <i>c</i> = 0, ....; where the probabilities of all possible "next-letters" sum to 1.0.</p>
<p>More concisely, an <i>n</i>-gram model predicts <span class="texhtml"><i>x</i><sub><i>i</i></sub></span> based on <img class="tex" alt="x_{i-1}, x_{i-2}, \dots, x_{i-n}" src="http://upload.wikimedia.org/math/9/7/f/97f14f419d00cd845299a18c3277c3ac.png" />. In Probability terms, this is nothing but <img class="tex" alt="P(x_{i} | x_{i-1}, x_{i-2}, \dots, x_{i-n})" src="http://upload.wikimedia.org/math/0/0/c/00cf9600e9e68cda875f9ac59b574aa6.png" />. When used for <a href="/wiki/Language_model" title="Language model">language modeling</a> independence assumptions are made so that each word depends only on the last <i>n</i> words. This <a href="/wiki/Markov_model" title="Markov model" class="mw-redirect">Markov model</a> is used as an approximation of the true underlying language. This assumption is important because it massively simplifies the problem of learning the language model from data. In addition, because of the open nature of language, it is common to group words unknown to the language model together.</p>
<p><i>n</i>-gram models are widely used in statistical <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>. In <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Phonemes" title="Phonemes" class="mw-redirect">phonemes</a> and sequences of phonemes are modeled using a <i>n</i>-gram distribution. For parsing, words are modeled such that each <i>n</i>-gram is composed of <i>n</i> words. For <a href="/wiki/Language_recognition" title="Language recognition">language recognition</a>, sequences of letters are modeled for different languages. For a sequence of words, (for example "the dog smelled like a skunk"), the trigrams would be: "# the dog", "the dog smelled", "dog smelled like", "smelled like a", "like a skunk" and "a skunk #". For sequences of characters, the 3-grams (sometimes referred to as "trigrams") that can be generated from "good morning" are "goo", "ood", "od ", "d m", " mo", "mor" and so forth. Some practitioners preprocess strings to remove spaces, most simply collapse whitespace to a single space while preserving paragraph marks. Punctuation is also commonly reduced or removed by preprocessing. <i>n</i>-grams can also be used for sequences of words or, in fact, for almost any type of data. They have been used for example for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from. They have also been very successful as the first pass in genetic sequence search and in the identification of which species short sequences of DNA were taken from.</p>
<p>N-gram models are often criticized because they lack any explicit representation of long range dependency. While it is true that the only explicit dependency range is (n-1) tokens for an n-gram model, it is also true that the effective range of dependency is significantly longer than this although long range correlations drop exponentially with distance for any Markov model. Alternative Markov language models that incorporate some degree of local state can exhibit very long range dependencies. This is often done using hand-crafted state variables that represent, for instance, the position in a sentence, the general topic of discourse or a grammatical state variable. Some of the best parsers of English currently in existence are roughly of this form.</p>
<p>Another criticism that has been leveled is that Markov models of language, including n-gram models, do not explicitly capture the performance/competence distinction introduced by <a href="/wiki/Noam_Chomsky" title="Noam Chomsky">Noam Chomsky</a>. This criticism fails to explain why parsers that are the best at parsing text seem to uniformly lack any such distinction and most even lack any clear distinction between semantics and syntax. Most proponents of n-gram and related language models opt for a fairly pragmatic approach to language modeling that emphasizes empirical results over theoretical purity.</p>
<p><a name="n-grams_for_approximate_matching" id="n-grams_for_approximate_matching"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=3" title="Edit section: n-grams for approximate matching">edit</a>]</span> <span class="mw-headline"><i>n</i>-grams for approximate matching</span></h2>
<p><i>n</i>-grams can also be used for efficient approximate matching. By converting a sequence of items to a set of <i>n</i>-grams, it can be embedded in a <a href="/wiki/Vector_space" title="Vector space">vector space</a> (in other words, represented as a <a href="/wiki/Histogram" title="Histogram">histogram</a>), thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into 3-grams, we get a <span class="texhtml">26<sup>3</sup></span>-dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings "abcba" and "bcbab" give rise to exactly the same 2-grams. However, we know empirically that if two strings of real text have a similar vector representation (as measured by <a href="/wiki/Cosine_similarity" title="Cosine similarity">cosine distance</a>) then they are likely to be similar. Other metrics have also been applied to vectors of <i>n</i>-grams with varying, sometimes better, results. For example <a href="/wiki/Z-score" title="Z-score" class="mw-redirect">z-scores</a> have been used to compare documents by examining how many standard deviations each <i>n</i>-gram differs from its mean occurrence in a large collection, or <a href="/wiki/Text_corpus" title="Text corpus">text corpus</a>, of documents (which form the "background" vector). In the event of small counts, the <a href="/w/index.php?title=G-score&amp;action=edit&amp;redlink=1" class="new" title="G-score (page does not exist)">g-score</a> may give better results for comparing alternative models.</p>
<p>It is also possible to take a more principled approach to the statistics of <i>n</i>-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a>.</p>
<p><a name="Other_applications" id="Other_applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=4" title="Edit section: Other applications">edit</a>]</span> <span class="mw-headline">Other applications</span></h2>
<p><i>n</i>-grams find use in several areas of computer science, <a href="/wiki/Computational_linguistics" title="Computational linguistics">computational linguistics</a>, and applied mathematics.</p>
<p>They have been used to:</p>
<ul>
<li>design <a href="/wiki/Kernel_(mathematics)" title="Kernel (mathematics)">kernels</a> that allow <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms such as <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machines</a> to learn from string data</li>
<li>find likely candidates for the correct spelling of a misspelled word</li>
<li>improve compression in <a href="/wiki/Data_compression" title="Data compression">compression algorithms</a> where a small area of data requires <i>n</i>-grams of greater length</li>
<li>assess the probability of a given word sequence appearing in text of a language of interest in pattern recognition systems, <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, OCR (<a href="/wiki/Optical_character_recognition" title="Optical character recognition">optical character recognition</a>), <a href="/wiki/Intelligent_Character_Recognition" title="Intelligent Character Recognition" class="mw-redirect">Intelligent Character Recognition</a> (<a href="/wiki/ICR" title="ICR">ICR</a>), <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a> and similar applications</li>
<li>improve retrieval in <a href="/wiki/Information_retrieval" title="Information retrieval">information retrieval</a> systems when it is hoped to find similar "documents" (a term for which the conventional meaning is sometimes stretched, depending on the data set) given a single query document and a database of reference documents</li>
<li>improve retrieval performance in genetic sequence analysis as in the <a href="/wiki/BLAST" title="BLAST">BLAST</a> family of programs</li>
<li>identify the language a text is in or the species a small sequence of DNA was taken from</li>
<li>predict letters or words at random in order to create text, as in the <a href="/wiki/Dissociated_press" title="Dissociated press">dissociated press</a> algorithm.</li>
</ul>
<p><a name="Bias-versus-variance_trade-off" id="Bias-versus-variance_trade-off"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=5" title="Edit section: Bias-versus-variance trade-off">edit</a>]</span> <span class="mw-headline">Bias-versus-variance trade-off</span></h2>
<p>What goes into picking the <i>n</i> for the <i>n</i>-gram?</p>
<p>There are problems of balance weight between <i>infrequent grams</i> (for example, if a proper name appeared in the training data) and <i>frequent grams</i>. Also, items not seen in the training data will be given a <a href="/wiki/Probability" title="Probability">probability</a> of 0.0 without <a href="/wiki/Smoothing" title="Smoothing">smoothing</a>. For unseen but plausible data from a sample, one can introduce <a href="/wiki/Pseudocount" title="Pseudocount">pseudocounts</a>. Pseudocounts are generally motivated on Bayesian grounds.</p>
<p><a name="Smoothing_techniques" id="Smoothing_techniques"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=6" title="Edit section: Smoothing techniques">edit</a>]</span> <span class="mw-headline">Smoothing techniques</span></h3>
<ul>
<li><a href="/wiki/Linear_interpolation" title="Linear interpolation">Linear interpolation</a> (e.g., taking the <a href="/wiki/Weighted_mean" title="Weighted mean">weighted mean</a> of the unigram, bigram, and trigram)</li>
<li><a href="/wiki/Good-Turing" title="Good-Turing" class="mw-redirect">Good-Turing</a> discounting</li>
<li><a href="/w/index.php?title=Witten-Bell_discounting&amp;action=edit&amp;redlink=1" class="new" title="Witten-Bell discounting (page does not exist)">Witten-Bell discounting</a></li>
<li><a href="/wiki/Katz%27s_back-off_model" title="Katz's back-off model">Katz's back-off model</a> (trigram)</li>
</ul>
<p><a name="Google_use_of_N-gram" id="Google_use_of_N-gram"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=7" title="Edit section: Google use of N-gram">edit</a>]</span> <span class="mw-headline">Google use of N-gram</span></h2>
<p><a href="/wiki/Google" title="Google">Google</a> uses n-gram models for a variety of R&amp;D projects, such as <a href="/wiki/Statistical_machine_translation" title="Statistical machine translation">statistical machine translation</a>, <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Spell_checker" title="Spell checker">checking spelling</a>, <a href="/wiki/Named_entity_recognition" title="Named entity recognition">entity recognition</a>, and <a href="/wiki/Information_extraction" title="Information extraction">data mining</a>. In September 2006 <a href="http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html" class="external text" title="http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html" rel="nofollow">Google announced</a> that they made their n-grams <a href="http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13" class="external text" title="http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13" rel="nofollow">public</a> at the <a href="/wiki/Linguistic_Data_Consortium" title="Linguistic Data Consortium">Linguistic Data Consortium</a> (<a href="http://www.ldc.upenn.edu/" class="external text" title="http://www.ldc.upenn.edu/" rel="nofollow">LDC</a>).</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=8" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov model</a></li>
<li><a href="/wiki/Trigram" title="Trigram">Trigram</a>, <a href="/wiki/Bigram" title="Bigram">Bigram</a></li>
<li><a href="/wiki/N-tuple" title="N-tuple" class="mw-redirect">n-tuple</a></li>
<li><a href="/wiki/K-mer" title="K-mer">k-mer</a></li>
</ul>
<p><a name="Bibliography" id="Bibliography"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=9" title="Edit section: Bibliography">edit</a>]</span> <span class="mw-headline">Bibliography</span></h2>
<ul>
<li>Christopher D. Manning, Hinrich SchÃ¼tze, <i>Foundations of Statistical Natural Language Processing</i>, MIT Press: 1999. <a href="/wiki/Special:BookSources/0262133601" class="internal">ISBN 0-262-13360-1</a>.</li>
<li>Ted Dunning, <i>Statistical Identification of Language</i>. Computing Research Laboratory Memorandum (1994) MCCS-94-273.</li>
<li>Owen White, Ted Dunning, Granger Sutton, Mark Adams, J.Craig Venter, and Chris Fields. A quality control algorithm for dna sequencing projects. Nucleic Acids Research, 21(16):3829--3838, 1993.</li>
<li>Frederick J. Damerau, <i>Markov Models and Linguistic Theory</i>. Mouton. The Hague, 1971.</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=N-gram&amp;action=edit&amp;section=10" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://n-gram-patterns.sourceforge.net/" class="external text" title="http://n-gram-patterns.sourceforge.net/" rel="nofollow">Google n-gram Information Extracter</a></li>
<li>Two visualizations of Google's n-gram dataset: <a href="http://www.chrisharrison.net/projects/wordassociation/" class="external text" title="http://www.chrisharrison.net/projects/wordassociation/" rel="nofollow">Word Association</a>, <a href="http://www.chrisharrison.net/projects/wordspectrum/" class="external text" title="http://www.chrisharrison.net/projects/wordspectrum/" rel="nofollow">Word Spectrum</a>.</li>
<li><a href="http://citeseer.ist.psu.edu/dunning94statistical.html" class="external text" title="http://citeseer.ist.psu.edu/dunning94statistical.html" rel="nofollow">N-gram language identification algorithm</a></li>
<li>SSN is a state of the art <a href="http://homepages.inf.ed.ac.uk/jhender6/" class="external text" title="http://homepages.inf.ed.ac.uk/jhender6/" rel="nofollow">statistical language parser</a> that is nearly Markov.</li>
<li><a href="http://ngram.sourceforge.net" class="external text" title="http://ngram.sourceforge.net" rel="nofollow">Ngram Statistics Package</a>, open source package to identify statistically significant Ngrams</li>
<li><a href="http://www.w3.org/TR/ngram-spec/" class="external text" title="http://www.w3.org/TR/ngram-spec/" rel="nofollow">Stochastic Language Models (N-Gram) Specification</a></li>
<li><a href="http://github.com/feedbackmine/language_detector/tree/master" class="external text" title="http://github.com/feedbackmine/language_detector/tree/master" rel="nofollow">language_detector</a>, open source N-Gram based language detector, written in ruby</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 172/1000000
Post-expand include size: 2617/2048000 bytes
Template argument size: 687/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:986182-0!1!0!default!!en!2 and timestamp 20090409051413 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/N-gram">http://en.wikipedia.org/wiki/N-gram</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Natural_language_processing" title="Category:Natural language processing">Natural language processing</a></span> | <span dir='ltr'><a href="/wiki/Category:Computational_linguistics" title="Category:Computational linguistics">Computational linguistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Speech_recognition" title="Category:Speech recognition">Speech recognition</a></span> | <span dir='ltr'><a href="/wiki/Category:Corpus_linguistics" title="Category:Corpus linguistics">Corpus linguistics</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Wikipedia_articles_needing_context" title="Category:Wikipedia articles needing context">Wikipedia articles needing context</a></span> | <span dir='ltr'><a href="/wiki/Category:Wikipedia_introduction_cleanup" title="Category:Wikipedia introduction cleanup">Wikipedia introduction cleanup</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/N-gram" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:N-gram" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=N-gram&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=N-gram&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=N-gram" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content â€” the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/N-gram" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/N-gram" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=N-gram&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=N-gram&amp;oldid=282707313" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=N-gram&amp;id=282707313">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ca"><a href="http://ca.wikipedia.org/wiki/N-grama">CatalÃ </a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/N-Gramm">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/N-grama">EspaÃ±ol</a></li>
				<li class="interwiki-eu"><a href="http://eu.wikipedia.org/wiki/N-grama">Euskara</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/N-gramme">FranÃ§ais</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/N-gramma">Italiano</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 9 April 2009, at 05:14 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv220 in 0.048 secs. --></body></html>
