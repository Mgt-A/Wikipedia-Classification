<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Chinese room,Ad hoc,Alan Turing,Allen Newell,Andy Clark,Argument,Artificial intelligence,Axon,Behavioral and Brain Sciences,Behaviourism,Biological naturalism" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Chinese_room&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Chinese_room&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Chinese room - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Chinese_room";
		var wgTitle = "Chinese room";
		var wgAction = "view";
		var wgArticleId = "6216";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281549721;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Chinese_room skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Chinese room</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<div class="floatright"><a href="/wiki/File:Chinese_Room.jpg" class="image" title="Chinese Room.jpg"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/4/48/Chinese_Room.jpg" width="273" height="212" border="0" /></a></div>
<p>The <b>Chinese Room</b> argument comprises a <a href="/wiki/Thought_experiment" title="Thought experiment">thought experiment</a> and associated <a href="/wiki/Argument" title="Argument">arguments</a> by <a href="/wiki/John_Searle" title="John Searle">John Searle</a> <a href="#CITEREFSearle1980" title="">(1980)</a>, which attempts to show that a symbol-processing machine like a computer can never be properly described as having a "<a href="/wiki/Mind" title="Mind">mind</a>" or "<a href="/wiki/Intentionality" title="Intentionality">understanding</a>", regardless of how intelligently it may behave.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Chinese_room_thought_experiment"><span class="tocnumber">1</span> <span class="toctext">Chinese room thought experiment</span></a></li>
<li class="toclevel-1"><a href="#History"><span class="tocnumber">2</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1"><a href="#Searle.27s_targets:_.22strong_AI.22_and_computationalism"><span class="tocnumber">3</span> <span class="toctext">Searle's targets: "strong AI" and computationalism</span></a>
<ul>
<li class="toclevel-2"><a href="#Strong_AI"><span class="tocnumber">3.1</span> <span class="toctext">Strong AI</span></a></li>
<li class="toclevel-2"><a href="#Strong_AI_as_philosophy"><span class="tocnumber">3.2</span> <span class="toctext">Strong AI as philosophy</span></a></li>
<li class="toclevel-2"><a href="#Strong_AI_v._AI_research"><span class="tocnumber">3.3</span> <span class="toctext">Strong AI v. AI research</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Replies"><span class="tocnumber">4</span> <span class="toctext">Replies</span></a>
<ul>
<li class="toclevel-2"><a href="#System_and_virtual_mind_replies:_finding_the_mind"><span class="tocnumber">4.1</span> <span class="toctext">System and virtual mind replies: finding the mind</span></a></li>
<li class="toclevel-2"><a href="#Robot_and_semantics_replies:_finding_the_meaning"><span class="tocnumber">4.2</span> <span class="toctext">Robot and semantics replies: finding the meaning</span></a></li>
<li class="toclevel-2"><a href="#Brain_simulation_and_connectionist_replies:_redesigning_the_room"><span class="tocnumber">4.3</span> <span class="toctext">Brain simulation and connectionist replies: redesigning the room</span></a></li>
<li class="toclevel-2"><a href="#Speed.2C_complexity_and_other_minds:_appeals_to_intuition"><span class="tocnumber">4.4</span> <span class="toctext">Speed, complexity and other minds: appeals to intuition</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Formal_arguments"><span class="tocnumber">5</span> <span class="toctext">Formal arguments</span></a></li>
<li class="toclevel-1"><a href="#Pop_culture_references"><span class="tocnumber">6</span> <span class="toctext">Pop culture references</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">7</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">9</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Further_reading"><span class="tocnumber">10</span> <span class="toctext">Further reading</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Chinese_room_thought_experiment" id="Chinese_room_thought_experiment"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=1" title="Edit section: Chinese room thought experiment">edit</a>]</span> <span class="mw-headline">Chinese room thought experiment</span></h2>
<p>Searle's <a href="/wiki/Thought_experiment" title="Thought experiment">thought experiment</a> begins with this hypothetical premise: suppose that <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> research has succeeded in constructing a computer that behaves as if it understands <a href="/wiki/Chinese_language" title="Chinese language">Chinese</a>. It takes <a href="/wiki/Chinese_character" title="Chinese character">Chinese characters</a> as input and, by following the instructions of a <a href="/wiki/Computer_program" title="Computer program">computer program</a>, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the <a href="/wiki/Turing_test" title="Turing test">Turing test</a>: it convinces a human Chinese speaker that the program is itself a human Chinese speaker. All of the questions that the human asks it receive appropriate responses, such that any Chinese speaker would be convinced that he or she is talking to another Chinese-speaking human being.</p>
<p>Some proponents of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> would conclude that the computer "understands" Chinese. This conclusion, a position he refers to as "<a href="#Strong_AI" title="">strong AI</a>", is the target of Searle's argument.</p>
<p>Searle then asks the reader to suppose that he is in a closed room and that he has a book with an English version of the aforementioned computer program, along with sufficient paper, pencils, erasers and filing cabinets. He can receive Chinese characters (perhaps through a slot in the door), process them according to the program's instructions, and produce Chinese characters as output. As the computer passed the Turing test this way, it is fair, says Searle, to deduce that he will be able to do so as well, simply by running the program manually.</p>
<p>And yet, Searle points out, he does not understand a word of Chinese. He asserts that there is no essential difference between the role the computer plays in the first case and the role he plays in the latter. Each is simply following a program, step-by-step, which simulates intelligent behavior. Since it is obvious that he does not understand Chinese, Searle argues, we must infer that computer does not understand Chinese either.</p>
<p>Searle argues that without "understanding" (what philosophers call "<a href="/wiki/Intentionality" title="Intentionality">intentionality</a>"), we can not describe what the machine is doing as "thinking". Because it does not think, it does not have a "mind" in anything like the normal sense of the word, according to Searle. Therefore, he concludes, "strong AI" is mistaken.</p>
<p><a name="History" id="History"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=2" title="Edit section: History">edit</a>]</span> <span class="mw-headline">History</span></h2>
<p>Searle's argument first appeared in his paper "Minds, Brains, and Programs", published in <i><a href="/wiki/Behavioral_and_Brain_Sciences" title="Behavioral and Brain Sciences">Behavioral and Brain Sciences</a></i> in 1980.<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup> It eventually became the journal's "most influential target article",<sup id="cite_ref-H1_1-0" class="reference"><a href="#cite_note-H1-1" title=""><span>[</span>2<span>]</span></a></sup> generating an enormous number of commentaries and responses in the ensuing decades.</p>
<p>Most of the discussion consists of attempts to refute it. "The overwhelming majority," notes <i><a href="/wiki/Behavioral_and_Brain_Sciences" title="Behavioral and Brain Sciences">BBS</a></i> editor <a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Stevan Harnad</a>, "still think that the Chinese Room Argument is dead wrong."<sup id="cite_ref-H2_2-0" class="reference"><a href="#cite_note-H2-2" title=""><span>[</span>3<span>]</span></a></sup> The sheer volume of the literature that has grown up around it inspired <a href="/wiki/Patrick_J._Hayes" title="Patrick J. Hayes">Pat Hayes</a> to quip that the field of <a href="/wiki/Cognitive_science" title="Cognitive science">cognitive science</a> ought to be redefined as "the ongoing research program of showing Searle's Chinese Room Argument to be false."<sup id="cite_ref-H1_1-1" class="reference"><a href="#cite_note-H1-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<p>Despite the controversy (or perhaps because of it), the paper has become "something of a classic in cognitive science," according to Harnad.<sup id="cite_ref-H2_2-1" class="reference"><a href="#cite_note-H2-2" title=""><span>[</span>3<span>]</span></a></sup> <a href="/wiki/Varol_Akman" title="Varol Akman">Varol Akman</a> agrees, and has described Searle's paper as "an exemplar of philosophical clarity and purity".<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup></p>
<p><a name="Searle.27s_targets:_.22strong_AI.22_and_computationalism" id="Searle.27s_targets:_.22strong_AI.22_and_computationalism"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=3" title="Edit section: Searle's targets: &quot;strong AI&quot; and computationalism">edit</a>]</span> <span class="mw-headline">Searle's targets: "strong AI" and computationalism</span></h2>
<p>Although the Chinese Room argument was originally presented in reaction to the statements of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">AI</a> researchers, philosophers have come to view it as an important part of the <a href="/wiki/Philosophy_of_mind" title="Philosophy of mind">philosophy of mind</a> — such as, for example, a challenge to <a href="/wiki/Functionalism_(philosophy_of_mind)" title="Functionalism (philosophy of mind)">functionalism</a> or the <a href="/wiki/Computational_theory_of_mind" title="Computational theory of mind">computational theory of mind</a>,<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup> and related to such questions as the <a href="/wiki/Mind-body_dichotomy" title="Mind-body dichotomy">mind-body problem</a>,<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup> the <a href="/wiki/Problem_of_other_minds" title="Problem of other minds">problem of other minds</a>,<sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup> the <a href="/wiki/Symbol_grounding" title="Symbol grounding">symbol-grounding</a> problem and the <a href="/wiki/Hard_problem_of_consciousness" title="Hard problem of consciousness">hard problem of consciousness</a>.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7" title=""><span>[</span>8<span>]</span></a></sup></p>
<p><a name="Strong_AI" id="Strong_AI"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=4" title="Edit section: Strong AI">edit</a>]</span> <span class="mw-headline">Strong AI</span></h3>
<p>Searle identified a <a href="/wiki/Philosophical_position" title="Philosophical position" class="mw-redirect">philosophical position</a> he calls "strong AI":</p>
<blockquote>
<p>The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8" title=""><span>[</span>9<span>]</span></a></sup></p>
</blockquote>
<p>The definition hinges on the distinction between <i>simulating</i> a mind and <i>actually having</i> a mind. Searle writes that "according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind."<sup id="cite_ref-9" class="reference"><a href="#cite_note-9" title=""><span>[</span>10<span>]</span></a></sup></p>
<p>The position is implicit in some of the statements of early AI researchers and analysts. For example, in 1955, AI founder <a href="/wiki/Herbert_Simon" title="Herbert Simon">Herbert Simon</a> declared that "there are now in the world machines that think, that learn and create"<sup id="cite_ref-10" class="reference"><a href="#cite_note-10" title=""><span>[</span>11<span>]</span></a></sup> and claimed that they had "solved the venerable <a href="/wiki/Mind-body_problem" title="Mind-body problem" class="mw-redirect">mind-body problem</a>, explaining how a system composed of matter can have the properties of <a href="/wiki/Mind" title="Mind">mind</a>."<sup id="cite_ref-11" class="reference"><a href="#cite_note-11" title=""><span>[</span>12<span>]</span></a></sup> <a href="/wiki/John_Haugeland" title="John Haugeland">John Haugeland</a> summarised the philosophical position of early AI researchers, writing that "AI wants only the genuine article: <i>machines with minds</i>, in the full and literal sense. This is not science fiction, but real science, based on a theoretical conception as deep as it is daring: namely, we are, at root, <i>computers ourselves</i>."<sup id="cite_ref-12" class="reference"><a href="#cite_note-12" title=""><span>[</span>13<span>]</span></a></sup></p>
<p>Searle also ascribes the following positions to advocates of strong AI:</p>
<ul>
<li>AI systems can be used to explain the mind;<sup id="cite_ref-13" class="reference"><a href="#cite_note-13" title=""><span>[</span>14<span>]</span></a></sup></li>
<li>The study of the brain is irrelevant to the study of the mind;<sup id="cite_ref-14" class="reference"><a href="#cite_note-14" title=""><span>[</span>15<span>]</span></a></sup> and</li>
<li>The <a href="/wiki/Turing_test" title="Turing test">Turing test</a> is adequate for establishing the existence of mental states.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15" title=""><span>[</span>16<span>]</span></a></sup></li>
</ul>
<p><a name="Strong_AI_as_philosophy" id="Strong_AI_as_philosophy"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=5" title="Edit section: Strong AI as philosophy">edit</a>]</span> <span class="mw-headline">Strong AI as philosophy</span></h3>
<p><a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Stevan Harnad</a> argues that Searle's depictions of strong AI can be reformulated as "recognizable tenets of <i><a href="/wiki/Computationalism" title="Computationalism" class="mw-redirect">computationalism</a></i>, a position (unlike 'strong AI') that is actually held by many thinkers, and hence one worth refuting."<sup id="cite_ref-16" class="reference"><a href="#cite_note-16" title=""><span>[</span>17<span>]</span></a></sup> <a href="/wiki/Computationalism" title="Computationalism" class="mw-redirect">Computationalism</a><sup id="cite_ref-17" class="reference"><a href="#cite_note-17" title=""><span>[</span>18<span>]</span></a></sup> is the position in the philosophy of mind which argues that the <a href="/wiki/Mind" title="Mind">mind</a> can be accurately described as an <a href="/wiki/Information_processing" title="Information processing">information-processing</a> system.</p>
<p>Each of the following, according to Harnad, is a "tenet" of computationalism:<sup id="cite_ref-18" class="reference"><a href="#cite_note-18" title=""><span>[</span>19<span>]</span></a></sup></p>
<ul>
<li>Mental states are computational states (which is why computers can have mental states and help to explain the mind);</li>
<li>Computational states are implementation-independent — in other words, it is the software that determines the computational state, not the hardware (which is why the brain, being hardware, is irrelevant); and that</li>
<li>Since implementation is unimportant, the only empirical data that matters is how the system functions; hence the <a href="/wiki/Turing_test" title="Turing test">Turing test</a> is definitive. This last point is a version of <a href="/wiki/Functionalism_(philosophy_of_mind)" title="Functionalism (philosophy of mind)">functionalism</a>.</li>
</ul>
<p>Searle accuses strong AI of <a href="/wiki/Dualism" title="Dualism">dualism</a>, the idea that the mind and the body are made up of different "substances". He writes that "strong AI only makes sense given the dualistic assumption that, where the mind is concerned, the brain doesn't matter."<sup id="cite_ref-lmecxp_19-0" class="reference"><a href="#cite_note-lmecxp-19" title=""><span>[</span>20<span>]</span></a></sup> He rejects any form of <a href="/wiki/Dualism" title="Dualism">dualism</a>, writing that "brains cause minds"<sup id="cite_ref-20" class="reference"><a href="#cite_note-20" title=""><span>[</span>21<span>]</span></a></sup> and that "actual human mental phenomena [are] dependent on actual physical-chemical properties of actual human brains",<sup id="cite_ref-lmecxp_19-1" class="reference"><a href="#cite_note-lmecxp-19" title=""><span>[</span>20<span>]</span></a></sup> a position called "<a href="/wiki/Biological_naturalism" title="Biological naturalism">biological naturalism</a>" (as opposed to alternatives like <a href="/wiki/Behaviourism" title="Behaviourism" class="mw-redirect">behaviourism</a>, <a href="/wiki/Functionalism_(philosophy_of_mind)" title="Functionalism (philosophy of mind)">functionalism</a>, <a href="/wiki/Type_physicalism" title="Type physicalism">identity theory</a> and <a href="/wiki/Dualism" title="Dualism">dualism</a>).<sup id="cite_ref-21" class="reference"><a href="#cite_note-21" title=""><span>[</span>22<span>]</span></a></sup></p>
<p>Searle's argument centers on "understanding" — that is, mental states with what philosophers call "<a href="/wiki/Intentionality" title="Intentionality">intentionality</a>" — and does not directly address other closely related ideas, such as "intelligence" or "consciousness". <a href="/wiki/David_Chalmers" title="David Chalmers">David Chalmers</a> has argued that, to the contrary, "it is fairly clear that <a href="/wiki/Consciousness" title="Consciousness">consciousness</a> is at the root of the matter".<sup id="cite_ref-22" class="reference"><a href="#cite_note-22" title=""><span>[</span>23<span>]</span></a></sup></p>
<p><a name="Strong_AI_v._AI_research" id="Strong_AI_v._AI_research"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=6" title="Edit section: Strong AI v. AI research">edit</a>]</span> <span class="mw-headline">Strong AI v. AI research</span></h3>
<p>Searle's argument does not limit the intelligence with which machines can behave or act; indeed, it fails to address this issue directly, leaving open the possibility that a machine could be built that <i>acts</i> intelligently but does not have a <a href="/wiki/Mind" title="Mind">mind</a> or <a href="/wiki/Intentionality" title="Intentionality">intentionality</a> in the same way that <a href="/wiki/Brain" title="Brain">brains</a> do.</p>
<p>Since the primary mission of AI research is only to create useful systems that act intelligently, Searle's arguments are not usually considered an issue for AI research. <a href="/wiki/Stuart_Russell" title="Stuart Russell">Stuart Russell</a> and <a href="/wiki/Peter_Norvig" title="Peter Norvig">Peter Norvig</a> observe that most AI researchers "don't care about the strong AI hypothesis—as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence."<sup id="cite_ref-23" class="reference"><a href="#cite_note-23" title=""><span>[</span>24<span>]</span></a></sup></p>
<p>Searle's "strong AI" should not be confused with "<a href="/wiki/Strong_AI" title="Strong AI">strong AI</a>" as defined by <a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil" class="mw-redirect">Ray Kurzweil</a> and other futurists,<sup id="cite_ref-K_24-0" class="reference"><a href="#cite_note-K-24" title=""><span>[</span>25<span>]</span></a></sup> who use the term to describe machine intelligence that rivals human intelligence. Kurzweil is concerned primarily with the <i>amount</i> of intelligence displayed by the machine, whereas Searle's argument sets no limit on this, as long as it understood that it is merely a simulation and not the real thing.</p>
<p><a name="Replies" id="Replies"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=7" title="Edit section: Replies">edit</a>]</span> <span class="mw-headline">Replies</span></h2>
<p>Replies to Searle's argument may be classified according to what they claim to show:<sup id="cite_ref-25" class="reference"><a href="#cite_note-25" title=""><span>[</span>26<span>]</span></a></sup></p>
<ul>
<li>Those which identify <i>who</i> speaks Chinese;</li>
<li>Those which demonstrate how meaningless symbols can become meaningful;</li>
<li>Those which suggest that the Chinese room should be redesigned more along the lines of a brain; and</li>
<li>Those which demonstrate the ways in which Searle's argument is misleading.</li>
</ul>
<p>Some of the arguments (robot and brain simulation, for example) fall into multiple categories.</p>
<p><a name="System_and_virtual_mind_replies:_finding_the_mind" id="System_and_virtual_mind_replies:_finding_the_mind"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=8" title="Edit section: System and virtual mind replies: finding the mind">edit</a>]</span> <span class="mw-headline">System and virtual mind replies: finding the mind</span></h3>
<p>These two replies attempt to answer the question: since the man in the room doesn't speak Chinese, <i>where</i> is the "mind" that does? These replies address the key <a href="/wiki/Ontological" title="Ontological" class="mw-redirect">ontological</a> issues of <a href="/wiki/Mind/body_problem" title="Mind/body problem" class="mw-redirect">mind vs. body</a> and simulation vs. reality.</p>
<p><b>Systems reply.</b><sup id="cite_ref-26" class="reference"><a href="#cite_note-26" title=""><span>[</span>27<span>]</span></a></sup> The "systems reply" argues that it is the <i>whole system</i> that understands Chinese, consisting of the room, the book, the man, the paper, the pencil and the filing cabinets. While the man by himself can only understand English, the complete system can understand Chinese. The man is part of the system, just as the <a href="/wiki/Hippocampus" title="Hippocampus">hippocampus</a> is a part of the brain. The fact that the man doesn't understand Chinese is irrelevant and is no more surprising than the fact that the hippocampus understands nothing by itself.</p>
<p>Searle's response is to consider what happens if the man memorizes the rules and keeps track of everything in his head. Then the only component of the system is the man himself—in this sense, the man <i>is</i> the system. Searle argues that if the man doesn't understand Chinese then the system (which consists of just the man) doesn't understand Chinese either and the fact that the man <i>appears</i> to understand Chinese proves nothing.<sup id="cite_ref-ihsqxx_27-0" class="reference"><a href="#cite_note-ihsqxx-27" title=""><span>[</span>28<span>]</span></a></sup></p>
<p><b>Virtual mind reply.</b><sup id="cite_ref-28" class="reference"><a href="#cite_note-28" title=""><span>[</span>29<span>]</span></a></sup> A more precise response is that there <i>is</i> a Chinese speaking mind in Searle's room, but that it is <i>virtual</i>. A fundamental property of computing machinery is that one machine can "implement" another: any (<a href="/wiki/Turing_complete" title="Turing complete" class="mw-redirect">Turing complete</a>) computer can do a step-by-step simulation of any other machine.<sup id="cite_ref-29" class="reference"><a href="#cite_note-29" title=""><span>[</span>30<span>]</span></a></sup> In this way, a machine can simultaneously be two machines at once: for example, it can be a <a href="/wiki/Macintosh" title="Macintosh">Macintosh</a> and a <a href="/wiki/Word_processor" title="Word processor">word processor</a> at the same time. A <a href="/wiki/Virtual_machine" title="Virtual machine">virtual machine</a> depends on the hardware (in that if you turn off the Macintosh, you turn off the word processor as well), yet is different from the hardware. (This is how the position resists <a href="/wiki/Dualism" title="Dualism">dualism</a>: there can be two machines in the same place, both made of the same substance, if one of them is <a href="/wiki/Virtual_machine" title="Virtual machine">virtual</a>.) A virtual machine is also "implementation independent" in that it doesn't matter what sort of hardware it runs on: a PC, a Macintosh, a supercomputer, a brain or Searle in his Chinese room.<sup id="cite_ref-30" class="reference"><a href="#cite_note-30" title=""><span>[</span>31<span>]</span></a></sup></p>
<p>To clarify the distinction between the systems reply and virtual mind reply, David Cole notes that a program could be written that implements two minds at once – for example, one speaking Chinese and the other Korean. While there is only one system and only one man in the room, there may be an unlimited number of "virtual minds."<sup id="cite_ref-31" class="reference"><a href="#cite_note-31" title=""><span>[</span>32<span>]</span></a></sup></p>
<p>Searle would respond that such a mind is only a simulation. He writes: "No one supposes that computer simulations of a five-alarm fire will burn the neighborhood down or that a computer simulation of a rainstorm will leave us all drenched."<sup id="cite_ref-32" class="reference"><a href="#cite_note-32" title=""><span>[</span>33<span>]</span></a></sup> Nicholas Fearn responds that, for some things, simulation is as good as the real thing. "When we call up the pocket calculator function on a desktop computer, the image of a pocket calculator appears on the screen. We don't complain that 'it isn't <i>really</i> a calculator', because the physical attributes of the device do not matter."<sup id="cite_ref-33" class="reference"><a href="#cite_note-33" title=""><span>[</span>34<span>]</span></a></sup> The question is, is the human mind like the pocket calculator, essentially composed of information? Or is it like the rainstorm, which can't be duplicated using digital information alone? (The issue of simulation is also discussed in the article <a href="/wiki/Synthetic_intelligence" title="Synthetic intelligence">synthetic intelligence</a>.)</p>
<p><b>What they do and don't prove.</b> These replies provide an explanation of exactly who it is that understands Chinese. If there is something <i>besides</i> the man in the room that can understand Chinese, Searle can't argue that (1) the man doesn't understand Chinese, therefore (2) nothing in the room understands Chinese. This, according to those who make this reply, shows that Searle's argument fails to prove that "strong AI" is false.<sup id="cite_ref-34" class="reference"><a href="#cite_note-34" title=""><span>[</span>35<span>]</span></a></sup></p>
<p>However, the replies, by themselves, do not prove that strong AI is <i>true</i>, either: they provide no evidence that the system (or the virtual mind) understands Chinese, other than the <a href="/wiki/Hypothetical" title="Hypothetical" class="mw-redirect">hypothetical</a> premise that it passes the <a href="/wiki/Turing_Test" title="Turing Test" class="mw-redirect">Turing Test</a>. As Searle writes "the systems reply simply begs the question by insisting that system must understand Chinese."<sup id="cite_ref-ihsqxx_27-1" class="reference"><a href="#cite_note-ihsqxx-27" title=""><span>[</span>28<span>]</span></a></sup></p>
<p><a name="Robot_and_semantics_replies:_finding_the_meaning" id="Robot_and_semantics_replies:_finding_the_meaning"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=9" title="Edit section: Robot and semantics replies: finding the meaning">edit</a>]</span> <span class="mw-headline">Robot and semantics replies: finding the meaning</span></h3>
<p>As far as the man in the room is concerned, the symbols he writes are just meaningless "squiggles." But if the Chinese room really "understands" what it's saying, then the symbols must get their meaning from somewhere. These arguments attempt to connect the symbols to the things they symbolize. These replies address Searle's concerns about <a href="/wiki/Intentionality" title="Intentionality">intentionality</a>, <a href="/wiki/Symbol_grounding" title="Symbol grounding">symbol grounding</a> and <a href="/wiki/Syntax" title="Syntax">syntax</a> vs. <a href="/wiki/Semantic" title="Semantic" class="mw-redirect">semantics</a>.</p>
<p><b>Robot reply</b>.<sup id="cite_ref-35" class="reference"><a href="#cite_note-35" title=""><span>[</span>36<span>]</span></a></sup> Suppose that instead of a room, the program was placed into a robot that could wander around and interact with its environment. This would allow a "<a href="/wiki/Causation" title="Causation">causal</a> connection" between the symbols and things they represent. <a href="/wiki/Hans_Moravec" title="Hans Moravec">Hans Moravec</a> comments: 'If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world."<sup id="cite_ref-36" class="reference"><a href="#cite_note-36" title=""><span>[</span>37<span>]</span></a></sup></p>
<p>Searle’s reply is to suppose that, unbeknownst to the individual in the Chinese room, some of the inputs he was receiving came directly from a camera mounted on a robot, and some of the outputs were used to manipulate the arms and legs of the robot. Nevertheless, the person in the room is still just following the rules, and <i>does not know what the symbols mean.</i> Searle writes "he doesn't <i>see</i> what comes into the robots eyes."<sup id="cite_ref-37" class="reference"><a href="#cite_note-37" title=""><span>[</span>38<span>]</span></a></sup> (See <a href="/wiki/Mary%27s_Room" title="Mary's Room" class="mw-redirect">Mary's Room</a> for a similar thought experiment.)</p>
<p><b>Derived meaning</b>.<sup id="cite_ref-38" class="reference"><a href="#cite_note-38" title=""><span>[</span>39<span>]</span></a></sup> Some respond that the room, as Searle describes it, <i>is</i> connected to the world: through the Chinese speakers that it is "talking" to and through the programmers who designed the <a href="/wiki/Knowledge_base" title="Knowledge base">knowledge base</a> in his file cabinet. The symbols he manipulates <i>are already meaningful</i>, they're just not meaningful to <i>him</i>.</p>
<p>Searle complains that the symbols only have a "derived" meaning, like the meaning of words in books. The meaning of the symbols depends on the conscious understanding of the Chinese speakers and the programmers outside the room. The room, according to Searle, has no understanding of its own.<sup id="cite_ref-39" class="reference"><a href="#cite_note-39" title=""><span>[</span>40<span>]</span></a></sup></p>
<p><b>Commonsense knowledge / contextualist reply</b>.<sup id="cite_ref-40" class="reference"><a href="#cite_note-40" title=""><span>[</span>41<span>]</span></a></sup> Some have argued that the meanings of the symbols would come from a vast "background" of <a href="/wiki/Commonsense_knowledge" title="Commonsense knowledge" class="mw-redirect">commonsense knowledge</a> encoded in the program and the filing cabinets. This would provide a "<a href="/wiki/Contextualism" title="Contextualism">context</a>" that would give the symbols their meaning.</p>
<p>Searle agrees that this background exists, but he does not agree that it can be built into programs. <a href="/wiki/Hubert_Dreyfus" title="Hubert Dreyfus">Hubert Dreyfus</a> has also criticized the idea that the "background" can be represented symbolically.<sup id="cite_ref-41" class="reference"><a href="#cite_note-41" title=""><span>[</span>42<span>]</span></a></sup></p>
<p><b>What they do and don't prove</b>. To each of these suggestions, Searle's response is the same: no matter how much knowledge is written into the program and no matter how the program is connected to the world, he is still in the room manipulating symbols according to rules. His actions are <a href="/wiki/Syntax" title="Syntax">syntactic</a> and this can never explain to him what the symbols stand for. Searle writes "syntax is insufficient for semantics."<sup id="cite_ref-42" class="reference"><a href="#cite_note-42" title=""><span>[</span>43<span>]</span></a></sup></p>
<p>However, for those who accept that Searle's actions simulate a mind, separate from his own, the important question is not what the symbols mean <i>to Searle</i>, what is important is what they mean <i>to the virtual mind.</i> While Searle is trapped in the room, the virtual mind is not: it is connected to the outside world through the Chinese speakers it speaks to, through the programmers who gave it world knowledge, and through the cameras and other sensors that roboticists can supply.</p>
<p><a name="Brain_simulation_and_connectionist_replies:_redesigning_the_room" id="Brain_simulation_and_connectionist_replies:_redesigning_the_room"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=10" title="Edit section: Brain simulation and connectionist replies: redesigning the room">edit</a>]</span> <span class="mw-headline">Brain simulation and connectionist replies: redesigning the room</span></h3>
<p>These arguments are all versions of the systems reply that identify a particular <i>kind</i> of system as being important. They try to outline what kind of a system would be able to pass the Turing test and give rise to conscious awareness in a machine. (Note that the "robot" and "commonsense knowledge" replies above also specify a certain kind of system as being important.)</p>
<p><b>Brain simulator reply</b>.<sup id="cite_ref-43" class="reference"><a href="#cite_note-43" title=""><span>[</span>44<span>]</span></a></sup> Suppose that the program simulated in fine detail the action of every neuron in the brain of a Chinese speaker. This strengthens the intuition that there would be no significant difference between the operation of the program and the operation of a live human brain.</p>
<p>Searle replies that such a simulation will not have reproduced the important features of the brain — its causal and intentional states. <a href="/wiki/Searle" title="Searle">Searle</a> is adamant that "human mental phenomena [are] dependent on actual physical-chemical properties of actual human brains."<sup id="cite_ref-lmecxp_19-2" class="reference"><a href="#cite_note-lmecxp-19" title=""><span>[</span>20<span>]</span></a></sup></p>
<p>Two variations on the brain simulator reply are:</p>
<dl>
<dd><b><a href="/wiki/China_brain" title="China brain">China brain</a></b>.<sup id="cite_ref-44" class="reference"><a href="#cite_note-44" title=""><span>[</span>45<span>]</span></a></sup> What if we ask each citizen of China to simulate one neuron, using the telephone system to simulate the connections between <a href="/wiki/Axon" title="Axon">axons</a> and <a href="/wiki/Dendrite" title="Dendrite">dendrites</a>? In this version, it seems obvious that no individual would have any understanding of what the brain might be saying.</dd>
</dl>
<dl>
<dd><b>Brain replacement scenario</b>.<sup id="cite_ref-45" class="reference"><a href="#cite_note-45" title=""><span>[</span>46<span>]</span></a></sup> In this, we are asked to imagine that engineers have invented a tiny computer that simulates the action of an individual neuron. What would happen if we replaced one neuron at a time? Replacing one would clearly do nothing to change conscious awareness. Replacing all of them would create a digital computer that simulates a brain. If Searle is right, then conscious awareness must disappear during the procedure (either gradually or all at once). Searle's critics argue that there would be no point during the procedure when he can claim that conscious awareness ends and mindless simulation begins.<sup id="cite_ref-46" class="reference"><a href="#cite_note-46" title=""><span>[</span>47<span>]</span></a></sup></dd>
</dl>
<p><b>Connectionist replies</b>.<sup id="cite_ref-47" class="reference"><a href="#cite_note-47" title=""><span>[</span>48<span>]</span></a></sup> Closely related to the brain simulator reply, this claims that a massively parallel connectionist architecture would be capable of understanding.</p>
<p><b>Combination reply</b>.<sup id="cite_ref-48" class="reference"><a href="#cite_note-48" title=""><span>[</span>49<span>]</span></a></sup> This response combines the robot reply with the brain simulation reply, arguing that a brain simulation connected to the world through a robot body could have a mind.</p>
<p><b>What they do and don't prove</b>. Arguments such as these (and the robot and commonsense knowledge replies above) recommend that Searle's room be redesigned. Searle's replies all point out that, however the program is written or however it is connected to the world, it is still being <i>simulated</i> by a simple step by step <a href="/wiki/Turing_complete" title="Turing complete" class="mw-redirect">Turing complete</a> machine (or machines). These machines are still just like the man in the room: they understand nothing and don't speak Chinese. They are merely manipulating symbols without knowing what they mean.</p>
<p>Searle also argues that, if features like a robot body or a connectionist architecture are <i>required</i>, then strong AI (as he understands it) has been abandoned.<sup id="cite_ref-49" class="reference"><a href="#cite_note-49" title=""><span>[</span>50<span>]</span></a></sup> Either (1) Searle's room can't pass the Turing test, because formal symbol manipulation (by a Turing complete machine) is not enough, or (2) Searle's room <i>could</i> pass the Turing test, but the Turing test is not sufficient to determine if the room has a "mind." Either way, it denies one or the other of the positions Searle thinks of "strong AI", proving his argument.</p>
<p>The brain arguments also suggests that computation can't provide an <i>explanation</i> of the human mind (another aspect of what Searle thinks of as "strong AI"). They assume that there is no simpler way to describe the mind than to create a program that is just as mysterious as the brain was. He writes "I thought the whole idea of strong AI was that we don't need to know how the brain works to know how the mind works."<sup id="cite_ref-50" class="reference"><a href="#cite_note-50" title=""><span>[</span>51<span>]</span></a></sup></p>
<p>Other critics don't argue that these improvements are <i>necessary</i> for the Chinese room to pass the <a href="/wiki/Turing_test" title="Turing test">Turing test</a> or to have a mind. They accept the premise that the room as Searle describes it does, in fact, have a mind, but they argue that it is difficult to see—Searle's description is correct, but <i>misleading.</i> By redesigning the room more realistically they hope to make this more obvious. In this case, these arguments are being used as appeals to intuition (see next section). Searle's intuition, however, is never shaken. He writes: "I can have any formal program you like, but I still understand nothing."<sup id="cite_ref-51" class="reference"><a href="#cite_note-51" title=""><span>[</span>52<span>]</span></a></sup></p>
<p>In fact, the room can just as easily be redesigned to <i>weaken</i> our intuitions. <a href="/wiki/Ned_Block" title="Ned Block">Ned Block</a>'s "<a href="/wiki/Blockhead_(Computer_system)" title="Blockhead (Computer system)" class="mw-redirect">blockhead</a>" argument <cite class="inline">(<a href="#CITEREFBlock1981" title="">Block 1981</a>)</cite> suggests that the program could, in theory, be rewritten into a simple <a href="/wiki/Lookup_table" title="Lookup table">lookup table</a> of <a href="/wiki/Production_system" title="Production system">rules</a> of the form "if the user writes <i>S</i>, reply with <i>P</i> and goto X". Any program can be rewritten (or "<a href="/wiki/Refactored" title="Refactored" class="mw-redirect">refactored</a>") into this form, even a brain simulation.<sup id="cite_ref-52" class="reference"><a href="#cite_note-52" title=""><span>[</span>53<span>]</span></a></sup> In the blockhead scenario, the entire mental state is hidden in the letter X, which represents a <a href="/wiki/Memory_address" title="Memory address">memory address</a>—a number associated with the next rule. It is hard to visualize that an instant of our conscious experience can be captured in a single large number, yet this is exactly what "strong AI" claims.</p>
<p><a name="Speed.2C_complexity_and_other_minds:_appeals_to_intuition" id="Speed.2C_complexity_and_other_minds:_appeals_to_intuition"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=11" title="Edit section: Speed, complexity and other minds: appeals to intuition">edit</a>]</span> <span class="mw-headline">Speed, complexity and other minds: appeals to intuition</span></h3>
<p>The following arguments (and the intuitive interpretations of the arguments above) do not directly explain how a Chinese speaking mind could exist in Searle's room, or how the symbols he manipulates could become meaningful. However, by raising doubts about Searle's intuitions they support other positions, such as the system and robot replies.</p>
<p><b>Speed and complexity replies</b>.<sup id="cite_ref-53" class="reference"><a href="#cite_note-53" title=""><span>[</span>54<span>]</span></a></sup> The speed at which our brains process information is (by some estimates) 100,000,000,000 operations per second.<sup id="cite_ref-54" class="reference"><a href="#cite_note-54" title=""><span>[</span>55<span>]</span></a></sup> Several critics point out that the man in the room would probably take millions of years to respond to a simple question, and would require "filing cabinets" of astronomical proportions. This brings the clarity of Searle's intuition into doubt.</p>
<p>An especially vivid version of the speed and complexity reply is from <a href="/wiki/Paul_Churchland" title="Paul Churchland">Paul</a> and <a href="/wiki/Patricia_Churchland" title="Patricia Churchland">Patricia Churchland</a>. They propose this analogous thought experiment:</p>
<dl>
<dd><b>Churchland's luminous room.</b><sup id="cite_ref-55" class="reference"><a href="#cite_note-55" title=""><span>[</span>56<span>]</span></a></sup> Suppose a philosopher finds it inconceivable that light is caused by waves of electromagnetism. He could go into a dark room and wave a magnet up and down. He would see no light, of course, and he could claim that he had proved light is not a magnetic wave and that he has refuted <a href="/wiki/Maxwell%27s_equations" title="Maxwell's equations">Maxwell's equations</a>. The problem is that he would have to wave the magnet up and down something like 450,000,000,000,000 times a second in order to see anything.</dd>
</dl>
<p>Several of the replies above address the issue of complexity. The connectionist reply emphasizes that a working artificial system would have to be as complex and as interconnected as the human brain. The commonsense knowledge reply emphasizes that any program that passed a Turing test would have to be "an extraordinarily supple, sophisticated, and multilayered system, brimming with 'world knowledge' and meta-knowledge and meta-meta-knowledge," as <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a> explains.<sup id="cite_ref-56" class="reference"><a href="#cite_note-56" title=""><span>[</span>57<span>]</span></a></sup></p>
<p><a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Stevan Harnad</a> is critical of speed and complexity replies when they stray beyond addressing our intuitions. He writes "Some have made a cult of speed and timing, holding that, when accelerated to the right speed, the computational may make a <a href="/wiki/Phase_transition" title="Phase transition">phase transition</a> into the mental. It should be clear that is not a counterargument but merely an <i><a href="/wiki/Ad_hoc" title="Ad hoc">ad hoc</a></i> speculation (as is the view that it is all just a matter of ratcheting up to the right degree of 'complexity.')"<sup id="cite_ref-57" class="reference"><a href="#cite_note-57" title=""><span>[</span>58<span>]</span></a></sup></p>
<p><b>Other minds reply</b>.<sup id="cite_ref-58" class="reference"><a href="#cite_note-58" title=""><span>[</span>59<span>]</span></a></sup> The <a href="/wiki/Problem_of_other_minds" title="Problem of other minds">problem of other minds</a> is that there is no way we can determine if other people's subjective experience is the same as our own. Searle's argument is a version of the <a href="/wiki/Problem_of_other_minds" title="Problem of other minds">problem of other minds</a>, applied to machines. We can only judge if other people have minds by studying their behavior (i.e., by giving them our own <a href="/wiki/Turing_test" title="Turing test">Turing test</a>). Critics of Searle argue that he is holding the Chinese room to a higher standard than we would hold an ordinary person.</p>
<p><a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a> (writing 30 years before Searle presented his argument) noted that people never consider the problem of other minds when dealing with each other. He writes that "instead of arguing continually over this point it is usual to have the polite convention that everyone thinks."<sup id="cite_ref-59" class="reference"><a href="#cite_note-59" title=""><span>[</span>60<span>]</span></a></sup> The <a href="/wiki/Turing_test" title="Turing test">Turing test</a> simply extends this "polite convention" to machines. He doesn't intend to solve the problem of other minds (for machines or people) and he doesn't think we need to. (One of Turing's motivations for devising the test is to avoid precisely the kind of philosophical problems that Searle is interested in. He writes "I do not wish to give the impression that I think there is no mystery ... [but] I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper."<sup id="cite_ref-60" class="reference"><a href="#cite_note-60" title=""><span>[</span>61<span>]</span></a></sup>)</p>
<p>According to those who make this reply, Searle's position implies that we can't prove that the Chinese room <i>or</i> the man in it has a mind. Searle believes that there are "causal properties" in our neurons that give rise to the mind. These causal properties can't be detected by anyone outside the mind, otherwise the Chinese Room couldn't pass the <a href="/wiki/Turing_test" title="Turing test">Turing test</a>—the people outside would be able to tell there wasn't a Chinese speaker in the room by detecting their causal properties. Since they can't detect causal properties, they can't detect the existence of the mental in either case. <a href="#CITEREFRussellNorvig2003" title="">Russell &amp; Norvig (2003)</a> argue that this implies the human mind, as Searle describes it, is <a href="/wiki/Epiphenomenal" title="Epiphenomenal" class="mw-redirect">epiphenomenal</a>: that it "casts no shadow." To make this point clear, <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a> suggests this version of the "other minds" reply:</p>
<dl>
<dd><b>Dennett's reply from natural selection</b>.<sup id="cite_ref-61" class="reference"><a href="#cite_note-61" title=""><span>[</span>62<span>]</span></a></sup> Suppose that, by some mutation, a human being is born that does not have Searle's "causal properties" but nevertheless acts exactly like a human being. (This sort of animal is a called a "<a href="/wiki/Philosophical_zombie" title="Philosophical zombie">zombie</a>" in thought experiments in the <a href="/wiki/Philosophy_of_mind" title="Philosophy of mind">philosophy of mind</a>). This new animal would reproduce just as any other human and eventually there would be more of these zombies. Natural selection would favor the zombies, since their design is (we could suppose) a bit simpler. Eventually the humans would die out. So therefore, if Searle is right, it's most likely that human beings (as we see them today) are actually "zombies," who nevertheless insist they are conscious. This suggests it's unlikely that Searle's "causal properties" would have ever evolved in the first place. Nature has no incentive to create them.</dd>
</dl>
<p>Searle disagrees with this analysis and insists that we must "presuppose the reality and knowability of the mental."<sup id="cite_ref-62" class="reference"><a href="#cite_note-62" title=""><span>[</span>63<span>]</span></a></sup> and that "The study of the mind starts with such facts as that humans have beliefs, while thermostats, telephones, and adding machines don't ... what we wanted to know is what distinguishes the mind from thermostats and livers."<sup id="cite_ref-63" class="reference"><a href="#cite_note-63" title=""><span>[</span>64<span>]</span></a></sup> He takes it as obvious that we can detect the presence of other minds and dismisses this reply as being off the point.</p>
<p><b>What they do and don't prove</b>. These arguments apply only to our intuitions. (As do the arguments above which are intended to make it seem more plausible that the Chinese room contains a mind, which can include the robot, commonsense knowledge, brain simulation and connectionist replies.) They do not directly prove that a machine can or can't have a mind.</p>
<p>However, some critics believe that Searle's argument relies entirely on intuitions. <a href="/wiki/Ned_Block" title="Ned Block">Ned Block</a> writes "Searle's argument depends for its force on intuitions that certain entities do not think."<sup id="cite_ref-64" class="reference"><a href="#cite_note-64" title=""><span>[</span>65<span>]</span></a></sup> <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a> describes the Chinese room argument as an "intuition pump"<sup id="cite_ref-65" class="reference"><a href="#cite_note-65" title=""><span>[</span>66<span>]</span></a></sup> and writes "Searle's thought experiment depends, illicitly, on your imagining too simple a case, an irrelevant case, and drawing the 'obvious' conclusion from it."<sup id="cite_ref-66" class="reference"><a href="#cite_note-66" title=""><span>[</span>67<span>]</span></a></sup></p>
<p>These arguments, if accepted, prevent Searle from claiming that his conclusion is obvious by undermining the intuitions that his certainty requires.</p>
<p><a name="Formal_arguments" id="Formal_arguments"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=12" title="Edit section: Formal arguments">edit</a>]</span> <span class="mw-headline">Formal arguments</span></h2>
<p>Searle has produced a more formal version of the argument of which the Chinese Room forms a part. He presented the first "excessively crude"<sup id="cite_ref-67" class="reference"><a href="#cite_note-67" title=""><span>[</span>68<span>]</span></a></sup> version in 1984. The version given below is from 1990.<sup id="cite_ref-68" class="reference"><a href="#cite_note-68" title=""><span>[</span>69<span>]</span></a></sup></p>
<p>The only premise or conclusion in the argument which should be controversial is A3 and it is this point which the Chinese room thought experiment is intended to prove.<sup id="cite_ref-CC1990_69-0" class="reference"><a href="#cite_note-CC1990-69" title=""><span>[</span>70<span>]</span></a></sup></p>
<p>He begins with three axioms:</p>
<dl>
<dd>(A1) "Programs are formal (<a href="/wiki/Syntax" title="Syntax">syntactic</a>)."
<dl>
<dd>A program uses <a href="/wiki/Syntax" title="Syntax">syntax</a> to manipulate symbols and pays no attention the <a href="/wiki/Semantics" title="Semantics">semantics</a> of the symbols. It knows where to put the symbols and how to move them around, but it doesn't know what they stand for or what they <a href="/wiki/Meaning" title="Meaning">mean</a>. For the program, the symbols are just physical objects like any others.</dd>
</dl>
</dd>
</dl>
<dl>
<dd>(A2) "Minds have mental contents (<a href="/wiki/Semantics" title="Semantics">semantics</a>)."
<dl>
<dd>Unlike the symbols used by a program, our thoughts have meaning: they represent things and we know what it is they represent.</dd>
</dl>
</dd>
</dl>
<dl>
<dd>(A3) "Syntax by itself is neither constitutive of nor sufficient for semantics."
<dl>
<dd>This is what the Chinese room argument is intended to prove: the Chinese room has syntax (because there is a man in there moving symbols around). The Chinese room has no semantics (because, according to Searle, there is no one or nothing in the room that understands what the symbols mean). Therefore, having syntax is not enough to generate semantics.</dd>
</dl>
</dd>
</dl>
<p>Searle posits that these lead directly to this conclusion:</p>
<dl>
<dd>(C1) Programs are neither constitutive of nor sufficient for minds.
<dl>
<dd>This should follow without controversy from the first three: Programs don't have semantics. Programs have only syntax, and syntax is insufficient for semantics. Every mind has semantics. Therefore programs are not minds.</dd>
</dl>
</dd>
</dl>
<p>This much of the argument is intended to show that <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> will never produce a machine with a mind by writing programs that manipulate symbols. The remainder of the argument addresses a different issue. Is the human brain running a program? In other words, is the <a href="/wiki/Computational_theory_of_mind" title="Computational theory of mind">computational theory of mind</a> correct?<sup id="cite_ref-70" class="reference"><a href="#cite_note-70" title=""><span>[</span>71<span>]</span></a></sup> He begins with an axiom that is intended to express the basic modern scientific consensus about brains and minds:</p>
<dl>
<dd>(A4) Brains cause minds.</dd>
</dl>
<p>Searle claims that we can derive "immediately" and "trivially"<sup id="cite_ref-71" class="reference"><a href="#cite_note-71" title=""><span>[</span>72<span>]</span></a></sup> that:</p>
<dl>
<dd>(C2) Any other system capable of causing minds would have to have causal powers (at least) equivalent to those of brains.
<dl>
<dd>Brains must have something that causes a mind to exist. Science has yet to determine exactly what it is, but it must exist, because minds exist. Searle calls it "causal powers". "Causal powers" is whatever the brain uses to create a mind. If anything else can cause a mind to exist, it must have "equivalent causal powers". "Equivalent causal powers" is whatever <i>else</i> that could be used to make a mind.</dd>
</dl>
</dd>
</dl>
<p>And from this he derives the further conclusions:</p>
<dl>
<dd>(C3) Any artifact that produced mental phenomena, any artificial brain, would have to be able to duplicate the specific causal powers of brains, and it could not do that just by running a formal program.
<dl>
<dd>This follows from C1 and C2: Since no program can produce a mind, and "equivalent causal powers" produce minds, it follows that programs do not have "equivalent causal powers."</dd>
</dl>
</dd>
</dl>
<dl>
<dd>(C4) The way that human brains actually produce mental phenomena cannot be solely by virtue of running a computer program.
<dl>
<dd>Since programs do not have "equivalent causal powers", "equivalent causal powers" produce minds, and brains produce minds, it follows that brains do not use programs to produce minds.</dd>
</dl>
</dd>
</dl>
<p><a name="Pop_culture_references" id="Pop_culture_references"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=13" title="Edit section: Pop culture references">edit</a>]</span> <span class="mw-headline">Pop culture references</span></h2>
<ul>
<li>The 2008 feature film <i>The Chinese Room</i> - <a href="http://www.imdb.com/title/tt1326199/" class="external text" title="http://www.imdb.com/title/tt1326199/" rel="nofollow">IMDb</a> concerns characters working in a Chinese Room-like office. An actual Chinese Room, with an inhabitant passing messages in and out, is visualized by the main character as he learns about the thought experiment.</li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=14" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<div class="references-small references-column-count references-column-count-2" style="-moz-column-count:2; column-count:2;">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a></li>
<li id="cite_note-H1-1">^ <a href="#cite_ref-H1_1-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-H1_1-1" title=""><sup><i><b>b</b></i></sup></a> <cite class="inline">(<a href="#CITEREFHarnad2001" title="">Harnad 2001</a>, p.&#160;1)</cite> <a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Harnad</a> edited <i>BBS</i> during the years which saw the introduction and popularisation of the Chinese Room argument.</li>
<li id="cite_note-H2-2">^ <a href="#cite_ref-H2_2-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-H2_2-1" title=""><sup><i><b>b</b></i></sup></a> <a href="#CITEREFHarnad2001" title="">Harnad 2001</a>, p.&#160;2</li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> In <a href="/wiki/Varol_Akman" title="Varol Akman">Akman</a>'s <a href="http://www.google.com/search?client=safari&amp;rls=en&amp;q=cogprints.org/539/0/md2.ps&amp;ie=UTF-8&amp;oe=UTF-8" class="external text" title="http://www.google.com/search?client=safari&amp;rls=en&amp;q=cogprints.org/539/0/md2.ps&amp;ie=UTF-8&amp;oe=UTF-8" rel="nofollow">review of <i>Mind Design II</i></a></li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> <a href="#CITEREFHarnad2005" title="">Harnad (2005)</a> holds that the Searle's argument is against the thesis that "has since come to be called 'computationalism,' according to which cognition is just computation, hence mental states are just computational states". <a href="#CITEREFCole2004" title="">Cole (2004)</a> agrees that "the argument also has broad implications for functionalist and computational theories of meaning and of mind".</li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> See the "Systems reply" below.</li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> See the "Other minds reply" below.</li>
<li id="cite_note-7"><b><a href="#cite_ref-7" title="">^</a></b> The relationship between Searle's argument and consciousness is detailed in <a href="#CITEREFChalmers1996" title="">Chalmers 1996</a></li>
<li id="cite_note-8"><b><a href="#cite_ref-8" title="">^</a></b> This version is from <a href="#CITEREFSearle1999" title="">Searle (1999)</a>, and is also quoted in <a href="#CITEREFDennett1991" title="">Dennett 1991</a>, p.&#160;435. Searle's original formulation was "The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states." <cite class="inline">(<a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;1)</cite>. Strong AI is defined similarly by <a href="#CITEREFRussellNorvig2003" title="">Russell &amp; Norvig (2003</a>, p.&#160;947): "The assertion that machines could possibly act intelligently (or, perhaps better, act as if they were intelligent) is called the 'weak AI' hypothesis by philosophers, and the assertion that machines that do so are actually thinking (as opposed to simulating thinking) is called the 'strong AI' hypothesis."</li>
<li id="cite_note-9"><b><a href="#cite_ref-9" title="">^</a></b> <a href="#CITEREFSearle2008" title="">Searle 2008</a></li>
<li id="cite_note-10"><b><a href="#cite_ref-10" title="">^</a></b> Quoted in <a href="#CITEREFRussellNorvig2003" title="">Russell &amp; Norvig 2003</a>, p.&#160;21. Simon, together with <a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a> and <a href="/wiki/Cliff_Shaw" title="Cliff Shaw">Cliff Shaw</a>, had just completed the first "AI" program, the <a href="/wiki/Logic_Theorist" title="Logic Theorist">Logic Theorist</a>.</li>
<li id="cite_note-11"><b><a href="#cite_ref-11" title="">^</a></b> Quoted in <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, p.&#160;46 and <a href="#CITEREFRussellNorvig2003" title="">Russell &amp; Norvig 2003</a>, p.&#160;17.</li>
<li id="cite_note-12"><b><a href="#cite_ref-12" title="">^</a></b> <a href="#CITEREFHaugeland1986" title="">Haugeland 1986</a>, p.&#160;2. (Italics his)</li>
<li id="cite_note-13"><b><a href="#cite_ref-13" title="">^</a></b> "Partisans of strong AI," Searle writes, "claim that in this question and answer sequence the machine is not only simulating a human ability but also (1) that the machine can literally be said to <i>understand</i> the story and provide the answers to questions, and (2) that what the machine and its program do <i>explains</i> the human ability to understand the story and answer questions about it." <cite class="inline">(<a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;2)</cite></li>
<li id="cite_note-14"><b><a href="#cite_ref-14" title="">^</a></b> Searle believes that "strong AI only makes sense given the dualistic assumption that, where the mind is concerned, the brain doesn't matter." <cite class="inline">(<a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;13)</cite> He writes elsewhere, "I thought the whole idea of strong AI was that we don't need to know how the brain works to know how the mind works." <cite class="inline">(<a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;8)</cite> This position owes its phrasing to <a href="#CITEREFHarnad2001" title="">Harnad (2001)</a>.</li>
<li id="cite_note-15"><b><a href="#cite_ref-15" title="">^</a></b> "One of the points at issue," writes Searle, "is the adequacy of the Turing test." <cite class="inline">(<a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;6)</cite></li>
<li id="cite_note-16"><b><a href="#cite_ref-16" title="">^</a></b> <a href="#CITEREFHarnad2001" title="">Harnad 2001</a>, p.&#160;3 (Italics his)</li>
<li id="cite_note-17"><b><a href="#cite_ref-17" title="">^</a></b> <a href="/wiki/Computationalism" title="Computationalism" class="mw-redirect">Computationalism</a> is associated with <a href="/wiki/Jerry_Fodor" title="Jerry Fodor">Jerry Fodor</a> and <a href="/wiki/Hilary_Putnam" title="Hilary Putnam">Hilary Putnam</a>. <cite class="inline">(<a href="#CITEREFHorst2005" title="">Horst 2005</a>, p.&#160;1)</cite> <a href="#CITEREFHarnad2001" title="">Harnad (2001)</a> also cites Allen Newell and <a href="/wiki/Zenon_Pylyshyn" title="Zenon Pylyshyn">Zenon Pylyshyn</a>. <a href="#CITEREFPinker1997" title="">Pinker (1997)</a> also advocates a version of computationalism.</li>
<li id="cite_note-18"><b><a href="#cite_ref-18" title="">^</a></b> <a href="#CITEREFHarnad2001" title="">Harnad 2001</a>, pp.&#160;3-5</li>
<li id="cite_note-lmecxp-19">^ <a href="#cite_ref-lmecxp_19-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lmecxp_19-1" title=""><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-lmecxp_19-2" title=""><sup><i><b>c</b></i></sup></a> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;13</li>
<li id="cite_note-20"><b><a href="#cite_ref-20" title="">^</a></b> <a href="#CITEREFSearle1990" title="">Searle 1990</a>, p.&#160;29</li>
<li id="cite_note-21"><b><a href="#cite_ref-21" title="">^</a></b> <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, p.&#160;8</li>
<li id="cite_note-22"><b><a href="#cite_ref-22" title="">^</a></b> <a href="#CITEREFChalmers1996" title="">Chalmers 1996</a>, p.&#160;322, quoted in Larry Hauser's <a href="http://host.uniroma3.it/progetti/kant/field/chinesebiblio.html" class="external text" title="http://host.uniroma3.it/progetti/kant/field/chinesebiblio.html" rel="nofollow">annotated bibliography</a>.</li>
<li id="cite_note-23"><b><a href="#cite_ref-23" title="">^</a></b> <a href="#CITEREFRussellNorvig2003" title="">Russell &amp; Norvig 2003</a>, p.&#160;947</li>
<li id="cite_note-K-24"><b><a href="#cite_ref-K_24-0" title="">^</a></b> <cite class="inline">(<a href="#CITEREFKurzweil2005" title="">Kurzweil 2005</a>, p.&#160;260)</cite> or see <a href="http://crnano.typepad.com/crnblog/2005/08/advanced_human_.html" class="external text" title="http://crnano.typepad.com/crnblog/2005/08/advanced_human_.html" rel="nofollow">Advanced Human Intelligence</a></li>
<li id="cite_note-25"><b><a href="#cite_ref-25" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole (2004</a>, pp.&#160;5-6) combines the middle two categories.</li>
<li id="cite_note-26"><b><a href="#cite_ref-26" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, pp.&#160;5-6, <a href="#CITEREFCole2004" title="">Cole 2004</a>, pp.&#160;6-7, <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, pp.&#160;2-3, <a href="#CITEREFRussellNorvig2003" title="">Russell &amp; Norvig 2003</a>, p.&#160;959, <a href="#CITEREFDennett1991" title="">Dennett 1991</a>, p.&#160;439, <a href="#CITEREFHearn2007" title="">Hearn 2007</a>, p.&#160;44, <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, p.&#160;269. Among those who hold to this position (according to <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;6)) are <a href="/wiki/Ned_Block" title="Ned Block">Ned Block</a>, <a href="/wiki/Jack_Copeland" title="Jack Copeland">Jack Copeland</a>, <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a>, <a href="/wiki/Jerry_Fodor" title="Jerry Fodor">Jerry Fodor</a>, <a href="/wiki/John_Haugeland" title="John Haugeland">John Haugeland</a>, <a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil" class="mw-redirect">Ray Kurzweil</a> and <a href="/wiki/Georges_Rey" title="Georges Rey">Georges Rey</a></li>
<li id="cite_note-ihsqxx-27">^ <a href="#cite_ref-ihsqxx_27-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ihsqxx_27-1" title=""><sup><i><b>b</b></i></sup></a> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;6</li>
<li id="cite_note-28"><b><a href="#cite_ref-28" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole (2004</a>, pp.&#160;7-9) ascribes this position to <a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a>, <a href="/w/index.php?title=Tim_Maudlin&amp;action=edit&amp;redlink=1" class="new" title="Tim Maudlin (page does not exist)">Tim Maudlin</a>, <a href="/wiki/David_Chalmers" title="David Chalmers">David Chalmers</a> and David Cole.</li>
<li id="cite_note-29"><b><a href="#cite_ref-29" title="">^</a></b> This is the point of the <a href="/wiki/Universal_Turing_machine" title="Universal Turing machine">universal Turing machine</a> and the <a href="/wiki/Church-Turing_thesis" title="Church-Turing thesis" class="mw-redirect">Church-Turing thesis</a>: what makes a system <a href="/wiki/Turing_complete" title="Turing complete" class="mw-redirect">Turing complete</a> is its ability to do a step-by-step simulation of any other machine.</li>
<li id="cite_note-30"><b><a href="#cite_ref-30" title="">^</a></b> The terminology "implementation independent" is due to <a href="#CITEREFHarnad2001" title="">Harnad (2001</a>, p.&#160;4).</li>
<li id="cite_note-31"><b><a href="#cite_ref-31" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;8</li>
<li id="cite_note-32"><b><a href="#cite_ref-32" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;12</li>
<li id="cite_note-33"><b><a href="#cite_ref-33" title="">^</a></b> <a href="#CITEREFHearn2007" title="">Hearn 2007</a>, p.&#160;47</li>
<li id="cite_note-34"><b><a href="#cite_ref-34" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;21) writes "From the intuition that in the CR thought experiment he would not understand Chinese by running a program, Searle infers that there is no understanding created by running a program. Clearly, whether that inference is valid or not turns on a metaphysical question about the identity of persons and minds. If the person understanding is not identical with the room operator, then the inference is unsound."</li>
<li id="cite_note-35"><b><a href="#cite_ref-35" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;7, <a href="#CITEREFCole2004" title="">Cole 2004</a>, pp.&#160;9-11, <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, p.&#160;3, <a href="#CITEREFHearn2007" title="">Hearn 2007</a>, p.&#160;44. <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;9) ascribes this position to <a href="/wiki/Margaret_Boden" title="Margaret Boden">Margaret Boden</a>, <a href="/wiki/Tim_Crane" title="Tim Crane">Tim Crane</a>, <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a>, <a href="/wiki/Jerry_Fodor" title="Jerry Fodor">Jerry Fodor</a>, <a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Stevan Harnad</a>, <a href="/wiki/Hans_Moravec" title="Hans Moravec">Hans Moravec</a> and <a href="/wiki/Georges_Rey" title="Georges Rey">Georges Rey</a></li>
<li id="cite_note-36"><b><a href="#cite_ref-36" title="">^</a></b> Quoted in <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, p.&#160;272. <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;18) calls this the "externalist" account of meaning.</li>
<li id="cite_note-37"><b><a href="#cite_ref-37" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;7</li>
<li id="cite_note-38"><b><a href="#cite_ref-38" title="">^</a></b> <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, p.&#160;11, <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;19. This argument is supported by <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a> and others.</li>
<li id="cite_note-39"><b><a href="#cite_ref-39" title="">^</a></b> Searle distinguishes between "intrinsic" intentionality and "derived" intentionality. "Intrinsic" intentionality is the kind that involves "conscious understanding" like you would have in a human mind. <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a> doesn't agree that there is a distinction. <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;19) writes "derived intentionality is all there is, according to Dennett."</li>
<li id="cite_note-40"><b><a href="#cite_ref-40" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;18 (where he calls this the "internalist" approach to meaning.) Proponents of this position include <a href="/wiki/Roger_Schank" title="Roger Schank">Roger Schank</a>, <a href="/wiki/Doug_Lenat" title="Doug Lenat" class="mw-redirect">Doug Lenat</a>, <a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a> and (with reservations) <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a>, who writes "The fact is that any program [that passed a Turing test] would have to be an extraordinarily supple, sophisticated, and multilayered system, brimming with 'world knowledge' and meta-knowledge and meta-meta-knowledge." <cite class="inline">(<a href="#CITEREFDennett1997" title="">Dennett 1997</a>, p.&#160;438)</cite></li>
<li id="cite_note-41"><b><a href="#cite_ref-41" title="">^</a></b> <a href="#CITEREFDreyfus1979" title="">Dreyfus 1979</a>. See "the <a href="/wiki/Epistemological" title="Epistemological" class="mw-redirect">epistemological</a> assumption".</li>
<li id="cite_note-42"><b><a href="#cite_ref-42" title="">^</a></b> <a href="#CITEREFSearle1984" title="">Searle 1984</a>. He also writes "Formal symbols by themselves can never be enough for mental contents, because the symbols, by definition, have no meaning (or <a href="/wiki/Interpretation_(logic)" title="Interpretation (logic)">interpretation</a>, or semantics) except insofar as someone outside the system gives it to them" <a href="#CITEREFSearle1989" title="">Searle 1989</a>, p.&#160;45 quoted in <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;16.</li>
<li id="cite_note-43"><b><a href="#cite_ref-43" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, pp.&#160;7-8, <a href="#CITEREFCole2004" title="">Cole 2004</a>, pp.&#160;12-13, <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, pp.&#160;3-4, <a href="#CITEREFChurchlandChurchland1990" title="">Churchland &amp; Churchland 1990</a>. <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;12) ascribes this position to <a href="/wiki/Paul_Churchland" title="Paul Churchland">Paul Churchland</a>, <a href="/wiki/Patricia_Churchland" title="Patricia Churchland">Patricia Churchland</a> and <a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil" class="mw-redirect">Ray Kurzweil</a>.</li>
<li id="cite_note-44"><b><a href="#cite_ref-44" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;4, <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, p.&#160;11. Early versions of this argument were put forward in 1974 by <a href="/wiki/Lawrence_Davis" title="Lawrence Davis" class="mw-redirect">Lawrence Davis</a> and in 1978 by <a href="/wiki/Ned_Block" title="Ned Block">Ned Block</a>. Block's version used walky talkies and was called the "Chinese Gym". <a href="#CITEREFChurchlandChurchland1990" title="">Churchland &amp; Churchland (1990)</a> described this scenario as well.</li>
<li id="cite_note-45"><b><a href="#cite_ref-45" title="">^</a></b> <a href="#CITEREFRussellNorvig" title="">Russell Norvig</a>, pp.&#160;956-8, <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;20, <a href="#CITEREFMoravec1988" title="">Moravec 1988</a>, p.&#160;? <a href="/w/index.php?title=CHECK&amp;action=edit&amp;redlink=1" class="new" title="CHECK (page does not exist)">CHECK</a>, <a href="#CITEREFKurzweil2005" title="">Kurzweil 2005</a>, p.&#160;262 <a href="/w/index.php?title=CHECK&amp;action=edit&amp;redlink=1" class="new" title="CHECK (page does not exist)">CHECK</a>, <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, pp.&#160;271 and 279 <a href="/w/index.php?title=CHECK&amp;action=edit&amp;redlink=1" class="new" title="CHECK (page does not exist)">CHECK</a>. An early version of this argument was put forward by <a href="/w/index.php?title=Clark_Glymour&amp;action=edit&amp;redlink=1" class="new" title="Clark Glymour (page does not exist)">Clark Glymour</a> in the mid-70s and was touched on by <a href="/wiki/Zenon_Pylyshyn" title="Zenon Pylyshyn">Zenon Pylyshyn</a> in 1980. <a href="#CITEREFMoravec1988" title="">Moravec (1988)</a> presented a vivid version of it, and it is now associated with <a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil" class="mw-redirect">Ray Kurzweil</a>'s version of <a href="/wiki/Transhumanism" title="Transhumanism">transhumanism</a>.</li>
<li id="cite_note-46"><b><a href="#cite_ref-46" title="">^</a></b> Searle predicts that, while going through the brain prosthesis, "you find, to your total amazement, that you are indeed losing control of you external behavior. You find, for example, that when doctors test your vision, you hear them say 'We are holding up a red object in front of you; pleas tell us what you see.' You want to cry out 'I can't see anything. I'm going totally blind.' But you hear your voice saying in a way that is completely out your control, 'I see a read object in front of me.' ... [Y]our conscious experience slowly shrinks to nothing, while your externally observable behavior remains the same." <a href="#CITEREFSearle1992" title="">Searle 1992</a> quoted in <a href="#CITEREFRussellNorvig2003" title="">Russell &amp; Norvig 2003</a>, p.&#160;957.</li>
<li id="cite_note-47"><b><a href="#cite_ref-47" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole (2004</a>, pp.&#160;12 &amp; 17) ascribes this position to <a href="/wiki/Andy_Clark" title="Andy Clark">Andy Clark</a> and <a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil" class="mw-redirect">Ray Kurzweil</a>. <a href="#CITEREFHauser2006" title="">Hauser (2006</a>, p.&#160;7) associates this position with <a href="/wiki/Paul_Churchland" title="Paul Churchland">Paul</a> and <a href="/wiki/Patricia_Churchland" title="Patricia Churchland">Patricia Churchland</a>.</li>
<li id="cite_note-48"><b><a href="#cite_ref-48" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, pp.&#160;8-9, <a href="#CITEREFHauser" title="">Hauser</a>,</li>
<li id="cite_note-49"><b><a href="#cite_ref-49" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle (1980</a>, p.&#160;7) writes that the robot reply "tacitly concedes that cognition is not solely a matter of formal symbol manipulation." <a href="#CITEREFHarnad2001" title="">Harnad (2001</a>, p.&#160;14) makes the same point, writing: "Now just as it is no refutation (but rather an affirmation) of the CRA to deny that [the Turing test] is a strong enough test, or to deny that a computer could ever pass it, it is merely special pleading to try to save computationalism by stipulating ad hoc (in the face of the CRA) that implementational details do matter after all, and that the computer's is the 'right' kind of implementation, whereas Searle's is the 'wrong' kind."</li>
<li id="cite_note-50"><b><a href="#cite_ref-50" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;8</li>
<li id="cite_note-51"><b><a href="#cite_ref-51" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;3</li>
<li id="cite_note-52"><b><a href="#cite_ref-52" title="">^</a></b> That is, any program running on a machine with a finite amount memory.</li>
<li id="cite_note-53"><b><a href="#cite_ref-53" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole 2004</a>, pp.&#160;14-15, <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, pp.&#160;269-270, <a href="#CITEREFPinker" title="">Pinker</a>, p.&#160;95. <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;14) ascribes this "speed" position to <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a>, <a href="/w/index.php?title=Tim_Maudlin&amp;action=edit&amp;redlink=1" class="new" title="Tim Maudlin (page does not exist)">Tim Maudlin</a>, <a href="/wiki/David_Chalmers" title="David Chalmers">David Chalmers</a>, <a href="/wiki/Steven_Pinker" title="Steven Pinker">Steven Pinker</a>, <a href="/wiki/Paul_Churchland" title="Paul Churchland">Paul Churchland</a>, <a href="/wiki/Patricia_Churchland" title="Patricia Churchland">Patricia Churchland</a> and others. <a href="#CITEREFDennett1991" title="">Dennett (1991</a>, p.&#160;438) points out the complexity of world knowledge.</li>
<li id="cite_note-54"><b><a href="#cite_ref-54" title="">^</a></b> <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, p.&#160;269</li>
<li id="cite_note-55"><b><a href="#cite_ref-55" title="">^</a></b> <a href="#CITEREFChurchlandChurchland1990" title="">Churchland &amp; Churchland 1990</a>, <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;12, <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, p.&#160;270, <a href="#CITEREFHearn2007" title="">Hearn 2007</a>, pp.&#160;45-46, <a href="#CITEREFPinker1997" title="">Pinker 1997</a>, p.&#160;94</li>
<li id="cite_note-56"><b><a href="#cite_ref-56" title="">^</a></b> <cite class="inline">(<a href="#CITEREFDennett1991" title="">Dennett 1991</a>, p.&#160;438)</cite></li>
<li id="cite_note-57"><b><a href="#cite_ref-57" title="">^</a></b> <a href="#CITEREFHarnad2001" title="">Harnad 2001</a>, p.&#160;7. Critics of the "phase transition" form of this argument include Harnad, <a href="/w/index.php?title=Tim_Maudlin&amp;action=edit&amp;redlink=1" class="new" title="Tim Maudlin (page does not exist)">Tim Maudlin</a>, <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a> and <a href="#CITEREFCole2004" title="">Cole (2004</a>, p.&#160;14). This "phase transition" idea is a version of <a href="/wiki/Strong_emergentism" title="Strong emergentism" class="mw-redirect">strong emergentism</a> (what <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a> derides as "Woo woo West Coast emergence" <cite class="inline">(<a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, p.&#160;275)</cite>). Harnad accuses <a href="/wiki/Paul_Churchland" title="Paul Churchland">Churchland</a> and <a href="/wiki/Patricia_Churchland" title="Patricia Churchland">Patricia Churchland</a> of espousing strong emergentism and <a href="#CITEREFKurzweil2005" title="">Kurzweil (2005)</a> seems to also agree with strong emergentism.</li>
<li id="cite_note-58"><b><a href="#cite_ref-58" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;13, <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, pp.&#160;4-5. <a href="#CITEREFTuring1950" title="">Turing (1950</a>, pp.&#160;11-12) makes this reply to what he calls "The Argument from Consciousness." <a href="#CITEREFCole2004" title="">Cole (2004</a>, pp.&#160;12-13) ascribes this position to <a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Daniel Dennett</a>, <a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil" class="mw-redirect">Ray Kurzweil</a> and <a href="/wiki/Hans_Moravec" title="Hans Moravec">Hans Moravec</a>.</li>
<li id="cite_note-59"><b><a href="#cite_ref-59" title="">^</a></b> <a href="#CITEREFTuring1950" title="">Turing 1950</a>, p.&#160;11</li>
<li id="cite_note-60"><b><a href="#cite_ref-60" title="">^</a></b> <a href="#CITEREFTuring1950" title="">Turing 1950</a>, p.&#160;12 Although Turing is discussing consciousness (not the mind or understanding or intentionality), <a href="#CITEREFNorvigRussell2003" title="">Norvig &amp; Russell (2003</a>, p.&#160;952-953) argue that Turing's comments apply the Chinese room.</li>
<li id="cite_note-61"><b><a href="#cite_ref-61" title="">^</a></b> <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;22, <a href="#CITEREFCrevier1993" title="">Crevier 1993</a>, p.&#160;271, <a href="#CITEREFHarnad2004" title="">Harnad 2004</a>, p.&#160;4</li>
<li id="cite_note-62"><b><a href="#cite_ref-62" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;10</li>
<li id="cite_note-63"><b><a href="#cite_ref-63" title="">^</a></b> <a href="#CITEREFSearle1980" title="">Searle 1980</a>, p.&#160;7</li>
<li id="cite_note-64"><b><a href="#cite_ref-64" title="">^</a></b> Quoted in <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;13.</li>
<li id="cite_note-65"><b><a href="#cite_ref-65" title="">^</a></b> <a href="#CITEREFDennett1991" title="">Dennett 1991</a>, pp.&#160;437 &amp; 440</li>
<li id="cite_note-66"><b><a href="#cite_ref-66" title="">^</a></b> <a href="#CITEREFDennett1991" title="">Dennett 1991</a>, p.&#160;438</li>
<li id="cite_note-67"><b><a href="#cite_ref-67" title="">^</a></b> <a href="#CITEREFSearle1984" title="">Searle 1984</a></li>
<li id="cite_note-68"><b><a href="#cite_ref-68" title="">^</a></b> <a href="#CITEREFSearle1984" title="">Searle 1984</a>, <a href="#CITEREFSearle1990" title="">Searle 1990</a>. The wording of each axiom and conclusion if from <a href="#CITEREFSearle1990" title="">Searle (1990)</a>. This version is based on <a href="#CITEREFHauser2006" title="">Hauser 2006</a>, p.&#160;5. (A1-3) and (C1) are described as 1,2,3 and 4 in <a href="#CITEREFCole2004" title="">Cole 2004</a>, p.&#160;5.</li>
<li id="cite_note-CC1990-69"><b><a href="#cite_ref-CC1990_69-0" title="">^</a></b> <a href="#CITEREFChurchlandChurchland1990" title="">Churchland &amp; Churchland (1990</a>, p.&#160;34) explain that the Chinese Room argument is intended to "shore up axiom 3".</li>
<li id="cite_note-70"><b><a href="#cite_ref-70" title="">^</a></b> <a href="#CITEREFHarnad2001" title="">Harnad (2001)</a> argues that Searle's primary target is <a href="/wiki/Computationalism" title="Computationalism" class="mw-redirect">computationalism</a>.</li>
<li id="cite_note-71"><b><a href="#cite_ref-71" title="">^</a></b> <a href="#CITEREFSearle1990" title="">Searle 1990</a></li>
</ol>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=15" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><cite style="font-style:normal" class="" id="CITEREFBlock1981"><a href="/wiki/Ned_Block" title="Ned Block">Block, Ned</a> (1981), "<a href="http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Psychologism.htm" class="external text" title="http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Psychologism.htm" rel="nofollow">Psychologism and Behaviourism</a>", <i>The Philosophical Review</i> <b>90</b>: 5–43, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.2307%2F2184371" class="external text" title="http://dx.doi.org/10.2307%2F2184371" rel="nofollow">10.2307/2184371</a></span><span class="printonly">, <a href="http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Psychologism.htm" class="external free" title="http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Psychologism.htm" rel="nofollow">http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Psychologism.htm</a></span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Psychologism+and+Behaviourism&amp;rft.jtitle=The+Philosophical+Review&amp;rft.aulast=Block&amp;rft.aufirst=Ned&amp;rft.au=Block%2C+Ned&amp;rft.date=1981&amp;rft.volume=90&amp;rft.pages=5%E2%80%9343&amp;rft_id=info:doi/10.2307%2F2184371&amp;rft_id=http%3A%2F%2Fwww.nyu.edu%2Fgsas%2Fdept%2Fphilo%2Ffaculty%2Fblock%2Fpapers%2FPsychologism.htm&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFChalmers1996"><a href="/wiki/David_Chalmers" title="David Chalmers">Chalmers, David</a> (1996), <i>The Conscious Mind: In Search of a Fundamental Theory</i>, Oxford University Press</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Conscious+Mind%3A+In+Search+of+a+Fundamental+Theory&amp;rft.aulast=Chalmers&amp;rft.aufirst=David&amp;rft.au=Chalmers%2C+David&amp;rft.date=1996&amp;rft.pub=Oxford+University+Press&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFChurchlandChurchland1990"><a href="/wiki/Paul_Churchland" title="Paul Churchland">Churchland, Paul</a>; <a href="/wiki/Patricia_Churchland" title="Patricia Churchland">Churchland, Patricia</a> (January 1990), "Could a machine think?", <i>Scientific American</i> <b>262</b>: 32–39</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Could+a+machine+think%3F&amp;rft.jtitle=Scientific+American&amp;rft.aulast=Churchland&amp;rft.aufirst=Paul&amp;rft.au=Churchland%2C+Paul&amp;rft.au=Churchland%2C+Patricia&amp;rft.date=January+1990&amp;rft.volume=262&amp;rft.pages=32%E2%80%9339&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="" id="CITEREFCole2004"><a href="/wiki/David_Cole" title="David Cole">Cole, David</a> (Fall 2004), <a href="http://plato.stanford.edu/archives/fall2004/entries/chinese-room/" class="external text" title="http://plato.stanford.edu/archives/fall2004/entries/chinese-room/" rel="nofollow">"The Chinese Room Argument"</a>, in Zalta, Edward N., <i>The Stanford Encyclopedia of Philosophy</i><span class="printonly">, <a href="http://plato.stanford.edu/archives/fall2004/entries/chinese-room/" class="external free" title="http://plato.stanford.edu/archives/fall2004/entries/chinese-room/" rel="nofollow">http://plato.stanford.edu/archives/fall2004/entries/chinese-room/</a></span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=The+Chinese+Room+Argument&amp;rft.atitle=The+Stanford+Encyclopedia+of+Philosophy&amp;rft.aulast=Cole&amp;rft.aufirst=David&amp;rft.au=Cole%2C+David&amp;rft.date=Fall+2004&amp;rft_id=http%3A%2F%2Fplato.stanford.edu%2Farchives%2Ffall2004%2Fentries%2Fchinese-room%2F&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>. <i>Page numbers above refer to a standard <a href="/wiki/Pdf" title="Pdf" class="mw-redirect">pdf</a> print of the article.</i></li>
<li><cite style="font-style:normal" class="" id="CITEREFCrevier1993"><a href="/wiki/Daniel_Crevier" title="Daniel Crevier">Crevier, Daniel</a> (1993), <i>AI: The Tumultuous Search for Artificial Intelligence</i>, New York, NY: BasicBooks, <a href="/wiki/Special:BookSources/0465029973" class="internal">ISBN 0-465-02997-3</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=AI%3A+The+Tumultuous+Search+for+Artificial+Intelligence&amp;rft.aulast=Crevier&amp;rft.aufirst=Daniel&amp;rft.au=Crevier%2C+Daniel&amp;rft.date=1993&amp;rft.place=New+York%2C+NY&amp;rft.pub=BasicBooks&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="book" id="CITEREFDennett1991"><a href="/wiki/Daniel_Dennett" title="Daniel Dennett">Dennett, Daniel</a> (1991). <i><a href="/wiki/Consciousness_Explained" title="Consciousness Explained">Consciousness Explained</a></i>. The Penguin Press. <a href="/wiki/Special:BookSources/0713990376" class="internal">ISBN 0-7139-9037-6</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=%5B%5BConsciousness+Explained%5D%5D&amp;rft.aulast=Dennett&amp;rft.aufirst=Daniel&amp;rft.au=Dennett%2C+Daniel&amp;rft.date=1991&amp;rft.pub=The+Penguin+Press&amp;rft.isbn=0-7139-9037-6&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFFearn2007">Fearn, Nicholas (2007), <i>The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers</i>, New York: Grove Press</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Latest+Answers+to+the+Oldest+Questions%3A+A+Philosophical+Adventure+with+the+World%27s+Greatest+Thinkers&amp;rft.aulast=Fearn&amp;rft.aufirst=Nicholas&amp;rft.au=Fearn%2C+Nicholas&amp;rft.date=2007&amp;rft.place=New+York&amp;rft.pub=Grove+Press&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="" id="CITEREFHarnad2001"><a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Harnad, Stevan</a> (2001), <a href="http://cogprints.org/4023/" class="external text" title="http://cogprints.org/4023/" rel="nofollow">"What's Wrong and Right About Searle's Chinese Room Argument"</a>, in M.; Preston, J., <i>Essays on Searle's Chinese Room Argument</i>, Oxford University Press<span class="printonly">, <a href="http://cogprints.org/4023/" class="external free" title="http://cogprints.org/4023/" rel="nofollow">http://cogprints.org/4023/</a></span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=What%27s+Wrong+and+Right+About+Searle%27s+Chinese+Room+Argument&amp;rft.atitle=Essays+on+Searle%27s+Chinese+Room+Argument&amp;rft.aulast=Harnad&amp;rft.aufirst=Stevan&amp;rft.au=Harnad%2C+Stevan&amp;rft.date=2001&amp;rft.pub=Oxford+University+Press&amp;rft_id=http%3A%2F%2Fcogprints.org%2F4023%2F&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>. <i>Page numbers above refer to a standard <a href="/wiki/Pdf" title="Pdf" class="mw-redirect">pdf</a> print of the article.</i></li>
<li><cite style="font-style:normal" class="" id="CITEREFHarnad2005"><a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Harnad, Stevan</a> (2005), <a href="http://eprints.ecs.soton.ac.uk/10424/01/chineseroom.html" class="external text" title="http://eprints.ecs.soton.ac.uk/10424/01/chineseroom.html" rel="nofollow">"Searle's Chinese Room Argument"</a>, <i>Encyclopedia of Philosophy</i>, Macmillan<span class="printonly">, <a href="http://eprints.ecs.soton.ac.uk/10424/01/chineseroom.html" class="external free" title="http://eprints.ecs.soton.ac.uk/10424/01/chineseroom.html" rel="nofollow">http://eprints.ecs.soton.ac.uk/10424/01/chineseroom.html</a></span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=Searle%27s+Chinese+Room+Argument&amp;rft.atitle=Encyclopedia+of+Philosophy&amp;rft.aulast=Harnad&amp;rft.aufirst=Stevan&amp;rft.au=Harnad%2C+Stevan&amp;rft.date=2005&amp;rft.pub=Macmillan&amp;rft_id=http%3A%2F%2Feprints.ecs.soton.ac.uk%2F10424%2F01%2Fchineseroom.html&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>. <i>Page numbers above refer to a standard <a href="/wiki/Pdf" title="Pdf" class="mw-redirect">pdf</a> print of the article.</i></li>
<li><cite style="font-style:normal" class="" id="CITEREFHauser1997"><a href="/w/index.php?title=Larry_Hauser&amp;action=edit&amp;redlink=1" class="new" title="Larry Hauser (page does not exist)">Hauser, Larry</a> (1997), "<a href="http://members.aol.com/lshauser2/chinabox.html" class="external text" title="http://members.aol.com/lshauser2/chinabox.html" rel="nofollow">Searle's Chinese Box: Debunking the Chinese Room Argument</a>", <i>Minds and Machines</i> <b>7</b>: 199–226, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1023%2FA%3A1008255830248" class="external text" title="http://dx.doi.org/10.1023%2FA%3A1008255830248" rel="nofollow">10.1023/A:1008255830248</a></span><span class="printonly">, <a href="http://members.aol.com/lshauser2/chinabox.html" class="external free" title="http://members.aol.com/lshauser2/chinabox.html" rel="nofollow">http://members.aol.com/lshauser2/chinabox.html</a></span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Searle%27s+Chinese+Box%3A+Debunking+the+Chinese+Room+Argument&amp;rft.jtitle=Minds+and+Machines&amp;rft.aulast=Hauser&amp;rft.aufirst=Larry&amp;rft.au=Hauser%2C+Larry&amp;rft.date=1997&amp;rft.volume=7&amp;rft.pages=199%E2%80%93226&amp;rft_id=info:doi/10.1023%2FA%3A1008255830248&amp;rft_id=http%3A%2F%2Fmembers.aol.com%2Flshauser2%2Fchinabox.html&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>. <i>Page numbers above refer to a standard <a href="/wiki/Pdf" title="Pdf" class="mw-redirect">pdf</a> print of the article.</i></li>
<li><cite style="font-style:normal" class="" id="CITEREFHauser2006"><a href="/w/index.php?title=Larry_Hauser&amp;action=edit&amp;redlink=1" class="new" title="Larry Hauser (page does not exist)">Hauser, Larry</a> (2006), <a href="http://www.iep.utm.edu/c/chineser.htm" class="external text" title="http://www.iep.utm.edu/c/chineser.htm" rel="nofollow">"Searle's Chinese Room"</a>, <i>Internet Encyclopedia of Philosophy</i><span class="printonly">, <a href="http://www.iep.utm.edu/c/chineser.htm" class="external free" title="http://www.iep.utm.edu/c/chineser.htm" rel="nofollow">http://www.iep.utm.edu/c/chineser.htm</a></span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=Searle%27s+Chinese+Room&amp;rft.atitle=Internet+Encyclopedia+of+Philosophy&amp;rft.aulast=Hauser&amp;rft.aufirst=Larry&amp;rft.au=Hauser%2C+Larry&amp;rft.date=2006&amp;rft_id=http%3A%2F%2Fwww.iep.utm.edu%2Fc%2Fchineser.htm&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>. <i>Page numbers above refer to a standard <a href="/wiki/Pdf" title="Pdf" class="mw-redirect">pdf</a> print of the article.</i></li>
<li><cite style="font-style:normal" class="" id="CITEREFKurzweil2005"><a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil" class="mw-redirect">Kurzweil, Ray</a> (2005), <i><a href="/wiki/The_Singularity_is_Near" title="The Singularity is Near" class="mw-redirect">The Singularity is Near</a></i>, Viking Press</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=%5B%5BThe+Singularity+is+Near%5D%5D&amp;rft.aulast=Kurzweil&amp;rft.aufirst=Ray&amp;rft.au=Kurzweil%2C+Ray&amp;rft.date=2005&amp;rft.pub=Viking+Press&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="" id="CITEREFMoravec1988"><a href="/wiki/Hans_Moravec" title="Hans Moravec">Moravec, Hans</a> (1988), <i>Mind Children</i>, Harvard University Press</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mind+Children&amp;rft.aulast=Moravec&amp;rft.aufirst=Hans&amp;rft.au=Moravec%2C+Hans&amp;rft.date=1988&amp;rft.pub=Harvard+University+Press&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="" id="CITEREFRussellNorvig2003"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart J.</a>; <a href="/wiki/Peter_Norvig" title="Peter Norvig">Norvig, Peter</a> (2003), <i><a href="http://aima.cs.berkeley.edu/" class="external text" title="http://aima.cs.berkeley.edu/" rel="nofollow">Artificial Intelligence: A Modern Approach</a></i> (2nd ed.), Upper Saddle River, NJ: Prentice Hall, <a href="/wiki/Special:BookSources/0137903952" class="internal">ISBN 0-13-790395-2</a><span class="printonly">, <a href="http://aima.cs.berkeley.edu/" class="external free" title="http://aima.cs.berkeley.edu/" rel="nofollow">http://aima.cs.berkeley.edu/</a></span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Russell%2C+Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rft.date=2003&amp;rft.edition=2nd&amp;rft.place=Upper+Saddle+River%2C+NJ&amp;rft.pub=Prentice+Hall&amp;rft_id=http%3A%2F%2Faima.cs.berkeley.edu%2F&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFPinker1997"><a href="/wiki/Steven_Pinker" title="Steven Pinker">Pinker, Steven</a> (1997), <i><a href="/wiki/How_the_Mind_Works" title="How the Mind Works">How the Mind Works</a></i>, New York, NY: W. W. Norton &amp; Company, Inc., <a href="/wiki/Special:BookSources/0393318486" class="internal">ISBN 0-393-31848-6</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=%5B%5BHow+the+Mind+Works%5D%5D&amp;rft.aulast=Pinker&amp;rft.aufirst=Steven&amp;rft.au=Pinker%2C+Steven&amp;rft.date=1997&amp;rft.place=New+York%2C+NY&amp;rft.pub=W.+W.+Norton+%26+Company%2C+Inc.&amp;rft.isbn=0-393-31848-6&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="" id="CITEREFSearle1980"><a href="/wiki/John_Searle" title="John Searle">Searle, John</a> (1980), "<a href="http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html" class="external text" title="http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html" rel="nofollow">Minds, Brains and Programs</a>", <i><a href="/wiki/Behavioral_and_Brain_Sciences" title="Behavioral and Brain Sciences">Behavioral and Brain Sciences</a></i> <b>3</b> (3): 417–457<span class="printonly">, <a href="http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html" class="external free" title="http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html" rel="nofollow">http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html</a></span><span class="reference-accessdate">, retrieved on October 8 2008</span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Minds%2C+Brains+and+Programs&amp;rft.jtitle=%5B%5BBehavioral+and+Brain+Sciences%5D%5D&amp;rft.aulast=Searle&amp;rft.aufirst=John&amp;rft.au=Searle%2C+John&amp;rft.date=1980&amp;rft.volume=3&amp;rft.issue=3&amp;rft.pages=417%E2%80%93457&amp;rft_id=http%3A%2F%2Fmembers.aol.com%2FNeoNoetics%2FMindsBrainsPrograms.html&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>. <i>Page numbers above refer to a standard <a href="/wiki/Pdf" title="Pdf" class="mw-redirect">pdf</a> print of the article. See also Searle's <a href="http://www.bbsonline.org/Preprints/OldArchive/bbs.searle2.html" class="external text" title="http://www.bbsonline.org/Preprints/OldArchive/bbs.searle2.html" rel="nofollow">original draft</a>.</i></li>
<li><cite style="font-style:normal" class="" id="CITEREFSearle1983"><a href="/wiki/John_Searle" title="John Searle">Searle, John</a> (1983), "Can Computers Think?", in <a href="/wiki/David_Chalmers" title="David Chalmers">Chalmers, David</a>, <i>Philosophy of Mind: Classical and Contemporary Readings</i>, Oxford: Oxford University Press, pp.&#160;669–675, <a href="/wiki/Special:BookSources/019514581X" class="internal">ISBN 0-19-514581-X</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=Can+Computers+Think%3F&amp;rft.atitle=Philosophy+of+Mind%3A+Classical+and+Contemporary+Readings&amp;rft.aulast=Searle&amp;rft.aufirst=John&amp;rft.au=Searle%2C+John&amp;rft.date=1983&amp;rft.pages=pp.%26nbsp%3B669%E2%80%93675&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.isbn=0-19-514581-X&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFSearle1984"><a href="/wiki/John_Searle" title="John Searle">Searle, John</a> (1984), <i>Minds, Brains and Science: The 1984 Reith Lectures</i>, Harvard University Press, <a href="/wiki/Special:BookSources/0674576314" class="internal">ISBN 0-67457631-4</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Minds%2C+Brains+and+Science%3A+The+1984+Reith+Lectures&amp;rft.aulast=Searle&amp;rft.aufirst=John&amp;rft.au=Searle%2C+John&amp;rft.date=1984&amp;rft.pub=Harvard+University+Press&amp;rft.isbn=0-67457631-4&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span> paperback: <a href="/wiki/Special:BookSources/0674576330" class="internal">ISBN 0-67457633-0</a>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFSearle1990"><a href="/wiki/John_Searle" title="John Searle">Searle, John</a> (January 1990), "Is the Brain's Mind a Computer Program?", <i>Scientific American</i> <b>262</b>: 26–31</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Is+the+Brain%27s+Mind+a+Computer+Program%3F&amp;rft.jtitle=Scientific+American&amp;rft.aulast=Searle&amp;rft.aufirst=John&amp;rft.au=Searle%2C+John&amp;rft.date=January+1990&amp;rft.volume=262&amp;rft.pages=26%E2%80%9331&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFSearle1992"><a href="/wiki/John_Searle" title="John Searle">Searle, John</a> (1992), <i>The Rediscovery of the Mind</i>, Cambridge, Massachusetts: M.I.T. Press</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Rediscovery+of+the+Mind&amp;rft.aulast=Searle&amp;rft.aufirst=John&amp;rft.au=Searle%2C+John&amp;rft.date=1992&amp;rft.place=Cambridge%2C+Massachusetts&amp;rft.pub=M.I.T.+Press&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>.</li>
<li><cite style="font-style:normal" class="" id="CITEREFSearle1999"><a href="/wiki/John_Searle" title="John Searle">Searle, John</a> (1999), <i>Mind, language and society</i>, New York, NY: Basic Books, <a href="/wiki/Special:BookSources/0465045219" class="internal">ISBN 0465045219</a>, <a href="/wiki/Online_Computer_Library_Center" title="Online Computer Library Center">OCLC</a> <a href="http://worldcat.org/oclc/231867665+43689264" class="external text" title="http://worldcat.org/oclc/231867665+43689264" rel="nofollow">231867665 43689264</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mind%2C+language+and+society&amp;rft.aulast=Searle&amp;rft.aufirst=John&amp;rft.au=Searle%2C+John&amp;rft.date=1999&amp;rft.place=New+York%2C+NY&amp;rft.pub=Basic+Books&amp;rft_id=info:oclcnum/231867665+43689264&amp;rft.isbn=0465045219&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="" id="CITEREFTuring1950"><a href="/wiki/Alan_Turing" title="Alan Turing">Turing, Alan</a> (October 1950), "<a href="http://loebner.net/Prizef/TuringArticle.html" class="external text" title="http://loebner.net/Prizef/TuringArticle.html" rel="nofollow">Computing Machinery and Intelligence</a>", <i><a href="/wiki/Mind_(journal)" title="Mind (journal)">Mind</a></i> <b>LIX</b> (236): 433–460, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1093%2Fmind%2FLIX.236.433" class="external text" title="http://dx.doi.org/10.1093%2Fmind%2FLIX.236.433" rel="nofollow">10.1093/mind/LIX.236.433</a></span>, <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a> <a href="http://worldcat.org/issn/0026-4423" class="external text" title="http://worldcat.org/issn/0026-4423" rel="nofollow">0026-4423</a><span class="printonly">, <a href="http://loebner.net/Prizef/TuringArticle.html" class="external free" title="http://loebner.net/Prizef/TuringArticle.html" rel="nofollow">http://loebner.net/Prizef/TuringArticle.html</a></span><span class="reference-accessdate">, retrieved on 2008-08-18</span></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Computing+Machinery+and+Intelligence&amp;rft.jtitle=%5B%5BMind+%28journal%29%7CMind%5D%5D&amp;rft.aulast=Turing&amp;rft.aufirst=Alan&amp;rft.au=Turing%2C+Alan&amp;rft.date=October+1950&amp;rft.volume=LIX&amp;rft.issue=236&amp;rft.pages=433%E2%80%93460&amp;rft_id=info:doi/10.1093%2Fmind%2FLIX.236.433&amp;rft.issn=0026-4423&amp;rft_id=http%3A%2F%2Floebner.net%2FPrizef%2FTuringArticle.html&amp;rfr_id=info:sid/en.wikipedia.org:Chinese_room"><span style="display: none;">&#160;</span></span>. <i>Page numbers above refer to a standard <a href="/wiki/Pdf" title="Pdf" class="mw-redirect">pdf</a> print of the article.</i></li>
</ul>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=16" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Turing_test" title="Turing test">Turing test</a></li>
</ul>
<p><a name="Further_reading" id="Further_reading"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Chinese_room&amp;action=edit&amp;section=17" title="Edit section: Further reading">edit</a>]</span> <span class="mw-headline">Further reading</span></h2>
<ul>
<li><a href="http://en.wikibooks.org/wiki/Consciousness_studies" class="extiw" title="b:Consciousness studies">Wikibooks: Consciousness Studies</a></li>
<li><a href="http://samvak.tripod.com/chinese.html" class="external text" title="http://samvak.tripod.com/chinese.html" rel="nofollow">Philosophical and analytic considerations in the Chinese Room thought experiment</a></li>
<li><a href="http://globetrotter.berkeley.edu/people/Searle/searle-con0.html" class="external text" title="http://globetrotter.berkeley.edu/people/Searle/searle-con0.html" rel="nofollow">Interview in which Searle discusses the Chinese Room</a></li>
<li><a href="http://www.zompist.com/searle.html" class="external text" title="http://www.zompist.com/searle.html" rel="nofollow">Understanding the Chinese Room (critical)</a> from <a href="/wiki/Zompist.com" title="Zompist.com">Zompist.com</a></li>
<li><a href="http://www.anti-state.com/article.php?article_id=247" class="external text" title="http://www.anti-state.com/article.php?article_id=247" rel="nofollow">A Refutation of John Searle's "Chinese Room Argument", by Bob Murphy</a></li>
<li>Nils Nilsson, "A Short Rebuttal to Searle", November 1984</li>
<li><a href="http://www.cs.bc.edu/~kugel/Publications/Searle%206.pdf" class="external text" title="http://www.cs.bc.edu/~kugel/Publications/Searle%206.pdf" rel="nofollow">Peter Kugel, "The Chinese Room Is A Trick"</a>, critical paper based on the assumption that the CR cannot use its inputs (which are in Chinese) to change its program (which is in English).</li>
<li>Wolfram Schmied "Demolishing Searle's Chinese Room", arXiv:cs.AI/0403009</li>
<li>Margaret Boden, "Escaping from the Chinese room" Heil, pp. 253–266 (1988)</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 17496/1000000
Post-expand include size: 107686/2048000 bytes
Template argument size: 22831/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:6216-0!1!0!default!!en!2 and timestamp 20090404143727 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Chinese_room">http://en.wikipedia.org/wiki/Chinese_room</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Philosophy_of_mind" title="Category:Philosophy of mind">Philosophy of mind</a></span> | <span dir='ltr'><a href="/wiki/Category:Philosophical_arguments" title="Category:Philosophical arguments">Philosophical arguments</a></span> | <span dir='ltr'><a href="/wiki/Category:Thought_experiments" title="Category:Thought experiments">Thought experiments</a></span> | <span dir='ltr'><a href="/wiki/Category:Philosophy_of_artificial_intelligence" title="Category:Philosophy of artificial intelligence">Philosophy of artificial intelligence</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Chinese_room" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Chinese_room" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Chinese_room&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Chinese_room&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Chinese_room" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Chinese_room" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Chinese_room" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Chinese_room&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Chinese_room&amp;oldid=281549721" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Chinese_room&amp;id=281549721">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-cs"><a href="http://cs.wikipedia.org/wiki/Argument_%C4%8D%C3%ADnsk%C3%A9ho_pokoje">Česky</a></li>
				<li class="interwiki-da"><a href="http://da.wikipedia.org/wiki/Det_kinesiske_rum">Dansk</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Chinesisches_Zimmer">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Habitaci%C3%B3n_china">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Chambre_chinoise">Français</a></li>
				<li class="interwiki-gl"><a href="http://gl.wikipedia.org/wiki/Sala_chinesa">Galego</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EC%A4%91%EA%B5%AD%EC%96%B4_%EB%B0%A9">한국어</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Stanza_cinese">Italiano</a></li>
				<li class="interwiki-he"><a href="http://he.wikipedia.org/wiki/%D7%94%D7%97%D7%93%D7%A8_%D7%94%D7%A1%D7%99%D7%A0%D7%99">עברית</a></li>
				<li class="interwiki-lt"><a href="http://lt.wikipedia.org/wiki/Kin%C5%B3_kambarys">Lietuvių</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Chinese_kamer">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E4%B8%AD%E5%9B%BD%E8%AA%9E%E3%81%AE%E9%83%A8%E5%B1%8B">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Chi%C5%84ski_pok%C3%B3j">Polski</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/Quarto_Chin%C3%AAs">Português</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9A%D0%B8%D1%82%D0%B0%D0%B9%D1%81%D0%BA%D0%B0%D1%8F_%D0%BA%D0%BE%D0%BC%D0%BD%D0%B0%D1%82%D0%B0">Русский</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/Kiinalaisen_huoneen_argumentti">Suomi</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Det_kinesiska_rummet">Svenska</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 3 April 2009, at 18:52.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv211 in 3.177 secs. --></body></html>
