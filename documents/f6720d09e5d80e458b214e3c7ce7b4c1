<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Fisher information,Bernoulli trial,Cauchy-Schwarz inequality,Covariance matrix,Cramér-Rao inequality,Derivative,Differential geometry,Dimension,Expected value,Fisher information metric,Formation matrix" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Fisher_information&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Fisher_information&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Fisher information - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Fisher_information";
		var wgTitle = "Fisher information";
		var wgAction = "view";
		var wgArticleId = "598971";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 273602656;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Fisher_information skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Fisher information</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Information_theory" title="Information theory">information theory</a>, the <b>Fisher information</b> (denoted <img class="tex" alt="\mathcal{I}(\theta)" src="http://upload.wikimedia.org/math/d/6/d/d6de2d90bc72136af43b11b1c332b604.png" />) is the <a href="/wiki/Variance" title="Variance">variance</a> of the <a href="/wiki/Score_(statistics)" title="Score (statistics)">score</a>. It is named in honor of its inventor, the <a href="/wiki/Statistician" title="Statistician">statistician</a> <a href="/wiki/Ronald_Fisher" title="Ronald Fisher">R.A. Fisher</a>.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Definition"><span class="tocnumber">1</span> <span class="toctext">Definition</span></a>
<ul>
<li class="toclevel-2"><a href="#Informal_derivation"><span class="tocnumber">1.1</span> <span class="toctext">Informal derivation</span></a></li>
<li class="toclevel-2"><a href="#Single-parameter_Bernoulli_experiment"><span class="tocnumber">1.2</span> <span class="toctext">Single-parameter Bernoulli experiment</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Matrix_form"><span class="tocnumber">2</span> <span class="toctext">Matrix form</span></a>
<ul>
<li class="toclevel-2"><a href="#Orthogonal_parameters"><span class="tocnumber">2.1</span> <span class="toctext">Orthogonal parameters</span></a></li>
<li class="toclevel-2"><a href="#Multivariate_normal_distribution"><span class="tocnumber">2.2</span> <span class="toctext">Multivariate normal distribution</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Properties"><span class="tocnumber">3</span> <span class="toctext">Properties</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">4</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">5</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#Further_weblinks"><span class="tocnumber">7</span> <span class="toctext">Further weblinks</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Definition" id="Definition"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=1" title="Edit section: Definition">edit</a>]</span> <span class="mw-headline">Definition</span></h2>
<p>The Fisher information is a way of measuring the amount of <a href="/wiki/Information" title="Information">information</a> that an observable <a href="/wiki/Random_variable" title="Random variable">random variable</a> <i>X</i> carries about an unknown <a href="/wiki/Parameter" title="Parameter">parameter</a> θ upon which the <a href="/wiki/Likelihood_function" title="Likelihood function">likelihood function</a> of <span class="texhtml">θ</span>, <span class="texhtml"><i>L</i>(θ) = <i>f</i>(<i>X</i>;θ)</span>, depends. The likelihood function is the joint probability of the data, the <i>X</i>s, conditional on the value of θ, <i>as a function of θ</i>. Since the <a href="/wiki/Expected_value" title="Expected value">expectation</a> of the <a href="/wiki/Score_(statistics)" title="Score (statistics)">score</a> is zero, the <a href="/wiki/Variance" title="Variance">variance</a> is simply the second <a href="/wiki/Moment_(mathematics)" title="Moment (mathematics)">moment</a> of the score, the derivative of the <a href="/wiki/Natural_logarithm" title="Natural logarithm">log</a> of the <a href="/wiki/Likelihood_function" title="Likelihood function">likelihood function</a> with respect to θ. Hence the Fisher information can be written</p>
<dl>
<dd><img class="tex" alt="
\mathcal{I}(\theta)
=
\mathrm{E}
\left\{\left.
 \left[
  \frac{\partial}{\partial\theta} \ln f(X;\theta)
 \right]^2
\right|\theta\right\},
" src="http://upload.wikimedia.org/math/4/7/8/478ebe791630c75766e38428f91a854b.png" /></dd>
</dl>
<p>which implies <img class="tex" alt="0 \leq \mathcal{I}(\theta) &lt; \infty" src="http://upload.wikimedia.org/math/d/6/4/d64368115c4a0a46ab6be316305caccc.png" />. The Fisher information is thus the expectation of the squared score. A random variable carrying high Fisher information implies that the absolute value of the score is often high.</p>
<p>The Fisher information is not a function of a particular observation, as the random variable <i>X</i> has been averaged out. The concept of information is useful when comparing two methods of observing a given random process.</p>
<p>If <span class="texhtml">ln<i>f</i>(<i>x</i>;θ)</span> is twice differentiable with respect to <span class="texhtml">θ</span>, and if the regularity condition</p>
<dl>
<dd><img class="tex" alt="\int \frac{\partial^2}{\partial \theta^2} f(x&#160;; \theta ) \, dx  = 0" src="http://upload.wikimedia.org/math/9/8/4/98420c47ae8d64d41df72e66bf6eb691.png" /></dd>
</dl>
<p>holds, then the Fisher information may also be written as<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup></p>
<dl>
<dd><img class="tex" alt="
\mathcal{I}(\theta) = - \mathrm{E} \left[\left. \frac{\partial^2}{\partial\theta^2} \ln f(X;\theta)\right|\theta \right].
" src="http://upload.wikimedia.org/math/d/c/0/dc04d0a88108c766c33650f7cf745eb0.png" /></dd>
</dl>
<p>Thus Fisher information is the negative of the expectation of the second <a href="/wiki/Derivative" title="Derivative">derivative</a> of the <a href="/wiki/Natural_logarithm" title="Natural logarithm">log</a> of <i>f</i> with respect to θ. Information may thus be seen to be a measure of the "sharpness" of the <a href="/wiki/Support_curve" title="Support curve">support curve</a> near the <a href="/wiki/Maximum_likelihood" title="Maximum likelihood">maximum likelihood estimate</a> of θ. A "blunt" support curve (one with a shallow maximum) would have a low expected second derivative, and thus low information; while a sharp one would have a high expected second derivative and thus high information.</p>
<p>Information is additive, in that the information yielded by two <a href="/wiki/Statistical_independence" title="Statistical independence" class="mw-redirect">independent</a> experiments is the sum of the information from each experiment separately:</p>
<dl>
<dd><img class="tex" alt=" \mathcal{I}_{X,Y}(\theta) = \mathcal{I}_X(\theta) + \mathcal{I}_Y(\theta). " src="http://upload.wikimedia.org/math/e/1/2/e123ad7e348676e5920c11001fe56abe.png" /></dd>
</dl>
<p>This result follows from the elementary fact that if random variables are independent, the variance of their sum is the sum of their variances. Hence the information in a random sample of size <i>n</i> is <i>n</i> times that in a sample of size 1 (if observations are independent).</p>
<p>The information provided by a <a href="/wiki/Sufficiency_(statistics)" title="Sufficiency (statistics)">sufficient statistic</a> is the same as that of the sample <i>X</i>. This may be seen by using <a href="/wiki/Sufficient_statistic#Fisher-Neyman.27s_factorization_theorem" title="Sufficient statistic" class="mw-redirect">Neyman's factorization criterion</a> for a sufficient statistic. If <span class="texhtml"><i>T</i>(<i>X</i>)</span> is sufficient for θ, then</p>
<dl>
<dd><img class="tex" alt=" f(X;\theta) = g(T(X), \theta) h(X) \!" src="http://upload.wikimedia.org/math/f/1/9/f19677f60a9b88afbf0ddc45f474ca2a.png" /></dd>
</dl>
<p>for some functions <i>g</i> and <i>h</i>. See <a href="/wiki/Sufficiency_(statistics)" title="Sufficiency (statistics)">sufficient statistic</a> for a more detailed explanation. The equality of information then follows from the following fact:</p>
<dl>
<dd><img class="tex" alt=" \frac{\partial}{\partial\theta} \ln \left[f(X  &#160;;\theta)\right]
= \frac{\partial}{\partial\theta} \ln \left[g(T(X);\theta)\right] " src="http://upload.wikimedia.org/math/4/1/6/4168cce91885a68c08aa7bdd2f43705a.png" /></dd>
</dl>
<p>which follows from the definition of Fisher information, and the independence of <span class="texhtml"><i>h</i>(<i>X</i>)</span> from θ. More generally, if <span class="texhtml"><i>T</i> = <i>t</i>(<i>X</i>)</span> is a <a href="/wiki/Statistic" title="Statistic">statistic</a>, then</p>
<dl>
<dd><img class="tex" alt="
\mathcal{I}_T(\theta)
\leq
\mathcal{I}_X(\theta)
" src="http://upload.wikimedia.org/math/7/b/0/7b040de9b3cf2d3add4a43e24e563be4.png" /></dd>
</dl>
<p>with equality <a href="/wiki/If_and_only_if" title="If and only if">if and only if</a> <i>T</i> is a <a href="/wiki/Sufficient_statistic" title="Sufficient statistic" class="mw-redirect">sufficient statistic</a>.</p>
<p>The <a href="/wiki/Cram%C3%A9r-Rao_inequality" title="Cramér-Rao inequality" class="mw-redirect">Cramér-Rao inequality</a> states that the inverse of the Fisher information is a lower bound on the variance of any <a href="/wiki/Unbiased_estimator" title="Unbiased estimator" class="mw-redirect">unbiased estimator</a> of θ.</p>
<p><a name="Informal_derivation" id="Informal_derivation"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=2" title="Edit section: Informal derivation">edit</a>]</span> <span class="mw-headline">Informal derivation</span></h3>
<p>Van Trees (1968) and Frieden (2004) provide the following method of deriving the Fisher information informally:</p>
<p>Consider an <a href="/wiki/Unbiased_estimator" title="Unbiased estimator" class="mw-redirect">unbiased estimator</a> <img class="tex" alt="\hat\theta(X)" src="http://upload.wikimedia.org/math/e/e/7/ee7ebfbea0f1dc3e05441142063c90b3.png" />. Mathematically, we write</p>
<dl>
<dd><img class="tex" alt="
\mathrm{E}\left[ \hat\theta(X) - \theta \right]
= \int \left[ \hat\theta(X) - \theta \right] \cdot f(X&#160;;\theta) \, dx = 0.
" src="http://upload.wikimedia.org/math/8/f/a/8facce62de3a85a9ff3005aebc6a58b8.png" /></dd>
</dl>
<p>The likelihood function <span class="texhtml"><i>f</i>(<i>X</i>;θ)</span> describes the probability that we observe a given sample <span class="texhtml"><i>x</i></span> <i>given</i> a known value of <span class="texhtml">θ</span>. If <span class="texhtml"><i>f</i></span> is sharply peaked, it is easy to intuit the "correct" value of <span class="texhtml">θ</span> given the data, and hence the data contains a lot of information about the parameter. If the likelihood <span class="texhtml"><i>f</i></span> is flat and spread-out, then it would take many, many samples of <span class="texhtml"><i>X</i></span> to estimate the actual "true" value of <span class="texhtml">θ</span>. Therefore, we would intuit that the data contain much less information about the parameter.</p>
<p>Now, we differentiate the unbiased-ness condition above to get</p>
<dl>
<dd><img class="tex" alt="
\frac{\partial}{\partial\theta} \int \left[ \hat\theta(X) - \theta \right] \cdot f(X&#160;;\theta) \, dx
= \int \left(\hat\theta-\theta\right) \frac{\partial f}{\partial\theta} \, dx - \int f \, dx = 0.
" src="http://upload.wikimedia.org/math/7/2/7/7277f132311b710ecc437c9060da3f0a.png" /></dd>
</dl>
<p>We now make use of two facts. The first is that the likelihood <span class="texhtml"><i>f</i></span> is just the probability of the data given the parameter. Since it is a probability, it must be normalized, implying that</p>
<dl>
<dd><img class="tex" alt="\int f \, dx = 1." src="http://upload.wikimedia.org/math/8/b/a/8baf1d18629dcf136ceaa56429827f9c.png" /></dd>
</dl>
<p>Second, we know from basic calculus that</p>
<dl>
<dd><img class="tex" alt="\frac{\partial f}{\partial\theta} = f \, \frac{\partial \ln f}{\partial\theta}" src="http://upload.wikimedia.org/math/7/1/a/71a61fc91cd7f885d2e7730f4cae1207.png" />.</dd>
</dl>
<p>Using these two facts in the above let us write</p>
<dl>
<dd><img class="tex" alt="
\int \left(\hat\theta-\theta\right) f \, \frac{\partial \ln f}{\partial\theta} \, dx = 1.
" src="http://upload.wikimedia.org/math/7/c/f/7cfe332f0d224d8f78e2fe08de76aa75.png" /></dd>
</dl>
<p>Factoring the integrand gives</p>
<dl>
<dd><img class="tex" alt="
\int \left(\left(\hat\theta-\theta\right) \sqrt{f} \right) \left( \sqrt{f} \, \frac{\partial \ln f}{\partial\theta} \right) \, dx = 1.
" src="http://upload.wikimedia.org/math/f/e/9/fe988b59c455777f5e03dab5f952da5f.png" /></dd>
</dl>
<p>If we square the equation, the <a href="/wiki/Cauchy-Schwarz_inequality" title="Cauchy-Schwarz inequality" class="mw-redirect">Cauchy-Schwarz inequality</a> lets us write</p>
<dl>
<dd><img class="tex" alt="
\left[ \int \left(\hat\theta - \theta\right)^2 f \, dx \right] \cdot \left[ \int \left( \frac{\partial \ln f}{\partial\theta} \right)^2 f \, dx \right] \geq 1.
" src="http://upload.wikimedia.org/math/b/f/c/bfc3e6b9897a565b103ad4c0c95ce153.png" /></dd>
</dl>
<p>The right-most factor is defined to be the Fisher Information</p>
<dl>
<dd><img class="tex" alt="
\mathcal{I}\left(\theta\right) = \int \left( \frac{\partial \ln f}{\partial\theta} \right)^2 f \, dx.
" src="http://upload.wikimedia.org/math/1/8/e/18e82effb73ef5f77f1ffdc6f4e2d917.png" /></dd>
</dl>
<p>The left-most factor is the expected mean-squared error of the estimator <span class="texhtml">θ</span>, since</p>
<dl>
<dd><img class="tex" alt="
\mathrm{E}\left[ \left( \hat\theta\left(X\right) - \theta \right)^2 \right] = \int \left(\hat\theta - \theta\right)^2 f \, dx.
" src="http://upload.wikimedia.org/math/e/4/9/e49e6d571f9d891e8170ee3ec48d5a64.png" /></dd>
</dl>
<p>Notice that the inequality tells us that, fundamentally,</p>
<dl>
<dd><img class="tex" alt="
\mbox{Var}\left[\hat\theta\right] \, \geq \, {1} / {\mathcal{I}\left(\theta\right)}.
" src="http://upload.wikimedia.org/math/2/0/2/20252bf0fa7f4af8b84a18f6d5255cc9.png" /></dd>
</dl>
<p>In other words, the precision to which we can estimate <span class="texhtml">θ</span> is fundamentally limited by the Fisher Information of likelihood function.</p>
<p><a name="Single-parameter_Bernoulli_experiment" id="Single-parameter_Bernoulli_experiment"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=3" title="Edit section: Single-parameter Bernoulli experiment">edit</a>]</span> <span class="mw-headline">Single-parameter Bernoulli experiment</span></h3>
<p>A <a href="/wiki/Bernoulli_trial" title="Bernoulli trial">Bernoulli trial</a> is a random variable with two possible outcomes, "success" and "failure", with "success" having a probability of <span class="texhtml">θ</span>. The outcome can be thought of as determined by a coin toss, with the probability of obtaining a "head" being <span class="texhtml">θ</span> and the probability of obtaining a "tail" being <span class="texhtml">1 − θ</span>.</p>
<p>The Fisher information contained in <i>n</i> independent <a href="/wiki/Bernoulli_trial" title="Bernoulli trial">Bernoulli trials</a> may be calculated as follows. In the following, <i>A</i> represents the number of successes, <i>B</i> the number of failures, and <span class="texhtml"><i>n</i> = <i>A</i> + <i>B</i></span> is the total number of trials.</p>
<dl>
<dd><img class="tex" alt="
\mathcal{I}(\theta)
=
-\mathrm{E}
\left[
 \frac{\partial^2}{\partial\theta^2} \ln(f(A;\theta))
\right] \qquad (1)
" src="http://upload.wikimedia.org/math/0/1/3/013e101234340d49bc784337b7b2263f.png" /></dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="
=
-\mathrm{E}
\left[
 \frac{\partial^2}{\partial\theta^2} \ln
 \left[
  \theta^A(1-\theta)^B\frac{(A+B)!}{A!B!}
 \right]
\right] \qquad (2)
" src="http://upload.wikimedia.org/math/5/c/3/5c3c2854672be5f876ca59b0a87c9f96.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="
=
-\mathrm{E}
\left[
 \frac{\partial^2}{\partial\theta^2} 
 \left[
  A \ln (\theta) + B \ln(1-\theta)
 \right]
\right] \qquad (3)
" src="http://upload.wikimedia.org/math/0/2/b/02b0541978a5b89451376f62adadf0c0.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="
=
-\mathrm{E}
\left[
 \frac{\partial}{\partial\theta}
 \left[
  \frac{A}{\theta} - \frac{B}{1-\theta}
 \right]
\right]
" src="http://upload.wikimedia.org/math/8/5/4/854179f57a6134bc1616e689d48f75dc.png" /> (on differentiating ln&#160;<i>x</i>, see <a href="/wiki/Logarithm" title="Logarithm">logarithm</a>) <img class="tex" alt="\qquad (4)" src="http://upload.wikimedia.org/math/0/1/7/017727dcae545b62dd2f14aa28682f72.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="
=
+\mathrm{E}
\left[
 \frac{A}{\theta^2} + \frac{B}{(1-\theta)^2}
\right] \qquad (5)
" src="http://upload.wikimedia.org/math/4/f/b/4fb8c1231df29e42e25d9e9d930a33a3.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="
=
\frac{n\theta}{\theta^2} + \frac{n(1-\theta)}{(1-\theta)^2}
" src="http://upload.wikimedia.org/math/7/0/3/70348f6e3e9aa59608a197df83edf8d8.png" /> (as the expected value of <span class="texhtml"><i>A</i> = <i>n</i>θ</span>, etc.) <img class="tex" alt="\qquad (6)" src="http://upload.wikimedia.org/math/1/0/e/10ecde953b6e8a2d246aea1e4dc5fe40.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="= \frac{n}{\theta(1-\theta)} \qquad (7)" src="http://upload.wikimedia.org/math/6/0/b/60b0a2f63153369c959f3db028ca9ebe.png" /></dd>
</dl>
</dd>
</dl>
<p>(1) defines Fisher information. (2) invokes the fact that the information in a <a href="/wiki/Sufficient_statistic" title="Sufficient statistic" class="mw-redirect">sufficient statistic</a> is the same as that of the sample itself. (3) expands the <a href="/wiki/Logarithm" title="Logarithm">log</a> term and drops a constant. (4) and (5) differentiate with respect to <span class="texhtml">θ</span>. (6) replaces <i>A</i> and <i>B</i> with their expectations. (7) is algebra.</p>
<p>The end result, namely,</p>
<dl>
<dd><img class="tex" alt="\mathcal{I}(\theta) = \frac{n}{\theta(1-\theta)}," src="http://upload.wikimedia.org/math/2/b/4/2b447372827ddd2c093911301c237c2e.png" /></dd>
</dl>
<p>is the reciprocal of the <a href="/wiki/Variance" title="Variance">variance</a> of the mean number of successes in <i>n</i> <a href="/wiki/Bernoulli_trial" title="Bernoulli trial">Bernoulli trials</a>, as expected (see last sentence of the preceding section).</p>
<p><a name="Matrix_form" id="Matrix_form"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=4" title="Edit section: Matrix form">edit</a>]</span> <span class="mw-headline">Matrix form</span></h2>
<p>When there are <i>N</i> parameters, so that θ is a <i>N</i>x1 <a href="/wiki/Vector_(geometric)" title="Vector (geometric)" class="mw-redirect">vector</a> <img class="tex" alt="\theta = \begin{bmatrix}
 \theta_{1}, \theta_{2}, \dots , \theta_{N} \end{bmatrix}," src="http://upload.wikimedia.org/math/8/f/6/8f654c6936d6b8f696c1acb61b70e9f0.png" /> then the Fisher information takes the form of an <i>N</i>x<i>N</i> <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a>, the Fisher Information Matrix (FIM), with typical element:</p>
<dl>
<dd><img class="tex" alt="
{\left(\mathcal{I} \left(\theta \right) \right)}_{i, j}
=
\mathrm{E}
\left[\left.
 \frac{\partial}{\partial\theta_i} \ln f(X;\theta)
 \frac{\partial}{\partial\theta_j} \ln f(X;\theta)
\right|\theta\right].
" src="http://upload.wikimedia.org/math/8/c/1/8c1bb67c125f639f91a4c2a07c319e80.png" /></dd>
</dl>
<p>The FIM is a <i>N</i>x<i>N</i> <a href="/wiki/Positive_definite_matrix" title="Positive definite matrix" class="mw-redirect">positive definite</a> <a href="/wiki/Symmetric_matrix" title="Symmetric matrix">symmetric matrix</a>, defining a <a href="/wiki/Fisher_information_metric" title="Fisher information metric">metric</a> on the <i>N</i>-<a href="/wiki/Dimension" title="Dimension">dimensional</a> <a href="/wiki/Parameter_space" title="Parameter space">parameter space</a>. Exploring this topic requires <a href="/wiki/Differential_geometry" title="Differential geometry">differential geometry</a>.</p>
<p>If the following regularity condition is met:</p>
<dl>
<dd><img class="tex" alt="\frac{\partial^2}{\partial \theta_i \partial \theta_j} \int f(X&#160;; \theta ) \, dx  = 0," src="http://upload.wikimedia.org/math/1/2/b/12b9e2443685f2e38c2b5faea8d5d86c.png" /></dd>
</dl>
<p>then the Fisher Information Matrix may also be written as:</p>
<dl>
<dd><img class="tex" alt="
{\left(\mathcal{I} \left(\theta \right) \right)}_{i, j}
=
- \mathrm{E}
\left[\left.
 \frac{\partial^2}{\partial\theta_i \partial\theta_j} \ln f(X;\theta)
\right|\theta\right].
" src="http://upload.wikimedia.org/math/3/4/7/3474df7a2e3e27be26f729825253952d.png" /></dd>
</dl>
<p><a name="Orthogonal_parameters" id="Orthogonal_parameters"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=5" title="Edit section: Orthogonal parameters">edit</a>]</span> <span class="mw-headline">Orthogonal parameters</span></h3>
<p>We say that two parameters <span class="texhtml">θ<sub><i>i</i></sub></span> and <span class="texhtml">θ<sub><i>j</i></sub></span> are orthogonal if the element of the i-th row and j-th column of the Fisher Information Matrix is zero. Orthogonal parameters are easy to deal with in the sense that their <a href="/wiki/Maximum_likelihood" title="Maximum likelihood">maximum likelihood estimates</a> are independent and can be calculated separately. When dealing with research problems, it is very common for the researcher to invest some time searching for an orthogonal parametrization of the densities involved in the problem.</p>
<p><a name="Multivariate_normal_distribution" id="Multivariate_normal_distribution"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=6" title="Edit section: Multivariate normal distribution">edit</a>]</span> <span class="mw-headline">Multivariate normal distribution</span></h3>
<p>The FIM for a <i>N</i>-variate <a href="/wiki/Multivariate_normal_distribution" title="Multivariate normal distribution">multivariate normal distribution</a> has a special form. Let <img class="tex" alt="\mu(\theta) = \begin{bmatrix}
 \mu_{1}(\theta), \mu_{2}(\theta), \dots , \mu_{N}(\theta) \end{bmatrix}^\top," src="http://upload.wikimedia.org/math/3/6/e/36ec536b7c417170c00bce71befecdec.png" /> and let <span class="texhtml">Σ(θ)</span> be the <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a>. Then the typical element <img class="tex" alt="\mathcal{I}_{m,n}" src="http://upload.wikimedia.org/math/f/1/7/f17b4e62f20870691cf6c350ca0eabee.png" />, 0 ≤ <i>m</i>, <i>n</i> &lt; <i>N</i>, of the FIM for <img class="tex" alt="X \sim N(\mu(\theta), \Sigma(\theta))" src="http://upload.wikimedia.org/math/d/a/6/da6955f875a0c73f6b4fa02fa49d1190.png" /> is:</p>
<dl>
<dd><img class="tex" alt="
\mathcal{I}_{m,n}
=
\frac{\partial \mu^\top}{\partial \theta_m}
\Sigma^{-1}
\frac{\partial \mu}{\partial \theta_n}
+
\frac{1}{2}
\mathrm{tr}
\left(
 \Sigma^{-1}
 \frac{\partial \Sigma}{\partial \theta_m}
 \Sigma^{-1}
 \frac{\partial \Sigma}{\partial \theta_n}
\right),
" src="http://upload.wikimedia.org/math/c/3/5/c35b79cbee78d8982865ad2749017635.png" /></dd>
</dl>
<p>where <img class="tex" alt="(..)^\top" src="http://upload.wikimedia.org/math/7/3/3/733ec15a51ac92e9951c68974f03c8ce.png" /> denotes the <a href="/wiki/Transpose" title="Transpose">transpose</a> of a vector, <span class="texhtml">tr(..)</span> denotes the <a href="/wiki/Trace_(matrix)" title="Trace (matrix)" class="mw-redirect">trace</a> of a <a href="/wiki/Square_matrix" title="Square matrix" class="mw-redirect">square matrix</a>, and:</p>
<ul>
<li><img class="tex" alt="
\frac{\partial \mu}{\partial \theta_m}
=
\begin{bmatrix}
 \frac{\partial \mu_1}{\partial \theta_m} &amp;
 \frac{\partial \mu_2}{\partial \theta_m} &amp;
 \cdots &amp;
 \frac{\partial \mu_N}{\partial \theta_m} &amp;
\end{bmatrix}^\top;
" src="http://upload.wikimedia.org/math/f/8/9/f8907247e4d7ed3250f3ad62865d8302.png" /></li>
</ul>
<ul>
<li><img class="tex" alt="
\frac{\partial \Sigma}{\partial \theta_m}
=
\begin{bmatrix}
 \frac{\partial \Sigma_{1,1}}{\partial \theta_m} &amp;
 \frac{\partial \Sigma_{1,2}}{\partial \theta_m} &amp;
 \cdots &amp;
 \frac{\partial \Sigma_{1,N}}{\partial \theta_m} \\  \\
 \frac{\partial \Sigma_{2,1}}{\partial \theta_m} &amp;
 \frac{\partial \Sigma_{2,2}}{\partial \theta_m} &amp;
 \cdots &amp;
 \frac{\partial \Sigma_{2,N}}{\partial \theta_m} \\  \\
 \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\  \\
 \frac{\partial \Sigma_{N,1}}{\partial \theta_m} &amp;
 \frac{\partial \Sigma_{N,2}}{\partial \theta_m} &amp;
 \cdots &amp;
 \frac{\partial \Sigma_{N,N}}{\partial \theta_m}
\end{bmatrix}.
" src="http://upload.wikimedia.org/math/b/9/5/b9508fabf169fe74f143af61a5b2fa33.png" /></li>
</ul>
<p>Note that a special, but very common case is the one where <span class="texhtml">Σ(θ) = Σ</span>, a constant. Then</p>
<p><img class="tex" alt="
\mathcal{I}_{m,n}
=
\frac{\partial \mu^\top}{\partial \theta_m}
\Sigma^{-1}
\frac{\partial \mu}{\partial \theta_n}
" src="http://upload.wikimedia.org/math/9/1/0/910620fd9e6e6ef88b454f5fd665d59e.png" /> .</p>
<p><br />
In this case the Fisher information matrix may be identified with the coefficient matrix of the normal equations of <a href="/wiki/Least_squares" title="Least squares">least squares</a> estimation theory.</p>
<p><a name="Properties" id="Properties"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=7" title="Edit section: Properties">edit</a>]</span> <span class="mw-headline">Properties</span></h2>
<p>The Fisher information depends on the parametrization of the problem. If θ and η are two different parameterizations of a problem, such that <span class="texhtml">θ = <i>h</i>(η)</span> and <i>h</i> is a differentiable function, then</p>
<dl>
<dd><img class="tex" alt="{\mathcal I}_\eta(\eta) = {\mathcal I}_\theta(h(\eta)) \left( h'(\eta) \right)^2" src="http://upload.wikimedia.org/math/d/2/9/d29b062034c4d2a3095368c4749e5c99.png" /></dd>
</dl>
<p>where <img class="tex" alt="{\mathcal I}_\eta" src="http://upload.wikimedia.org/math/6/e/7/6e719404abe83e187ade08e344597076.png" /> and <img class="tex" alt="{\mathcal I}_\theta" src="http://upload.wikimedia.org/math/d/b/b/dbb773ef071d96caa29aec47a8c98236.png" /> are the Fisher information measures of η and θ, respectively.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=8" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Formation_matrix" title="Formation matrix">Formation matrix</a></li>
<li><a href="/wiki/Jeffreys_prior" title="Jeffreys prior">Jeffreys prior</a></li>
</ul>
<p>Other measures employed in <a href="/wiki/Information_theory" title="Information theory">information theory</a>:</p>
<ul>
<li><a href="/wiki/Self-information" title="Self-information">Self-information</a></li>
<li><a href="/wiki/Kullback-Leibler_divergence" title="Kullback-Leibler divergence" class="mw-redirect">Kullback-Leibler divergence</a></li>
<li><a href="/wiki/Shannon_entropy" title="Shannon entropy" class="mw-redirect">Shannon entropy</a></li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=9" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> Lehmann and Casella, eq. (2.5.16).</li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> Lehmann and Casella, eq. (2.5.11).</li>
</ol>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=10" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFSchervish1995">Schervish, Mark J. (1995). <i>Theory of Statistics</i>. New York: Springer. Section 2.3.1. <a href="/wiki/Special:BookSources/0387945466" class="internal">ISBN 0387945466</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Theory+of+Statistics&amp;rft.aulast=Schervish&amp;rft.aufirst=Mark+J.&amp;rft.au=Schervish%2C+Mark+J.&amp;rft.date=1995&amp;rft.pages=Section+2.3.1&amp;rft.place=New+York&amp;rft.pub=Springer&amp;rft.isbn=0387945466&amp;rfr_id=info:sid/en.wikipedia.org:Fisher_information"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFVan_Trees1968">Van Trees, H. L. (1968). <i>Detection, Estimation, and Modulation Theory, Part I</i>. New York: Wiley. <a href="/wiki/Special:BookSources/0471095176" class="internal">ISBN 0471095176</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Detection%2C+Estimation%2C+and+Modulation+Theory%2C+Part+I&amp;rft.aulast=Van+Trees&amp;rft.aufirst=H.+L.&amp;rft.au=Van+Trees%2C+H.+L.&amp;rft.date=1968&amp;rft.place=New+York&amp;rft.pub=Wiley&amp;rft.isbn=0471095176&amp;rfr_id=info:sid/en.wikipedia.org:Fisher_information"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFLehmannCasella.2C_G.1998">Lehmann, E. L.; Casella, G. (1998). <i>Theory of Point Estimation</i>. Springer. pp.&#160;2nd ed. <a href="/wiki/Special:BookSources/0387985026" class="internal">ISBN 0-387-98502-6</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Theory+of+Point+Estimation&amp;rft.aulast=Lehmann&amp;rft.aufirst=E.+L.&amp;rft.au=Lehmann%2C+E.+L.&amp;rft.au=Casella%2C+G.&amp;rft.date=1998&amp;rft.pages=pp.%26nbsp%3B2nd+ed&amp;rft.pub=Springer&amp;rft.isbn=0-387-98502-6&amp;rfr_id=info:sid/en.wikipedia.org:Fisher_information"><span style="display: none;">&#160;</span></span></li>
</ul>
<p><a name="Further_weblinks" id="Further_weblinks"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Fisher_information&amp;action=edit&amp;section=11" title="Edit section: Further weblinks">edit</a>]</span> <span class="mw-headline">Further weblinks</span></h2>
<ul>
<li><a href="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=20008&amp;objectType=File" class="external text" title="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=20008&amp;objectType=File" rel="nofollow">Fisher4Cast: a Matlab, GUI-based Fisher information tool</a> for research and teaching, primarily aimed at cosmological forecasting applications.</li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 1763/1000000
Post-expand include size: 7206/2048000 bytes
Template argument size: 1669/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:598971-0!1!0!default!!en!2 and timestamp 20090327103200 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Fisher_information">http://en.wikipedia.org/wiki/Fisher_information</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Estimation_theory" title="Category:Estimation theory">Estimation theory</a></span> | <span dir='ltr'><a href="/wiki/Category:Information_theory" title="Category:Information theory">Information theory</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Fisher_information" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Fisher_information" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Fisher_information&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Fisher_information&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Fisher_information" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Fisher_information" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Fisher_information" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Fisher_information&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Fisher_information&amp;oldid=273602656" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Fisher_information&amp;id=273602656">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Fisher-Information">Deutsch</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Information_de_Fisher">Français</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Informazione_di_Fisher">Italiano</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A3%E3%83%83%E3%82%B7%E3%83%A3%E3%83%BC%E6%83%85%E5%A0%B1%E9%87%8F">日本語</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%8F_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0">Русский</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 27 February 2009, at 06:42.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv189 in 0.055 secs. --></body></html>
