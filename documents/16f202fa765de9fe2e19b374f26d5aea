<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Doomsday argument,Articles with disputed statements from March 2009,Doomsday argument,1945,1949,1983,1992,1993,2000,2002,2026" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Doomsday_argument&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Doomsday_argument&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Doomsday argument - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Doomsday_argument";
		var wgTitle = "Doomsday argument";
		var wgAction = "view";
		var wgArticleId = "574311";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282013108;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Doomsday_argument skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Doomsday argument</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<div class="thumb tright">
<div class="thumbinner" style="width:352px;"><a href="/wiki/File:Population_curve.svg" class="image" title="World population from 10,000 BC to AD 2000"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Population_curve.svg/350px-Population_curve.svg.png" width="350" height="175" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Population_curve.svg" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
World population from <a href="/wiki/Prehistoric_times" title="Prehistoric times" class="mw-redirect">10,000 BC</a> to <a href="/wiki/2000" title="2000">AD 2000</a></div>
</div>
</div>
<p>The <b>Doomsday argument</b> (<b>DA</b>) is a <a href="/wiki/Probabilistic_argument" title="Probabilistic argument">probabilistic argument</a> that claims to <a href="/wiki/Predict" title="Predict" class="mw-redirect">predict</a> the <a href="/wiki/Future" title="Future">future</a> lifetime of the <a href="/wiki/Human_species" title="Human species" class="mw-redirect">human species</a> given only an estimate of the total number of humans born so far. Simply put, it says that supposing the humans alive today are in a random place in the whole human history timeline, chances are we are about halfway through it.</p>
<p>It was first proposed in an explicit way by the astrophysicist <a href="/wiki/Brandon_Carter" title="Brandon Carter">Brandon Carter</a> in <a href="/wiki/1983" title="1983">1983</a>,<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup> from which it is sometimes called the <b>Carter catastrophe</b>; the argument was subsequently championed by the <a href="/wiki/Philosopher" title="Philosopher" class="mw-redirect">philosopher</a> <a href="/wiki/John_A._Leslie" title="John A. Leslie">John A. Leslie</a> and has since been independently discovered by <a href="/wiki/J._Richard_Gott" title="J. Richard Gott">J. Richard Gott</a><sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup> and <a href="/wiki/Holger_Bech_Nielsen" title="Holger Bech Nielsen">Holger Bech Nielsen</a>.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2" title=""><span>[</span>3<span>]</span></a></sup> Similar principles of <a href="/wiki/Eschatology" title="Eschatology">eschatology</a> were proposed earlier by <a href="/wiki/Heinz_von_Foerster" title="Heinz von Foerster">Heinz von Foerster</a>, among others.</p>
<p>Denoting by <i>N</i> the total number of humans who were ever or will ever be born, the <a href="/wiki/Copernican_principle" title="Copernican principle">Copernican principle</a> suggests that we are equally likely (along with the other <i>N</i>&#160;−&#160;1 humans) to find ourselves at any position <i>n</i>, so we assume that our fractional position <i>f</i>&#160;=&#160;<i>n</i>/<i>N</i> is <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniformly distributed</a> on the <a href="/wiki/Interval_(mathematics)" title="Interval (mathematics)">interval</a> (0,&#160;1] <a href="/wiki/Prior_probability" title="Prior probability">prior</a> to learning our absolute position.</p>
<p>Let us further assume that our fractional position <i>f</i> is uniformly distributed on (0,&#160;1] even after we learn of our absolute position <i>n</i>. (This is equivalent to the assumption that we have no prior information about the total number of humans, <i>N</i>.) That is, for example, there is 95% chance that <i>f</i> is in the interval (0.05,&#160;1], that is <i>f</i>&#160;&gt;&#160;0.05. In other words we could assume that we could be 95% certain that we would be within the last 95% of all the humans ever to be born. If we know our absolute position <i>n</i>, this implies<sup class="noprint Inline-Template"><span title="The material in the vicinity of this tag may not be factual or accurate&#160;from March 2009" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Disputed_statement" title="Wikipedia:Disputed statement">dubious</a> <span class="metadata">– <a href="/wiki/Talk:Doomsday_argument#The_article_is_misleading.2C_and_the_argument_itself_is_absurd" title="Talk:Doomsday argument">discuss</a></span></i>]</span></sup> an upper bound for <i>N</i> obtained by rearranging <i>n</i>/<i>N</i>&#160;&gt;&#160;0.05 to give <i>N</i>&#160;&lt;&#160;20<i>n</i>.</p>
<p>If we take that 60 billion humans have been born so far (Leslie's figure), then we can estimate that there is a 95% chance that the total number of humans <i>N</i> will be less than 20&#160;×&#160;60 billion&#160;=&#160;1.2 trillion. Assuming that the <a href="/wiki/World_population" title="World population">world population</a> stabilizes <a href="/wiki/World_Population#Forecast_of_world_population" title="World Population" class="mw-redirect">at 10 billion</a> and a <a href="/wiki/Life_expectancy" title="Life expectancy">life expectancy</a> of <a href="/wiki/Longevity#Future" title="Longevity">80 years</a>, it can be estimated that the remaining 1140 billion humans will be born in 9120 years. Depending on the projection of world population in the forthcoming centuries, estimates may vary, but the main point of the argument is that it is unlikely that more than 1.2 trillion humans will ever live.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Remarks"><span class="tocnumber">1</span> <span class="toctext">Remarks</span></a></li>
<li class="toclevel-1"><a href="#Simplification:_two_possible_total_number_of_humans"><span class="tocnumber">2</span> <span class="toctext">Simplification: two possible total number of humans</span></a></li>
<li class="toclevel-1"><a href="#What_the_argument_is_not"><span class="tocnumber">3</span> <span class="toctext">What the argument is not</span></a></li>
<li class="toclevel-1"><a href="#Variations"><span class="tocnumber">4</span> <span class="toctext">Variations</span></a>
<ul>
<li class="toclevel-2"><a href="#Gott.27s_formulation:_.27vague_prior.27_total_population"><span class="tocnumber">4.1</span> <span class="toctext">Gott's formulation: 'vague prior' total population</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Reference_classes"><span class="tocnumber">5</span> <span class="toctext">Reference classes</span></a>
<ul>
<li class="toclevel-2"><a href="#Sampling_only_WMD-era_humans"><span class="tocnumber">5.1</span> <span class="toctext">Sampling only WMD-era humans</span></a></li>
<li class="toclevel-2"><a href="#SSSA:_Sampling_from_observer-moments"><span class="tocnumber">5.2</span> <span class="toctext">SSSA: Sampling from observer-moments</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Rebuttals"><span class="tocnumber">6</span> <span class="toctext">Rebuttals</span></a>
<ul>
<li class="toclevel-2"><a href="#We_are_in_the_earliest_5.25.2C_a_priori"><span class="tocnumber">6.1</span> <span class="toctext">We are in the earliest 5%, a priori</span></a></li>
<li class="toclevel-2"><a href="#Critique:_Human_extinction_is_distant.2C_a_posteriori"><span class="tocnumber">6.2</span> <span class="toctext">Critique: Human extinction is distant, a posteriori</span></a></li>
<li class="toclevel-2"><a href="#The_prior_N_distribution_may_make_n_very_uninformative"><span class="tocnumber">6.3</span> <span class="toctext">The prior N distribution may make n very uninformative</span></a></li>
<li class="toclevel-2"><a href="#Infinite_Expectation"><span class="tocnumber">6.4</span> <span class="toctext">Infinite Expectation</span></a></li>
<li class="toclevel-2"><a href="#SIA:_The_possibility_of_not_existing_at_all"><span class="tocnumber">6.5</span> <span class="toctext">SIA: The possibility of not existing at all</span></a></li>
<li class="toclevel-2"><a href="#Many_worlds"><span class="tocnumber">6.6</span> <span class="toctext">Many worlds</span></a></li>
<li class="toclevel-2"><a href="#Caves.27_rebuttal"><span class="tocnumber">6.7</span> <span class="toctext">Caves' rebuttal</span></a></li>
<li class="toclevel-2"><a href="#Self-referencing_doomsday_argument_rebuttal"><span class="tocnumber">6.8</span> <span class="toctext">Self-referencing doomsday argument rebuttal</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Mathematics-free_explanation_by_analogy"><span class="tocnumber">7</span> <span class="toctext">Mathematics-free explanation by analogy</span></a>
<ul>
<li class="toclevel-2"><a href="#Analogy_to_the_estimated_final_score_of_a_cricket_batsman"><span class="tocnumber">7.1</span> <span class="toctext">Analogy to the estimated final score of a cricket batsman</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">9</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">10</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Remarks" id="Remarks"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=1" title="Edit section: Remarks">edit</a>]</span> <span class="mw-headline">Remarks</span></h3>
<ul>
<li>The step that converts <i>N</i> into an extinction time depends upon a finite human lifespan. If <a href="/wiki/Immortality" title="Immortality">immortality</a> becomes common, and the birth rate drops to zero, <i>N</i> will never be reached.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup></li>
<li>The total number of humans born so far may depend on one's definition of "<a href="/wiki/Human" title="Human">human</a>".</li>
<li>A precise formulation of the Doomsday Argument requires the <a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian</a> interpretation of probability, which is widely, if not universally, accepted.</li>
<li>Even among Bayesians some of the assumptions of the argument's logic would not be acceptable; for instance, the fact that it is applied to a temporal phenomenon (how long something lasts) means that <i>N'</i>s distribution simultaneously represents an "<a href="/wiki/Aleatory_probability" title="Aleatory probability" class="mw-redirect">aleatory probability</a>" (as a future event), and an "<a href="/wiki/Epistemic_probability" title="Epistemic probability" class="mw-redirect">epistemic probability</a>" (as a decided value about which we are uncertain).</li>
<li>The <i>U</i>(0,1] <i>f</i> distribution is derived from two choices, which whilst being the default are also arbitrary:
<ul>
<li>The <a href="/wiki/Principle_of_indifference" title="Principle of indifference">principle of indifference</a>, so that it is as likely for any other randomly selected person to be born after you as before you.</li>
<li>The <i>assumption</i> of no 'prior' knowledge on the distribution of <i>N</i>.</li>
</ul>
</li>
</ul>
<p><a name="Simplification:_two_possible_total_number_of_humans" id="Simplification:_two_possible_total_number_of_humans"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=2" title="Edit section: Simplification: two possible total number of humans">edit</a>]</span> <span class="mw-headline">Simplification: two possible total number of humans</span></h3>
<p>Assume for simplicity that the total number of humans who will ever be born is 60 billion (<i>N</i><sub>1</sub>), or 6,000 billion (<i>N</i><sub>2</sub>).<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup> If there is no prior knowledge of the position that a currently living individual, <i>X</i>, has in the history of humanity, we may instead compute how many humans were born before <i>X</i>, and arrive at (say) 59,854,795,447, which would roughly place <i>X</i> amongst the first 60 billion humans who have ever lived.</p>
<p>Now, if we assume that the number of humans who will ever be born equals <i>N</i><sub>1</sub>, the probability that <i>X</i> is amongst the first 60 billion humans who have ever lived is of course 100%. However, if the number of humans who will ever be born equals <i>N</i><sub>2</sub>, then the probability that <i>X</i> is amongst the first 60 billion humans who have ever lived is only 1%. Since X is in fact amongst the first 60 billion humans who have ever lived, this means that the total number of humans who will ever be born is more likely to be much closer to 60 billion than to 6,000 billion. In essence the DA therefore suggests that <a href="/wiki/Human_extinction" title="Human extinction">human extinction</a> is more likely to occur sooner rather than later.</p>
<p>It is possible to sum the probabilities for each value of <i>N</i> and therefore to compute a statistical 'confidence limit' on <i>N</i>. For example, taking the numbers above, it is 99% certain that <i>N</i> is smaller than 6,000 billion.</p>
<p><a name="What_the_argument_is_not" id="What_the_argument_is_not"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=3" title="Edit section: What the argument is not">edit</a>]</span> <span class="mw-headline">What the argument is not</span></h3>
<p>The Doomsday argument (DA) does <i>not</i> say that humanity cannot or will not exist indefinitely. It does not put any upper limit on the number of humans that will ever exist, nor provide a date for when humanity will become <a href="/wiki/Extinct" title="Extinct" class="mw-redirect">extinct</a>.</p>
<p>An abbreviated form of the argument <i>does</i> make these claims, by confusing probability with certainty. However, the actual DA's conclusion is:</p>
<dl>
<dd>There is a 95% <i>chance</i> of extinction within 9120 years.</dd>
</dl>
<p>The DA gives a 5% chance that some humans will still be alive in about the year 11125. (These dates are based on the assumptions above; the precise numbers vary among specific <i>Doomsday arguments</i>.)</p>
<p><a name="Variations" id="Variations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=4" title="Edit section: Variations">edit</a>]</span> <span class="mw-headline">Variations</span></h2>
<p>This argument has generated a lively philosophical debate, and no consensus has yet emerged on its solution. The variants described below produce the DA by separate derivations.</p>
<p><a name="Gott.27s_formulation:_.27vague_prior.27_total_population" id="Gott.27s_formulation:_.27vague_prior.27_total_population"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=5" title="Edit section: Gott's formulation: 'vague prior' total population">edit</a>]</span> <span class="mw-headline">Gott's formulation: 'vague prior' total population</span></h3>
<p>Gott specifically proposes the functional form for the <a href="/wiki/Prior_probability" title="Prior probability">prior distribution</a> of the number of people who will ever be born (<i>N</i>). Gott's DA used the <a href="/wiki/Prior_probability#Uninformative_priors" title="Prior probability">vague prior distribution</a>:</p>
<dl>
<dd><img class="tex" alt="P(N) = \frac{k}{N}" src="http://upload.wikimedia.org/math/3/4/6/346cbbcb8f195e5b1a7e2a20dd36dfe7.png" />.</dd>
</dl>
<p>where</p>
<ul>
<li>P(N) is the probability prior to discovering <i>n</i>, the total number of humans who have <i>yet</i> been born.</li>
<li>The constant, <i>k</i>, is chosen to <a href="/wiki/Normalizing_constant" title="Normalizing constant">normalize</a> the sum of P(<i>N</i>). The value chosen isn't important here, just the functional form (this is an <a href="/wiki/Improper_prior" title="Improper prior" class="mw-redirect">improper prior</a>, so no value of <i>k</i> gives a valid distribution, but <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a> is still possible using it.)</li>
</ul>
<p>Since Gott specifies the <a href="/wiki/Prior_probability" title="Prior probability">prior</a> distribution of total humans, <i>P(N)</i>, <a href="/wiki/Bayes%27s_theorem" title="Bayes's theorem" class="mw-redirect">Bayes's theorem</a> and the <a href="/wiki/Principle_of_indifference" title="Principle of indifference">principle of indifference</a> alone give us <i>P(N|n)</i>, the probability of <i>N</i> humans being born if <i>n</i> is a random draw from <i>N</i>:</p>
<dl>
<dd><img class="tex" alt="P(N\mid n) = \frac{P(n\mid N) P(N)}{P(n)}." src="http://upload.wikimedia.org/math/3/3/5/3357b43bc249c47ea2f8667d4f8499e6.png" /></dd>
</dl>
<p>This is Bayes's theorem for the <a href="/wiki/Posterior_probability" title="Posterior probability">posterior probability</a> of total population exactly <i>N</i>, <a href="/wiki/Conditioned" title="Conditioned" class="mw-redirect">conditioned</a> on current population exactly <i>n</i>. Now, using the indifference principle:</p>
<dl>
<dd><img class="tex" alt="P(n\mid N) = \frac{1}{N}" src="http://upload.wikimedia.org/math/6/e/b/6eb5c28d8a5e1b19fc69aa5908bc7ae5.png" />.</dd>
</dl>
<p>The unconditioned <i>n</i> distribution of the current population is identical to the vague prior <i>N</i> probability density function,<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup> so:</p>
<dl>
<dd><img class="tex" alt="P(n) = \frac{k}{n}" src="http://upload.wikimedia.org/math/c/5/2/c527fb57fac7eb05fc195fd7aa38f2d4.png" />,</dd>
</dl>
<p>giving P (<i>N</i> | <i>n</i>) for each specific <i>N</i> (through a substitution into the posterior probability equation):</p>
<dl>
<dd><img class="tex" alt="P(N\mid n) = \frac{n}{N^2}" src="http://upload.wikimedia.org/math/1/4/a/14a4e5807c53b900ef14b13c010c25e4.png" />.</dd>
</dl>
<p>The easiest way to produce the doomsday estimate with a given confidence (say 95%) is to pretend that <i>N</i> is a <a href="/wiki/Continuous_random_variable" title="Continuous random variable" class="mw-redirect">continuous variable</a> (since it is very large) and <a href="/wiki/Integral" title="Integral">integrate</a> over the probability density from <i>N</i> = <i>n</i> to <i>N</i> = <i>Z</i>. (This will give a function for the probability that <i>N</i> ≤ <i>Z</i>):</p>
<dl>
<dd><img class="tex" alt="P(N \leq Z) = \int_{N=n}^{N=Z} P(N|n)\,dN" src="http://upload.wikimedia.org/math/0/4/c/04c2caa4eb3773b53d354273f540b4e7.png" /> <img class="tex" alt=" = \frac{Z-n}{Z}" src="http://upload.wikimedia.org/math/4/6/8/4680067972b14340dd714587f39d3db1.png" /></dd>
</dl>
<p>Defining <i>Z</i> = 20<i>n</i> gives:</p>
<dl>
<dd><img class="tex" alt="P(N \leq 20n) = \frac{19}{20}" src="http://upload.wikimedia.org/math/a/9/0/a9091ec0ae7116c3491ef79e1384caa8.png" />.</dd>
</dl>
<p>This is the simplest <a href="/wiki/Bayesian" title="Bayesian">Bayesian</a> derivation of the Doomsday Argument:</p>
<dl>
<dd>The chance that the total number of humans that will ever be born (<i>N</i>) is greater than twenty times the total that have been is below 5%</dd>
</dl>
<p>The use of a <a href="/wiki/Prior_probability#Uninformative_priors" title="Prior probability">vague prior</a> distribution seems well-motivated as it assumes as little knowledge as possible about <i>N</i>, given that any particular function must be chosen. It is equivalent to the assumption that the probability density of one's fractional position remains uniformly distributed even after learning of one's absolute position (<i>n</i>).</p>
<p>Gott's 'reference class' in his original 1993 paper was not the number of births, but the number of years 'humans' had existed as a species, which he put <a href="/wiki/Human_evolution#H._sapiens" title="Human evolution">at 200,000</a>. Also, Gott tried to give a 95% confidence interval between a <i>minimum</i> survival time and a maximum. Because of the 2.5% chance that he gives to underestimating the minimum he has only a 2.5% chance of overestimating the maximum. This equates to 97.5% confidence that extinction occurs before the upper boundary of his confidence interval.</p>
<p>97.5% is one chance in forty, which can be used in the integral above with <i>Z</i> = 40<i>n</i>, and <i>n</i> = 200,000 years:</p>
<dl>
<dd><img class="tex" alt="P(N \leq 40[200000]) = \frac{39}{40}" src="http://upload.wikimedia.org/math/4/7/6/47618d63ae4ed64da6933daa6a537668.png" /></dd>
</dl>
<p>This is how Gott produces a 97.5% confidence of extinction within <i>N</i> ≤ 8,000,000 years. The number he quoted was the likely time remaining, <i>N</i>&#160;−&#160;<i>n</i> = <b>7.8 million years</b>. This was much higher than the temporal confidence bound produced by counting births, because it applied the principle of indifference to time. (Producing different estimates by sampling different parameters in the same hypothesis is <a href="/wiki/Bertrand%27s_paradox_(probability)" title="Bertrand's paradox (probability)">Bertrand's paradox</a>.)</p>
<p>His choice of 95% confidence bounds (rather than 80% or 99.9%, say) matched the scientifically accepted limit of <a href="/wiki/Statistical_significance" title="Statistical significance">statistical significance</a> for hypothesis rejection. Therefore, he argued that the <a href="/wiki/Hypothesis" title="Hypothesis">hypothesis</a>: “humanity will cease to exist before 5,100 years or thrive beyond 7.8 million years” can be rejected.</p>
<p>Leslie's argument differs from Gott's version in that he does not assume a <i>vague prior</i> probability distribution for <i>N</i>. Instead he argues that the force of the Doomsday Argument resides purely in the increased probability of an early Doomsday once you take into account your birth position, regardless of your prior probability distribution for <i>N</i>. He calls this the <i>probability shift</i>.</p>
<p><a href="/wiki/Heinz_von_Foerster" title="Heinz von Foerster">Heinz von Foerster</a> argued that humanity's abilities to construct societies, civilizations and technologies do not result in self inhibition. Rather, societies' success varies directly with population size. Von Foerster found that this model fit some 25 data points from the birth of <a href="/wiki/Jesus" title="Jesus">Jesus</a> to 1958, with only 7% of the <a href="/wiki/Variance" title="Variance">variance</a> left unexplained. Several follow-up letters (1961, 1962, …) were published in <i>Science</i> showing that von Foerster's equation was still on track. The data continued to fit up until 1973. The most remarkable thing about von Foerster's model was it predicted that the human population would reach infinity or a mathematical singularity, on Friday, <span class="mw-formatted-date" title="2026-11-13"><span class="mw-formatted-date" title="11-13"><a href="/wiki/November_13" title="November 13">November 13</a></span>, <a href="/wiki/2026" title="2026">2026</a></span>. In fact, von Foerster did not imply that the world population on that day could actually become infinite. The real implication was that the world population growth pattern followed for many centuries prior to 1960 was about to come to an end and be transformed into a radically different pattern. Note that this prediction began to be fulfilled just in a few years after the "Doomsday" was published.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup></p>
<p><a name="Reference_classes" id="Reference_classes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=6" title="Edit section: Reference classes">edit</a>]</span> <span class="mw-headline">Reference classes</span></h2>
<p>One of the major areas of Doomsday Argument debate is the <a href="/w/index.php?title=Reference_class&amp;action=edit&amp;redlink=1" class="new" title="Reference class (page does not exist)">reference class</a> from which <i>n</i> is drawn, and of which <i>N</i> is the ultimate size. The 'standard' Doomsday Argument <a href="/wiki/Hypothesis" title="Hypothesis">hypothesis</a> doesn't spend very much time on this point, and simply says that the reference class is the number of 'humans'. Given that you are human, the Copernican principle could be applied to ask if you were born unusually early, but the grouping of 'human' has been widely challenged on <a href="/wiki/Anthropology" title="Anthropology">practical</a> and <a href="/wiki/Philosophy" title="Philosophy">philosophical</a> grounds. <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> has argued that <a href="/wiki/Consciousness" title="Consciousness">consciousness</a> is (part of) the discriminator between what is in and what is out of the reference class, and that <a href="/wiki/Extraterrestrial_intelligence" title="Extraterrestrial intelligence" class="mw-redirect">extraterrestrial intelligences</a> might affect the calculation dramatically.</p>
<p>The following sub-sections relate to different suggested reference classes, each of which has had the standard Doomsday Argument applied to it.</p>
<p><a name="Sampling_only_WMD-era_humans" id="Sampling_only_WMD-era_humans"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=7" title="Edit section: Sampling only WMD-era humans">edit</a>]</span> <span class="mw-headline">Sampling only <a href="/wiki/Weapons_of_mass_destruction" title="Weapons of mass destruction" class="mw-redirect">WMD</a>-era humans</span></h3>
<p>The <a href="/wiki/Doomsday_clock" title="Doomsday clock" class="mw-redirect">Doomsday clock</a> shows the expected time to nuclear <a href="/wiki/Doomsday_event" title="Doomsday event">doomsday</a> by the judgment of an <a href="/wiki/Bulletin_of_the_Atomic_Scientists" title="Bulletin of the Atomic Scientists">expert board</a>, rather than a Bayesian model. If the twelve hours of the clock symbolize the lifespan of the human species, its current time of 11:55 implies that we are among the last 1% of people who will ever be born (i.e. that <i>n</i> &gt; 0.99<i>N</i>). <a href="/wiki/J._Richard_Gott" title="J. Richard Gott">J. Richard Gott</a>'s temporal version of the Doomsday argument (DA) would require very strong prior evidence to overcome the improbability of being born in such a <a href="/wiki/Copernican_principle" title="Copernican principle">special</a> time.</p>
<dl>
<dd>If the clock's doomsday estimate is correct, there is less than 1 chance in 100 of seeing it show such a late time in human history, if observed at a random time within that history.</dd>
</dl>
<p>The <a href="/wiki/Bulletin_of_the_Atomic_Scientists" title="Bulletin of the Atomic Scientists">scientists'</a> warning can be reconciled with the DA, however: The Doomsday clock specifically estimates the proximity of <a href="/wiki/Nuclear_weapon" title="Nuclear weapon">atomic</a> self-destruction - which has only been possible for sixty years.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7" title=""><span>[</span>8<span>]</span></a></sup> If doomsday requires nuclear weaponry then the Doomsday Argument 'reference class' is: people contemporaneous with nuclear weapons. In this model, the number of people living through, or born after <a href="/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki" title="Atomic bombings of Hiroshima and Nagasaki">Hiroshima</a> is <i>n</i>, and the number of people who ever will is <i>N</i>. Applying <a href="/wiki/J._Richard_Gott" title="J. Richard Gott">Gott's</a> DA to these variable definitions gives a 50% chance of doomsday within 50 years.</p>
<dl>
<dd>In this model, the clock's hands are so close to midnight because a <a href="/wiki/Conditional_probability" title="Conditional probability">condition</a> of doomsday is living post-<a href="/wiki/1945" title="1945">1945</a>, a condition which applies now but not to the earlier 11 hours and 53 minutes of the clock's metaphorical human 'day'.</dd>
</dl>
<p>If your life is randomly selected from all lives lived under the shadow of the bomb, this simple model gives a 95% chance of doomsday within 1000 years.</p>
<p>The scientists' recent use of moving the clock forward to warn of the dangers posed by <a href="/wiki/Global_warming" title="Global warming">global warming</a> muddles this reasoning, however.</p>
<p><a name="SSSA:_Sampling_from_observer-moments" id="SSSA:_Sampling_from_observer-moments"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=8" title="Edit section: SSSA: Sampling from observer-moments">edit</a>]</span> <span class="mw-headline">SSSA: Sampling from observer-moments</span></h3>
<p><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a>, <a href="/wiki/Anthropic_principle#Anthropic_bias_and_anthropic_reasoning" title="Anthropic principle">considering observation selection effects</a>, has produced a Self-Sampling Assumption (SSA): "<i>that you should think of yourself as if you were a random observer from a suitable reference class</i>". If the 'reference class' is the set of humans to ever be born, this gives <i>N</i> &lt; 20<i>n</i> with 95% confidence (the standard Doomsday argument). However, he has <a href="/wiki/Anthropic_bias" title="Anthropic bias" class="mw-redirect">refined</a> this idea to apply to <i>observer-moments</i> rather than just observers. He has formalized this (<a href="http://anthropic-principle.com/preprints/self-location.html" class="external autonumber" title="http://anthropic-principle.com/preprints/self-location.html" rel="nofollow">[2]</a> as:</p>
<dl>
<dd>The Strong Self-Sampling Assumption (<b>SSSA</b>): Each observer-moment should reason as if it were randomly selected from the class of all observer-moments in its reference class.</dd>
</dl>
<p>If the minute in which you read this article is randomly selected from every minute in every human's lifespan then (with 95% confidence) this event has occurred after the first 5% of human observer-moments. If future mean lifespan is twice historic, this implies 95% confidence that <i>N</i> &lt; 10<i>n</i> (the average future human will account for twice the observer-moments of the average historic human). Therefore, the 95th percentile extinction-time estimate in this version is <b>4560 years</b>.</p>
<p><a name="Rebuttals" id="Rebuttals"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=9" title="Edit section: Rebuttals">edit</a>]</span> <span class="mw-headline">Rebuttals</span></h2>
<p><a name="We_are_in_the_earliest_5.25.2C_a_priori" id="We_are_in_the_earliest_5.25.2C_a_priori"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=10" title="Edit section: We are in the earliest 5%, a priori">edit</a>]</span> <span class="mw-headline">We are in the earliest 5%, a priori</span></h3>
<p>If you agree with the statistical methods, still disagreeing with the Doomsday argument (DA) implies that:</p>
<ol>
<li>We <b>are</b> within the first 5% of humans to be born.</li>
<li>This is <b>not</b> purely a coincidence.</li>
</ol>
<p>Therefore, these rebuttals try to give reasons for believing that we are some of the earliest humans.</p>
<p>For instance, you are member 50,000 in a collaborative project, the Doomsday Argument implies a 95% chance that there will never be more than a million members of that project. This can be refuted if your other characteristics are typical of the <a href="/wiki/Early_adopter" title="Early adopter">early adopter</a>. The mainstream of potential users will prefer to be involved when the project is nearly complete. If you enjoy the project's incompleteness, we already know that you are unusual, prior to the discovery of your early involvement.</p>
<p>If you have measurable attributes that set you apart from the typical long run user, the project DA can be refuted based on the fact that you would expect to be within the first 5% of members, <a href="/wiki/A_priori_and_a_posteriori_(philosophy)" title="A priori and a posteriori (philosophy)" class="mw-redirect">a priori</a>. The analogy to the total-human-population form of the argument is: Confidence in a prediction of the <a href="/wiki/Probability_distribution" title="Probability distribution">distribution</a> of human characteristics that places modern &amp; historic humans outside the mainstream, implies that we already know, before examining <i>n</i> that it is likely to be very early in <i>N</i>.</p>
<p>For example, if you are certain that 99% of humans who will ever live will be <a href="/wiki/Cyborg" title="Cyborg">cyborgs</a>, but you are not a cyborg, you could be equally certain that at least one hundred times as many people remain to be born as have been.</p>
<p><a href="/wiki/Robin_Hanson" title="Robin Hanson">Robin Hanson</a>'s paper sums up these criticisms of the DA:</p>
<dl>
<dd>"All else is not equal; we have good reasons for thinking we are not randomly selected humans from all who will ever live."</dd>
</dl>
<p>Drawbacks of this rebuttal:</p>
<ol>
<li>The question of how the confident prediction is derived. We need an uncannily <a href="/wiki/Prescient" title="Prescient" class="mw-redirect">prescient</a> picture of humanity's statistical <a href="/wiki/Probability_distribution" title="Probability distribution">distribution</a> through all time, before we can pronounce ourselves extreme members of that <a href="/wiki/Population" title="Population">population</a>. (In contrast, project pioneers have clearly distinct psychology from the mainstream.)</li>
<li>If the majority of humans have characteristics we do not share, some would argue that this is equivalent to the Doomsday argument, since <i>people like us</i> will become extinct. (<a href="/wiki/Friedrich_Nietzsche" title="Friedrich Nietzsche">Friedrich Nietzsche</a> outlines this <a href="/wiki/Pseudoextinction" title="Pseudoextinction">pseudoextinction</a> point of view in <a href="/wiki/Also_sprach_Zarathustra" title="Also sprach Zarathustra" class="mw-redirect">Also sprach Zarathustra</a>.)</li>
</ol>
<p><a name="Critique:_Human_extinction_is_distant.2C_a_posteriori" id="Critique:_Human_extinction_is_distant.2C_a_posteriori"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=11" title="Edit section: Critique: Human extinction is distant, a posteriori">edit</a>]</span> <span class="mw-headline">Critique: Human extinction is distant, a posteriori</span></h3>
<p>The <a href="/wiki/A_posteriori" title="A posteriori" class="mw-redirect">a posteriori</a> observation that <a href="/wiki/Extinction_level_event" title="Extinction level event" class="mw-redirect">extinction level events</a> are rare could be offered as evidence that the DA's predictions are implausible; typically, <a href="/wiki/Extinction" title="Extinction">extinctions</a> of a dominant <a href="/wiki/Species" title="Species">species</a> happens less often than once in a million years. Therefore, it is argued that <a href="/wiki/Human_extinction" title="Human extinction">Human extinction</a> is unlikely within the next ten millennia. (Another <a href="/wiki/Probabilistic_argument" title="Probabilistic argument">probabilistic argument</a>, drawing a different conclusion from the DA.)</p>
<p>In Bayesian terms, this response to the DA says that our knowledge of history (or ability to prevent disaster) produces a prior marginal for <i>N</i> with a minimum value in the trillions. If <i>N</i> is distributed uniformly from <span class="texhtml">10<sup>12</sup></span> to <span class="texhtml">10<sup>13</sup></span>, for example, then the probability of <i>N</i> &lt; 1,200 billion inferred from <i>n</i> = 60 billion will be extremely small. This is an equally impeccable Bayesian calculation, rejecting the <a href="/wiki/Copernican_principle" title="Copernican principle">Copernican principle</a> on the grounds that we must be 'special observers' since there is no likely mechanism for humanity to go extinct within the next hundred thousand years.</p>
<p>This response is accused of overlooking the <a href="/wiki/Human_extinction#Human_extinction_scenarios" title="Human extinction">technological threats to humanity's survival</a>, to which earlier life was not subject, and is specifically rejected by most of the DA's academic critics (arguably excepting <a href="/wiki/Robin_Hanson" title="Robin Hanson">Robin Hanson</a>).</p>
<p>In fact, many <a href="/wiki/Futurologists" title="Futurologists" class="mw-redirect">futurologists</a> believe the empirical situation is worse than Gott's DA estimate. For instance, Sir <a href="/wiki/Martin_Rees" title="Martin Rees" class="mw-redirect">Martin Rees</a> believes that the technological dangers give an estimated human survival duration of ninety-five years (with <a href="/wiki/Our_Final_Hour" title="Our Final Hour">50% confidence</a>.) Earlier prophets made similar predictions and were 'proven' wrong (e.g. on <a href="/wiki/Doomsday_argument#Sampling_only_WMD-era_humans" title="Doomsday argument">surviving the nuclear arms race</a>). It is possible that their estimates were accurate, and that their common image as alarmists is a <a href="/wiki/Survivorship_bias" title="Survivorship bias">survivorship bias</a>.</p>
<p><a name="The_prior_N_distribution_may_make_n_very_uninformative" id="The_prior_N_distribution_may_make_n_very_uninformative"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=12" title="Edit section: The prior N distribution may make n very uninformative">edit</a>]</span> <span class="mw-headline">The prior <i>N</i> distribution may make <i>n</i> very uninformative</span></h3>
<p><a href="/wiki/Robin_Hanson" title="Robin Hanson">Robin Hanson</a> argues that <i>N'</i>s prior may be <a href="/wiki/Exponential_distribution" title="Exponential distribution">exponentially distributed</a>:</p>
<dl>
<dd><img class="tex" alt="N = \frac{e^{U(0, q]}}{c}" src="http://upload.wikimedia.org/math/5/7/e/57e64b6560d3b6d2a95cbc80d931c266.png" /></dd>
</dl>
<p>Here, <i>c</i> and <i>q</i> are constants. If <i>q</i> is large, then our 95% confidence upper bound is on the uniform draw, not the exponential value of <i>N</i>.</p>
<p>The best way to compare this with Gott's Bayesian argument is to flatten the distribution from the vague prior by having the probability fall off more slowly with <i>N</i> (that inverse proportionally). This corresponds to the idea that humanity's growth may be exponential in time with doomsday having a vague prior <a href="/wiki/Probability_density_function" title="Probability density function">pdf</a> in <i>time</i>. This would mean than <i>N</i>, the last birth, would have a distribution looking like the following:</p>
<dl>
<dd><img class="tex" alt="\Pr(N) = \frac{k}{N^\alpha}, 0 &lt; \alpha &lt; 1.
" src="http://upload.wikimedia.org/math/8/d/6/8d68a0238d253a016b15fd9fe3c5aa01.png" /></dd>
</dl>
<p>This prior <i>N</i> distribution is all that is required (with the principle of indifference) to produce the inference of <i>N</i> from <i>n</i>, and this is done in an identical way to the standard case, as described by Gott (equivalent to <span class="texhtml">α</span> = 1 in this distribution):</p>
<dl>
<dd><img class="tex" alt=" \Pr(n) = \int_{N=n}^{N=\infty} \Pr(n\mid N) \Pr(N) \,dN = \int_{n}^{\infty} \frac{k}{N^{(\alpha+1)}} \,dN = \frac{k}{{\alpha}n^{\alpha}}" src="http://upload.wikimedia.org/math/c/a/9/ca9210f5ee56770cc3236683f92f3d87.png" /></dd>
</dl>
<p>Substituting into the posterior probability equation):</p>
<dl>
<dd><img class="tex" alt="\Pr(N\mid n) = \frac{{\alpha}n^{\alpha}}{N^{(1+\alpha)}}." src="http://upload.wikimedia.org/math/1/9/3/193e477e2e59e19dc82c8217634f9079.png" /></dd>
</dl>
<p>Integrating the probability of any <i>N</i> above <i>xn</i>:</p>
<dl>
<dd><img class="tex" alt="\Pr(N &gt; xn) = \int_{N=xn}^{N=\infty} \Pr(N\mid n)\,dN = \frac{1}{x^{\alpha}}." src="http://upload.wikimedia.org/math/1/9/a/19a717dd222fec7157dbdc2950c76be9.png" /></dd>
</dl>
<p>For example, if <i>x</i> = 20, and <span class="texhtml">α</span> = 0.5, this becomes:</p>
<dl>
<dd><img class="tex" alt="\Pr(N &gt; 20n) = \frac{1}{\sqrt{20}} \simeq 22.3\%. " src="http://upload.wikimedia.org/math/e/b/0/eb00c78c968f73c22c0ddf1e2c96b888.png" /></dd>
</dl>
<p>Therefore, with this prior, the chance of a trillion births is well over 20%, rather than the 5% chance given by the standard DA. If <span class="texhtml">α</span> is reduced further by assuming a flatter prior <i>N</i> distribution, then the limits on <i>N</i> given by <i>n</i> become weaker. An <span class="texhtml">α</span> of one reproduces Gott's calculation with a birth reference class, and <span class="texhtml">α</span> around 0.5 could approximate his temporal confidence interval calculation (if the population were expanding exponentially). As <img class="tex" alt="\alpha \to 0" src="http://upload.wikimedia.org/math/2/1/c/21c1e0489fb47e3c0fc1afe02d937719.png" /> (gets smaller) <i>n</i> becomes less and less <a href="/wiki/Uninformative_prior" title="Uninformative prior" class="mw-redirect">informative</a> about <i>N</i>. In the limit this distribution approaches an (unbounded) <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniform distribution</a>, where all values of <i>N</i> are equally likely. This is Page et al.'s <b>"Assumption 3"</b>, which they find few reasons to reject, a priori. (Although all distributions with <img class="tex" alt="\alpha \leq 1" src="http://upload.wikimedia.org/math/7/5/7/757608bc3dfb07daf3c1d0e95e68b90e.png" /> are improper priors, this applies to Gott's vague-prior distribution also, and they can all be converted to produce <a href="/wiki/Improper_integral" title="Improper integral">proper integrals</a> by postulating a finite upper population limit.) Since the probability of reaching a population of size 2<i>N</i> is usually thought of as the chance of reaching <i>N</i> multiplied by the survival probability from <i>N</i> to 2<i>N</i> it seems that Pr(<i>N</i>) must be a <a href="/wiki/Monotonic_function" title="Monotonic function">monotonically</a> decreasing function of <i>N</i>, but this doesn't necessarily require an inverse proportionality.</p>
<p>A prior distribution with a very low <span class="texhtml">α</span> <a href="/wiki/Parameter" title="Parameter">parameter</a> makes the DA's ability to constrain the ultimate size of humanity very weak.</p>
<p><a name="Infinite_Expectation" id="Infinite_Expectation"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=13" title="Edit section: Infinite Expectation">edit</a>]</span> <span class="mw-headline">Infinite Expectation</span></h3>
<p>Another objection to the Doomsday Argument is that the <a href="/wiki/Expected_value" title="Expected value">expected</a> total human population is actually <a href="/wiki/Infinite" title="Infinite" class="mw-redirect">infinite</a>. The calculation is as follows:</p>
<dl>
<dd>The total human population <var>N</var> = <var>n</var>/<var>f</var>, where <var>n</var> is the human population to date and <var>f</var> is our fractional position in the total.</dd>
<dd>We assume that <var>f</var> is uniformly distributed on (0,1].</dd>
<dd>The expectation of <var>N</var> is <img class="tex" alt=" E(N) = \int_{0}^{1} {n \over f} \, df = n \ln (1) - n \ln (0) = + \infty ." src="http://upload.wikimedia.org/math/d/0/5/d054bff222dbd0fb6860e8501cfb4be9.png" /></dd>
</dl>
<p>This infinite expectation shows that, under the framework of the DA, humanity still has some chance of surviving an arbitrarily long time.</p>
<p>For a similar example of counterintuitive infinite expectations, see the <a href="/wiki/St._Petersburg_paradox" title="St. Petersburg paradox">St. Petersburg paradox</a>.</p>
<p><a name="SIA:_The_possibility_of_not_existing_at_all" id="SIA:_The_possibility_of_not_existing_at_all"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=14" title="Edit section: SIA: The possibility of not existing at all">edit</a>]</span> <span class="mw-headline">SIA: The possibility of not existing at all</span></h3>
<p>One objection is that the possibility of you existing at all depends on how many humans will ever exist (<i>N</i>). If this is a high number, then the possibility of you existing is higher than if only a few humans will ever exist. Since you do indeed exist, this is evidence that the number of humans that will ever exist is high.</p>
<p>This objection, originally by <a href="/w/index.php?title=Dennis_Dieks&amp;action=edit&amp;redlink=1" class="new" title="Dennis Dieks (page does not exist)">Dennis Dieks</a> (<a href="/wiki/1992" title="1992">1992</a>), is now known by <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a>'s name for it: the "<a href="/wiki/Self-Indication_Assumption" title="Self-Indication Assumption">Self-Indication Assumption</a> objection". It can be shown that some <a href="/wiki/Self-Indication_Assumption" title="Self-Indication Assumption">SIAs</a> prevent any inference of <i>N</i> from <i>n</i> (the current population); for details of this argument from the Bayesian inference perspective see: <a href="/wiki/Self-Indication_Assumption_Doomsday_argument_rebuttal" title="Self-Indication Assumption Doomsday argument rebuttal">Self-Indication Assumption Doomsday argument rebuttal</a>.</p>
<p><a name="Many_worlds" id="Many_worlds"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=15" title="Edit section: Many worlds">edit</a>]</span> <span class="mw-headline">Many worlds</span></h3>
<p><a href="/w/index.php?title=John_Eastmond&amp;action=edit&amp;redlink=1" class="new" title="John Eastmond (page does not exist)">John Eastmond</a>'s "Many-Worlds Resolution of the Doomsday Argument" claims that when the Doomsday Argument is extended from a form that deals with a single historic timeline into one dealing with the many bifurcating simultaneous histories suggested by the <a href="/wiki/Many-worlds_interpretation" title="Many-worlds interpretation">many-worlds interpretation</a> of <a href="/wiki/Quantum_mechanics" title="Quantum mechanics">quantum mechanics</a> then one finds that the generalized argument no longer makes any prediction about <i>the</i> future total size of the human species. More specifically, if each finite value of total population size is realized in a different future, then learning of our present position from the beginning of the human species does not change our prior belief about which particular total population size we will find ourselves experiencing in one of humanity's many futures (assuming that versions of us live long enough to see versions of Doomsday).</p>
<p><a name="Caves.27_rebuttal" id="Caves.27_rebuttal"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=16" title="Edit section: Caves' rebuttal">edit</a>]</span> <span class="mw-headline">Caves' rebuttal</span></h3>
<p>Caves' <a href="/wiki/Bayesian" title="Bayesian">Bayesian</a> argument says that the uniform distribution assumption is incompatible with the <a href="/wiki/Copernican_principle" title="Copernican principle">Copernican principle</a>, not a consequence of it.</p>
<p>He gives a number of examples to argue that Gott's rule is implausible. For instance, he says, imagine stumbling into a birthday party, about which you know nothing:</p>
<dl>
<dd>Your friendly enquiry about the age of the celebrant elicits the reply that she is celebrating her (<i>t</i><sub><i>p</i></sub> = ) 50th birthday. According to Gott, you can predict with 95% confidence that the woman will survive between [50]/39 = 1.28 years and 39[×50] = 1,950 years into the future. Since the wide range encompasses reasonable expectations regarding the woman's survival, it might not seem so bad, till one realizes that [Gott's rule] predicts that with probability 1/2 the woman will survive beyond 100 years old and with probability 1/3 beyond 150. Few of us would want to bet on the woman's survival using Gott's rule. <i>(See Caves' online paper <a href="/wiki/Doomsday_argument#External_links" title="Doomsday argument">below</a>.)</i></dd>
</dl>
<p>Although this example exposes a weakness in <a href="/wiki/J._Richard_Gott" title="J. Richard Gott">J. Richard Gott</a>'s "Copernicus method" DA (that he does not specify when the "Copernicus method" can be applied) it is not precisely analogous with the <a href="/wiki/Doomsday_argument#Numerical_estimates_of_the_Doomsday_argument" title="Doomsday argument">modern DA</a>; <a href="/wiki/Epistemology" title="Epistemology">epistemological</a> refinements of Gott's argument by <a href="/wiki/Philosopher" title="Philosopher" class="mw-redirect">philosophers</a> such as <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> specify that:</p>
<dl>
<dd>Knowing the absolute birth rank (<i>n</i>) must give no information on the total population (<i>N</i>).</dd>
</dl>
<p>Careful DA variants specified with this rule aren't shown implausible by Caves' "Old Lady" example above, because, the woman's age is given prior to the estimate of her lifespan. Since human age gives an estimate of survival time (via <a href="/wiki/Actuary" title="Actuary">actuarial</a> tables) Caves' Birthday party age-estimate could not fall into the class of DA problems defined with this proviso.</p>
<p>To produce a comparable "Birthday party example" of the carefully specified Bayesian DA we would need to completely exclude all prior knowledge of likely human life spans; in principle this could be done (e.g.: hypothetical <a href="/w/index.php?title=Amnesia_chamber&amp;action=edit&amp;redlink=1" class="new" title="Amnesia chamber (page does not exist)">Amnesia chamber</a>). However, this would remove the modified example from everyday experience. To keep it in the everyday realm the lady's age must be <i>hidden</i> prior to the survival estimate being made. (Although this is no longer exactly the DA, it is much more comparable to it.)</p>
<p>Without knowing the lady’s age, the DA reasoning produces a <i>rule</i> to convert the birthday (<i>n</i>) into a maximum lifespan with 50% confidence (<i>N</i>). Gott's <a href="/w/index.php?title=Copernicus_method&amp;action=edit&amp;redlink=1" class="new" title="Copernicus method (page does not exist)">Copernicus method</a> rule is simply: Prob (<i>N</i> &lt; 2<i>n</i>) = 50%. How accurate would this estimate turn out to be? Western <a href="/wiki/Demographics" title="Demographics">demographics</a> are now fairly <a href="/wiki/Uniform" title="Uniform">uniform</a> across ages, so a random birthday (<i>n</i>) could be (very roughly) approximated by a U(0,<i>M</i>] draw where <i>M</i> is the maximum lifespan in the census. In this 'flat' model, everyone shares the same lifespan so <i>N</i> = <i>M</i>. If <i>n</i> happens to be less than (<i>M</i>)/2 then Gott's 2<i>n</i> estimate of <i>N</i> will be under <i>M</i>, its true figure. The other half of the time 2<i>n</i> underestimates <i>M</i>, and in this case (the one Caves highlights in his example) the subject will die before the 2<i>n</i> estimate is reached. In this 'flat demographics' model Gott's 50% confidence figure is proven right 50% of the time.</p>
<p><a name="Self-referencing_doomsday_argument_rebuttal" id="Self-referencing_doomsday_argument_rebuttal"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=17" title="Edit section: Self-referencing doomsday argument rebuttal">edit</a>]</span> <span class="mw-headline">Self-referencing doomsday argument rebuttal</span></h3>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/Self-referencing_doomsday_argument_rebuttal" title="Self-referencing doomsday argument rebuttal">Self-referencing doomsday argument rebuttal</a></div>
<p>Some philosophers have been bold enough to suggest that only people who have contemplated the Doomsday argument (DA) belong in the reference class '<a href="/wiki/Human" title="Human">human</a>'. If that is the appropriate reference class, <a href="/wiki/Brandon_Carter" title="Brandon Carter">Carter</a> defied his own prediction when he first described the argument (to the <a href="/wiki/Royal_Society" title="Royal Society">Royal Society</a>). A member present could have argued thus:</p>
<dl>
<dd>"Presently, only one person in the world understands the Doomsday argument, so by its own logic there is a 95% chance that it is a minor problem which will only ever interest twenty people, and I should ignore it."</dd>
</dl>
<p><a href="/w/index.php?title=Jeff_Dewynne&amp;action=edit&amp;redlink=1" class="new" title="Jeff Dewynne (page does not exist)">Jeff Dewynne</a> and Professor <a href="/w/index.php?title=Peter_Landsberg&amp;action=edit&amp;redlink=1" class="new" title="Peter Landsberg (page does not exist)">Peter Landsberg</a> suggested that this line of reasoning will create a <a href="/wiki/Paradox" title="Paradox">paradox</a> for the Doomsday argument:</p>
<p>If a member did pass such a comment, it would indicate that they understood the DA sufficiently well that in fact 2 people could be considered to understand it, and thus there would a 95% chance that 40 people would actually be interested. Also, of course, ignoring something because you only expect a small number of people to be interested in it is extremely short sighted - if this approach were to be taken, nothing new would ever be explored, if we assume no <i>a priori</i> knowledge of the nature of interest and attentional mechanisms.</p>
<p>Additionally, it should be considered that because <a href="/wiki/Brandon_Carter" title="Brandon Carter">Carter</a> did present and describe his argument, in which case the people to whom he explained it did contemplate the DA, as it was inevitable, the conclusion could then be drawn that in the moment of explanation <a href="/wiki/Brandon_Carter" title="Brandon Carter">Carter</a> created the basis for his own prediction.</p>
<p><a name="Mathematics-free_explanation_by_analogy" id="Mathematics-free_explanation_by_analogy"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=18" title="Edit section: Mathematics-free explanation by analogy">edit</a>]</span> <span class="mw-headline">Mathematics-free explanation by analogy</span></h2>
<p>Think of the human species like a car driver. We've had some bumps, but no catastrophes, and our car (<a href="/wiki/Earth" title="Earth">Earth</a>) is still road-worthy, but we want insurance. We ask the cosmic insurer how much a millennium’s cover will be, but they haven't dealt with humanity before. How should they work out the premium? The Doomsday Argument says that all they have to ask is how long we've been on the road (at least 40,000 years without an accident), they should calculate our insurance based on us having a 50% chance of having a fatal accident inside another 40,000 years.</p>
<p>Insurance companies try to attract drivers with long accident-free histories not because they necessarily drive more safely than newly qualified drivers, but for statistical reasons: They calculate that each driver looks for insurance quotes every year, so that the time since the last <a href="/wiki/Accident" title="Accident">accident</a> is a random sample between accidents. The chance of being more than halfway through a random sample is half, and if they are more than half way between accidents then they are heading for an accident in less time than the time since their last. A driver who hasn't had a scratch in 40 years will be quoted a very low premium for this reason, but you shouldn't expect cheap insurance if you've only passed your test two hours ago (equivalent to the accident-free record of the human species in relation to 40 years of <a href="/wiki/Geological_time" title="Geological time" class="mw-redirect">geological time</a>.)</p>
<p><a name="Analogy_to_the_estimated_final_score_of_a_cricket_batsman" id="Analogy_to_the_estimated_final_score_of_a_cricket_batsman"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=19" title="Edit section: Analogy to the estimated final score of a cricket batsman">edit</a>]</span> <span class="mw-headline">Analogy to the estimated final score of a cricket batsman</span></h3>
<p>A random in-progress <a href="/wiki/Cricket" title="Cricket">cricket</a> <a href="/wiki/Test_cricket" title="Test cricket">test match</a> is sampled for a single piece of information: the current <a href="/wiki/Batsman" title="Batsman">batsman</a>'s run tally so far. If the batsman is dismissed (rather than declaring), what is the chance that he will end up with a score more than double his current total?</p>
<dl>
<dd>A <i>rough</i> <a href="/wiki/Empirical" title="Empirical">empirical</a> result is that the chance is half (on average).</dd>
</dl>
<p>The <b>Doomsday argument</b> (DA) is that even if we were completely ignorant of the game we could make the same prediction, or profit by offering a bet paying <a href="/wiki/Odds" title="Odds">odds</a> of 2-to-3 on the batsmen doubling his current score.</p>
<p>Importantly, we can only offer the bet before the current score is given (this is necessary because the absolute value of the current score would give a cricket expert a lot of information about the chance of that tally doubling). It is necessary to be ignorant of the absolute run tally before making the prediction because this is linked to the likely total, but if the likely total and absolute value are <i>not</i> linked the survival prediction can be made <i>after</i> discovering the batter's current score. Analogously, the DA says that <i>if the absolute number of humans born gives no information on the number that will be</i>, we can predict the species’ total number of births after discovering that 60 billion people have ever been born: with 50% confidence it is 120 billion people, so that there is better-chance-than-not that <b>the last human birth will occur before the 23rd century</b>.</p>
<p>It is <i>not</i> true that the chance is half, <i>whatever</i> is the number of runs currently scored; <a href="/wiki/Batting" title="Batting">batting</a> records give an empirical <a href="/wiki/Correlation" title="Correlation">correlation</a> between reaching a given score (50 say) and reaching any other, higher score (say 100). On the average, the chance of doubling the current tally may be half, but the chance of reaching a century having scored fifty is much lower than reaching ten from five. Thus, the <i>absolute</i> value of the score gives information about the likely final total the batsman will reach, beyond the “scale invariant”.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8" title=""><span>[</span>9<span>]</span></a></sup></p>
<p>An analogous Bayesian critique of the DA is that we somehow possess <a href="/wiki/Prior_probability" title="Prior probability">prior</a> knowledge of the all-time human population distribution (total runs scored), and that this is more significant than the finding of a low number of births until now (a low current run count).</p>
<p>There are two alternative methods of making <a href="/wiki/Uniform" title="Uniform">uniform</a> draws from the current score (<i>n</i>):</p>
<ol>
<li>Put the runs actually scored by dismissed player in order, say 200, and randomly choose between these scoring increments by U(0, 200].</li>
<li>Select a <i>time</i> randomly from the beginning of the match to the final dismissal.</li>
</ol>
<p>The second sampling-scheme will include those lengthy periods of a game where a dismissed player is replaced, during which the ‘current batsman’ is preparing to take the field and has no runs. If we sample based on time-of-day rather than running-score we will often find that a new batsman has a score of zero <i>when the total score that day was low</i>, but we will rarely sample a zero if one batsman stayed at the <a href="/wiki/Crease" title="Crease" class="mw-redirect">crease</a>, piling on runs all day long. Therefore, the fact that we sample a non-zero score would tell us something about the likely final score that the current batsman will achieve.</p>
<p>Choosing sampling method 2 rather than method 1 would give a different statistical link between current and final score: any non-zero score would imply that the batsman reached a high final total, especially if the time to replace batsman is <i>very</i> long. This is analogous to the <a href="/wiki/Self-Indication_Assumption" title="Self-Indication Assumption">SIA</a>-DA-refutation that <i>N'</i>s distribution should include <i>N</i> = 0 states, which leads to the DA having reduced <a href="/wiki/Predictive_power" title="Predictive power">predictive power</a> (in the extreme, no power to predict <i>N</i> from <i>n</i> at all).</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=20" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Doomsday" title="Doomsday">Doomsday</a></li>
<li><a href="/wiki/Doomsday_event" title="Doomsday event">Doomsday events</a></li>
<li><a href="/wiki/Fermi_paradox" title="Fermi paradox">Fermi paradox</a></li>
<li><a href="/wiki/Final_anthropic_principle" title="Final anthropic principle">Final anthropic principle</a></li>
<li><a href="/wiki/List_of_disasters#Causes_of_hypothetical_future_disasters" title="List of disasters" class="mw-redirect">Hypothetical disasters</a></li>
<li><a href="/wiki/Mediocrity_principle" title="Mediocrity principle">Mediocrity principle</a></li>
<li><a href="/wiki/Quantum_immortality" title="Quantum immortality" class="mw-redirect">Quantum immortality</a></li>
<li><a href="/wiki/Simulated_reality" title="Simulated reality">Simulated reality</a></li>
<li><a href="/wiki/Sic_transit_gloria_mundi" title="Sic transit gloria mundi">Sic transit gloria mundi</a></li>
<li><a href="/wiki/Survival_analysis" title="Survival analysis">Survival analysis</a></li>
<li><a href="/wiki/Survivalism" title="Survivalism">Survivalism</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li>
<li><a href="/wiki/Black_swan_theory" title="Black swan theory">Black swan theory</a></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=21" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREF.5B.5BBrandon_Carter.5D.5D1983"><a href="/wiki/Brandon_Carter" title="Brandon Carter">Brandon Carter</a> (1983). "<a href="http://journals.royalsociety.org/content/2lk3776367111470/?p=9b69d8d942944c3f9af2c3354ead559e&amp;pi=17" class="external text" title="http://journals.royalsociety.org/content/2lk3776367111470/?p=9b69d8d942944c3f9af2c3354ead559e&amp;pi=17" rel="nofollow">The anthropic principle and its implications for biological evolution</a>" (PDF). <i><a href="/wiki/Philosophical_Transactions_of_the_Royal_Society_of_London" title="Philosophical Transactions of the Royal Society of London" class="mw-redirect">Philosophical Transactions of the Royal Society of London</a></i> <b>A310</b>: 347–363<span class="printonly">. <a href="http://journals.royalsociety.org/content/2lk3776367111470/?p=9b69d8d942944c3f9af2c3354ead559e&amp;pi=17" class="external free" title="http://journals.royalsociety.org/content/2lk3776367111470/?p=9b69d8d942944c3f9af2c3354ead559e&amp;pi=17" rel="nofollow">http://journals.royalsociety.org/content/2lk3776367111470/?p=9b69d8d942944c3f9af2c3354ead559e&amp;pi=17</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The+anthropic+principle+and+its+implications+for+biological+evolution&amp;rft.jtitle=%5B%5BPhilosophical+Transactions+of+the+Royal+Society+of+London%5D%5D&amp;rft.aulast=%5B%5BBrandon+Carter%5D%5D&amp;rft.au=%5B%5BBrandon+Carter%5D%5D&amp;rft.date=1983&amp;rft.volume=A310&amp;rft.pages=347%26ndash%3B363&amp;rft_id=http%3A%2F%2Fjournals.royalsociety.org%2Fcontent%2F2lk3776367111470%2F%3Fp%3D9b69d8d942944c3f9af2c3354ead559e%26pi%3D17&amp;rfr_id=info:sid/en.wikipedia.org:Doomsday_argument"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFJ._Richard_Gott.2C_III.5B.5B1993.5D.5D">J. Richard Gott, III (<a href="/wiki/1993" title="1993">1993</a>). "Implications of the Copernican principle for our future prospects". <i><a href="/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i> <b>363</b>: 315–319. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1038%2F363315a0" class="external text" title="http://dx.doi.org/10.1038%2F363315a0" rel="nofollow">10.1038/363315a0</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Implications+of+the+Copernican+principle+for+our+future+prospects&amp;rft.jtitle=%5B%5BNature+%28journal%29%7CNature%5D%5D&amp;rft.aulast=J.+Richard+Gott%2C+III&amp;rft.au=J.+Richard+Gott%2C+III&amp;rft.date=%5B%5B1993%5D%5D&amp;rft.volume=363&amp;rft.pages=315%26ndash%3B319&amp;rft_id=info:doi/10.1038%2F363315a0&amp;rfr_id=info:sid/en.wikipedia.org:Doomsday_argument"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-2"><b><a href="#cite_ref-2" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREF.5B.5BHolger_Bech_Nielsen.5D.5D1989"><a href="/wiki/Holger_Bech_Nielsen" title="Holger Bech Nielsen">Holger Bech Nielsen</a> (1989). "Random dynamics and relations between the number of fermion generations and the fine structure constants". <i><a href="/w/index.php?title=Acta_Physica_Polonica&amp;action=edit&amp;redlink=1" class="new" title="Acta Physica Polonica (page does not exist)">Acta Physica Polonica</a></i> <b>B20</b>: 427–468.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Random+dynamics+and+relations+between+the+number+of+fermion+generations+and+the+fine+structure+constants&amp;rft.jtitle=%5B%5BActa+Physica+Polonica%5D%5D&amp;rft.aulast=%5B%5BHolger+Bech+Nielsen%5D%5D&amp;rft.au=%5B%5BHolger+Bech+Nielsen%5D%5D&amp;rft.date=1989&amp;rft.volume=B20&amp;rft.pages=427%26ndash%3B468&amp;rfr_id=info:sid/en.wikipedia.org:Doomsday_argument"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> The Doomsday argument formulation would still apply if humans developed unlimited lifespan. <a href="/w/index.php?title=John_Eastmond&amp;action=edit&amp;redlink=1" class="new" title="John Eastmond (page does not exist)">John Eastmond</a>'s <a href="/wiki/2002" title="2002">2002</a> critique (<a href="http://xxx.lanl.gov/abs/gr-qc/0208038" class="external autonumber" title="http://xxx.lanl.gov/abs/gr-qc/0208038" rel="nofollow">[1]</a>) concludes that "<i>an infinite conscious lifetime is not possible, even in principle</i>" because, he contends, that the DA's uniform prior probability distribution applied over a <a href="/wiki/Countable" title="Countable" class="mw-redirect">countable</a> infinity of observer-moments implies an <a href="/wiki/Uncountable" title="Uncountable" class="mw-redirect">uncountable</a> number of the countably infinite bit-strings needed to specify each moment.</li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> Doomsday argument two-case section is partially based on <a href="http://www.findarticles.com/p/articles/mi_m2346/is_n426_v107/ai_20550244" class="external text" title="http://www.findarticles.com/p/articles/mi_m2346/is_n426_v107/ai_20550244" rel="nofollow">a refutation of the Doomsday Argument by Korb and Oliver</a>.</li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> The only <a href="/wiki/Probability_density_function" title="Probability density function">probability density functions</a> that must be specified a priori are:
<ul>
<li>Pr(<i>N</i>) - the ultimate number of people that will be born, assumed by J. Richard Gott to have a vague prior distribution, Pr(<i>N</i>) = <i>k</i>/<i>N</i></li>
<li>Pr(<i>n</i>|<i>N</i>) - the chance of being born in any position based on a total population <i>N</i> - all DA forms assume the <a href="/wiki/Copernican_principle" title="Copernican principle">Copernican principle</a>, making Pr(<i>n</i>|<i>N</i>) = 1/<i>N</i></li>
</ul>
From these two distributions, the Doomsday Argument proceeds to create a Bayesian inference on the distribution of <i>N</i> from <i>n</i>, through <a href="/wiki/Bayes%27_theorem#Bayes.27_theorem_for_probability_densities" title="Bayes' theorem">Bayes' rule</a>, which requires P(<i>n</i>); to produce this, integrate over all the possible values of <i>N</i> which might contain an individual born <i>n</i>th (that is, wherever <i>N</i> &gt; <i>n</i>):
<dl>
<dd><img class="tex" alt=" P(n) = \int_{N=n}^{N=\infty} P(n\mid N) P(N) \,dN = \int_{n}^{\infty}\frac{k}{N^2} \,dN " src="http://upload.wikimedia.org/math/0/c/0/0c04d235e8c7c7935128fad3525d60f6.png" /> <img class="tex" alt="= \frac{k}{n}" src="http://upload.wikimedia.org/math/0/2/9/02915ed5d97dd3af8631288540580616.png" /></dd>
</dl>
This is why the marginal distribution of n and N are identical in the case of P(<i>N</i>) = <i>k</i>/<i>N'</i></li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> See, for example, <a href="http://urss.ru/cgi-bin/db.pl?cp=&amp;page=Book&amp;id=34250&amp;lang=en&amp;blang=en&amp;list=38" class="external text" title="http://urss.ru/cgi-bin/db.pl?cp=&amp;page=Book&amp;id=34250&amp;lang=en&amp;blang=en&amp;list=38" rel="nofollow"><i>Introduction to Social Macrodynamics</i></a>by <a href="/wiki/Andrey_Korotayev" title="Andrey Korotayev">Andrey Korotayev</a> <i>et al.</i></li>
<li id="cite_note-7"><b><a href="#cite_ref-7" title="">^</a></b> The clock first appeared in <a href="/wiki/1949" title="1949">1949</a>, and the date on which humanity gained the power to destroy itself is debatable, but to simplify the argument the numbers here are based on an assumption of fifty years.</li>
<li id="cite_note-8"><b><a href="#cite_ref-8" title="">^</a></b> The cricketing rationale for the lengthening of future survival time with current score is that batting is a test of skill that a high-scoring batsman has passed. Therefore, higher scores are correlated with better players who will then be more likely to continue scoring heavily. Historic batting records give a <a href="/wiki/Prior_probability" title="Prior probability">prior</a> distribution that provides other useful data. In particular, we know the <a href="/wiki/Mean" title="Mean">mean</a> score across all players and matches. High and low <a href="/wiki/Posterior_probability" title="Posterior probability">posterior</a> information (the current score) only gives a weak indication of the player's skill, which is more strongly described by this <i>prior</i> mean. (This statistical phenomenon of <a href="/wiki/Prior_probability#Informative_priors" title="Prior probability">informative</a> averages is called <a href="/wiki/Regression_toward_the_mean" title="Regression toward the mean">Regression toward the mean</a>.)</li>
</ol>
</div>
<div class="references-small" style="margin-left:1.5em;">
<ul>
<li>John Leslie, <i>The End of the World: The Science and Ethics of Human Extinction</i>, Routledge, 1998, <a href="/wiki/Special:BookSources/0415184479" class="internal">ISBN 0-415-18447-9</a>.</li>
<li>J. R. Gott III, <i>Future Prospects Discussed</i>, Nature, vol. 368, p. 108, 1994.</li>
<li>This argument plays a central role in <a href="/wiki/Stephen_Baxter" title="Stephen Baxter">Stephen Baxter</a>'s science fiction book, <i><a href="/wiki/Manifold:_Time" title="Manifold: Time">Manifold: Time</a></i>, Del Rey Books, 2000, <a href="/wiki/Special:BookSources/034543076X" class="internal">ISBN 0-345-43076-X</a>.</li>
</ul>
</div>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Doomsday_argument&amp;action=edit&amp;section=22" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://flatrock.org.nz/topics/environment/doom_soon.htm" class="external text" title="http://flatrock.org.nz/topics/environment/doom_soon.htm" rel="nofollow">A non-mathematical, unpartisan introduction to the DA</a></li>
<li><a href="http://youtube.com/watch?v=F-QA2rkpBSY" class="external text" title="http://youtube.com/watch?v=F-QA2rkpBSY" rel="nofollow">A compelling lecture from the University of Colorado-Boulder</a></li>
<li><a href="http://www.anthropic-principle.com/preprints/ali/alive.html" class="external text" title="http://www.anthropic-principle.com/preprints/ali/alive.html" rel="nofollow">Nick Bostrom's response to Korb and Oliver</a></li>
<li><a href="http://www.anthropic-principle.com/primer1.html" class="external text" title="http://www.anthropic-principle.com/primer1.html" rel="nofollow">Nick Bostrom's summary version of the argument</a></li>
<li><a href="http://www.anthropic-principle.com/preprints.html#doomsday" class="external text" title="http://www.anthropic-principle.com/preprints.html#doomsday" rel="nofollow">Nick Bostrom's annotated collection of references</a></li>
<li><a href="http://arxiv.org/abs/gr-qc/9407002" class="external text" title="http://arxiv.org/abs/gr-qc/9407002" rel="nofollow">Kopf, Krtouš &amp; Page's early (1994) refutation</a> based on the <a href="/wiki/SIA" title="SIA" class="mw-redirect">SIA</a>, which they called "Assumption 2".</li>
<li><a href="http://xxx.lanl.gov/abs/gr-qc/0208038" class="external text" title="http://xxx.lanl.gov/abs/gr-qc/0208038" rel="nofollow">The Doomsday Argument, Consciousness and Many Worlds by John Eastmond</a></li>
<li><a href="http://xxx.lanl.gov/abs/gr-qc/0009081" class="external text" title="http://xxx.lanl.gov/abs/gr-qc/0009081" rel="nofollow">The Doomsday argument and the number of possible observers by Ken Olum</a> In 1993 <a href="/wiki/J._Richard_Gott" title="J. Richard Gott">J. Richard Gott</a> used his "Copernicus method" to predict the lifetime of Broadway shows. One part of this paper uses the same reference class as an empirical counter-example to Gott's method.</li>
<li><a href="http://hanson.gmu.edu/nodoom.html" class="external text" title="http://hanson.gmu.edu/nodoom.html" rel="nofollow">A Critique of the Doomsday Argument by Robin Hanson</a></li>
<li><a href="http://cogprints.ecs.soton.ac.uk/archive/00002990/" class="external text" title="http://cogprints.ecs.soton.ac.uk/archive/00002990/" rel="nofollow">A third route to the doomsday argument by Paul Franceschi</a></li>
<li><a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=82931" class="external text" title="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=82931" rel="nofollow">Chambers' Ussherian Corollary Objection</a></li>
<li><a href="http://info.phys.unm.edu/papers/2000/Caves2000a.pdf" class="external text" title="http://info.phys.unm.edu/papers/2000/Caves2000a.pdf" rel="nofollow">Caves' Bayesian critique of Gott's argument. C. M. Caves, "Predicting future duration from present age: A critical assessment", Contemporary Physics 41, 143-153 (2000).</a></li>
<li><a href="http://arxiv.org/abs/0806.3538v1" class="external text" title="http://arxiv.org/abs/0806.3538v1" rel="nofollow">C.M. Caves, "Predicting future duration from present age: Revisiting a critical assessment of Gott's rule.</a></li>
<li><a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=2400044" class="external text" title="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=2400044" rel="nofollow">"Infinitely Long Afterlives and the Doomsday Argument" by John Leslie shows that Leslie has recently modified his analysis and conclusion (Philosophy 83 (4) 2008 pp. 519–524): Abstract -- A recent book of mine defends three distinct varieties of immortality. One of them is an infinitely lengthy afterlife; however, any hopes of it might seem destroyed by something like Brandon Carter's ‘doomsday argument’ against viewing ourselves as extremely early humans. The apparent difficulty might be overcome in two ways. First, if the world is non-deterministic then anything on the lines of the doomsday argument may prove unable to deliver a strongly pessimistic conclusion. Secondly, anything on those lines may break down when an infinite sequence of experiences is in question.</a></li>
<li><a href="http://www.lrb.co.uk/v21/n13/gree04_.html" class="external text" title="http://www.lrb.co.uk/v21/n13/gree04_.html" rel="nofollow">Mark Greenberg, "Apocalypse Not Just Now" in London Review of Books</a></li>
<li><a href="http://pthbb.org/manual/services/grim/laster.html" class="external text" title="http://pthbb.org/manual/services/grim/laster.html" rel="nofollow">Laster</a>: A simple webpage applet giving the min &amp; max survival times of anything with 50% and 95% confidence requiring only that you input how old it is. It is designed to use the same mathematics as <a href="/wiki/J._Richard_Gott" title="J. Richard Gott">J. Richard Gott</a>'s form of the DA, and was programmed by <a href="/wiki/Sustainable_development" title="Sustainable development">sustainable development</a> researcher Jerrad Pierce.</li>
</ul>
<table class="navbox" cellspacing="0" style=";">
<tr>
<td style="padding:2px;">
<table cellspacing="0" class="nowraplinks collapsible autocollapse" style="width:100%;background:transparent;color:inherit;;">
<tr>
<th style=";" colspan="2" class="navbox-title">
<div style="float:left; width:6em;text-align:left;">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;;;border:none;; font-size:xx-small;"><a href="/wiki/Template:Doomsday" title="Template:Doomsday"><span title="View this template" style=";;border:none;">v</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/wiki/Template_talk:Doomsday" title="Template talk:Doomsday"><span title="Discussion about this template" style=";;border:none;">d</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="http://en.wikipedia.org/w/index.php?title=Template:Doomsday&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Template:Doomsday&amp;action=edit" rel="nofollow"><span title="Edit this template" style=";;border:none;;">e</span></a></div>
</div>
<span style="font-size:110%;"><a href="/wiki/Doomsday" title="Doomsday">Doomsday</a></span></th>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td colspan="2" style="width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em">
<div>
<p><a href="/wiki/Apocalypse" title="Apocalypse">Apocalypse</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Apocalyptic_and_post-apocalyptic_fiction" title="Apocalyptic and post-apocalyptic fiction">Apocalyptic and post-apocalyptic fiction</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Armageddon" title="Armageddon">Armageddon</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Big_Crunch" title="Big Crunch">Big Crunch</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Big_Rip" title="Big Rip">Big Rip</a><span style="font-weight:bold;">&#160;·</span> <strong class="selflink">Doomsday argument</strong><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Doomsday_cult" title="Doomsday cult">Doomsday cult</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Doomsday_Clock" title="Doomsday Clock">Doomsday Clock</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Doomsday_device" title="Doomsday device">Doomsday device</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Doomsday_event" title="Doomsday event">Doomsday event</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Doomsday_film" title="Doomsday film">Doomsday film</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Earth#Future" title="Earth">End of planet Earth</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/End_time" title="End time">End time</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Eschatology" title="Eschatology">Eschatology</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Human_extinction" title="Human extinction">Human extinction</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Last_Judgement" title="Last Judgement" class="mw-redirect">Last Judgement</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Planetary_Phase_of_Civilization" title="Planetary Phase of Civilization">Planetary Phase of Civilization</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Ragnar%C3%B6k" title="Ragnarök">Ragnarök</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Risks_to_civilization,_humans_and_planet_Earth" title="Risks to civilization, humans and planet Earth">Risks to civilization, humans and planet Earth</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Societal_collapse" title="Societal collapse">Societal collapse</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Ten_Threats" title="Ten Threats">Ten Threats</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/World_War_III" title="World War III">World War III</a></p>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
</table>


<!-- 
NewPP limit report
Preprocessor node count: 2139/1000000
Post-expand include size: 24826/2048000 bytes
Template argument size: 8642/2048000 bytes
Expensive parser function count: 1/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:574311-0!1!0!default!!en!2 and timestamp 20090406020221 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Doomsday_argument">http://en.wikipedia.org/wiki/Doomsday_argument</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Eschatology" title="Category:Eschatology">Eschatology</a></span> | <span dir='ltr'><a href="/wiki/Category:Probability_theory" title="Category:Probability theory">Probability theory</a></span> | <span dir='ltr'><a href="/wiki/Category:Theories_of_history" title="Category:Theories of history">Theories of history</a></span> | <span dir='ltr'><a href="/wiki/Category:Sociocultural_evolution" title="Category:Sociocultural evolution">Sociocultural evolution</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:All_pages_needing_cleanup" title="Category:All pages needing cleanup">All pages needing cleanup</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_disputed_statements_from_March_2009" title="Category:Articles with disputed statements from March 2009">Articles with disputed statements from March 2009</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Doomsday_argument" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Doomsday_argument" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Doomsday_argument&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Doomsday_argument&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Doomsday_argument" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Doomsday_argument" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Doomsday_argument" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Doomsday_argument&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Doomsday_argument&amp;oldid=282013108" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Doomsday_argument&amp;id=282013108">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Doomsday-Argument">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Argumento_del_juicio_final">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Argument_de_l%27apocalypse">Français</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/Doomsday_argument">Русский</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 6 April 2009, at 02:01.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv184 in 1.166 secs. --></body></html>
