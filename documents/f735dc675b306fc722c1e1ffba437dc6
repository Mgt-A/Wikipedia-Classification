<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Principal component analysis,Compression Methods,Compression Formats,Compression Software Implementations,A-law algorithm,Acoustics,Adaptive Huffman coding,Algebraic Code Excited Linear Prediction,Approximation,Arg max,Arithmetic coding" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Principal_component_analysis&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Principal_component_analysis&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Principal component analysis - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Principal_component_analysis";
		var wgTitle = "Principal component analysis";
		var wgAction = "view";
		var wgArticleId = "76340";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281653103;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Principal_component_analysis skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Principal component analysis</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<div class="dablink">"KLT" redirects here. For the Kanade-Lucas-Tomasi <a href="/wiki/Interest_point_detection" title="Interest point detection">feature</a> tracker used in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, see <a href="/wiki/Lucas%E2%80%93Kanade_method" title="Lucas–Kanade method" class="mw-redirect">Lucas–Kanade method</a>.</div>
<table class="metadata plainlinks ambox ambox-content" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_contradict.svg" class="image" title="Accuracy dispute"><img alt="Accuracy dispute" src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Ambox_contradict.svg/38px-Ambox_contradict.svg.png" width="38" height="38" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article or section appears to <b>contradict</b> itself. Please help <a href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit" rel="nofollow">fix&#160;this&#160;problem</a>.</td>
</tr>
</table>
<p><b>Principal component analysis (PCA)</b> involves a mathematical procedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables called principal components. The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible. Depending on the field of application, it is also named the discrete <b><a href="/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem" title="Karhunen–Loève theorem">Karhunen–Loève</a> transform (KLT)</b>, the <b><a href="/wiki/Harold_Hotelling" title="Harold Hotelling">Hotelling</a> transform</b> or <b>proper orthogonal decomposition (POD)</b>.</p>
<p>PCA was invented in 1901 by <a href="/wiki/Karl_Pearson" title="Karl Pearson">Karl Pearson</a><sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup>. Now it is mostly used as a tool in <a href="/wiki/Exploratory_data_analysis" title="Exploratory data analysis">exploratory data analysis</a> and for making predictive models. PCA involves the calculation of the <a href="/wiki/Eigendecomposition_of_a_matrix" title="Eigendecomposition of a matrix">eigenvalue decomposition</a> of a data <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> or <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a> of a <a href="/wiki/Data_matrix" title="Data matrix">data matrix</a>, usually after mean centering the data for each attribute. The results of a PCA are usually discussed in terms of component scores and loadings (Shaw, 2003).</p>
<p>PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way which best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA supplies the user with a lower-dimensional picture, a "shadow" of this object when viewed from its (in some sense) most informative viewpoint.</p>
<p>PCA is closely related to <a href="/wiki/Factor_analysis" title="Factor analysis">factor analysis</a>; indeed, some statistical packages deliberately conflate the two techniques. True factor analysis makes different assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Details"><span class="tocnumber">1</span> <span class="toctext">Details</span></a></li>
<li class="toclevel-1"><a href="#Discussion"><span class="tocnumber">2</span> <span class="toctext">Discussion</span></a></li>
<li class="toclevel-1"><a href="#Table_of_symbols_and_abbreviations"><span class="tocnumber">3</span> <span class="toctext">Table of symbols and abbreviations</span></a></li>
<li class="toclevel-1"><a href="#Properties_and_Limitations_of_PCA"><span class="tocnumber">4</span> <span class="toctext">Properties and Limitations of PCA</span></a></li>
<li class="toclevel-1"><a href="#Computing_PCA_using_the_Covariance_Method"><span class="tocnumber">5</span> <span class="toctext">Computing PCA using the Covariance Method</span></a>
<ul>
<li class="toclevel-2"><a href="#Organize_the_data_set"><span class="tocnumber">5.1</span> <span class="toctext">Organize the data set</span></a></li>
<li class="toclevel-2"><a href="#Calculate_the_empirical_mean"><span class="tocnumber">5.2</span> <span class="toctext">Calculate the empirical mean</span></a></li>
<li class="toclevel-2"><a href="#Calculate_the_deviations_from_the_mean"><span class="tocnumber">5.3</span> <span class="toctext">Calculate the deviations from the mean</span></a></li>
<li class="toclevel-2"><a href="#Find_the_covariance_matrix"><span class="tocnumber">5.4</span> <span class="toctext">Find the covariance matrix</span></a></li>
<li class="toclevel-2"><a href="#Find_the_eigenvectors_and_eigenvalues_of_the_covariance_matrix"><span class="tocnumber">5.5</span> <span class="toctext">Find the eigenvectors and eigenvalues of the covariance matrix</span></a></li>
<li class="toclevel-2"><a href="#Rearrange_the_eigenvectors_and_eigenvalues"><span class="tocnumber">5.6</span> <span class="toctext">Rearrange the eigenvectors and eigenvalues</span></a></li>
<li class="toclevel-2"><a href="#Compute_the_cumulative_energy_content_for_each_eigenvector"><span class="tocnumber">5.7</span> <span class="toctext">Compute the cumulative energy content for each eigenvector</span></a></li>
<li class="toclevel-2"><a href="#Select_a_subset_of_the_eigenvectors_as_basis_vectors"><span class="tocnumber">5.8</span> <span class="toctext">Select a subset of the eigenvectors as basis vectors</span></a></li>
<li class="toclevel-2"><a href="#Convert_the_source_data_to_z-scores"><span class="tocnumber">5.9</span> <span class="toctext">Convert the source data to z-scores</span></a></li>
<li class="toclevel-2"><a href="#Project_the_z-scores_of_the_data_onto_the_new_basis"><span class="tocnumber">5.10</span> <span class="toctext">Project the z-scores of the data onto the new basis</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Derivation_of_PCA_using_the_covariance_method"><span class="tocnumber">6</span> <span class="toctext">Derivation of PCA using the covariance method</span></a></li>
<li class="toclevel-1"><a href="#Relation_between_PCA_and_K-means_clustering"><span class="tocnumber">7</span> <span class="toctext">Relation between PCA and K-means clustering</span></a></li>
<li class="toclevel-1"><a href="#Correspondence_analysis"><span class="tocnumber">8</span> <span class="toctext">Correspondence analysis</span></a></li>
<li class="toclevel-1"><a href="#Generalizations"><span class="tocnumber">9</span> <span class="toctext">Generalizations</span></a>
<ul>
<li class="toclevel-2"><a href="#Nonlinear_generalizations"><span class="tocnumber">9.1</span> <span class="toctext">Nonlinear generalizations</span></a></li>
<li class="toclevel-2"><a href="#Higher_order"><span class="tocnumber">9.2</span> <span class="toctext">Higher order</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Software.2Fsource_code"><span class="tocnumber">10</span> <span class="toctext">Software/source code</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">11</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">12</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">13</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">14</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Details" id="Details"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=1" title="Edit section: Details">edit</a>]</span> <span class="mw-headline">Details</span></h2>
<p>PCA is mathematically defined<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup> as an <a href="/wiki/Orthogonal_transformation" title="Orthogonal transformation" class="mw-redirect">orthogonal</a> <a href="/wiki/Linear_transformation" title="Linear transformation" class="mw-redirect">linear transformation</a> that transforms the data to a new <a href="/wiki/Coordinate_system" title="Coordinate system">coordinate system</a> such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. PCA is theoretically the optimum transform for a given data in <a href="/wiki/Least_squares" title="Least squares">least square</a> terms.</p>
<p>PCA can be used for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction" class="mw-redirect">dimensionality reduction</a> in a data set by retaining those characteristics of the data set that contribute most to its <a href="/wiki/Variance" title="Variance">variance</a>, by keeping lower-order principal components and ignoring higher-order ones. Such low-order components often contain the "most important" aspects of the data. However, depending on the application this may not always be the case.</p>
<p>For a data <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a>, <b>X<sup>T</sup></b>, with zero <a href="/wiki/Empirical_mean" title="Empirical mean" class="mw-redirect">empirical mean</a> (the empirical mean of the distribution has been subtracted from the data set), where each row represents a different repetition of the experiment, and each column gives the results from a particular probe, the PCA transformation is given by:</p>
<dl>
<dd><img class="tex" alt="\mathbf{Y}^{\rm T}=\mathbf{X}^{\rm T}\mathbf{W}" src="http://upload.wikimedia.org/math/4/0/7/40774ade43c14dbe14830bca2222513c.png" />
<dl>
<dd><img class="tex" alt=" = \mathbf{V}\mathbf{\Sigma} " src="http://upload.wikimedia.org/math/a/4/1/a41bfa8f3ebed9bff4aba4a89d22a3ce.png" /></dd>
</dl>
</dd>
</dl>
<p>where <b>V Σ W<sup>T</sup></b> is the <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a> (svd) of <b>X<sup>T</sup></b>.</p>
<p>PCA has the distinction of being the optimal <a href="/wiki/Linear_transformation" title="Linear transformation" class="mw-redirect">linear transformation</a> for keeping the subspace that has largest variance. This advantage, however, comes at the price of greater computational requirement if compared, for example, to the <a href="/wiki/Discrete_cosine_transform" title="Discrete cosine transform">discrete cosine transform</a>.</p>
<p><a name="Discussion" id="Discussion"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=2" title="Edit section: Discussion">edit</a>]</span> <span class="mw-headline">Discussion</span></h2>
<p>Though most derivations and implementations fail to identify the importance of mean subtraction, data centering is carried out because it is part of the solution towards finding a basis that <a href="/wiki/Minimum_mean_square_error" title="Minimum mean square error">minimizes the mean square error</a> of approximating the data<sup id="cite_ref-2" class="reference"><a href="#cite_note-2" title=""><span>[</span>3<span>]</span></a></sup>. Assuming zero <a href="/wiki/Empirical_mean" title="Empirical mean" class="mw-redirect">empirical mean</a> (the empirical mean of the distribution has been subtracted from the data set), the principal component <i>w</i><sub>1</sub> of a data set <i>x</i> can be defined as:</p>
<dl>
<dd><img class="tex" alt="\mathbf{w}_1
 = \arg\max_{\Vert \mathbf{w} \Vert = 1} \operatorname{var}\{ \mathbf{w}^T \mathbf{x} \}
 = \arg\max_{\Vert \mathbf{w} \Vert = 1} E\left\{ \left( \mathbf{w}^T \mathbf{x}\right)^2 \right\}" src="http://upload.wikimedia.org/math/5/d/6/5d64395ef8f85c6ffdea775504da4545.png" /></dd>
</dl>
<p>(See <a href="/wiki/Arg_max" title="Arg max">arg max</a> for the notation.) With the first <i>k</i>&#160;−&#160;1 components, the <i>k</i>th component can be found by subtracting the first <span class="texhtml"><i>k</i> − 1</span> principal components from <i>x</i>:</p>
<dl>
<dd><img class="tex" alt="\mathbf{\hat{x}}_{k - 1}
 = \mathbf{x} -
 \sum_{i = 1}^{k - 1}
 \mathbf{w}_i \mathbf{w}_i^T \mathbf{x}" src="http://upload.wikimedia.org/math/2/f/0/2f03d7c399d96787d5e3639c08780e4c.png" /></dd>
</dl>
<p>and by substituting this as the new data set to find a principal component in</p>
<dl>
<dd><img class="tex" alt="\mathbf{w}_k
 = \arg\max_{\Vert \mathbf{w} \Vert = 1} E\left\{
 \left( \mathbf{w}^T \mathbf{\hat{x}}_{k - 1}
 \right)^2 \right\}." src="http://upload.wikimedia.org/math/a/1/e/a1e7cc107ee2e0428f0d0aa250485ba7.png" /></dd>
</dl>
<p>The Karhunen–Loève transform is therefore equivalent to finding the <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a> of the data matrix <i>X</i>,</p>
<dl>
<dd><img class="tex" alt="\mathbf{X}=\mathbf{W}\mathbf{\Sigma}\mathbf{V}^T," src="http://upload.wikimedia.org/math/f/7/0/f702ff3fe390dc05c7af181fa54dace3.png" /></dd>
</dl>
<p>and then obtaining the reduced-space data matrix <b>Y</b> by projecting <b>X</b> down into the reduced space defined by only the first <i>L</i> singular vectors, <b>W<sub>L</sub></b>:</p>
<dl>
<dd><img class="tex" alt="\mathbf{Y}=\mathbf{W_L}^T\mathbf{X} = \mathbf{\Sigma_L}\mathbf{V_L}^T" src="http://upload.wikimedia.org/math/a/f/8/af8f42d9aaca5b333df01d982d86fa64.png" /></dd>
</dl>
<p>The matrix <b>W</b> of singular vectors of <b>X</b> is equivalently the matrix <b>W</b> of eigenvectors of the matrix of observed covariances <b>C</b> = <b>X X<sup>T</sup></b>,</p>
<dl>
<dd><img class="tex" alt="\mathbf{X}\mathbf{X}^T = \mathbf{W}\mathbf{\Sigma}\mathbf{\Sigma}^T\mathbf{W}^T" src="http://upload.wikimedia.org/math/1/d/0/1d0c3ebbdbb9b13a7c68b80aa95386ff.png" /></dd>
</dl>
<p>The <a href="/wiki/Eigenvectors" title="Eigenvectors" class="mw-redirect">eigenvectors</a> with the largest <a href="/wiki/Eigenvalues" title="Eigenvalues" class="mw-redirect">eigenvalues</a> correspond to the dimensions that have the strongest <a href="/wiki/Correlation" title="Correlation">correlation</a> in the data set (see <a href="/wiki/Rayleigh_quotient" title="Rayleigh quotient">Rayleigh quotient</a>).</p>
<p>PCA is equivalent to <a href="/wiki/Empirical_orthogonal_functions" title="Empirical orthogonal functions">empirical orthogonal functions</a> (EOF).</p>
<p>An <a href="/wiki/Autoencoder" title="Autoencoder" class="mw-redirect">autoencoder</a> <a href="/wiki/Artificial_neural_network" title="Artificial neural network">neural network</a> with a linear hidden layer is similar to PCA. Upon convergence, the weight vectors of the <i>K</i> neurons in the hidden layer will form a basis for the space spanned by the first <i>K</i> principal components. Unlike PCA, this technique will not necessarily produce <a href="/wiki/Orthogonal" title="Orthogonal" class="mw-redirect">orthogonal</a> vectors.</p>
<p>PCA is a popular technique in <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>. But it is not optimized for class separability<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup>. An alternative is the <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a>, which does take this into account. PCA optimally minimizes reconstruction error under the <a href="/wiki/Lp_space" title="Lp space">L2 norm</a>.</p>
<p><a name="Table_of_symbols_and_abbreviations" id="Table_of_symbols_and_abbreviations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=3" title="Edit section: Table of symbols and abbreviations">edit</a>]</span> <span class="mw-headline">Table of symbols and abbreviations</span></h2>
<table class="wikitable">
<tr>
<th>Symbol</th>
<th>Meaning</th>
<th>Dimensions</th>
<th>Indices</th>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{X} = \{ X[m,n] \}" src="http://upload.wikimedia.org/math/f/8/6/f867e7c98e184b05ed01eedf48d48da5.png" /></td>
<td>data matrix, consisting of the set of all data vectors, one vector per column</td>
<td><img class="tex" alt=" M \times N" src="http://upload.wikimedia.org/math/7/7/a/77a72b608a1beff4bf12b46d0ee0f3bb.png" /></td>
<td><img class="tex" alt=" m = 1 \ldots M " src="http://upload.wikimedia.org/math/4/d/0/4d08549f75bda4aa10c97dae0102f4f5.png" /><br />
<img class="tex" alt=" n = 1 \ldots N " src="http://upload.wikimedia.org/math/2/9/5/2952b718dcfb00ae3e968eced35aae04.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="N \," src="http://upload.wikimedia.org/math/b/e/e/beed584371120e11bf20723d0f22e52e.png" /></td>
<td>the number of column vectors in the data set</td>
<td><img class="tex" alt="1 \times 1" src="http://upload.wikimedia.org/math/b/9/9/b99a31f00d8eff828fb2b1657efe2f4b.png" /></td>
<td><i>scalar</i></td>
</tr>
<tr>
<td><img class="tex" alt="M \," src="http://upload.wikimedia.org/math/1/d/d/1dd4ab77983ec94cab2e7ff337a739e8.png" /></td>
<td>the number of elements in each column vector (dimension)</td>
<td><img class="tex" alt="1 \times 1" src="http://upload.wikimedia.org/math/b/9/9/b99a31f00d8eff828fb2b1657efe2f4b.png" /></td>
<td><i>scalar</i></td>
</tr>
<tr>
<td><img class="tex" alt="L \," src="http://upload.wikimedia.org/math/4/3/a/43afc2e242876990f6bf778f2a2278d7.png" /></td>
<td>the number of dimensions in the dimensionally reduced subspace, <img class="tex" alt=" 1 \le L \le M " src="http://upload.wikimedia.org/math/b/8/4/b84657493e92092fc1a8a364ff62270e.png" /></td>
<td><img class="tex" alt="1 \times 1" src="http://upload.wikimedia.org/math/b/9/9/b99a31f00d8eff828fb2b1657efe2f4b.png" /></td>
<td><i>scalar</i></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{u} = \{ u[m] \}" src="http://upload.wikimedia.org/math/5/c/8/5c8208a0df7d9bf4da7d597b1870f23d.png" /></td>
<td>vector of empirical <a href="/wiki/Mean" title="Mean">means</a>, one mean for each row <i>m</i> of the data matrix</td>
<td><img class="tex" alt=" M \times 1" src="http://upload.wikimedia.org/math/0/8/2/082f7fb86728e4c1c5d4c3053756b038.png" /></td>
<td><img class="tex" alt=" m = 1 \ldots M " src="http://upload.wikimedia.org/math/4/d/0/4d08549f75bda4aa10c97dae0102f4f5.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{s} = \{ s[m] \}" src="http://upload.wikimedia.org/math/c/6/7/c676f10f15d0881db172967871693572.png" /></td>
<td>vector of empirical <a href="/wiki/Standard_deviation" title="Standard deviation">standard deviations</a>, one standard deviation for each row <i>m</i> of the data matrix</td>
<td><img class="tex" alt=" M \times 1" src="http://upload.wikimedia.org/math/0/8/2/082f7fb86728e4c1c5d4c3053756b038.png" /></td>
<td><img class="tex" alt=" m = 1 \ldots M " src="http://upload.wikimedia.org/math/4/d/0/4d08549f75bda4aa10c97dae0102f4f5.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{h} = \{ h[n] \}" src="http://upload.wikimedia.org/math/0/5/c/05c290900324b15ef4123a2293a2c544.png" /></td>
<td>vector of all 1's</td>
<td><img class="tex" alt=" 1 \times N" src="http://upload.wikimedia.org/math/a/4/8/a48c766b7d3907922863031c5140af8b.png" /></td>
<td><img class="tex" alt=" n = 1 \ldots N " src="http://upload.wikimedia.org/math/2/9/5/2952b718dcfb00ae3e968eced35aae04.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{B} = \{ B[m,n] \}" src="http://upload.wikimedia.org/math/3/b/0/3b00e80fb5f767ceaba11c14ffbe7ffb.png" /></td>
<td><a href="/wiki/Deviation" title="Deviation">deviations</a> from the mean of each row <i>m</i> of the data matrix</td>
<td><img class="tex" alt=" M \times N" src="http://upload.wikimedia.org/math/7/7/a/77a72b608a1beff4bf12b46d0ee0f3bb.png" /></td>
<td><img class="tex" alt=" m = 1 \ldots M " src="http://upload.wikimedia.org/math/4/d/0/4d08549f75bda4aa10c97dae0102f4f5.png" /><br />
<img class="tex" alt=" n = 1 \ldots N " src="http://upload.wikimedia.org/math/2/9/5/2952b718dcfb00ae3e968eced35aae04.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{Z} = \{ Z[m,n] \} " src="http://upload.wikimedia.org/math/4/e/3/4e3dbd4fb1b5ef217ba716a1a90d2511.png" /></td>
<td><a href="/wiki/Z-score" title="Z-score" class="mw-redirect">z-scores</a>, computed using the mean and standard deviation for each row <i>m</i> of the data matrix</td>
<td><img class="tex" alt=" M \times N" src="http://upload.wikimedia.org/math/7/7/a/77a72b608a1beff4bf12b46d0ee0f3bb.png" /></td>
<td><img class="tex" alt=" m = 1 \ldots M " src="http://upload.wikimedia.org/math/4/d/0/4d08549f75bda4aa10c97dae0102f4f5.png" /><br />
<img class="tex" alt=" n = 1 \ldots N " src="http://upload.wikimedia.org/math/2/9/5/2952b718dcfb00ae3e968eced35aae04.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{C} = \{ C[p,q] \} " src="http://upload.wikimedia.org/math/a/2/a/a2ae59fecd9a30529c7ab8260e534f1a.png" /></td>
<td><a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a></td>
<td><img class="tex" alt=" M \times M " src="http://upload.wikimedia.org/math/6/2/7/627dfe3da8c871a94e9f209e28cc7121.png" /></td>
<td><img class="tex" alt=" p = 1 \ldots M " src="http://upload.wikimedia.org/math/e/e/8/ee89e25c5629e9ce5c55751102c22695.png" /><br />
<img class="tex" alt=" q = 1 \ldots M " src="http://upload.wikimedia.org/math/e/7/1/e71be601212cc102f41ab296a52ea483.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{R} = \{ R[p,q] \} " src="http://upload.wikimedia.org/math/0/4/2/042cc0a47cdc31cb96cd0720c0ac51b2.png" /></td>
<td><a href="/wiki/Correlation_matrix" title="Correlation matrix" class="mw-redirect">correlation matrix</a></td>
<td><img class="tex" alt=" M \times M " src="http://upload.wikimedia.org/math/6/2/7/627dfe3da8c871a94e9f209e28cc7121.png" /></td>
<td><img class="tex" alt=" p = 1 \ldots M " src="http://upload.wikimedia.org/math/e/e/8/ee89e25c5629e9ce5c55751102c22695.png" /><br />
<img class="tex" alt=" q = 1 \ldots M " src="http://upload.wikimedia.org/math/e/7/1/e71be601212cc102f41ab296a52ea483.png" /></td>
</tr>
<tr>
<td><img class="tex" alt=" \mathbf{V} = \{ V[p,q] \} " src="http://upload.wikimedia.org/math/0/5/f/05f86efc393e12e96b879e1b48e9fb4f.png" /></td>
<td>matrix consisting of the set of all <a href="/wiki/Eigenvectors" title="Eigenvectors" class="mw-redirect">eigenvectors</a> of <b>C</b>, one eigenvector per column</td>
<td><img class="tex" alt=" M \times M " src="http://upload.wikimedia.org/math/6/2/7/627dfe3da8c871a94e9f209e28cc7121.png" /></td>
<td><img class="tex" alt=" p = 1 \ldots M " src="http://upload.wikimedia.org/math/e/e/8/ee89e25c5629e9ce5c55751102c22695.png" /><br />
<img class="tex" alt=" q = 1 \ldots M " src="http://upload.wikimedia.org/math/e/7/1/e71be601212cc102f41ab296a52ea483.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{D} = \{ D[p,q] \} " src="http://upload.wikimedia.org/math/a/c/d/acdc2b89fb7f17e8ba1abbda6f4f0a01.png" /></td>
<td><a href="/wiki/Diagonal_matrix" title="Diagonal matrix">diagonal matrix</a> consisting of the set of all <a href="/wiki/Eigenvalues" title="Eigenvalues" class="mw-redirect">eigenvalues</a> of <b>C</b> along its <a href="/wiki/Principal_diagonal" title="Principal diagonal" class="mw-redirect">principal diagonal</a>, and 0 for all other elements</td>
<td><img class="tex" alt=" M \times M " src="http://upload.wikimedia.org/math/6/2/7/627dfe3da8c871a94e9f209e28cc7121.png" /></td>
<td><img class="tex" alt=" p = 1 \ldots M " src="http://upload.wikimedia.org/math/e/e/8/ee89e25c5629e9ce5c55751102c22695.png" /><br />
<img class="tex" alt=" q = 1 \ldots M " src="http://upload.wikimedia.org/math/e/7/1/e71be601212cc102f41ab296a52ea483.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{W} = \{ W[p,q] \} " src="http://upload.wikimedia.org/math/0/6/1/0616329127868d5b6c7a63513a096761.png" /></td>
<td>matrix of basis vectors, one vector per column, where each basis vector is one of the <a href="/wiki/Eigenvectors" title="Eigenvectors" class="mw-redirect">eigenvectors</a> of <b>C</b>, and where the vectors in <b>W</b> are a sub-set of those in <b>V</b></td>
<td><img class="tex" alt=" M \times L" src="http://upload.wikimedia.org/math/e/b/7/eb796e2c1bb1c75bbf23abf6aab2c3fc.png" /></td>
<td><img class="tex" alt=" p = 1 \ldots M " src="http://upload.wikimedia.org/math/e/e/8/ee89e25c5629e9ce5c55751102c22695.png" /><br />
<img class="tex" alt=" q = 1 \ldots L" src="http://upload.wikimedia.org/math/b/1/0/b1052cf45aaa22f121ca91086847b53b.png" /></td>
</tr>
<tr>
<td><img class="tex" alt="\mathbf{Y} = \{ Y[m,n] \} " src="http://upload.wikimedia.org/math/9/3/f/93ff4f2a52b9c57cb8d4d718c2ec8377.png" /></td>
<td>matrix consisting of <i>N</i> column vectors, where each vector is the projection of the corresponding data vector from matrix <b>X</b> onto the basis vectors contained in the columns of matrix <b>W</b>.</td>
<td><img class="tex" alt=" L \times N" src="http://upload.wikimedia.org/math/7/d/d/7ddb5c4baf504e06e73640cf19bfac01.png" /></td>
<td><img class="tex" alt=" m = 1 \ldots L " src="http://upload.wikimedia.org/math/a/c/0/ac08f0e10ca256f9b346ab33eb2dbf74.png" /><br />
<img class="tex" alt=" n = 1 \ldots N" src="http://upload.wikimedia.org/math/2/9/5/2952b718dcfb00ae3e968eced35aae04.png" /></td>
</tr>
</table>
<p><a name="Properties_and_Limitations_of_PCA" id="Properties_and_Limitations_of_PCA"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=4" title="Edit section: Properties and Limitations of PCA">edit</a>]</span> <span class="mw-headline">Properties and Limitations of PCA</span></h2>
<p>PCA is theoretically the optimal linear scheme, in terms of <a href="/wiki/Minimum_mean_square_error" title="Minimum mean square error">least mean square error</a>, for compressing a set of high dimensional vectors into a set of lower dimensional vectors and then reconstructing the original set. It is a <a href="/wiki/Non-parametric" title="Non-parametric" class="mw-redirect">non-parametric</a> analysis and the answer is unique and independent of any hypothesis about data <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a>. However, the latter two properties are regarded as weakness as well as strength, in that being non-parametric, no prior knowledge can be incorporated and that PCA compressions often incur loss of information.</p>
<p>The applicability of PCA is limited by the assumptions<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup> made in its derivation. These assumptions are:</p>
<ul>
<li>Assumption on Linearity</li>
</ul>
<p>We assumed the observed data set to be <a href="/wiki/Linear_combination" title="Linear combination">linear combinations</a> of certain basis. Non-linear methods such as <a href="/wiki/Kernel_principal_component_analysis" title="Kernel principal component analysis">kernel PCA</a> have been developed without assuming linearity.</p>
<ul>
<li>Assumption on the statistical importance of mean and covariance</li>
</ul>
<p>PCA uses the <a href="/wiki/Eigenvector" title="Eigenvector" class="mw-redirect">eigenvectors</a> of the <a href="/wiki/Covariance" title="Covariance">covariance</a> matrix and it only finds the independent axes of the data under the Gaussian assumption. For non-Gaussian or multi-modal Gaussian data, PCA simply de-correlates the axes. When PCA is used for clustering, its main limitation is that it does not account for class separability since it makes no use of the class label of the feature vector. There is no guarantee that the directions of maximum variance will contain good features for discrimination.</p>
<ul>
<li>Assumption that large variances have important dynamics</li>
</ul>
<p>PCA simply performs a coordinate rotation that aligns the transformed axes with the directions of maximum variance. It is only when we believe that the observed data has a high signal-to-noise ratio that the principal components with larger variance correspond to interesting dynamics and lower ones correspond to noise.</p>
<p>Essentially, PCA involves only rotation and scaling. The above assumptions are made in order to simplify the algebraic computation on the data set. Some other methods have been developed without one or more of these assumptions; these are described below.</p>
<p><a name="Computing_PCA_using_the_Covariance_Method" id="Computing_PCA_using_the_Covariance_Method"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=5" title="Edit section: Computing PCA using the Covariance Method">edit</a>]</span> <span class="mw-headline">Computing PCA using the Covariance Method</span></h2>
<p>Following is a detailed description of PCA using the covariance method. The goal is to transform a given data set <b>X</b> of dimension <i>M</i> to an alternative data set <b>Y</b> of smaller dimension <i>L</i>. Equivalently, we are seeking to find the matrix <b>Y</b>, where <b>Y</b> is the Karhunen–Loeve transform (KLT) of matrix <b>X</b>:</p>
<dl>
<dd><img class="tex" alt=" \mathbf{Y} = \mathbb{KLT} \{ \mathbf{X} \} " src="http://upload.wikimedia.org/math/2/5/8/258f9edf465e35e61c82416fd98f0657.png" /></dd>
</dl>
<p><a name="Organize_the_data_set" id="Organize_the_data_set"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=6" title="Edit section: Organize the data set">edit</a>]</span> <span class="mw-headline">Organize the data set</span></h3>
<p><b>Suppose</b> you have data comprising a set of observations of <i>M</i> variables, and you want to reduce the data so that each observation can be described with only <i>L</i> variables, <i>L</i> &lt; <i>M</i>. Suppose further, that the data are arranged as a set of <i>N</i> data vectors <img class="tex" alt="\mathbf{x}_1 \ldots \mathbf{x}_N" src="http://upload.wikimedia.org/math/c/e/a/cea8f38160bedce54cb178149a830181.png" /> with each <img class="tex" alt="\mathbf{x}_n " src="http://upload.wikimedia.org/math/8/e/1/8e11284eb0afcf6f88e1cdce8b5a26d0.png" /> representing a single grouped observation of the <i>M</i> variables.</p>
<ul>
<li>Write <img class="tex" alt="\mathbf{x}_1 \ldots \mathbf{x}_N" src="http://upload.wikimedia.org/math/c/e/a/cea8f38160bedce54cb178149a830181.png" /> as column vectors, each of which has <i>M</i> rows.</li>
<li>Place the column vectors into a single matrix <b>X</b> of dimensions <i>M</i> × <i>N</i>.</li>
</ul>
<p><a name="Calculate_the_empirical_mean" id="Calculate_the_empirical_mean"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=7" title="Edit section: Calculate the empirical mean">edit</a>]</span> <span class="mw-headline">Calculate the empirical mean</span></h3>
<ul>
<li>Find the empirical mean along each dimension <i>m</i> = 1,&#160;...,&#160;<i>M</i>.</li>
<li>Place the calculated mean values into an empirical mean vector <b>u</b> of dimensions <i>M</i> × 1.</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="u[m] = {1 \over N} \sum_{n=1}^N X[m,n] " src="http://upload.wikimedia.org/math/8/9/5/8957f38829f7b6c873a6248176f16b58.png" /></dd>
</dl>
</dd>
</dl>
<p><a name="Calculate_the_deviations_from_the_mean" id="Calculate_the_deviations_from_the_mean"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=8" title="Edit section: Calculate the deviations from the mean">edit</a>]</span> <span class="mw-headline">Calculate the deviations from the mean</span></h3>
<p>Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup>. Hence we proceed by centering the data as follows:</p>
<ul>
<li>Subtract the empirical mean vector <b>u</b> from each column of the data matrix <b>X</b>.</li>
<li>Store mean-subtracted data in the <i>M</i> × <i>N</i> matrix <b>B</b>.</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="\mathbf{B} = \mathbf{X} - \mathbf{u}\mathbf{h} " src="http://upload.wikimedia.org/math/3/e/c/3ec81b924c855d2ae0d7becc1c11bb7d.png" /></dd>
<dd>where <b>h</b> is a 1 x <i>N</i> row vector of all 1's:</dd>
</dl>
</dd>
</dl>
<dl>
<dd>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="h[n] = 1 \, \qquad \qquad \mathrm{for \ } n = 1 \ldots N " src="http://upload.wikimedia.org/math/1/7/a/17a2338fabe95808174754e820755123.png" /></dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p><a name="Find_the_covariance_matrix" id="Find_the_covariance_matrix"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=9" title="Edit section: Find the covariance matrix">edit</a>]</span> <span class="mw-headline">Find the covariance matrix</span></h3>
<ul>
<li>Find the <i>M</i> × <i>M</i> empirical covariance matrix <b>C</b> from the <a href="/wiki/Outer_product" title="Outer product">outer product</a> of matrix <b>B</b> with itself:</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="\mathbf{C} = \mathbb{ E } \left[ \mathbf{B} \otimes \mathbf{B} \right] = \mathbb{ E } \left[ \mathbf{B} \cdot \mathbf{B}^{*} \right] = { 1 \over N } \mathbf{B} \cdot \mathbf{B}^{*}" src="http://upload.wikimedia.org/math/5/d/7/5d799134e511a494447178b8128021c6.png" /></dd>
<dd>where
<dl>
<dd><img class="tex" alt="\mathbb{E} " src="http://upload.wikimedia.org/math/5/6/b/56bda1c0c911f27b99dec7ff663a12c2.png" /> is the <a href="/wiki/Expected_value" title="Expected value">expected value</a> operator,</dd>
<dd><img class="tex" alt=" \otimes " src="http://upload.wikimedia.org/math/e/9/d/e9dd9013ec300ceba41484dfc2c9a876.png" /> is the <a href="/wiki/Outer_product" title="Outer product">outer product</a> operator, and</dd>
<dd><img class="tex" alt=" * \ " src="http://upload.wikimedia.org/math/d/1/0/d104d819cb49fadb70952da66a0dc281.png" /> is the <a href="/wiki/Conjugate_transpose" title="Conjugate transpose">conjugate transpose</a> operator. Note that if B consists entirely of real numbers, which is the case in many applications, the "conjugate transpose" is the same as the regular <a href="/wiki/Transpose" title="Transpose">transpose</a>.</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<ul>
<li>Please note that the information in this section is indeed a bit fuzzy. See the covariance matrix sections on the discussion page for more information.</li>
</ul>
<p><a name="Find_the_eigenvectors_and_eigenvalues_of_the_covariance_matrix" id="Find_the_eigenvectors_and_eigenvalues_of_the_covariance_matrix"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=10" title="Edit section: Find the eigenvectors and eigenvalues of the covariance matrix">edit</a>]</span> <span class="mw-headline">Find the eigenvectors and eigenvalues of the covariance matrix</span></h3>
<ul>
<li>Compute the matrix <b>V</b> of <a href="/wiki/Eigenvector" title="Eigenvector" class="mw-redirect">eigenvectors</a> which <a href="/wiki/Diagonalizable_matrix" title="Diagonalizable matrix">diagonalizes</a> the covariance matrix <b>C</b>:</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="\mathbf{V}^{-1} \mathbf{C} \mathbf{V} = \mathbf{D} " src="http://upload.wikimedia.org/math/3/c/b/3cb46a0530b1320c6a7bc3bd72b7ed3f.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>where <b>D</b> is the <a href="/wiki/Diagonal_matrix" title="Diagonal matrix">diagonal matrix</a> of <a href="/wiki/Eigenvalue" title="Eigenvalue" class="mw-redirect">eigenvalues</a> of <b>C</b>. This step will typically involve the use of a computer-based algorithm for computing eigenvectors and eigenvalues. These algorithms are readily available as sub-components of most <a href="/wiki/Matrix_algebra" title="Matrix algebra">matrix algebra</a> systems, such as <a href="/wiki/MATLAB" title="MATLAB">MATLAB</a><sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup>, <a href="/wiki/Mathematica" title="Mathematica">Mathematica</a><sup id="cite_ref-7" class="reference"><a href="#cite_note-7" title=""><span>[</span>8<span>]</span></a></sup>, <a href="/wiki/SciPy" title="SciPy">SciPy</a>, <a href="/wiki/IDL" title="IDL">IDL</a>(<a href="/wiki/Interactive_Data_Language" title="Interactive Data Language" class="mw-redirect">Interactive Data Language</a>), or <a href="/wiki/GNU_Octave" title="GNU Octave">GNU Octave</a>.</dd>
</dl>
<ul>
<li>Matrix <b>D</b> will take the form of an <i>M</i> × <i>M</i> <a href="/wiki/Diagonal_matrix" title="Diagonal matrix">diagonal matrix</a>, where</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="D[p,q] = \lambda_m \qquad \mathrm{for} \qquad p = q = m" src="http://upload.wikimedia.org/math/e/6/4/e64dd570e3822288e9e85e0e21bda469.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>is the <i>m</i>th eigenvalue of the covariance matrix <b>C</b>, and</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="D[p,q] = 0 \qquad \mathrm{for} \qquad p \ne q." src="http://upload.wikimedia.org/math/b/7/f/b7f35034926231465eb6593364e75729.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>Matrix <b>V</b>, also of dimension <i>M</i> × <i>M</i>, contains <i>M</i> column vectors, each of length <i>M</i>, which represent the <i>M</i> eigenvectors of the covariance matrix <b>C</b>.</li>
<li>The eigenvalues and eigenvectors are ordered and paired. The <i>m</i>th eigenvalue corresponds to the <i>m</i>th eigenvector.</li>
</ul>
<p><a name="Rearrange_the_eigenvectors_and_eigenvalues" id="Rearrange_the_eigenvectors_and_eigenvalues"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=11" title="Edit section: Rearrange the eigenvectors and eigenvalues">edit</a>]</span> <span class="mw-headline">Rearrange the eigenvectors and eigenvalues</span></h3>
<ul>
<li>Sort the columns of the eigenvector matrix <b>V</b> and eigenvalue matrix <b>D</b> in order of <i>decreasing</i> eigenvalue.</li>
<li>Make sure to maintain the correct pairings between the columns in each matrix.</li>
</ul>
<p><a name="Compute_the_cumulative_energy_content_for_each_eigenvector" id="Compute_the_cumulative_energy_content_for_each_eigenvector"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=12" title="Edit section: Compute the cumulative energy content for each eigenvector">edit</a>]</span> <span class="mw-headline">Compute the cumulative energy content for each eigenvector</span></h3>
<ul>
<li>The eigenvalues represent the distribution of the source data's energy among each of the eigenvectors, where the eigenvectors form a <a href="/wiki/Basis_(linear_algebra)" title="Basis (linear algebra)">basis</a> for the data. The cumulative energy content <i>g</i> for the <i>m</i>th eigenvector is the sum of the energy content across all of the eigenvalues from 1 through <i>m</i>:</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="g[m] = \sum_{q=1}^m D[p,q] \qquad \mathrm{for} \qquad p = q \qquad \mathrm{and} \qquad m = 1,\dots,M " src="http://upload.wikimedia.org/math/d/a/6/da66b77a83e247f0bfb0f4183d039698.png" /></dd>
</dl>
</dd>
</dl>
<p><a name="Select_a_subset_of_the_eigenvectors_as_basis_vectors" id="Select_a_subset_of_the_eigenvectors_as_basis_vectors"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=13" title="Edit section: Select a subset of the eigenvectors as basis vectors">edit</a>]</span> <span class="mw-headline">Select a subset of the eigenvectors as basis vectors</span></h3>
<ul>
<li>Save the first <i>L</i> columns of <b>V</b> as the <i>M</i> × <i>L</i> matrix <b>W</b>:</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" W[p,q] = V[p,q] \qquad \mathrm{for} \qquad p = 1,\dots,M \qquad q = 1,\dots,L " src="http://upload.wikimedia.org/math/5/a/7/5a77ea6fd7f837c31fc3726421950b56.png" /></dd>
</dl>
</dd>
</dl>
<dl>
<dd>where</dd>
</dl>
<dl>
<dd>
<dl>
<dd><img class="tex" alt="1 \leq L \leq M." src="http://upload.wikimedia.org/math/6/1/b/61b8fda08d84949e36a83dd6dd7c1d76.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>Use the vector <b>g</b> as a guide in choosing an appropriate value for <i>L</i>. The goal is to choose as small a value of <i>L</i> as possible while achieving a reasonably high value of <i>g</i> on a percentage basis. For example, you may want to choose <i>L</i> so that the cumulative energy <i>g</i> is above a certain threshold, like 90 percent. In this case, choose the smallest value of <i>L</i> such that</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" g[m=L] \ge 90%\, " src="http://upload.wikimedia.org/math/f/5/3/f534234e2b3f080c025e414e8438ebf1.png" /></dd>
</dl>
</dd>
</dl>
<p><a name="Convert_the_source_data_to_z-scores" id="Convert_the_source_data_to_z-scores"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=14" title="Edit section: Convert the source data to z-scores">edit</a>]</span> <span class="mw-headline">Convert the source data to z-scores</span></h3>
<ul>
<li>Create an <i>M</i> × 1 empirical standard deviation vector <b>s</b> from the square root of each element along the main diagonal of the covariance matrix <b>C</b>:</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" \mathbf{s} = \{ s[m] \} = \sqrt{C[p,q]} \qquad \mathrm{for \ } p = q = m = 1 \ldots M " src="http://upload.wikimedia.org/math/b/7/7/b7721571befb1ed62f64d0a3fa49f426.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>Calculate the <i>M</i> × <i>N</i> <a href="/wiki/Standard_score" title="Standard score">z-score</a> matrix:</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" \mathbf{Z} = { \mathbf{B} \over \mathbf{s} \cdot \mathbf{h} } " src="http://upload.wikimedia.org/math/4/d/d/4dda8c62ae717e6c2a16449e3e2747c6.png" /> (divide element-by-element)</dd>
</dl>
</dd>
</dl>
<ul>
<li>Note: While this step is useful for various applications as it normalizes the data set with respect to its variance, it is not integral part of PCA/KLT!</li>
</ul>
<p><a name="Project_the_z-scores_of_the_data_onto_the_new_basis" id="Project_the_z-scores_of_the_data_onto_the_new_basis"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=15" title="Edit section: Project the z-scores of the data onto the new basis">edit</a>]</span> <span class="mw-headline">Project the z-scores of the data onto the new basis</span></h3>
<ul>
<li>The projected vectors are the columns of the matrix</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" \mathbf{Y} = \mathbf{W}^* \cdot \mathbf{Z} = \mathbb{KLT} \{ \mathbf{X} \}." src="http://upload.wikimedia.org/math/c/1/d/c1d186cd6743d216ae15913a17c53c9e.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>The columns of matrix <b>Y</b> represent the Karhunen-Loeve transforms (KLT) of the data vectors in the columns of matrix <b>X</b>.</li>
</ul>
<p><a name="Derivation_of_PCA_using_the_covariance_method" id="Derivation_of_PCA_using_the_covariance_method"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=16" title="Edit section: Derivation of PCA using the covariance method">edit</a>]</span> <span class="mw-headline"><i>Derivation</i> of PCA using the covariance method</span></h2>
<p>Let <b>X</b> be a <i>d</i>-dimensional random vector expressed as column vector. Without loss of generality, assume <b>X</b> has zero empirical mean. We want to find a <img class="tex" alt="d \times d" src="http://upload.wikimedia.org/math/c/0/d/c0d8b4b800fcd5f789307e7b57bd6c2f.png" /> <a href="/wiki/Orthonormal_basis" title="Orthonormal basis">orthonormal transformation matrix</a> <b>P</b> such that</p>
<dl>
<dd><img class="tex" alt="\mathbf{Y} = \mathbf{P}^\top \mathbf{X}" src="http://upload.wikimedia.org/math/7/8/7/7872a5de26a89bb859486e092eba45f4.png" /></dd>
</dl>
<p>with the constraint that</p>
<dl>
<dd><img class="tex" alt="\operatorname{cov}(\mathbf{Y})" src="http://upload.wikimedia.org/math/0/d/a/0daf46117b30fb9fad55fcaf86220abe.png" /> is a <a href="/wiki/Diagonal_matrix" title="Diagonal matrix">diagonal matrix</a> and <img class="tex" alt="\mathbf{P}^{-1} = \mathbf{P}^\top." src="http://upload.wikimedia.org/math/1/a/6/1a6dff08b94c7ce5532cb1a7bf724c5c.png" /></dd>
</dl>
<p>By substitution, and matrix algebra, we obtain:</p>
<dl>
<dd><img class="tex" alt="
\begin{matrix}
\operatorname{cov}(\mathbf{Y}) &amp;=&amp; \mathbb{E}[ \mathbf{Y} \mathbf{Y}^\top]\\
\ &amp;=&amp; \mathbb{E}[( \mathbf{P}^\top \mathbf{X} ) ( \mathbf{P}^\top \mathbf{X} )^\top]\\
\ &amp;=&amp; \mathbb{E}[(\mathbf{P}^\top \mathbf{X}) (\mathbf{X}^\top \mathbf{P})] \\
\ &amp;=&amp; \mathbf{P}^\top \mathbb{E}[\mathbf{X} \mathbf{X}^\top] \mathbf{P} \\
\ &amp;=&amp; \mathbf{P}^\top \operatorname{cov}(\mathbf{X}) \mathbf{P}
\end{matrix}
" src="http://upload.wikimedia.org/math/1/9/d/19d1d26bfd1a71054cb397c331787f83.png" /></dd>
</dl>
<p>We now have:</p>
<dl>
<dd><img class="tex" alt="
\begin{matrix}
\mathbf{P}\operatorname{cov}(\mathbf{Y}) &amp;=&amp; \mathbf{P} \mathbf{P}^\top \operatorname{cov}(\mathbf{X}) \mathbf{P}\\
\ &amp;=&amp; \operatorname{cov}(\mathbf{X}) \mathbf{P}\\
\end{matrix}
" src="http://upload.wikimedia.org/math/e/2/9/e2909491849756cb28d6587694c11a8c.png" /></dd>
</dl>
<p>Rewrite <b>P</b> as d <img class="tex" alt="d \times 1" src="http://upload.wikimedia.org/math/0/e/a/0ea4bb5d9ea9d5b56be8e2a1007cf73e.png" /> column vectors, so</p>
<dl>
<dd><img class="tex" alt="\mathbf{P} = [P_1, P_2, \ldots, P_d]" src="http://upload.wikimedia.org/math/c/1/7/c1703cd93c71377fe45dab83bcb23dc4.png" /></dd>
</dl>
<p>and <img class="tex" alt="\operatorname{cov}(\mathbf{Y})" src="http://upload.wikimedia.org/math/0/d/a/0daf46117b30fb9fad55fcaf86220abe.png" /> as:</p>
<dl>
<dd><img class="tex" alt="
\begin{bmatrix}
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_d
\end{bmatrix}.
" src="http://upload.wikimedia.org/math/2/0/2/202fb379b1b540978fda04d18ecf05cd.png" /></dd>
</dl>
<p>Substituting into equation above, we obtain:</p>
<dl>
<dd><img class="tex" alt="[\lambda_1 P_1, \lambda_2 P_2, \ldots, \lambda_d P_d] =
[\operatorname{cov}(\mathbf{X})P_1, \operatorname{cov}(\mathbf{X})P_2,
\ldots, \operatorname{cov}(\mathbf{X})P_d]." src="http://upload.wikimedia.org/math/3/6/5/365d7a86dbcc0a19ec18400fc11f417a.png" /></dd>
</dl>
<p>Notice that in <img class="tex" alt="\lambda_i P_i = \operatorname{cov}(\mathbf{X})P_i" src="http://upload.wikimedia.org/math/a/0/0/a0057fd51ca1fda7ea2d851efdc4bf6b.png" />, <i>P</i><sub><i>i</i></sub> is an <a href="/wiki/Eigenvector" title="Eigenvector" class="mw-redirect">eigenvector</a> of the covariance matrix of <b>X</b>. Therefore, by finding the eigenvectors of the covariance matrix of <b>X</b>, we find a projection matrix <b>P</b> that satisfies the original constraints.</p>
<p><a name="Relation_between_PCA_and_K-means_clustering" id="Relation_between_PCA_and_K-means_clustering"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=17" title="Edit section: Relation between PCA and K-means clustering">edit</a>]</span> <span class="mw-headline">Relation between PCA and K-means clustering</span></h2>
<p>It has been shown recently (2007) <sup id="cite_ref-8" class="reference"><a href="#cite_note-8" title=""><span>[</span>9<span>]</span></a></sup> <sup id="cite_ref-9" class="reference"><a href="#cite_note-9" title=""><span>[</span>10<span>]</span></a></sup> that the relaxed solution of <a href="/wiki/K-means_clustering" title="K-means clustering" class="mw-redirect">K-means clustering</a>, specified by the cluster indicators, is given by the PCA principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace specified by the between-class <a href="/wiki/Scatter_matrix" title="Scatter matrix">scatter matrix</a>. Thus PCA automatically projects to the subspace where the global solution of K-means clustering lie, and thus facilitate K-means clustering to find near-optimal solutions.</p>
<p><a name="Correspondence_analysis" id="Correspondence_analysis"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=18" title="Edit section: Correspondence analysis">edit</a>]</span> <span class="mw-headline">Correspondence analysis</span></h2>
<p><b>Correspondence analysis</b> (CA) was developed by <a href="/wiki/Jean-Paul_Benz%C3%A9cri" title="Jean-Paul Benzécri">Jean-Paul Benzécri</a><sup id="cite_ref-10" class="reference"><a href="#cite_note-10" title=""><span>[</span>11<span>]</span></a></sup> and is conceptually similar to PCA, but scales the data (which must be positive) so that rows and columns are treated equivalently. It is traditionally applied to <a href="/wiki/Contingency_tables" title="Contingency tables" class="mw-redirect">contingency tables</a>. CA decomposes the <a href="/wiki/Chi-square" title="Chi-square" class="mw-redirect">Chi-square</a> statistic associated to this table into orthogonal factors<sup id="cite_ref-11" class="reference"><a href="#cite_note-11" title=""><span>[</span>12<span>]</span></a></sup>. Because CA is a descriptive technique, it can be applied to tables for which the Chi-square statistic is appropriate or not. Several variants of CA are available including <a href="/wiki/Detrended_Correspondence_Analysis" title="Detrended Correspondence Analysis" class="mw-redirect">Detrended Correspondence Analysis</a> and <a href="/w/index.php?title=Canonical_Correspondence_Analysis&amp;action=edit&amp;redlink=1" class="new" title="Canonical Correspondence Analysis (page does not exist)">Canonical Correspondence Analysis</a>.</p>
<p><a name="Generalizations" id="Generalizations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=19" title="Edit section: Generalizations">edit</a>]</span> <span class="mw-headline">Generalizations</span></h2>
<p><a name="Nonlinear_generalizations" id="Nonlinear_generalizations"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=20" title="Edit section: Nonlinear generalizations">edit</a>]</span> <span class="mw-headline">Nonlinear generalizations</span></h3>
<p>Most of the modern methods for <a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">nonlinear dimensionality reduction</a> find their theoretical and algorithmic roots in PCA or K-means. The original Pearson's idea was to take a straight line (or plane) which will be "the best fit" to a set of data points. <b>Principal <a href="/wiki/Curve" title="Curve">curves</a> and <a href="/wiki/Manifold" title="Manifold">manifolds</a></b> give the natural geometric framework for PCA generalization and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold for data <a href="/wiki/Approximation" title="Approximation">approximation</a>, and by encoding using standard geometric <a href="/wiki/Projection_(mathematics)" title="Projection (mathematics)">projection</a> onto the manifold<sup id="cite_ref-12" class="reference"><a href="#cite_note-12" title=""><span>[</span>13<span>]</span></a></sup>. See <a href="/wiki/Principal_geodesic_analysis" title="Principal geodesic analysis">principal geodesic analysis</a>.</p>
<p><a name="Higher_order" id="Higher_order"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=21" title="Edit section: Higher order">edit</a>]</span> <span class="mw-headline">Higher order</span></h3>
<p>N-way principal component analysis may be performed with models like <a href="/wiki/PARAFAC" title="PARAFAC">PARAFAC</a> and <a href="/wiki/Tucker_decomposition" title="Tucker decomposition">Tucker decomposition</a>.</p>
<p><a name="Software.2Fsource_code" id="Software.2Fsource_code"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=22" title="Edit section: Software/source code">edit</a>]</span> <span class="mw-headline">Software/source code</span></h2>
<ul>
<li><a href="http://www.mdp.edu.ar/psicologia/vista/vista.htm" class="external text" title="http://www.mdp.edu.ar/psicologia/vista/vista.htm" rel="nofollow">"ViSta: The Visual Statistics System"</a> a free software that provides principal components analysis, simple and multiple correspondence analysis.</li>
<li><a href="http://www.coloritto.com" class="external text" title="http://www.coloritto.com" rel="nofollow">"Spectramap"</a> is software to create a <a href="/wiki/Biplot" title="Biplot">biplot</a> using principal components analysis, correspondence analysis or spectral map analysis.</li>
<li><a href="http://sourceforge.net/projects/opencvlibrary/" class="external text" title="http://sourceforge.net/projects/opencvlibrary/" rel="nofollow">Computer Vision Library</a></li>
<li><a href="http://astro.u-strasbg.fr/~fmurtagh/mda-sw/" class="external text" title="http://astro.u-strasbg.fr/~fmurtagh/mda-sw/" rel="nofollow">Multivariate Data Analysis Software</a></li>
<li>in <a href="/wiki/MATLAB" title="MATLAB">Matlab</a>, the functions "princomp" and "wmspca" give the principal components</li>
<li>in <a href="/wiki/GNU_Octave" title="GNU Octave">Octave</a>, the free software equivalent to <a href="/wiki/MATLAB" title="MATLAB">Matlab</a>, the function <a href="http://octave.sourceforge.net/doc/statistics.html" class="external text" title="http://octave.sourceforge.net/doc/statistics.html" rel="nofollow">princomp</a> gives the principal component</li>
<li>in the open source statistical package <a href="/wiki/R_(programming_language)" title="R (programming language)">R</a>, the functions <a href="http://rweb.stat.umn.edu/R/library/stats/html/princomp.html" class="external text" title="http://rweb.stat.umn.edu/R/library/stats/html/princomp.html" rel="nofollow">"princomp"</a> and <a href="http://rweb.stat.umn.edu/R/library/stats/html/prcomp.html" class="external text" title="http://rweb.stat.umn.edu/R/library/stats/html/prcomp.html" rel="nofollow">"prcomp"</a> can be used for principal component analysis; prcomp uses <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a> which generally gives better numerical accuracy.</li>
<li><a href="http://www.datascope.be/spm_page.htm" class="external text" title="http://www.datascope.be/spm_page.htm" rel="nofollow">"spm"</a> is a generic package developed in <a href="/wiki/R_(programming_language)" title="R (programming language)">R</a> for multivariate projection methods that allows principal components analysis, correspondence analysis, and spectral map analysis</li>
<li>In <i>XLMiner</i>, the Principles Component tab can be used for principal component analysis.</li>
<li><a href="http://www.scilab.org" class="external text" title="http://www.scilab.org" rel="nofollow">SciLab</a></li>
<li>In <a href="/wiki/IDL_(programming_language)" title="IDL (programming language)">IDL</a>, the principal components can be calculated using the function pcomp.</li>
<li><a href="/wiki/Weka_(machine_learning)" title="Weka (machine learning)">Weka</a> computes principal components (<a href="http://weka.sourceforge.net/doc/weka/attributeSelection/PrincipalComponents.html" class="external text" title="http://weka.sourceforge.net/doc/weka/attributeSelection/PrincipalComponents.html" rel="nofollow">javadoc</a>).</li>
</ul>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=23" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFPearson.2C_K.1901">Pearson, K. (1901). "<a href="http://stat.smmu.edu.cn/history/pearson1901.pdf" class="external text" title="http://stat.smmu.edu.cn/history/pearson1901.pdf" rel="nofollow">On Lines and Planes of Closest Fit to Systems of Points in Space</a>" (PDF). <i>Philosophical Magazine</i> <b>2</b> (6): 559–572<span class="printonly">. <a href="http://stat.smmu.edu.cn/history/pearson1901.pdf" class="external free" title="http://stat.smmu.edu.cn/history/pearson1901.pdf" rel="nofollow">http://stat.smmu.edu.cn/history/pearson1901.pdf</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=On+Lines+and+Planes+of+Closest+Fit+to+Systems+of+Points+in+Space&amp;rft.jtitle=Philosophical+Magazine&amp;rft.aulast=Pearson%2C+K.&amp;rft.au=Pearson%2C+K.&amp;rft.date=1901&amp;rft.volume=2&amp;rft.issue=6&amp;rft.pages=559%E2%80%93572&amp;rft_id=http%3A%2F%2Fstat.smmu.edu.cn%2Fhistory%2Fpearson1901.pdf&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> Jolliffe I.T. <a href="http://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0" class="external text" title="http://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0" rel="nofollow">Principal Component Analysis</a>, Series: <a href="http://www.springer.com/west/home/statistics/statistical+theory+and+methods?SGWID=4-10129-69-173621571-0" class="external text" title="http://www.springer.com/west/home/statistics/statistical+theory+and+methods?SGWID=4-10129-69-173621571-0" rel="nofollow">Springer Series in Statistics</a>, 2nd ed., Springer, NY, 2002, XXIX, 487 p. 28 illus. <a href="/wiki/Special:BookSources/9780387954424" class="internal">ISBN 978-0-387-95442-4</a></li>
<li id="cite_note-2"><b><a href="#cite_ref-2" title="">^</a></b> A.A. Miranda, Y.-A. Le Borgne, and G. Bontempi. <a href="http://www.springerlink.com/content/w6114741w8728567" class="external text" title="http://www.springerlink.com/content/w6114741w8728567" rel="nofollow">New Routes from Minimal Approximation Error to Principal Components</a>, Volume 27, Number 3 / June, 2008, Neural Processing Letters, Springer</li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFFukunaga.2C_Keinosuke1990">Fukunaga, Keinosuke (1990). <i><a href="http://books.google.com/books?visbn=0122698517" class="external text" title="http://books.google.com/books?visbn=0122698517" rel="nofollow">Introduction to Statistical Pattern Recognition</a></i>. Elsevier<span class="printonly">. <a href="http://books.google.com/books?visbn=0122698517" class="external free" title="http://books.google.com/books?visbn=0122698517" rel="nofollow">http://books.google.com/books?visbn=0122698517</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Statistical+Pattern+Recognition&amp;rft.aulast=Fukunaga%2C+Keinosuke&amp;rft.au=Fukunaga%2C+Keinosuke&amp;rft.date=1990&amp;rft.pub=Elsevier&amp;rft_id=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fvisbn%3D0122698517&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> Jon Shlens, <a href="http://www.cs.cmu.edu/~elaw/papers/pca.pdf" class="external text" title="http://www.cs.cmu.edu/~elaw/papers/pca.pdf" rel="nofollow">A Tutorial on Principal Component Analysis.</a></li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> A.A. Miranda, Y.-A. Le Borgne, and G. Bontempi. <a href="http://www.springerlink.com/content/w6114741w8728567" class="external text" title="http://www.springerlink.com/content/w6114741w8728567" rel="nofollow">New Routes from Minimal Approximation Error to Principal Components</a>, Volume 27, Number 3 / June, 2008, Neural Processing Letters, Springer</li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> <a href="http://www.mathworks.com/access/helpdesk/help/techdoc/ref/eig.html#998306" class="external text" title="http://www.mathworks.com/access/helpdesk/help/techdoc/ref/eig.html#998306" rel="nofollow">eig function</a> Matlab documentation</li>
<li id="cite_note-7"><b><a href="#cite_ref-7" title="">^</a></b> <a href="http://reference.wolfram.com/mathematica/ref/Eigenvalues.html" class="external text" title="http://reference.wolfram.com/mathematica/ref/Eigenvalues.html" rel="nofollow">Eigenvalues function</a> Mathematica documentation</li>
<li id="cite_note-8"><b><a href="#cite_ref-8" title="">^</a></b> H. Zha, C. Ding, M. Gu, X. He and H.D. Simon. "Spectral Relaxation for K-means Clustering", <a href="http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf" class="external free" title="http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf" rel="nofollow">http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf</a>, Neural Information Processing Systems vol.14 (NIPS 2001). pp. 1057-1064, Vancouver, Canada. Dec. 2001.</li>
<li id="cite_note-9"><b><a href="#cite_ref-9" title="">^</a></b> C. Ding and X. He. "K-means Clustering via Principal Component Analysis". Proc. of Int'l Conf. Machine Learning (ICML 2004), pp 225-232. July 2004. <a href="http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf" class="external free" title="http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf" rel="nofollow">http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf</a></li>
<li id="cite_note-10"><b><a href="#cite_ref-10" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFBenz.C3.A9cri.2C_J.-P.1973">Benzécri, J.-P. (1973). <i>L'Analyse des Données. Volume II. L'Analyse des Correspondences</i>. Paris, France: Dunod.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=L%27Analyse+des+Donn%C3%A9es.+Volume+II.+L%27Analyse+des+Correspondences&amp;rft.aulast=Benz%C3%A9cri%2C+J.-P.&amp;rft.au=Benz%C3%A9cri%2C+J.-P.&amp;rft.date=1973&amp;rft.place=Paris%2C+France&amp;rft.pub=Dunod&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-11"><b><a href="#cite_ref-11" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFGreenacre.2C_Michael1983">Greenacre, Michael (1983). <i>Theory and Applications of Correspondence Analysis</i>. London: Academic Press. <a href="/wiki/Special:BookSources/0122990501" class="internal">ISBN 0-12-299050-1</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Theory+and+Applications+of+Correspondence+Analysis&amp;rft.aulast=Greenacre%2C+Michael&amp;rft.au=Greenacre%2C+Michael&amp;rft.date=1983&amp;rft.place=London&amp;rft.pub=Academic+Press&amp;rft.isbn=0-12-299050-1&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-12"><b><a href="#cite_ref-12" title="">^</a></b> A. Gorban, B. Kegl, D. Wunsch, A. Zinovyev (Eds.), <a href="http://pca.narod.ru/contentsgkwz.htm" class="external text" title="http://pca.narod.ru/contentsgkwz.htm" rel="nofollow">Principal Manifolds for Data Visualisation and Dimension Reduction,</a> LNCSE 58, Springer, Berlin – Heidelberg – New York, 2007. <a href="/wiki/Special:BookSources/9783540737490" class="internal">ISBN 978-3-540-73749-0</a></li>
</ol>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=24" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li>R. Kramer, Chemometric Techniques for Quantitative Analysis, (1998) Marcel-Dekker, <a href="/wiki/Special:BookSources/0824701984" class="internal">ISBN 0-8247-0198-4</a>.</li>
<li>Shaw PJA, Multivariate statistics for the Environmental Sciences, (2003) Hodder-Arnold.</li>
<li>Patra sk et al., J- Photochemistry &amp; Photobiology A:Chemistry, (1999) 122:23–31</li>
</ul>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=25" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<div style="-moz-column-count:2; column-count:2;">
<ul>
<li><a href="/wiki/Sparse_PCA" title="Sparse PCA">Sparse PCA</a></li>
<li><a href="/wiki/Biplot" title="Biplot">Biplot</a></li>
<li><a href="/wiki/Eigenface" title="Eigenface">Eigenface</a></li>
<li><a href="http://en.wikiversity.org/wiki/Exploratory_factor_analysis" class="extiw" title="v:Exploratory factor analysis">Exploratory factor analysis</a> (Wikiversity)</li>
<li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Geometric_data_analysis" title="Geometric data analysis">Geometric data analysis</a></li>
<li><a href="/wiki/Factorial_code" title="Factorial code">Factorial code</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a></li>
<li><a href="/wiki/Kernel_PCA" title="Kernel PCA" class="mw-redirect">Kernel PCA</a></li>
<li><a href="/wiki/Matrix_decomposition" title="Matrix decomposition">Matrix decomposition</a></li>
<li><a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">Nonlinear dimensionality reduction</a></li>
<li><a href="/wiki/Oja%27s_rule" title="Oja's rule">Oja's rule</a></li>
<li><a href="/w/index.php?title=PCA_network&amp;action=edit&amp;redlink=1" class="new" title="PCA network (page does not exist)">PCA network</a></li>
<li><a href="/w/index.php?title=PCA_applied_to_yield_curves&amp;action=edit&amp;redlink=1" class="new" title="PCA applied to yield curves (page does not exist)">PCA applied to yield curves</a></li>
<li><a href="/wiki/Point_distribution_model" title="Point distribution model">Point distribution model</a> (PCA applied to morphometry and computer vision)</li>
<li><a href="/wiki/Principal_component_regression" title="Principal component regression">Principal component regression</a></li>
<li><a href="http://en.wikibooks.org/wiki/Statistics/Multivariate_Data_Analysis/Principal_Component_Analysis" class="extiw" title="wikibooks:Statistics/Multivariate Data Analysis/Principal Component Analysis">Principal component analysis</a> (Wikibooks)</li>
<li><a href="/wiki/Singular_spectrum_analysis" title="Singular spectrum analysis">Singular spectrum analysis</a></li>
<li><a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">Singular value decomposition</a></li>
<li><a href="/wiki/Transform_coding" title="Transform coding">Transform coding</a></li>
<li><a href="/wiki/Weighted_least_squares" title="Weighted least squares" class="mw-redirect">Weighted least squares</a></li>
<li><a href="/wiki/Dynamic_mode_decomposition" title="Dynamic mode decomposition">Dynamic mode decomposition</a></li>
</ul>
</div>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=26" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://ideas.repec.org/p/pra/mprapa/12723.html" class="external text" title="http://ideas.repec.org/p/pra/mprapa/12723.html" rel="nofollow">The Most Representative Composite Rank Ordering of Multi-Attribute Objects by the Particle Swarm Optimization</a></li>
<li><a href="http://ssrn.com/abstract=1321369" class="external text" title="http://ssrn.com/abstract=1321369" rel="nofollow">Sub-Optimality of Rank Ordering of Objects on the Basis of the Leading Principal Component Factor Scores</a></li>
<li><a href="http://neon.otago.ac.nz/chemlect/chem306/pca/index.html" class="external text" title="http://neon.otago.ac.nz/chemlect/chem306/pca/index.html" rel="nofollow">Spectroscopy and PCA</a></li>
<li><a href="http://www.statsoft.com/textbook/stfacan.html" class="external text" title="http://www.statsoft.com/textbook/stfacan.html" rel="nofollow">An introductory explanation of PCA from StatSoft</a></li>
<li><a href="http://www.snl.salk.edu/~shlens/pub/notes/pca.pdf" class="external text" title="http://www.snl.salk.edu/~shlens/pub/notes/pca.pdf" rel="nofollow">A Tutorial on Principal Component Analysis</a> (PDF)</li>
<li><a href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf" class="external text" title="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf" rel="nofollow">A tutorial on PCA by Lindsay I. Smith</a> (PDF)</li>
<li><a href="http://www.umetrics.com/default.asp/pagename/methods_MVA_intro/c/1" class="external text" title="http://www.umetrics.com/default.asp/pagename/methods_MVA_intro/c/1" rel="nofollow">A layman's explanation from Umetrics</a></li>
<li><a href="http://blog.peltarion.com/2006/06/20/the-talented-drhebb-part-2-pca/" class="external text" title="http://blog.peltarion.com/2006/06/20/the-talented-drhebb-part-2-pca/" rel="nofollow">Principal Component Analysis using Hebbian learning tutorial</a></li>
<li><a href="http://brandon-merkl.blogspot.com/2006/04/principal-components-analysis.html" class="external text" title="http://brandon-merkl.blogspot.com/2006/04/principal-components-analysis.html" rel="nofollow">Presentation of Principal Component Analysis used in Biomedical Engineering</a></li>
<li><a href="http://public.lanl.gov/mewall/kluwer2002.html" class="external text" title="http://public.lanl.gov/mewall/kluwer2002.html" rel="nofollow">Application to microarray and other biomedical data</a></li>
<li><a href="http://feinsteinneuroscience.org/" class="external text" title="http://feinsteinneuroscience.org/" rel="nofollow">PCA in functional neuroimaging, free software</a></li>
<li><a href="http://www.chemometry.com/Research/PCA.html" class="external text" title="http://www.chemometry.com/Research/PCA.html" rel="nofollow">Uncertainty estimation for PCA</a></li>
<li><a href="http://factominer.free.fr/" class="external text" title="http://factominer.free.fr/" rel="nofollow">FactoMineR, an R package dedicated to exploratory multivariate analysis</a></li>
<li><a href="http://www.datascope.be/" class="external text" title="http://www.datascope.be/" rel="nofollow">A web-site with presentations and open source software on exploratory multivariate data analysis</a></li>
<li><a href="http://transp-or2.epfl.ch/pagesPerso/javierFiles/software.php" class="external text" title="http://transp-or2.epfl.ch/pagesPerso/javierFiles/software.php" rel="nofollow">EasyPCA, a very simple and small PCA program under the GPL license</a></li>
<li><a href="http://www.iiap.res.in/astrostat/" class="external text" title="http://www.iiap.res.in/astrostat/" rel="nofollow">A very intuitive R tutorial on cluster and principal component analysis including example data can be found here</a></li>
</ul>
<table class="navbox" cellspacing="0" style=";">
<tr>
<td style="padding:2px;">
<table cellspacing="0" class="nowraplinks collapsible autocollapse" style="width:100%;background:transparent;color:inherit;;">
<tr>
<th style=";" colspan="2" class="navbox-title">
<div style="float:left; width:6em;text-align:left;">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;;;border:none;; font-size:xx-small;"><a href="/wiki/Template:Compression_Methods" title="Template:Compression Methods"><span title="View this template" style=";;border:none;">v</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/wiki/Template_talk:Compression_Methods" title="Template talk:Compression Methods"><span title="Discussion about this template" style=";;border:none;">d</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="http://en.wikipedia.org/w/index.php?title=Template:Compression_Methods&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Template:Compression_Methods&amp;action=edit" rel="nofollow"><span title="Edit this template" style=";;border:none;;">e</span></a></div>
</div>
<span style="font-size:110%;"><a href="/wiki/Data_compression" title="Data compression">Data compression</a> methods</span></th>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Lossless_data_compression" title="Lossless data compression">Lossless</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Information_theory" title="Information theory">Theory</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">Entropy</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Complexity</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Redundancy_(information_theory)" title="Redundancy (information theory)">Redundancy</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Entropy_encoding" title="Entropy encoding">Entropy encoding</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Huffman_coding" title="Huffman coding">Huffman</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Adaptive_Huffman_coding" title="Adaptive Huffman coding">Adaptive Huffman</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Arithmetic_coding" title="Arithmetic coding">Arithmetic</a> (<a href="/wiki/Shannon%E2%80%93Fano_coding" title="Shannon–Fano coding">Shannon-Fano</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Range_encoding" title="Range encoding">Range</a>)<span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Golomb_coding" title="Golomb coding">Golomb</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Exponential-Golomb_coding" title="Exponential-Golomb coding">Exp-Golomb</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Universal_code_(data_compression)" title="Universal code (data compression)">Universal</a> (<a href="/wiki/Elias_gamma_coding" title="Elias gamma coding">Elias</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Fibonacci_coding" title="Fibonacci coding">Fibonacci</a>)</div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Dictionary_coder" title="Dictionary coder">Dictionary</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Run-length_encoding" title="Run-length encoding">RLE</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/DEFLATE" title="DEFLATE">DEFLATE</a><span style="font-weight:bold;">&#160;·</span> <a href="/w/index.php?title=LZ_Family&amp;action=edit&amp;redlink=1" class="new" title="LZ Family (page does not exist)">LZ Family</a> (<a href="/wiki/LZ77_and_LZ78" title="LZ77 and LZ78">LZ77/78</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Lempel-Ziv-Storer-Szymanski" title="Lempel-Ziv-Storer-Szymanski">LZSS</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Lempel-Ziv-Welch" title="Lempel-Ziv-Welch">LZW</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/LZWL" title="LZWL">LZWL</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Lempel-Ziv-Oberhumer" title="Lempel-Ziv-Oberhumer">LZO</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Lempel-Ziv-Markov_chain_algorithm" title="Lempel-Ziv-Markov chain algorithm">LZMA</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/LZX_(algorithm)" title="LZX (algorithm)">LZX</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/LZRW" title="LZRW">LZRW</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/LZJB" title="LZJB">LZJB</a><span style="font-weight:bold;">&#160;·</span> <a href="/w/index.php?title=Lempel-Ziv-Tamayo&amp;action=edit&amp;redlink=1" class="new" title="Lempel-Ziv-Tamayo (page does not exist)">LZT</a>)</div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;">Others</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Context_tree_weighting" title="Context tree weighting">CTW</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Burrows-Wheeler_transform" title="Burrows-Wheeler transform">BWT</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Prediction_by_Partial_Matching" title="Prediction by Partial Matching">PPM</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Dynamic_Markov_Compression" title="Dynamic Markov Compression" class="mw-redirect">DMC</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Audio_data_compression" title="Audio data compression" class="mw-redirect">Audio</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Acoustics" title="Acoustics">Theory</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Convolution" title="Convolution">Convolution</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Sampling_(signal_processing)" title="Sampling (signal processing)">Sampling</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" title="Nyquist–Shannon sampling theorem">Nyquist–Shannon theorem</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Audio_codec" title="Audio codec">Audio codec</a> parts</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Linear_predictive_coding" title="Linear predictive coding">LPC</a> (<a href="/wiki/Log_Area_Ratios" title="Log Area Ratios">LAR</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Line_spectral_pairs" title="Line spectral pairs">LSP</a>)<span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Warped_Linear_Predictive_Coding" title="Warped Linear Predictive Coding">WLPC</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Code_Excited_Linear_Prediction" title="Code Excited Linear Prediction" class="mw-redirect">CELP</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Algebraic_Code_Excited_Linear_Prediction" title="Algebraic Code Excited Linear Prediction" class="mw-redirect">ACELP</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/A-law_algorithm" title="A-law algorithm">A-law</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/%CE%9C-law_algorithm" title="Μ-law algorithm">μ-law</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Modified_discrete_cosine_transform" title="Modified discrete cosine transform">MDCT</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Fourier_transform" title="Fourier transform">Fourier transform</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Psychoacoustic_model" title="Psychoacoustic model" class="mw-redirect">Psychoacoustic model</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;">Others</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Audio_level_compression" title="Audio level compression" class="mw-redirect">Dynamic range compression</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Speech_encoding" title="Speech encoding" class="mw-redirect">Speech compression</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Sub-band_coding" title="Sub-band coding">Sub-band coding</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Image_compression" title="Image compression">Image</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;">Terms</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Color_space" title="Color space">Color space</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Pixel" title="Pixel">Pixel</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Chroma_subsampling" title="Chroma subsampling">Chroma subsampling</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Compression_artifact" title="Compression artifact">Compression artifact</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;">Methods</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Run-length_encoding" title="Run-length encoding">RLE</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/DPCM" title="DPCM">DPCM</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Fractal_compression" title="Fractal compression">Fractal</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Wavelet_compression" title="Wavelet compression">Wavelet</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/EZW" title="EZW" class="mw-redirect">EZW</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Set_partitioning_in_hierarchical_trees" title="Set partitioning in hierarchical trees">SPIHT</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Pyramid_(image_processing)" title="Pyramid (image processing)">LP</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Discrete_cosine_transform" title="Discrete cosine transform">DCT</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Karhunen-Lo%C3%A8ve_transform" title="Karhunen-Loève transform" class="mw-redirect">KLT</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;">Others</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Bit_rate" title="Bit rate">Bit rate</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Standard_test_image" title="Standard test image">Test images</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Peak_signal-to-noise_ratio" title="Peak signal-to-noise ratio">PSNR quality measure</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Quantization_(image_processing)" title="Quantization (image processing)">Quantization</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Video_compression" title="Video compression">Video</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;">Terms</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Video#Characteristics_of_video_streams" title="Video">Video Characteristics</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Film_frame" title="Film frame">Frame</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Video_compression_picture_types" title="Video compression picture types">Frame types</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Video_quality" title="Video quality">Video quality</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Video_codec" title="Video codec">Video codec parts</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Motion_compensation" title="Motion compensation">Motion compensation</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Discrete_cosine_transform" title="Discrete cosine transform">DCT</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Quantization_(signal_processing)" title="Quantization (signal processing)">Quantization</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;width:11em;;">
<div style="padding:0em 0.75em;">Others</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Video_codec" title="Video codec">Video codecs</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Rate_distortion_theory" title="Rate distortion theory" class="mw-redirect">Rate distortion theory</a> (<a href="/wiki/Constant_bitrate" title="Constant bitrate">CBR</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Average_bitrate" title="Average bitrate">ABR</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Variable_bitrate" title="Variable bitrate">VBR</a>)</div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td colspan="2" style="width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Timeline_of_information_theory" title="Timeline of information theory">Timeline of information theory, data compression, and error-correcting codes</a></div>
</td>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-abovebelow" style=";" colspan="2">See <a href="/wiki/Template:Compression_Formats" title="Template:Compression Formats" class="mw-redirect">Compression Formats and Standards</a> for formats and <a href="/wiki/Template:Compression_Software_Implementations" title="Template:Compression Software Implementations">Compression Software Implementations</a> for codecs</td>
</tr>
</table>
</td>
</tr>
</table>


<!-- 
NewPP limit report
Preprocessor node count: 4650/1000000
Post-expand include size: 91460/2048000 bytes
Template argument size: 66319/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:76340-0!1!0!default!!en!2 and timestamp 20090404064244 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Principal_component_analysis">http://en.wikipedia.org/wiki/Principal_component_analysis</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Multivariate_statistics" title="Category:Multivariate statistics">Multivariate statistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Singular_value_decomposition" title="Category:Singular value decomposition">Singular value decomposition</a></span> | <span dir='ltr'><a href="/wiki/Category:Data_mining" title="Category:Data mining">Data mining</a></span> | <span dir='ltr'><a href="/wiki/Category:Data_analysis" title="Category:Data analysis">Data analysis</a></span> | <span dir='ltr'><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Self-contradictory_articles" title="Category:Self-contradictory articles">Self-contradictory articles</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Principal_component_analysis" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Principal_component_analysis" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Principal_component_analysis&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Principal_component_analysis&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Principal_component_analysis" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Principal_component_analysis" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Principal_component_analysis" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Principal_component_analysis&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Principal_component_analysis&amp;oldid=281653103" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Principal_component_analysis&amp;id=281653103">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D8%AA%D8%AD%D9%84%D9%8A%D9%84_%D8%A7%D9%84%D8%B9%D9%86%D8%B5%D8%B1_%D8%A7%D9%84%D8%B1%D8%A6%D9%8A%D8%B3%D9%8A">العربية</a></li>
				<li class="interwiki-cs"><a href="http://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent">Česky</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Hauptkomponentenanalyse">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/An%C3%A1lisis_de_componentes_principales">Español</a></li>
				<li class="interwiki-eo"><a href="http://eo.wikipedia.org/wiki/Analizo_al_precipaj_konsisteroj">Esperanto</a></li>
				<li class="interwiki-fa"><a href="http://fa.wikipedia.org/wiki/%D8%AA%D8%AD%D9%84%DB%8C%D9%84_%D9%85%D9%88%D9%84%D9%81%D9%87%E2%80%8C%D9%87%D8%A7%DB%8C_%D8%A7%D8%B5%D9%84%DB%8C">فارسی</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Analyse_en_composantes_principales">Français</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D">한국어</a></li>
				<li class="interwiki-id"><a href="http://id.wikipedia.org/wiki/Analisis_komponen_utama">Bahasa Indonesia</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Analisi_delle_componenti_principali">Italiano</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Hoofdcomponenten">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Analiza_g%C5%82%C3%B3wnych_sk%C5%82adowych">Polski</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82">Русский</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/P%C3%A4%C3%A4komponenttianalyysi">Suomi</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Principalkomponentanalys">Svenska</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 4 April 2009, at 06:42.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv204 in 0.046 secs. --></body></html>
