<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Cluster analysis,Akaike information criterion,Animal,Arterial,Artificial neural network,As the crow flies,Bayesian information criterion,Biclustering,Bioinformatics,Biology,Bipartite graph" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Cluster_analysis&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Cluster_analysis&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Cluster analysis - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Cluster_analysis";
		var wgTitle = "Cluster analysis";
		var wgAction = "view";
		var wgArticleId = "669675";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282007120;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Cluster_analysis skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Cluster analysis</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p><b>Clustering</b> is the assignment of objects into groups (called <i>clusters</i>) so that objects from the same cluster are more similar to each other than objects from different clusters. Often similarity is assessed according to a <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">distance measure</a>. Clustering is a common technique for <a href="/wiki/Statistics" title="Statistics">statistical</a> <a href="/wiki/Data_analysis" title="Data analysis">data analysis</a>, which is used in many fields, including <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <a href="/wiki/Data_mining" title="Data mining">data mining</a>, <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, <a href="/wiki/Image_analysis" title="Image analysis">image analysis</a> and <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>.</p>
<p>Besides the term <i>data clustering</i> (or just <i>clustering</i>), there are a number of terms with similar meanings, including <i>cluster analysis</i>, <i>automatic classification</i>, <i>numerical taxonomy</i>, <i>botryology</i> and <i>typological analysis</i>.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Types_of_clustering"><span class="tocnumber">1</span> <span class="toctext">Types of clustering</span></a></li>
<li class="toclevel-1"><a href="#Distance_measure"><span class="tocnumber">2</span> <span class="toctext">Distance measure</span></a></li>
<li class="toclevel-1"><a href="#Hierarchical_clustering"><span class="tocnumber">3</span> <span class="toctext">Hierarchical clustering</span></a>
<ul>
<li class="toclevel-2"><a href="#Creating_clusters"><span class="tocnumber">3.1</span> <span class="toctext">Creating clusters</span></a></li>
<li class="toclevel-2"><a href="#Agglomerative_hierarchical_clustering"><span class="tocnumber">3.2</span> <span class="toctext">Agglomerative hierarchical clustering</span></a></li>
<li class="toclevel-2"><a href="#Concept_clustering"><span class="tocnumber">3.3</span> <span class="toctext">Concept clustering</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Partitional_clustering"><span class="tocnumber">4</span> <span class="toctext">Partitional clustering</span></a>
<ul>
<li class="toclevel-2"><a href="#K-means_and_derivatives"><span class="tocnumber">4.1</span> <span class="toctext">K-means and derivatives</span></a>
<ul>
<li class="toclevel-3"><a href="#K-means_clustering"><span class="tocnumber">4.1.1</span> <span class="toctext">K-means clustering</span></a></li>
<li class="toclevel-3"><a href="#Fuzzy_c-means_clustering"><span class="tocnumber">4.1.2</span> <span class="toctext">Fuzzy c-means clustering</span></a></li>
<li class="toclevel-3"><a href="#QT_clustering_algorithm"><span class="tocnumber">4.1.3</span> <span class="toctext">QT clustering algorithm</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="#Locality-sensitive_hashing"><span class="tocnumber">4.2</span> <span class="toctext">Locality-sensitive hashing</span></a></li>
<li class="toclevel-2"><a href="#Graph-theoretic_methods"><span class="tocnumber">4.3</span> <span class="toctext">Graph-theoretic methods</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Determining_the_number_of_clusters"><span class="tocnumber">5</span> <span class="toctext">Determining the number of clusters</span></a></li>
<li class="toclevel-1"><a href="#Spectral_clustering"><span class="tocnumber">6</span> <span class="toctext">Spectral clustering</span></a></li>
<li class="toclevel-1"><a href="#Applications"><span class="tocnumber">7</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2"><a href="#Biology"><span class="tocnumber">7.1</span> <span class="toctext">Biology</span></a></li>
<li class="toclevel-2"><a href="#Medicine"><span class="tocnumber">7.2</span> <span class="toctext">Medicine</span></a></li>
<li class="toclevel-2"><a href="#Market_research"><span class="tocnumber">7.3</span> <span class="toctext">Market research</span></a></li>
<li class="toclevel-2"><a href="#Other_applications"><span class="tocnumber">7.4</span> <span class="toctext">Other applications</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Comparisons_between_data_clusterings"><span class="tocnumber">8</span> <span class="toctext">Comparisons between data clusterings</span></a></li>
<li class="toclevel-1"><a href="#Algorithms"><span class="tocnumber">9</span> <span class="toctext">Algorithms</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">10</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Bibliography"><span class="tocnumber">11</span> <span class="toctext">Bibliography</span></a>
<ul>
<li class="toclevel-2"><a href="#Others"><span class="tocnumber">11.1</span> <span class="toctext">Others</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">12</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Types_of_clustering" id="Types_of_clustering"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=1" title="Edit section: Types of clustering">edit</a>]</span> <span class="mw-headline">Types of clustering</span></h2>
<p>Data clustering algorithms can be <a href="/wiki/Hierarchical" title="Hierarchical" class="mw-redirect">hierarchical</a>. Hierarchical algorithms find successive clusters using previously established clusters. Hierarchical algorithms can be agglomerative ("bottom-up") or divisive ("top-down"). Agglomerative algorithms begin with each element as a separate cluster and merge them into successively larger clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters.</p>
<p><a href="/wiki/Partition_of_a_set" title="Partition of a set">Partitional</a> algorithms typically determine all clusters at once, but can also be used as divisive algorithms in the <a href="/wiki/Hierarchical" title="Hierarchical" class="mw-redirect">hierarchical</a> clustering.</p>
<p>Density-based clustering algorithms are devised to discover arbitrary-shaped clusters. In this approach, a cluster is regarded as a region in which the density of data objects exceeds a threshold. <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a> and <a href="/w/index.php?title=OPTICS&amp;action=edit&amp;redlink=1" class="new" title="OPTICS (page does not exist)">OPTICS</a> are two typical algorithms of this kind.</p>
<p><i>Two-way clustering</i>, <i>co-clustering</i> or <a href="/wiki/Biclustering" title="Biclustering">biclustering</a> are clustering methods where not only the objects are clustered but also the features of the objects, i.e., if the data is represented in a <a href="/wiki/Data_matrix_(statistics)" title="Data matrix (statistics)" class="mw-redirect">data matrix</a>, the rows and columns are clustered simultaneously.</p>
<p>Another important distinction is whether the clustering uses symmetric or asymmetric distances. A property of <a href="/wiki/Euclidean_space" title="Euclidean space">Euclidean space</a> is that distances are symmetric (the distance from object <i>A</i> to <i>B</i> is the same as the distance from <i>B</i> to <i>A</i>). In other applications (e.g., sequence-alignment methods, see Prinzie &amp; Van den Poel (2006)), this is not the case.</p>
<p><a name="Distance_measure" id="Distance_measure"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=2" title="Edit section: Distance measure">edit</a>]</span> <span class="mw-headline">Distance measure</span></h2>
<p>An important step in any clustering is to select a <a href="/wiki/Distance" title="Distance">distance measure</a>, which will determine how the <i>similarity</i> of two elements is calculated. This will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a 2-dimensional space, the distance between the point (x=1, y=0) and the origin (x=0, y=0) is always 1 according to the usual norms, but the distance between the point (x=1, y=1) and the origin can be 2,<img class="tex" alt="\sqrt[2]{2}" src="http://upload.wikimedia.org/math/4/9/0/4909b6bd770bacde08d5a68e38665804.png" /> or 1 if you take respectively the 1-norm, 2-norm or infinity-norm distance.</p>
<p>Common distance functions:</p>
<ul>
<li>The <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> (also called distance <a href="/wiki/As_the_crow_flies" title="As the crow flies">as the crow flies</a> or 2-norm distance). A review of cluster analysis in health psychology research found that the most common distance measure in published studies in that research area is the Euclidean distance or the squared Euclidean distance.</li>
<li>The <a href="/wiki/Manhattan_distance" title="Manhattan distance" class="mw-redirect">Manhattan distance</a> (also called taxicab norm or 1-norm)</li>
<li>The <a href="/wiki/Maximum_norm" title="Maximum norm" class="mw-redirect">maximum norm</a></li>
<li>The <a href="/wiki/Mahalanobis_distance" title="Mahalanobis distance">Mahalanobis distance</a> corrects data for different scales and correlations in the variables</li>
<li>The angle between two vectors can be used as a distance measure when clustering high dimensional data. See <a href="/wiki/Inner_product_space" title="Inner product space">Inner product space</a>.</li>
<li>The <a href="/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a> measures the minimum number of substitutions required to change one member into another.</li>
</ul>
<p><a name="Hierarchical_clustering" id="Hierarchical_clustering"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=3" title="Edit section: Hierarchical clustering">edit</a>]</span> <span class="mw-headline">Hierarchical clustering</span></h2>
<p><a name="Creating_clusters" id="Creating_clusters"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=4" title="Edit section: Creating clusters">edit</a>]</span> <span class="mw-headline">Creating clusters</span></h3>
<p>Hierarchical clustering builds (agglomerative), or breaks up (divisive), a hierarchy of clusters. The traditional representation of this hierarchy is a <a href="/wiki/Tree_data_structure" title="Tree data structure" class="mw-redirect">tree</a> (called a <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a>), with individual elements at one end and a single cluster containing every element at the other. Agglomerative algorithms begin at the leaves of the tree, whereas divisive algorithms begin at the root. (In the figure, the arrows indicate an agglomerative clustering.)</p>
<p>Cutting the tree at a given height will give a clustering at a selected precision. In the following example, cutting after the second row will yield clusters {a} {b c} {d e} {f}. Cutting after the third row will yield clusters {a} {b c} {d e f}, which is a coarser clustering, with a smaller number of larger clusters.</p>
<p><a name="Agglomerative_hierarchical_clustering" id="Agglomerative_hierarchical_clustering"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=5" title="Edit section: Agglomerative hierarchical clustering">edit</a>]</span> <span class="mw-headline">Agglomerative hierarchical clustering</span></h3>
<p>For example, suppose this data is to be clustered, and the <a href="/wiki/Euclidean_distance" title="Euclidean distance">euclidean distance</a> is the <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">distance metric</a>.</p>
<div class="thumb tnone">
<div class="thumbinner" style="width:268px;"><a href="/wiki/File:Clusters.PNG" class="image" title="Raw data"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/c/ce/Clusters.PNG" width="266" height="260" border="0" class="thumbimage" /></a>
<div class="thumbcaption">Raw data</div>
</div>
</div>
<p>The hierarchical clustering <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a> would be as such:</p>
<div class="thumb tnone">
<div class="thumbinner" style="width:435px;"><a href="/wiki/File:Hierarchical_clustering_diagram.png" class="image" title="Traditional representation"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/a/ab/Hierarchical_clustering_diagram.png" width="433" height="344" border="0" class="thumbimage" /></a>
<div class="thumbcaption">Traditional representation</div>
</div>
</div>
<p>This method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.</p>
<p>Optionally, one can also construct a <a href="/wiki/Distance_matrix" title="Distance matrix">distance matrix</a> at this stage, where the number in the <i>i</i>-th row <i>j</i>-th column is the distance between the <i>i</i>-th and <i>j</i>-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the <a href="/wiki/Single-linkage_clustering" title="Single-linkage clustering">single-linkage clustering</a> page; it can easily be adapted to different types of linkage (see below).</p>
<p>Suppose we have merged the two closest elements <i>b</i> and <i>c</i>, we now have the following clusters {<i>a</i>}, {<i>b</i>, <i>c</i>}, {<i>d</i>}, {<i>e</i>} and {<i>f</i>}, and want to merge them further. To do that, we need to take the distance between {a} and {b c}, and therefore define the distance between two clusters. Usually the distance between two clusters <img class="tex" alt="\mathcal{A}" src="http://upload.wikimedia.org/math/8/4/c/84cc21a1ecbbe55e01e12e575a52cca2.png" /> and <img class="tex" alt="\mathcal{B}" src="http://upload.wikimedia.org/math/8/d/7/8d7c27e339945f6c96cc234d1248d3fc.png" /> is one of the following:</p>
<ul>
<li>The maximum distance between elements of each cluster (also called complete linkage clustering):</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" \max \{\, d(x,y)&#160;: x \in \mathcal{A},\, y \in \mathcal{B}\,\}. " src="http://upload.wikimedia.org/math/1/7/8/1780326fbfb1c8b03777066749f1a38e.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>The minimum distance between elements of each cluster (also called <a href="/wiki/Single-linkage_clustering" title="Single-linkage clustering">single-linkage clustering</a>):</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" \min \{\, d(x,y)&#160;: x \in \mathcal{A},\, y \in \mathcal{B} \,\}. " src="http://upload.wikimedia.org/math/6/5/1/65113399bcb89c3bf49f080bbf9eb5c3.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>The mean distance between elements of each cluster (also called average linkage clustering, used e.g. in <a href="/wiki/UPGMA" title="UPGMA">UPGMA</a>):</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" {1 \over {|\mathcal{A}|\cdot|\mathcal{B}|}}\sum_{x \in \mathcal{A}}\sum_{ y \in \mathcal{B}} d(x,y). " src="http://upload.wikimedia.org/math/7/8/f/78fa745ce59b8c5ea8b6b00f36cb97ae.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>The sum of all intra-cluster variance.</li>
<li>The increase in variance for the cluster being merged (<a href="/w/index.php?title=Ward%27s_criterion&amp;action=edit&amp;redlink=1" class="new" title="Ward's criterion (page does not exist)">Ward's criterion</a>).</li>
<li>The probability that candidate clusters spawn from the same distribution function (V-linkage).</li>
</ul>
<p>Each agglomeration occurs at a greater distance between clusters than the previous agglomeration, and one can decide to stop clustering either when the clusters are too far apart to be merged (distance criterion) or when there is a sufficiently small number of clusters (number criterion).</p>
<p><a name="Concept_clustering" id="Concept_clustering"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=6" title="Edit section: Concept clustering">edit</a>]</span> <span class="mw-headline">Concept clustering</span></h3>
<p>Another variation of the agglomerative clustering approach is <a href="/wiki/Conceptual_clustering" title="Conceptual clustering">conceptual clustering</a>.</p>
<p><a name="Partitional_clustering" id="Partitional_clustering"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=7" title="Edit section: Partitional clustering">edit</a>]</span> <span class="mw-headline">Partitional clustering</span></h2>
<p><a name="K-means_and_derivatives" id="K-means_and_derivatives"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=8" title="Edit section: K-means and derivatives">edit</a>]</span> <span class="mw-headline"><i>K</i>-means and derivatives</span></h3>
<p><a name="K-means_clustering" id="K-means_clustering"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=9" title="Edit section: K-means clustering">edit</a>]</span> <span class="mw-headline"><i>K</i>-means clustering</span></h4>
<p>The <a href="/wiki/K-means_algorithm" title="K-means algorithm"><i>K</i>-means algorithm</a> assigns each point to the cluster whose center (also called centroid) is nearest. The center is the average of all the points in the cluster — that is, its coordinates are the arithmetic mean for each dimension separately over all the points in the cluster.</p>
<dl>
<dd><i>Example:</i> The data set has three dimensions and the cluster has two points: <i>X</i> = (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, <i>x</i><sub>3</sub>) and <i>Y</i> = (<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, <i>y</i><sub>3</sub>). Then the centroid <i>Z</i> becomes <i>Z</i> = (<i>z</i><sub>1</sub>, <i>z</i><sub>2</sub>, <i>z</i><sub>3</sub>), where <i>z</i><sub>1</sub> = (<i>x</i><sub>1</sub>&#160;+&#160;<i>y</i><sub>1</sub>)/2 and <i>z</i><sub>2</sub> = (<i>x</i><sub>2</sub>&#160;+&#160;<i>y</i><sub>2</sub>)/2 and <i>z</i><sub>3</sub> = (<i>x</i><sub>3</sub>&#160;+&#160;<i>y</i><sub>3</sub>)/2.</dd>
</dl>
<p>The algorithm steps are <sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup>:</p>
<ul>
<li>Choose the number of clusters, <i>k</i>.</li>
<li>Randomly generate <i>k</i> clusters and determine the cluster centers, or directly generate <i>k</i> random points as cluster centers.</li>
<li>Assign each point to the nearest cluster center.</li>
<li>Recompute the new cluster centers.</li>
<li>Repeat the two previous steps until some convergence criterion is met (usually that the assignment hasn't changed).</li>
</ul>
<p>The main advantages of this algorithm are its simplicity and speed which allows it to run on large datasets. Its disadvantage is that it does not yield the same result with each run, since the resulting clusters depend on the initial random assignments. It minimizes intra-cluster variance, but does not ensure that the result has a global minimum of variance. Another disadvantage is the requirement for the concept of a mean to be definable which is not always the case. For such datasets the <a href="/wiki/K-medoids" title="K-medoids">k-medoids</a> variant is appropriate.</p>
<p><a name="Fuzzy_c-means_clustering" id="Fuzzy_c-means_clustering"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=10" title="Edit section: Fuzzy c-means clustering">edit</a>]</span> <span class="mw-headline">Fuzzy <i>c</i>-means clustering</span></h4>
<p>In <a href="/wiki/Fuzzy_clustering" title="Fuzzy clustering">fuzzy clustering</a>, each point has a degree of belonging to clusters, as in <a href="/wiki/Fuzzy_logic" title="Fuzzy logic">fuzzy logic</a>, rather than belonging completely to just one cluster. Thus, points on the edge of a cluster, may be <i>in the cluster</i> to a lesser degree than points in the center of cluster. For each point <i>x</i> we have a coefficient giving the degree of being in the <i>k</i>th cluster <span class="texhtml"><i>u</i><sub><i>k</i></sub>(<i>x</i>)</span>. Usually, the sum of those coefficients for any given <i>x</i> is defined to be 1:</p>
<dl>
<dd><img class="tex" alt=" \forall x \left(\sum_{k=1}^{\mathrm{num.}\ \mathrm{clusters}} u_k(x) \ =1\right)." src="http://upload.wikimedia.org/math/5/9/6/59613d616335a28500273119886438a9.png" /></dd>
</dl>
<p>With fuzzy <i>c</i>-means, the centroid of a cluster is the mean of all points, weighted by their degree of belonging to the cluster:</p>
<dl>
<dd><img class="tex" alt="\mathrm{center}_k = {{\sum_x u_k(x)^m x} \over {\sum_x u_k(x)^m}}." src="http://upload.wikimedia.org/math/b/a/3/ba34d1989b6d1d97381fe9fd6de2ee29.png" /></dd>
</dl>
<p>The degree of belonging is related to the inverse of the distance to the cluster center:</p>
<dl>
<dd><img class="tex" alt="u_k(x) = {1 \over d(\mathrm{center}_k,x)}," src="http://upload.wikimedia.org/math/b/5/4/b5466200135cb28aa1fb958081870147.png" /></dd>
</dl>
<p>then the coefficients are normalized and fuzzyfied with a real parameter <span class="texhtml"><i>m</i> &gt; 1</span> so that their sum is 1. So</p>
<dl>
<dd><img class="tex" alt="u_k(x) = \frac{1}{\sum_j \left(\frac{d(\mathrm{center}_k,x)}{d(\mathrm{center}_j,x)}\right)^{2/(m-1)}}." src="http://upload.wikimedia.org/math/a/6/d/a6d09f4eafb6f35b95a6c4cacf298e46.png" /></dd>
</dl>
<p>For <i>m</i> equal to 2, this is equivalent to normalising the coefficient linearly to make their sum 1. When <i>m</i> is close to 1, then cluster center closest to the point is given much more weight than the others, and the algorithm is similar to <i>k</i>-means.</p>
<p>The fuzzy <i>c</i>-means algorithm is very similar to the <i>k</i>-means algorithm:</p>
<ul>
<li>Choose a number of clusters.</li>
<li>Assign randomly to each point coefficients for being in the clusters.</li>
<li>Repeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than <span class="texhtml">ε</span>, the given sensitivity threshold)&#160;:
<ul>
<li>Compute the centroid for each cluster, using the formula above.</li>
<li>For each point, compute its coefficients of being in the clusters, using the formula above.</li>
</ul>
</li>
</ul>
<p>The algorithm minimizes intra-cluster variance as well, but has the same problems as <i>k</i>-means, the minimum is a local minimum, and the results depend on the initial choice of weights. The <a href="/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm">Expectation-maximization algorithm</a> is a more statistically formalized method which includes some of these ideas: partial membership in classes. It has better convergence properties and is in general preferred to fuzzy-c-means.</p>
<p><a name="QT_clustering_algorithm" id="QT_clustering_algorithm"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=11" title="Edit section: QT clustering algorithm">edit</a>]</span> <span class="mw-headline">QT clustering algorithm</span></h4>
<p>QT (quality threshold) clustering (Heyer, <a href="/w/index.php?title=Kruglyak&amp;action=edit&amp;redlink=1" class="new" title="Kruglyak (page does not exist)">Kruglyak</a>, Yooseph, 1999) is an alternative method of partitioning data, invented for gene clustering. It requires more computing power than <i>k</i>-means, but does not require specifying the number of clusters <i>a priori</i>, and always returns the same result when run several times.</p>
<p>The algorithm is:</p>
<ul>
<li>The user chooses a maximum diameter for clusters.</li>
<li>Build a candidate cluster for each point by including the closest point, the next closest, and so on, until the diameter of the cluster surpasses the threshold.</li>
<li>Save the candidate cluster with the most points as the first true cluster, and remove all points in the cluster from further consideration. Must clarify what happens if more than 1 cluster has the maximum number of points&#160;?</li>
<li><a href="/wiki/Recursion" title="Recursion">Recurse</a> with the reduced set of points.</li>
</ul>
<p>The distance between a point and a group of points is computed using complete linkage, i.e. as the maximum distance from the point to any member of the group (see the "Agglomerative hierarchical clustering" section about distance between clusters).</p>
<p><a name="Locality-sensitive_hashing" id="Locality-sensitive_hashing"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=12" title="Edit section: Locality-sensitive hashing">edit</a>]</span> <span class="mw-headline">Locality-sensitive hashing</span></h3>
<p><a href="/wiki/Locality-sensitive_hashing" title="Locality-sensitive hashing" class="mw-redirect">Locality-sensitive hashing</a> can be used for clustering. Feature space vectors are sets, and the metric used is the <a href="/wiki/Jaccard_distance" title="Jaccard distance" class="mw-redirect">Jaccard distance</a>. The feature space can be considered high-dimensional. The <i>min-wise independent permutations</i> LSH scheme (sometimes MinHash) is then used to put similar items into buckets. With just one set of hashing methods, there are only clusters of very similar elements. By seeding the hash functions several times (eg 20), it is possible to get bigger clusters. <sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<p><a name="Graph-theoretic_methods" id="Graph-theoretic_methods"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=13" title="Edit section: Graph-theoretic methods">edit</a>]</span> <span class="mw-headline">Graph-theoretic methods</span></h3>
<p><a href="/wiki/Formal_concept_analysis" title="Formal concept analysis">Formal concept analysis</a> is a technique for generating clusters of objects and attributes, given a <a href="/wiki/Bipartite_graph" title="Bipartite graph">bipartite graph</a> representing the relations between the objects and attributes. Other methods for generating <i>overlapping clusters</i> (a <a href="/wiki/Cover_(topology)" title="Cover (topology)">cover</a> rather than a <a href="/wiki/Partition_of_a_set" title="Partition of a set">partition</a>) are discussed by Jardine and Sibson (1968) and Cole and Wishart (1970).</p>
<p><a name="Determining_the_number_of_clusters" id="Determining_the_number_of_clusters"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=14" title="Edit section: Determining the number of clusters">edit</a>]</span> <span class="mw-headline">Determining the number of clusters</span></h2>
<p>Many clustering algorithms require that you specify up front the number of clusters to find. If that number is not apparent from prior knowledge, it should be chosen in some way. Several methods for this have been suggested within the statistical literature,<sup id="cite_ref-2" class="reference"><a href="#cite_note-2" title=""><span>[</span>3<span>]</span></a></sup><span class="reference plainlinksneverexpand" style="white-space:nowrap"><sup>:365</sup></span> where one <a href="/wiki/Rule_of_thumb" title="Rule of thumb">rule of thumb</a> sets the number to</p>
<dl>
<dd><img class="tex" alt=" k \approx (n/2)^{1/2} " src="http://upload.wikimedia.org/math/f/b/c/fbce2cdee91160c3ccd81851b78dc92e.png" /></dd>
</dl>
<p>with <i>n</i> as the number of objects (data points).</p>
<div class="thumb tright">
<div class="thumbinner" style="width:302px;"><a href="/wiki/File:DataClustering_ElbowCriterion.JPG" class="image" title="Explained Variance. The &quot;elbow&quot; is indicated by the red circle. The number of clusters chosen should therefore be 4."><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/DataClustering_ElbowCriterion.JPG/300px-DataClustering_ElbowCriterion.JPG" width="300" height="240" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:DataClustering_ElbowCriterion.JPG" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
Explained Variance. The "elbow" is indicated by the red circle. The number of clusters chosen should therefore be 4.</div>
</div>
</div>
<p>Another rule of thumb looks at the percentage of variance explained as a function of the number of clusters: You should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if you graph the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters are chosen at this point, hence the "elbow criterion". This "elbow" cannot always be unambiguously identified.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup> Percentage of variance explained is the ratio of the between-group variance to the total variance. A slight variation of this method plots the curvature of the within group variance.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup> The method can be traced to <a href="/wiki/Robert_L._Thorndike" title="Robert L. Thorndike">Robert L. Thorndike</a> in 1953.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup></p>
<p>Other ways to determine the number of clusters use <a href="/wiki/Akaike_information_criterion" title="Akaike information criterion">Akaike information criterion</a> (AIC) or <a href="/wiki/Bayesian_information_criterion" title="Bayesian information criterion">Bayesian information criterion</a> (BIC) — if it is possible to make a likelihood function for the clustering model. For example: The k-means model is "almost" a <a href="/wiki/Gaussian_mixture_model" title="Gaussian mixture model" class="mw-redirect">Gaussian mixture model</a> and one can construct a likelihood for the Gaussian mixture model and thus also determine AIC and BIC values.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup></p>
<p>The average <a href="/wiki/Silhouette_(clustering)" title="Silhouette (clustering)">silhouette</a> of the data is a useful criterion for assessing the natural number of clusters. The silhouette of a datum is a measure of how closely it is matched to data within its cluster and how loosely it is matched to data of the neighbouring cluster, i.e. the cluster whose average distance from the datum is lowest<sup id="cite_ref-7" class="reference"><a href="#cite_note-7" title=""><span>[</span>8<span>]</span></a></sup>. A silhouette close to <span class="texhtml">1</span> implies the datum is in an appropriate cluster, whilst a silhouette close to <span class="texhtml">− 1</span> implies the datum is in the wrong cluster. Optimization techniques such as <a href="/wiki/Genetic_algorithms" title="Genetic algorithms" class="mw-redirect">genetic algorithms</a> are useful in determining the number clusters that gives rise to the largest silhouette<sup id="cite_ref-8" class="reference"><a href="#cite_note-8" title=""><span>[</span>9<span>]</span></a></sup>.</p>
<p><a name="Spectral_clustering" id="Spectral_clustering"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=15" title="Edit section: Spectral clustering">edit</a>]</span> <span class="mw-headline">Spectral clustering</span></h2>
<p>Given a set of data points A, the <a href="/wiki/Similarity_matrix" title="Similarity matrix">similarity matrix</a> may be defined as a matrix <span class="texhtml"><i>S</i></span> where <span class="texhtml"><i>S</i><sub><i>i</i><i>j</i></sub></span> represents a measure of the similarity between points <img class="tex" alt="i, j\in A" src="http://upload.wikimedia.org/math/d/0/b/d0bded5f72bb7a71e0c39ccc2f045859.png" />. Spectral clustering techniques make use of the <a href="/wiki/Spectrum_of_a_matrix" title="Spectrum of a matrix" class="mw-redirect">spectrum</a> of the similarity matrix of the data to perform <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction" class="mw-redirect">dimensionality reduction</a> for clustering in fewer dimensions.</p>
<p>One such technique is the <i><a href="/w/index.php?title=Shi-Malik_algorithm&amp;action=edit&amp;redlink=1" class="new" title="Shi-Malik algorithm (page does not exist)">Shi-Malik algorithm</a></i>, commonly used for <a href="/wiki/Segmentation_(image_processing)" title="Segmentation (image processing)">image segmentation</a>. It partitions points into two sets <span class="texhtml">(<i>S</i><sub>1</sub>,<i>S</i><sub>2</sub>)</span> based on the <a href="/wiki/Eigenvector" title="Eigenvector" class="mw-redirect">eigenvector</a> <span class="texhtml"><i>v</i></span> corresponding to the second-smallest <a href="/wiki/Eigenvalue" title="Eigenvalue" class="mw-redirect">eigenvalue</a> of the <a href="/wiki/Laplacian_matrix" title="Laplacian matrix">Laplacian matrix</a></p>
<dl>
<dd><span class="texhtml"><i>L</i> = <i>I</i> − <i>D</i> <sup>− 1 / 2</sup><i>S</i><i>D</i> <sup>− 1 / 2</sup></span></dd>
</dl>
<p>of <span class="texhtml"><i>S</i></span>, where <span class="texhtml"><i>D</i></span> is the diagonal matrix</p>
<dl>
<dd>
<table>
<tr align='center'>
<td><i>D</i><sub><i>i</i><i>i</i></sub> =</td>
<td><font size='+2'>∑</font></td>
<td><i>S</i><sub><i>i</i><i>j</i></sub>.</td>
</tr>
<tr align='center' valign='top'>
<td></td>
<td><i>j</i></td>
<td></td>
</tr>
</table>
</dd>
</dl>
<p>This partitioning may be done in various ways, such as by taking the median <span class="texhtml"><i>m</i></span> of the components in <span class="texhtml"><i>v</i></span>, and placing all points whose component in <span class="texhtml"><i>v</i></span> is greater than <span class="texhtml"><i>m</i></span> in <span class="texhtml"><i>S</i><sub>1</sub></span>, and the rest in <span class="texhtml"><i>S</i><sub>2</sub></span>. The algorithm can be used for hierarchical clustering by repeatedly partitioning the subsets in this fashion.</p>
<p>A related algorithm is the <i><a href="/w/index.php?title=Meila-Shi_algorithm&amp;action=edit&amp;redlink=1" class="new" title="Meila-Shi algorithm (page does not exist)">Meila-Shi algorithm</a></i>, which takes the <a href="/wiki/Eigenvector" title="Eigenvector" class="mw-redirect">eigenvectors</a> corresponding to the <i>k</i> largest <a href="/wiki/Eigenvalue" title="Eigenvalue" class="mw-redirect">eigenvalues</a> of the matrix <span class="texhtml"><i>P</i> = <i>S</i><i>D</i> <sup>− 1</sup></span> for some <i>k</i>, and then invokes another (e.g. <i>k</i>-means) to cluster points by their respective <i>k</i> components in these eigenvectors.</p>
<p><a name="Applications" id="Applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=16" title="Edit section: Applications">edit</a>]</span> <span class="mw-headline">Applications</span></h2>
<p><a name="Biology" id="Biology"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=17" title="Edit section: Biology">edit</a>]</span> <span class="mw-headline">Biology</span></h3>
<p>In <a href="/wiki/Biology" title="Biology">biology</a> <b>clustering</b> has many applications</p>
<ul>
<li>In imaging, data clustering may take different form based on the data dimensionality. For example, the <a href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture" class="external text" title="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture" rel="nofollow">SOCR EM Mixture model segmentation activity and applet</a> shows how to obtain point, region or volume classification using the online <a href="/wiki/SOCR" title="SOCR" class="mw-redirect">SOCR</a> computational libraries.</li>
<li>In the fields of <a href="/wiki/Plant" title="Plant">plant</a> and <a href="/wiki/Animal" title="Animal">animal</a> <a href="/wiki/Ecology" title="Ecology">ecology</a>, clustering is used to describe and to make spatial and temporal comparisons of communities (assemblages) of organisms in heterogeneous environments; it is also used in <a href="/wiki/Systematics" title="Systematics">plant systematics</a> to generate artificial <a href="/wiki/Phylogeny" title="Phylogeny" class="mw-redirect">phylogenies</a> or clusters of organisms (individuals) at the species, genus or higher level that share a number of attributes</li>
<li>In computational biology and <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>:
<ul>
<li>In <a href="/wiki/Transcriptome" title="Transcriptome">transcriptomics</a>, clustering is used to build groups of <a href="/wiki/Genes" title="Genes" class="mw-redirect">genes</a> with related expression patterns (also known as coexpressed genes). Often such groups contain functionally related proteins, such as <a href="/wiki/Enzyme" title="Enzyme">enzymes</a> for a specific <a href="/wiki/Metabolic_pathway" title="Metabolic pathway">pathway</a>, or genes that are co-regulated. High throughput experiments using <a href="/wiki/Expressed_sequence_tag" title="Expressed sequence tag">expressed sequence tags</a> (ESTs) or <a href="/wiki/DNA_microarray" title="DNA microarray">DNA microarrays</a> can be a powerful tool for <a href="/wiki/Genome_annotation" title="Genome annotation" class="mw-redirect">genome annotation</a>, a general aspect of <a href="/wiki/Genomics" title="Genomics">genomics</a>.</li>
<li>In <a href="/wiki/Sequence_analysis" title="Sequence analysis">sequence analysis</a>, clustering is used to group homologous sequences into <a href="/wiki/List_of_gene_families" title="List of gene families">gene families</a>. This is a very important concept in bioinformatics, and <a href="/wiki/Evolutionary_biology" title="Evolutionary biology">evolutionary biology</a> in general. See evolution by <a href="/wiki/Gene_duplication" title="Gene duplication">gene duplication</a>.</li>
<li>In high-throughput genotyping platforms clustering algorithms are used to automatically assign <a href="/wiki/Genotypes" title="Genotypes" class="mw-redirect">genotypes</a>.</li>
</ul>
</li>
<li>In QSAR and molecular modeling studies as also chemoinformatics</li>
</ul>
<p><a name="Medicine" id="Medicine"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=18" title="Edit section: Medicine">edit</a>]</span> <span class="mw-headline">Medicine</span></h3>
<p>In <a href="/wiki/Medical_imaging" title="Medical imaging">medical imaging</a>, such as <a href="/wiki/PET_scan" title="PET scan" class="mw-redirect">PET scans</a>, cluster analysis can be used to differentiate between different types of <a href="/wiki/Tissue_(biology)" title="Tissue (biology)">tissue</a> and <a href="/wiki/Blood" title="Blood">blood</a> in a three dimensional image. In this application, actual position does not matter, but the <a href="/wiki/Voxel" title="Voxel">voxel</a> intensity is considered as a <a href="/wiki/Coordinate_vector" title="Coordinate vector">vector</a>, with a dimension for each image that was taken over time. This technique allows, for example, accurate measurement of the rate a radioactive tracer is delivered to the area of interest, without a separate sampling of <a href="/wiki/Arterial" title="Arterial" class="mw-redirect">arterial</a> blood, an intrusive technique that is most common today.</p>
<p><a name="Market_research" id="Market_research"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=19" title="Edit section: Market research">edit</a>]</span> <span class="mw-headline">Market research</span></h3>
<p>Cluster analysis is widely used in <a href="/wiki/Market_research" title="Market research">market research</a> when working with multivariate data from <a href="/wiki/Statistical_survey" title="Statistical survey">surveys</a> and test panels. Market researchers use cluster analysis to partition the general <a href="/wiki/Population" title="Population">population</a> of <a href="/wiki/Consumers" title="Consumers" class="mw-redirect">consumers</a> into market segments and to better understand the relationships between different groups of consumers/potential <a href="/wiki/Customers" title="Customers" class="mw-redirect">customers</a>.</p>
<ul>
<li>Segmenting the market and determining <a href="/wiki/Target_market" title="Target market">target markets</a></li>
<li><a href="/wiki/Positioning_(marketing)" title="Positioning (marketing)">Product positioning</a></li>
<li><a href="/wiki/New_product_development" title="New product development">New product development</a></li>
<li>Selecting test markets (see&#160;: <a href="/wiki/Experimental_techniques" title="Experimental techniques">experimental techniques</a>)</li>
</ul>
<p><a name="Other_applications" id="Other_applications"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=20" title="Edit section: Other applications">edit</a>]</span> <span class="mw-headline">Other applications</span></h3>
<dl>
<dt>Social network analysis</dt>
<dd>In the study of <a href="/wiki/Social_networks" title="Social networks" class="mw-redirect">social networks</a>, clustering may be used to recognize <a href="/wiki/Communities" title="Communities" class="mw-redirect">communities</a> within large groups of people.</dd>
<dt>Image segmentation</dt>
<dd>Clustering can be used to divide a <a href="/wiki/Digital" title="Digital">digital</a> <a href="/wiki/Image" title="Image">image</a> into distinct regions for <a href="/wiki/Border_detection" title="Border detection" class="mw-redirect">border detection</a> or <a href="/wiki/Object_recognition" title="Object recognition">object recognition</a>.</dd>
<dt>Data mining</dt>
<dd>Many <a href="/wiki/Data_mining" title="Data mining">data mining</a> applications involve partitioning data items into related subsets; the marketing applications discussed above represent some examples. Another common application is the division of documents, such as <a href="/wiki/World_Wide_Web" title="World Wide Web">World Wide Web</a> pages, into genres.</dd>
<dt>Search result grouping</dt>
<dd>In the process of intelligent grouping of the files and websites, clustering may be used to create a more relevant set of search results compared to normal search engines like <a href="/wiki/Google" title="Google">Google</a>. There are currently a number of web based clustering tools such as <a href="/wiki/Clusty" title="Clusty">Clusty</a>.</dd>
<dt>Slippy map optimization</dt>
<dd><a href="/wiki/Flickr" title="Flickr">Flickr</a>'s map of photos and other map sites use clustering to reduce the number of markers on a map. This makes it both faster and reduces the amount of visual clutter.</dd>
<dt>IMRT segmentation</dt>
<dd>Clustering can be used to divide a fluence map into distinct regions for conversion into deliverable fields in MLC-based Radiation Therapy.</dd>
<dt>Grouping of Shopping Items</dt>
<dd>Clustering can be used to group all the shopping items available on the web into a set of unique products. For example, all the items on eBay can be grouped into unique products. (eBay doesn't have the concept of a <a href="/wiki/Stock-keeping_unit" title="Stock-keeping unit">SKU</a>)</dd>
<dt><a href="/wiki/Mathematical_chemistry" title="Mathematical chemistry">Mathematical chemistry</a></dt>
<dd>To find structural similarity, etc., for example, 3000 chemical compounds were clustered in the space of 90 <a href="/wiki/Topological_index" title="Topological index">topological indices</a>.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9" title=""><span>[</span>10<span>]</span></a></sup></dd>
<dt>Petroleum Geology</dt>
<dd>Cluster Analysis is used to reconstruct missing bottom hole core data or missing log curves in order to evaluate reservoir properties.</dd>
<dt>Fysical Geography</dt>
<dd>The clustering of chemical properties in different sample locations.</dd>
</dl>
<p><a name="Comparisons_between_data_clusterings" id="Comparisons_between_data_clusterings"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=21" title="Edit section: Comparisons between data clusterings">edit</a>]</span> <span class="mw-headline">Comparisons between data clusterings</span></h2>
<p>There have been several suggestions for a measure of similarity between two clusterings. Such a measure can be used to compare how well different data clustering algorithms perform on a set of data. Many of these measures are derived from the <a href="/wiki/Matching_matrix" title="Matching matrix" class="mw-redirect">matching matrix</a> (aka <a href="/wiki/Confusion_matrix" title="Confusion matrix">confusion matrix</a>), e.g., the <a href="/wiki/Rand_index" title="Rand index">Rand measure</a> and the Fowlkes-Mallows <i>B</i><sub><i>k</i></sub> measures.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10" title=""><span>[</span>11<span>]</span></a></sup> With small variations, such kind of measures also include set matching based clustering criteria, e.g., the Meila and Heckerman.</p>
<p>Several different clustering systems based on <a href="/wiki/Mutual_information" title="Mutual information">mutual information</a> have been proposed. One is Marina Meila's 'Variation of Information' metric (see ref below); another provides hierarchical clustering<sup id="cite_ref-11" class="reference"><a href="#cite_note-11" title=""><span>[</span>12<span>]</span></a></sup>.</p>
<p>Recently, a new Mallows Distance based metric was also proposed for soft clustering comparisons.</p>
<p><a name="Algorithms" id="Algorithms"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=22" title="Edit section: Algorithms">edit</a>]</span> <span class="mw-headline">Algorithms</span></h2>
<p>In recent years considerable effort has been put into improving algorithm performance (Z. Huang, 1998). Among the most popular are <i><a href="/w/index.php?title=CLARANS&amp;action=edit&amp;redlink=1" class="new" title="CLARANS (page does not exist)">CLARANS</a></i> (Ng and Han,1994), <i><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></i> (Ester et al., 1996) and <i>BIRCH</i> (Zhang et al., 1996).</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=23" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a> (ANN)</li>
<li><a href="/wiki/Canopy_clustering_algorithm" title="Canopy clustering algorithm">Canopy clustering algorithm</a></li>
<li><a href="/wiki/Cluster-weighted_modeling" title="Cluster-weighted modeling">Cluster-weighted modeling</a></li>
<li><a href="/wiki/Consensus_clustering" title="Consensus clustering">Consensus clustering</a></li>
<li><a href="/wiki/Constrained_clustering" title="Constrained clustering">Constrained clustering</a></li>
<li><a href="/wiki/Cophenetic_correlation" title="Cophenetic correlation">Cophenetic correlation</a></li>
<li><a href="/wiki/Datamining" title="Datamining" class="mw-redirect">Datamining</a></li>
<li><a href="/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm">Expectation maximization</a> (EM)</li>
<li><a href="/wiki/FLAME_clustering" title="FLAME clustering">FLAME clustering</a></li>
<li><a href="/wiki/K-means" title="K-means" class="mw-redirect">K-means</a></li>
<li><a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">Multidimensional scaling</a></li>
<li><a href="/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">Neighbourhood components analysis</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">Self-organizing map</a></li>
<li><a href="/wiki/Silhouette_(clustering)" title="Silhouette (clustering)">Silhouette</a></li>
<li><a href="/wiki/Structured_data_analysis_(statistics)" title="Structured data analysis (statistics)">Structured data analysis (statistics)</a></li>
</ul>
<p><a name="Bibliography" id="Bibliography"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=24" title="Edit section: Bibliography">edit</a>]</span> <span class="mw-headline">Bibliography</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> J. MacQueen, 1967</li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <a href="http://www2007.org/program/paper.php?id=570" class="external text" title="http://www2007.org/program/paper.php?id=570" rel="nofollow">Google News personalization: scalable online collaborative filtering</a></li>
<li id="cite_note-2"><b><a href="#cite_ref-2" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREF.5B.5BKanti_Mardia.5D.5D_et_al.1979"><a href="/wiki/Kanti_Mardia" title="Kanti Mardia" class="mw-redirect">Kanti Mardia</a> et al. (1979). <i>Multivariate Analysis</i>. Academic Press.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Multivariate+Analysis&amp;rft.aulast=%5B%5BKanti+Mardia%5D%5D+et+al.&amp;rft.au=%5B%5BKanti+Mardia%5D%5D+et+al.&amp;rft.date=1979&amp;rft.pub=Academic+Press&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> See, e.g., <cite style="font-style:normal" class="" id="CITEREFDavid_J._Ketchen.2C_Jr_.26_Christopher_L._Shook1996">David J. Ketchen, Jr &amp; Christopher L. Shook (1996). "<a href="http://www3.interscience.wiley.com/cgi-bin/fulltext/17435/PDFSTART" class="external text" title="http://www3.interscience.wiley.com/cgi-bin/fulltext/17435/PDFSTART" rel="nofollow">The application of cluster analysis in Strategic Management Research: An analysis and critique</a>". <i><a href="/w/index.php?title=Strategic_Management_Journal&amp;action=edit&amp;redlink=1" class="new" title="Strategic Management Journal (page does not exist)">Strategic Management Journal</a></i> <b>17</b> (6): 441–458<span class="printonly">. <a href="http://www3.interscience.wiley.com/cgi-bin/fulltext/17435/PDFSTART" class="external free" title="http://www3.interscience.wiley.com/cgi-bin/fulltext/17435/PDFSTART" rel="nofollow">http://www3.interscience.wiley.com/cgi-bin/fulltext/17435/PDFSTART</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The+application+of+cluster+analysis+in+Strategic+Management+Research%3A+An+analysis+and+critique&amp;rft.jtitle=%5B%5BStrategic+Management+Journal%5D%5D&amp;rft.aulast=David+J.+Ketchen%2C+Jr+%26+Christopher+L.+Shook&amp;rft.au=David+J.+Ketchen%2C+Jr+%26+Christopher+L.+Shook&amp;rft.date=1996&amp;rft.volume=17&amp;rft.issue=6&amp;rft.pages=441%26ndash%3B458&amp;rft_id=http%3A%2F%2Fwww3.interscience.wiley.com%2Fcgi-bin%2Ffulltext%2F17435%2FPDFSTART&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> See, e.g., Figure 6 in
<ul>
<li><cite style="font-style:normal" class="" id="CITEREFCyril_Goutte.2C_Peter_Toft.2C_Egill_Rostrup.2C_Finn_.C3.85rup_Nielsen.2C_.5B.5BLars_Kai_Hansen.5D.5D1999">Cyril Goutte, Peter Toft, Egill Rostrup, Finn Årup Nielsen, <a href="/w/index.php?title=Lars_Kai_Hansen&amp;action=edit&amp;redlink=1" class="new" title="Lars Kai Hansen (page does not exist)">Lars Kai Hansen</a> (March 1999). "On Clustering fMRI Time Series". <i><a href="/wiki/NeuroImage" title="NeuroImage">NeuroImage</a></i> <b>9</b> (3): 298–310. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1006%2Fnimg.1998.0391" class="external text" title="http://dx.doi.org/10.1006%2Fnimg.1998.0391" rel="nofollow">10.1006/nimg.1998.0391</a></span>. <a href="http://www.ncbi.nlm.nih.gov/pubmed/10075900" class="external" title="http://www.ncbi.nlm.nih.gov/pubmed/10075900">PMID 10075900</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=On+Clustering+fMRI+Time+Series&amp;rft.jtitle=%5B%5BNeuroImage%5D%5D&amp;rft.aulast=Cyril+Goutte%2C+Peter+Toft%2C+Egill+Rostrup%2C+Finn+%C3%85rup+Nielsen%2C+%5B%5BLars+Kai+Hansen%5D%5D&amp;rft.au=Cyril+Goutte%2C+Peter+Toft%2C+Egill+Rostrup%2C+Finn+%C3%85rup+Nielsen%2C+%5B%5BLars+Kai+Hansen%5D%5D&amp;rft.date=March+1999&amp;rft.volume=9&amp;rft.issue=3&amp;rft.pages=298%26ndash%3B310&amp;rft_id=info:doi/10.1006%2Fnimg.1998.0391&amp;rft_id=info:pmid/10075900&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span></li>
</ul>
</li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREF.5B.5BRobert_L._Thorndike.5D.5D1953"><a href="/wiki/Robert_L._Thorndike" title="Robert L. Thorndike">Robert L. Thorndike</a> (December 1953). "Who Belong in the Family?". <i><a href="/wiki/Psychometrika" title="Psychometrika">Psychometrika</a></i> <b>18</b> (4).</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Who+Belong+in+the+Family%3F&amp;rft.jtitle=%5B%5BPsychometrika%5D%5D&amp;rft.aulast=%5B%5BRobert+L.+Thorndike%5D%5D&amp;rft.au=%5B%5BRobert+L.+Thorndike%5D%5D&amp;rft.date=December+1953&amp;rft.volume=18&amp;rft.issue=4&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFCyril_Goutte.2C_.5B.5BLars_Kai_Hansen.5D.5D.2C_Matthew_G._Liptrot_.26_Egill_Rostrup2001">Cyril Goutte, <a href="/w/index.php?title=Lars_Kai_Hansen&amp;action=edit&amp;redlink=1" class="new" title="Lars Kai Hansen (page does not exist)">Lars Kai Hansen</a>, Matthew G. Liptrot &amp; Egill Rostrup (2001). "<a href="http://www3.interscience.wiley.com/cgi-bin/fulltext/82002382/" class="external text" title="http://www3.interscience.wiley.com/cgi-bin/fulltext/82002382/" rel="nofollow">Feature-Space Clustering for fMRI Meta-Analysis</a>". <i><a href="/wiki/Human_Brain_Mapping" title="Human Brain Mapping">Human Brain Mapping</a></i> <b>13</b> (3): 165–183<span class="printonly">. <a href="http://www3.interscience.wiley.com/cgi-bin/fulltext/82002382/" class="external free" title="http://www3.interscience.wiley.com/cgi-bin/fulltext/82002382/" rel="nofollow">http://www3.interscience.wiley.com/cgi-bin/fulltext/82002382/</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Feature-Space+Clustering+for+fMRI+Meta-Analysis&amp;rft.jtitle=%5B%5BHuman+Brain+Mapping%5D%5D&amp;rft.aulast=Cyril+Goutte%2C+%5B%5BLars+Kai+Hansen%5D%5D%2C+Matthew+G.+Liptrot+%26+Egill+Rostrup&amp;rft.au=Cyril+Goutte%2C+%5B%5BLars+Kai+Hansen%5D%5D%2C+Matthew+G.+Liptrot+%26+Egill+Rostrup&amp;rft.date=2001&amp;rft.volume=13&amp;rft.issue=3&amp;rft.pages=165%26ndash%3B183&amp;rft_id=http%3A%2F%2Fwww3.interscience.wiley.com%2Fcgi-bin%2Ffulltext%2F82002382%2F&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span> see especially Figure 14 and appendix.</li>
<li id="cite_note-7"><b><a href="#cite_ref-7" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFPeter_J._Rousseuw1987">Peter J. Rousseuw (1987). "Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis". <i>Computational and Applied Mathematics</i> <b>20</b>: 53–65.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Silhouettes%3A+a+Graphical+Aid+to+the+Interpretation+and+Validation+of+Cluster+Analysis&amp;rft.jtitle=Computational+and+Applied+Mathematics&amp;rft.aulast=Peter+J.+Rousseuw&amp;rft.au=Peter+J.+Rousseuw&amp;rft.date=1987&amp;rft.volume=20&amp;rft.pages=53%26ndash%3B65&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-8"><b><a href="#cite_ref-8" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFR._Lleti.2C_M.C._Ortiz.2C_L.A._Sarabia.2C_M.S._S.C3.A1nchez2004">R. Lleti, M.C. Ortiz, L.A. Sarabia, M.S. Sánchez (2004). "Selecting Variables for k-Means Cluster Analysis by Using a Genetic Algorithm that Optimises the Silhouettes". <i>Analytica Chimica Acta</i> <b>515</b>: 87–100.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Selecting+Variables+for+k-Means+Cluster+Analysis+by+Using+a+Genetic+Algorithm+that+Optimises+the+Silhouettes&amp;rft.jtitle=Analytica+Chimica+Acta&amp;rft.aulast=R.+Lleti%2C+M.C.+Ortiz%2C+L.A.+Sarabia%2C+M.S.+S%C3%A1nchez&amp;rft.au=R.+Lleti%2C+M.C.+Ortiz%2C+L.A.+Sarabia%2C+M.S.+S%C3%A1nchez&amp;rft.date=2004&amp;rft.volume=515&amp;rft.pages=87%26ndash%3B100&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-9"><b><a href="#cite_ref-9" title="">^</a></b> Basak S.C., Magnuson V.R., Niemi C.J., Regal R.R. "Determing Structural Similarity of Chemicals Using Graph Theoretic Indices". <i>Discr. Appl. Math.</i>, <b>19</b>, 1988: 17-44.</li>
<li id="cite_note-10"><b><a href="#cite_ref-10" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFE._B._Fowlkes_.26_C._L._Mallows1983">E. B. Fowlkes &amp; C. L. Mallows (September 1983). "A Method for Comparing Two Hierarchical Clusterings". <i><a href="/wiki/Journal_of_the_American_Statistical_Association" title="Journal of the American Statistical Association">Journal of the American Statistical Association</a></i> <b>78</b> (383): 553–584. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.2307%2F2288117" class="external text" title="http://dx.doi.org/10.2307%2F2288117" rel="nofollow">10.2307/2288117</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A+Method+for+Comparing+Two+Hierarchical+Clusterings&amp;rft.jtitle=%5B%5BJournal+of+the+American+Statistical+Association%5D%5D&amp;rft.aulast=E.+B.+Fowlkes+%26+C.+L.+Mallows&amp;rft.au=E.+B.+Fowlkes+%26+C.+L.+Mallows&amp;rft.date=September+1983&amp;rft.volume=78&amp;rft.issue=383&amp;rft.pages=553%26ndash%3B584&amp;rft_id=info:doi/10.2307%2F2288117&amp;rfr_id=info:sid/en.wikipedia.org:Cluster_analysis"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-11"><b><a href="#cite_ref-11" title="">^</a></b> Alexander Kraskov, Harald Stögbauer, Ralph G. Andrzejak, and Peter Grassberger, "Hierarchical Clustering Based on Mutual Information", (2003) <i><a href="http://arxiv.org/abs/q-bio/0311039" class="external text" title="http://arxiv.org/abs/q-bio/0311039" rel="nofollow">ArXiv q-bio/0311039</a></i></li>
</ol>
</div>
<p><a name="Others" id="Others"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=25" title="Edit section: Others">edit</a>]</span> <span class="mw-headline">Others</span></h3>
<ul>
<li>Clatworthy, J., Buick, D., Hankins, M., Weinman, J., &amp; Horne, R. (2005). The use and reporting of cluster analysis in health psychology: A review. <i>British Journal of Health Psychology</i> 10: 329-358.</li>
<li>Cole, A. J. &amp; Wishart, D. (1970). An improved algorithm for the Jardine-Sibson method of generating overlapping clusters. <i>The Computer Journal</i> 13(2):156-163.</li>
</ul>
<ul>
<li>Ester, M., Kriegel, H.P., Sander, J., and Xu, X. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, Oregon, USA: AAAI Press, pp. 226–231.</li>
</ul>
<ul>
<li>Heyer, L.J., Kruglyak, S. and Yooseph, S., Exploring Expression Data: Identification and Analysis of Coexpressed Genes, <i>Genome Research</i> 9:1106-1115.</li>
</ul>
<ul>
<li>S. Kotsiantis, P. Pintelas, Recent Advances in Clustering: A Brief Survey, WSEAS Transactions on Information Science and Applications, Vol 1, No 1 (73-81), 2004.</li>
</ul>
<ul>
<li>Huang, Z. (1998). Extensions to the K-means Algorithm for Clustering Large Datasets with Categorical Values. <i>Data Mining and Knowledge Discovery</i>, 2, p. 283-304.</li>
</ul>
<ul>
<li>Jardine, N. &amp; Sibson, R. (1968). The construction of hierarchic and non-hierarchic classifications. <i>The Computer Journal</i> 11:177.</li>
<li><a href="http://www.inference.phy.cam.ac.uk/mackay/itila/" class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itila/" rel="nofollow">The on-line textbook: Information Theory, Inference, and Learning Algorithms</a>, by <a href="/wiki/David_J.C._MacKay" title="David J.C. MacKay" class="mw-redirect">David J.C. MacKay</a> includes chapters on k-means clustering, soft k-means clustering, and derivations including the E-M algorithm and the variational view of the E-M algorithm.</li>
<li>MacQueen, J. B. (1967). Some Methods for classification and Analysis of Multivariate Observations, Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, University of California Press, 1:281-297</li>
<li>Ng, R.T. and Han, J. 1994. Efficient and effective clustering methods for spatial data mining. Proceedings of the 20th VLDB Conference, Santiago, Chile, pp. 144–155.</li>
<li>Prinzie A., D. Van den Poel (2006), <a href="http://econpapers.repec.org/paper/rugrugwps/05_2F292.htm" class="external text" title="http://econpapers.repec.org/paper/rugrugwps/05_2F292.htm" rel="nofollow">Incorporating sequential information into traditional classification models by using an element/position-sensitive SAM</a>. <i>Decision Support Systems</i> 42 (2): 508-526.</li>
<li>Romesburg, H. Clarles, <i>Cluster Analysis for Researchers</i>, 2004, 340 pp. <a href="/wiki/Special:BookSources/1411606175" class="internal">ISBN 1-4116-0617-5</a>, reprint of 1990 edition published by <a href="/w/index.php?title=Krieger_Pub._Co.&amp;action=edit&amp;redlink=1" class="new" title="Krieger Pub. Co. (page does not exist)">Krieger Pub. Co.</a>.. A Japanese language translation is available from <a href="/w/index.php?title=Uchida_Rokakuho_Publishing_Co.&amp;action=edit&amp;redlink=1" class="new" title="Uchida Rokakuho Publishing Co. (page does not exist)">Uchida Rokakuho Publishing Co.</a>, Ltd., Tokyo, Japan.</li>
<li>Sheppard, A. G. (1996). The sequence of factor analysis and cluster analysis: Differences in segmentation and dimensionality through the use of raw and factor scores. Tourism Analysis, 1(Inaugural Volume), 49-57.</li>
<li>Zhang, T., Ramakrishnan, R., and Livny, M. 1996. BIRCH: An efficient data clustering method for very large databases. Proceedings of ACM SIGMOD Conference, Montreal, Canada, pp. 103–114.</li>
</ul>
<p>For spectral clustering:</p>
<ul>
<li>Jianbo Shi and Jitendra Malik, "Normalized Cuts and Image Segmentation", IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888-905, August 2000. Available on <a href="http://www.cs.berkeley.edu/~malik/malik-pubs-ptrs.html" class="external text" title="http://www.cs.berkeley.edu/~malik/malik-pubs-ptrs.html" rel="nofollow">Jitendra Malik's homepage</a></li>
<li>Marina Meila and Jianbo Shi, "Learning Segmentation with Random Walk", Neural Information Processing Systems, NIPS, 2001. Available from <a href="http://www.cis.upenn.edu/~jshi/jshi_publication.htm" class="external text" title="http://www.cis.upenn.edu/~jshi/jshi_publication.htm" rel="nofollow">Jianbo Shi's homepage</a></li>
<li>see referenced articles <a href="http://www.luigidragone.com/datamining/spectral-clustering.html#references" class="external text" title="http://www.luigidragone.com/datamining/spectral-clustering.html#references" rel="nofollow">here</a></li>
</ul>
<p>For estimating number of clusters:</p>
<ul>
<li>I. O. Kyrgyzov, O. O. Kyrgyzov, H. Maître and M. Campedel. <a href="http://www.tsi.enst.fr/~kyrgyzov/publications.html" class="external text" title="http://www.tsi.enst.fr/~kyrgyzov/publications.html" rel="nofollow">Kernel MDL to Determine the Number of Clusters</a>, <a href="http://www.springerlink.com/content/j646uqx4p435j530/" class="external text" title="http://www.springerlink.com/content/j646uqx4p435j530/" rel="nofollow">MLDM, pp. 203-217, 2007</a>.</li>
</ul>
<ul>
<li>Stan Salvador and Philip Chan, <a href="http://cs.fit.edu/~pkc/papers/ictai04salvador.pdf" class="external text" title="http://cs.fit.edu/~pkc/papers/ictai04salvador.pdf" rel="nofollow">Determining the Number of Clusters/Segments in Hierarchical Clustering/Segmentation Algorithms</a>, Proc. 16th IEEE Intl. Conf. on Tools with AI, pp. 576-584, 2004.</li>
<li>Can, F., Ozkarahan, E. A. (1990) "Concepts and effectiveness of the cover coefficient-based clustering methodology for text databases." ACM Transactions on Database Systems. 15 (4) 483-517.</li>
</ul>
<p>For discussion of the elbow criterion:</p>
<ul>
<li>Aldenderfer, M.S., Blashfield, R.K, <i>Cluster Analysis</i>, (1984), Newbury Park (CA): Sage.</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Cluster_analysis&amp;action=edit&amp;section=26" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_style.png" class="image" title="Ambox style.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/d/d6/Ambox_style.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article's <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">external links</a> <b>may not follow Wikipedia's <a href="/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_mirror_or_a_repository_of_links.2C_images.2C_or_media_files" title="Wikipedia:What Wikipedia is not">content policies</a> or <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">guidelines</a></b>. Please <a href="http://en.wikipedia.org/w/index.php?title=Cluster_analysis&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Cluster_analysis&amp;action=edit" rel="nofollow">improve this article</a> by removing excessive or inappropriate external links.</td>
</tr>
</table>
<ul>
<li><i><a href="http://adios.tau.ac.il/compact/" class="external text" title="http://adios.tau.ac.il/compact/" rel="nofollow">COMPACT - Comparative Package for Clustering Assessment</a></i>. A free Matlab package, 2006.</li>
<li>P. Berkhin, <i><a href="http://citeseer.ist.psu.edu/berkhin02survey.html" class="external text" title="http://citeseer.ist.psu.edu/berkhin02survey.html" rel="nofollow">Survey of Clustering Data Mining Techniques</a></i>, Accrue Software, 2002.</li>
<li>Jain, Murty and Flynn: <i><a href="http://citeseer.ist.psu.edu/jain99data.html" class="external text" title="http://citeseer.ist.psu.edu/jain99data.html" rel="nofollow">Data Clustering: A Review</a></i>, ACM Comp. Surv., 1999.</li>
<li><a href="http://www.phpandme.net/2009/03/clustering-web-pages-into-similar-topics/" class="external text" title="http://www.phpandme.net/2009/03/clustering-web-pages-into-similar-topics/" rel="nofollow">Clustering web documents using PHP</a></li>
<li>for another presentation of hierarchical, k-means and fuzzy c-means see this <a href="http://www.elet.polimi.it/upload/matteucc/Clustering/tutorial_html/index.html" class="external text" title="http://www.elet.polimi.it/upload/matteucc/Clustering/tutorial_html/index.html" rel="nofollow">introduction to clustering</a>. Also has an explanation on mixture of <a href="/wiki/Normal_distribution" title="Normal distribution">Gaussians</a>.</li>
<li>David Dowe, <i><a href="http://www.csse.monash.edu.au/~dld/cluster.html" class="external text" title="http://www.csse.monash.edu.au/~dld/cluster.html" rel="nofollow">Mixture Modelling page</a></i> - other clustering and mixture model links.</li>
<li>A tutorial on clustering <a href="http://gauss.nmsu.edu/~lludeman/video/ch6pr.html" class="external autonumber" title="http://gauss.nmsu.edu/~lludeman/video/ch6pr.html" rel="nofollow">[1]</a></li>
<li><a href="http://www.inference.phy.cam.ac.uk/mackay/itila/" class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itila/" rel="nofollow">The on-line textbook: Information Theory, Inference, and Learning Algorithms</a>, by <a href="/wiki/David_J.C._MacKay" title="David J.C. MacKay" class="mw-redirect">David J.C. MacKay</a> includes chapters on k-means clustering, soft k-means clustering, and derivations including the E-M algorithm and the variational view of the E-M algorithm.</li>
<li><a href="http://people.revoledu.com/kardi/tutorial/Clustering/index.html" class="external text" title="http://people.revoledu.com/kardi/tutorial/Clustering/index.html" rel="nofollow">Numerical example of Hierarchical Clustering</a></li>
<li><a href="http://cran.r-project.org/web/packages/kernlab/index.html" class="external text" title="http://cran.r-project.org/web/packages/kernlab/index.html" rel="nofollow">kernlab</a> - R package for kernel based machine learning (includes spectral clustering implementation)</li>
<li><a href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/" class="external text" title="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/" rel="nofollow">Tutorial</a> - Tutorial with introduction of Clustering Algorithms (k-means, fuzzy-c-means, hierarchical, mixture of gaussians) + some interactive demos (java applets)</li>
<li><a href="http://www.dmoz.org/Computers/Software/Databases/Data_Mining/Public_Domain_Software/" class="external text" title="http://www.dmoz.org/Computers/Software/Databases/Data_Mining/Public_Domain_Software/" rel="nofollow">Data Mining Software</a> at the <a href="/wiki/Open_Directory_Project" title="Open Directory Project">Open Directory Project</a></li>
<li><a href="http://homepages.feis.herts.ac.uk/~nngroup/software.php" class="external text" title="http://homepages.feis.herts.ac.uk/~nngroup/software.php" rel="nofollow">Java Competitive Learning Application</a> A suite of Unsupervised Neural Networks for clustering. Written in Java. Complete with all source code.</li>
<li><a href="http://dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Software/" class="external text" title="http://dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Software/" rel="nofollow">Machine Learning Software</a> - Also contains much clustering software.</li>
<li><a href="http://www.youtube.com/watch?v=1ZDybXl212Q" class="external text" title="http://www.youtube.com/watch?v=1ZDybXl212Q" rel="nofollow">Cluster Computing and MapReduce Lecture 4</a></li>
<li><a href="http://factominer.free.fr/" class="external text" title="http://factominer.free.fr/" rel="nofollow">FactoMineR</a> (free exploratory multivariate data analysis software linked to <a href="/wiki/R_programming_language" title="R programming language" class="mw-redirect">R</a>)</li>
<li><a href="http://www.springer.com/statistics/statistical+theory+and+methods/journal/357" class="external text" title="http://www.springer.com/statistics/statistical+theory+and+methods/journal/357" rel="nofollow">The Journal of Classification</a>. A publication of the <a href="http://thames.cs.rhul.ac.uk/~fionn/classification-society" class="external text" title="http://thames.cs.rhul.ac.uk/~fionn/classification-society" rel="nofollow">Classification Society of North America</a> that specializes on the mathematical and statistical theory of cluster analysis.</li>
<li><a href="http://ai4r.rubyforge.org/index.html" class="external text" title="http://ai4r.rubyforge.org/index.html" rel="nofollow">Data clustering algorithms implementation in Ruby (AI4R)</a></li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 4235/1000000
Post-expand include size: 32226/2048000 bytes
Template argument size: 11820/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:669675-0!1!0!default!!en!2 and timestamp 20090406012311 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Cluster_analysis">http://en.wikipedia.org/wiki/Cluster_analysis</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Data_mining" title="Category:Data mining">Data mining</a></span> | <span dir='ltr'><a href="/wiki/Category:Data_analysis" title="Category:Data analysis">Data analysis</a></span> | <span dir='ltr'><a href="/wiki/Category:Data_clustering_algorithms" title="Category:Data clustering algorithms">Data clustering algorithms</a></span> | <span dir='ltr'><a href="/wiki/Category:Geostatistics" title="Category:Geostatistics">Geostatistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></span> | <span dir='ltr'><a href="/wiki/Category:Multivariate_statistics" title="Category:Multivariate statistics">Multivariate statistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Knowledge_discovery_in_databases" title="Category:Knowledge discovery in databases">Knowledge discovery in databases</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Wikipedia_external_links_cleanup" title="Category:Wikipedia external links cleanup">Wikipedia external links cleanup</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Cluster_analysis" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Cluster_analysis" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Cluster_analysis&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Cluster_analysis&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Cluster_analysis" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Cluster_analysis" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Cluster_analysis" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Cluster_analysis&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Cluster_analysis&amp;oldid=282007120" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Cluster_analysis&amp;id=282007120">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ca"><a href="http://ca.wikipedia.org/wiki/Clusteritzaci%C3%B3_de_dades">Català</a></li>
				<li class="interwiki-cs"><a href="http://cs.wikipedia.org/wiki/Shlukov%C3%A1_anal%C3%BDza">Česky</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Clusteranalyse">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Algoritmo_de_agrupamiento">Español</a></li>
				<li class="interwiki-eu"><a href="http://eu.wikipedia.org/wiki/Multzokatze_(estatistika)">Euskara</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Partitionnement_de_donn%C3%A9es">Français</a></li>
				<li class="interwiki-hr"><a href="http://hr.wikipedia.org/wiki/Grupiranje">Hrvatski</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Clustering">Italiano</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%BB%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%AA%E3%83%B3%E3%82%B0">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Analiza_skupie%C5%84">Polski</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/Clustering">Português</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7">Русский</a></li>
				<li class="interwiki-sl"><a href="http://sl.wikipedia.org/wiki/Grupiranje">Slovenščina</a></li>
				<li class="interwiki-th"><a href="http://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%81%E0%B8%9A%E0%B9%88%E0%B8%87%E0%B8%81%E0%B8%A5%E0%B8%B8%E0%B9%88%E0%B8%A1%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5">ไทย</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/Ph%C3%A2n_nh%C3%B3m_d%E1%BB%AF_li%E1%BB%87u">Tiếng Việt</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 6 April 2009, at 01:23.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv188 in 0.057 secs. --></body></html>
