<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="CPU cache,Articles with unsourced statements since March 2008,CPU technologies,128-bit,32-bit,64-bit,8-bit,AMD,Address space,Advanced Configuration and Power Interface,Advanced Power Management" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=CPU_cache&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=CPU_cache&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>CPU cache - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "CPU_cache";
		var wgTitle = "CPU cache";
		var wgAction = "view";
		var wgArticleId = "849181";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281582324;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [""];
		var wgRestrictionMove = [""];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-CPU_cache skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">CPU cache</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<div class="dablink">"Cache memory" redirects here. For the general use, see <a href="/wiki/Cache" title="Cache">cache</a>.</div>
<div class="thumb tright">
<div class="thumbinner" style="width:252px;"><a href="/wiki/File:Cache,basic.svg" class="image" title="Diagram of a CPU cache"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Cache%2Cbasic.svg/250px-Cache%2Cbasic.svg.png" width="250" height="109" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Cache,basic.svg" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
Diagram of a CPU cache</div>
</div>
</div>
<p>A <b>CPU cache</b> is a <a href="/wiki/Cache" title="Cache">cache</a> used by the <a href="/wiki/Central_processing_unit" title="Central processing unit">central processing unit</a> of a <a href="/wiki/Computer" title="Computer">computer</a> to reduce the average time to access <a href="/wiki/Computer_storage" title="Computer storage" class="mw-redirect">memory</a>. The cache is a smaller, faster memory which stores copies of the data from the most frequently used <a href="/wiki/Main_memory" title="Main memory" class="mw-redirect">main memory</a> locations. As long as most memory accesses are cached memory locations, the average <a href="/wiki/RAM_latency" title="RAM latency" class="mw-redirect">latency</a> of memory accesses will be closer to the cache latency than to the latency of main memory.</p>
<p>When the processor needs to read from or write to a location in main memory, it first checks whether a copy of that data is in the cache. If so, the processor immediately reads from or writes to the cache, which is much faster than reading from or writing to main memory.</p>
<p>The diagram on the right shows two memories. Each location in each memory has a datum (a <i>cache line</i>), which in different designs ranges in size from 8 to 512 <a href="/wiki/Byte" title="Byte">bytes</a>. The size of the cache line is usually larger than the size of the usual access requested by a CPU instruction, which ranges from 1 to 16 bytes. Each location in each memory also has an index, which is a unique number used to refer to that location. The index for a location in main memory is called an <a href="/wiki/Memory_address" title="Memory address">address</a>. Each location in the cache has a tag that contains the index of the datum in main memory that has been cached. In a CPU's data cache these entries are called <i>cache lines</i> or <i>cache blocks</i>.</p>
<p>Most modern desktop and server CPUs have at least three independent caches: an <b>instruction cache</b> to speed up executable instruction fetch, a <b>data cache</b> to speed up data fetch and store, and a <a href="/wiki/Translation_lookaside_buffer" title="Translation lookaside buffer">translation lookaside buffer</a> used to speed up virtual-to-physical address translation for both executable instructions and data.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Details_of_operation"><span class="tocnumber">1</span> <span class="toctext">Details of operation</span></a></li>
<li class="toclevel-1"><a href="#Structure"><span class="tocnumber">2</span> <span class="toctext">Structure</span></a></li>
<li class="toclevel-1"><a href="#Associativity"><span class="tocnumber">3</span> <span class="toctext">Associativity</span></a>
<ul>
<li class="toclevel-2"><a href="#Pseudo-associative_cache"><span class="tocnumber">3.1</span> <span class="toctext">Pseudo-associative cache</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Cache_misses"><span class="tocnumber">4</span> <span class="toctext">Cache misses</span></a></li>
<li class="toclevel-1"><a href="#Address_translation"><span class="tocnumber">5</span> <span class="toctext">Address translation</span></a>
<ul>
<li class="toclevel-2"><a href="#Virtual_indexing_and_virtual_aliases"><span class="tocnumber">5.1</span> <span class="toctext">Virtual indexing and virtual aliases</span></a></li>
<li class="toclevel-2"><a href="#Virtual_tags_and_vhints"><span class="tocnumber">5.2</span> <span class="toctext">Virtual tags and vhints</span></a></li>
<li class="toclevel-2"><a href="#Page_coloring"><span class="tocnumber">5.3</span> <span class="toctext">Page coloring</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Cache_hierarchy_in_a_modern_processor"><span class="tocnumber">6</span> <span class="toctext">Cache hierarchy in a modern processor</span></a>
<ul>
<li class="toclevel-2"><a href="#Specialized_caches"><span class="tocnumber">6.1</span> <span class="toctext">Specialized caches</span></a>
<ul>
<li class="toclevel-3"><a href="#Victim_cache"><span class="tocnumber">6.1.1</span> <span class="toctext">Victim cache</span></a></li>
<li class="toclevel-3"><a href="#Trace_cache"><span class="tocnumber">6.1.2</span> <span class="toctext">Trace cache</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="#Multi-level_caches"><span class="tocnumber">6.2</span> <span class="toctext">Multi-level caches</span></a>
<ul>
<li class="toclevel-3"><a href="#Exclusive_versus_inclusive"><span class="tocnumber">6.2.1</span> <span class="toctext">Exclusive versus inclusive</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="#Example:_the_K8"><span class="tocnumber">6.3</span> <span class="toctext">Example: the K8</span></a></li>
<li class="toclevel-2"><a href="#More_hierarchies"><span class="tocnumber">6.4</span> <span class="toctext">More hierarchies</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Implementation"><span class="tocnumber">7</span> <span class="toctext">Implementation</span></a>
<ul>
<li class="toclevel-2"><a href="#History"><span class="tocnumber">7.1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-2"><a href="#History_of_cache_in_x86_architecture"><span class="tocnumber">7.2</span> <span class="toctext">History of cache in x86 architecture</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes_and_references"><span class="tocnumber">9</span> <span class="toctext">Notes and references</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">10</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Details_of_operation" id="Details_of_operation"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=1" title="Edit section: Details of operation">edit</a>]</span> <span class="mw-headline">Details of operation</span></h2>
<p>When the processor needs to read or write a location in main memory, it first checks whether that memory location is in the cache. This is accomplished by comparing the address of the memory location to all tags in the cache that might contain that address. If the processor finds that the memory location is in the cache, we say that a <i>cache hit</i> has occurred, otherwise we speak of a <i>cache miss</i>. In the case of a cache hit, the processor immediately reads or writes the data in the cache line. The proportion of accesses that result in a cache hit is known as the <i>hit rate</i>, and is a measure of the effectiveness of the cache.</p>
<p>In the case of a cache miss, most caches allocate a new entry, which comprises the tag just missed and a copy of the data from memory. The reference can then be applied to the new entry just as in the case of a hit. Misses are comparatively slow because they require the data to be transferred from main memory. This transfer incurs a delay since main memory is much slower than cache memory, and also incurs the overhead for recording the new data in the cache before it is delivered to the processor.</p>
<p>In order to make room for the new entry on a cache miss, the cache generally has to <i>evict</i> one of the existing entries. The <a href="/wiki/Heuristic_(computer_science)" title="Heuristic (computer science)" class="mw-redirect">heuristic</a> that it uses to choose the entry to evict is called the <i><a href="/wiki/Cache_algorithms" title="Cache algorithms">replacement policy</a></i>. The fundamental problem with any replacement policy is that it must predict which existing cache entry is least likely to be used in the future. Predicting the future is difficult, especially for hardware caches that use simple rules amenable to implementation in circuitry, so there are a variety of replacement policies to choose from and no perfect way to decide among them. One popular replacement policy, <a href="/wiki/Cache_algorithms" title="Cache algorithms">LRU</a>, replaces the least recently used entry.</p>
<p>When data is written to the cache, it must at some point be written to main memory as well. The timing of this write is controlled by what is known as the <i>write policy</i>. In a <i>write-through</i> cache, every write to the cache causes a write to main memory. Alternatively, in a <i>write-back</i> or <i>copy-back</i> cache, writes are not immediately mirrored to memory. Instead, the cache tracks which locations have been written over (these locations are marked <i>dirty</i>). The data in these locations are written back to main memory when that data is evicted from the cache. For this reason, a miss in a write-back cache will often require <i>two</i> memory accesses to service: one to first write the dirty location to memory and then another to read the new location from memory.</p>
<p>There are intermediate policies as well. The cache may be write-through, but the writes may be held in a store data queue temporarily, usually so that multiple stores can be processed together (which can reduce <a href="/wiki/Computer_bus" title="Computer bus" class="mw-redirect">bus</a> turnarounds and so improve bus utilization).</p>
<p>The data in main memory being cached may be changed by other entities, in which case the copy in the cache may become out-of-date or <i>stale</i>. Alternatively, when the CPU updates the data in the cache, copies of data in other caches will become stale. Communication protocols between the cache managers which keep the data consistent are known as <i><a href="/wiki/Cache_coherence" title="Cache coherence">cache coherence</a> protocols</i>.</p>
<p>The time taken to fetch a datum from memory (read latency) matters because a CPU will often run out of things to do while waiting for the datum. When a CPU reaches this state, it is called a <i>stall</i>. As CPUs become faster, stalls due to cache misses displace more potential computation; modern CPUs can execute hundreds of instructions in the time taken to fetch a single datum from memory. Various techniques have been employed to keep the CPU busy during this time. <i><a href="/wiki/Out-of-order_execution" title="Out-of-order execution">Out-of-order</a></i> CPUs (<a href="/wiki/Pentium_Pro" title="Pentium Pro">Pentium Pro</a> and later <a href="/wiki/Intel" title="Intel" class="mw-redirect">Intel</a> designs, for example) attempt to execute independent instructions after the instruction that is waiting for the cache miss data. Another technology, used by many processors, is <a href="/wiki/Simultaneous_multithreading" title="Simultaneous multithreading">simultaneous multithreading</a> (SMT), or in Intel's terminology <a href="/wiki/Hyper-threading" title="Hyper-threading">hyper-threading</a> (HT), which allows an alternate <a href="/wiki/Thread_(computer_science)" title="Thread (computer science)">thread</a> to use the CPU core while a first thread waits for data to come from main memory.</p>
<p><a name="Structure" id="Structure"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=2" title="Edit section: Structure">edit</a>]</span> <span class="mw-headline">Structure</span></h2>
<p>Cache row entries usually have the following structure:</p>
<table style="width:30%; text-align:center" border="1">
<tr>
<td>Data blocks</td>
<td>Tag</td>
<td>Index</td>
<td>Displacement</td>
<td>Valid bit</td>
</tr>
</table>
<p>The data blocks contain the actual data fetched from the main memory. The memory address is split (MSB to LSB) into the tag, the index and the displacement (offset), while the valid bit denotes that this particular entry has valid data. The index length is <span class="texhtml"><i>l</i><i>o</i><i>g</i><sub>2</sub>(<i>c</i><i>a</i><i>c</i><i>h</i><i>e</i>_<i>r</i><i>o</i><i>w</i><i>s</i>)</span> bits and describes which row the data has been put in. The displacement length is <span class="texhtml"><i>l</i><i>o</i><i>g</i><sub>2</sub>(<i>d</i><i>a</i><i>t</i><i>a</i>_<i>b</i><i>l</i><i>o</i><i>c</i><i>k</i><i>s</i>)</span> and specifies which block of the ones we have stored we need. The tag length is <span class="texhtml"><i>a</i><i>d</i><i>d</i><i>r</i><i>e</i><i>s</i><i>s</i> − <i>i</i><i>n</i><i>d</i><i>e</i><i>x</i> − <i>d</i><i>i</i><i>s</i><i>p</i><i>l</i><i>a</i><i>c</i><i>e</i><i>m</i><i>e</i><i>n</i><i>t</i></span> and contains the most significant bits of the address, which are checked against the current row (the row has been retrieved by index) to see if it is the one we need or another, irrelevant memory location that happened to have the same index bits as the one we want.</p>
<p><a name="Associativity" id="Associativity"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=3" title="Edit section: Associativity">edit</a>]</span> <span class="mw-headline">Associativity</span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:452px;"><a href="/wiki/File:Cache,associative-fill-both.png" class="image" title="Which memory locations can be cached by which cache locations"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Cache%2Cassociative-fill-both.png/450px-Cache%2Cassociative-fill-both.png" width="450" height="223" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Cache,associative-fill-both.png" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
Which memory locations can be cached by which cache locations</div>
</div>
</div>
<p>The replacement policy decides where in the cache a copy of a particular entry of main memory will go. If the replacement policy is free to choose any entry in the cache to hold the copy, the cache is called <b>fully associative</b>. At the other extreme, if each entry in main memory can go in just one place in the cache, the cache is <b>direct mapped</b>. Many caches implement a compromise in which each entry in main memory can go to any one of N places in the cache, and are described as <b>N-way set associative</b>. For example, the level-1 data cache in an <a href="/wiki/AMD" title="AMD" class="mw-redirect">AMD</a> <a href="/wiki/Athlon" title="Athlon">Athlon</a> is 2-way set associative, which means that any particular location in main memory can be cached in either of 2 locations in the level-1 data cache.</p>
<p>Associativity is a <a href="/wiki/Trade-off" title="Trade-off">trade-off</a>. If there are ten places the replacement policy can put a new cache entry, then when the cache is checked for a hit, all ten places must be searched. Checking more places takes more power, chip area, and potentially time. On the other hand, caches with more associativity suffer fewer misses (see conflict misses, below), so that the CPU spends less time servicing those misses. The rule of thumb is that doubling the associativity, from direct mapped to 2-way, or from 2-way to 4-way, has about the same effect on hit rate as doubling the cache size. Associativity increases beyond 4-way have much less effect on the hit rate, and are generally done for other reasons (see virtual aliasing, below).</p>
<p>In order of increasing (worse) hit times and decreasing (better) miss rates,</p>
<ul>
<li>direct mapped cache -- the best (fastest) hit times, and so the best tradeoff for "large" caches</li>
<li>2-way set associative cache</li>
<li>2-way skewed associative cache -- "the best tradeoff for .... caches whose sizes are in the range 4K-8K bytes" -- André Seznec<sup id="cite_ref-Seznec_0-0" class="reference"><a href="#cite_note-Seznec-0" title=""><span>[</span>1<span>]</span></a></sup></li>
<li>4-way set associative cache</li>
<li>fully associative cache -- the best (lowest) miss rates, and so the best tradeoff when the miss penalty is very high</li>
</ul>
<p>If each location in main memory can be cached in either of two locations in the cache, one logical question is: <i>which two?</i> The simplest and most commonly used scheme, shown in the right-hand diagram above, is to use the least significant bits of the memory location's index as the index for the cache memory, and to have two entries for each index. One good property of this scheme is that the tags stored in the cache do not have to include that part of the main memory address which is implied by the cache memory's index. Since the cache tags are fewer bits, they take less area [on the microprocessor chip] and can be read and compared faster.</p>
<p>One of the advantages of a direct mapped cache is that it allows simple and fast <a href="/wiki/Speculative_execution" title="Speculative execution">speculation</a>. Once the address has been computed, the one cache index which might have a copy of that datum is known. That cache entry can be read, and the processor can continue to work with that data before it finishes checking that the tag actually matches the requested address.</p>
<p>The idea of having the processor use the cached data before the tag match completes can be applied to associative caches as well. A subset of the tag, called a <i>hint</i>, can be used to pick just one of the possible cache entries mapping to the requested address. This datum can then be used in parallel with checking the full tag. The hint technique works best when used in the context of address translation, as explained below.</p>
<p>Other schemes have been suggested, such as the <i>skewed cache</i><sup id="cite_ref-Seznec_0-1" class="reference"><a href="#cite_note-Seznec-0" title=""><span>[</span>1<span>]</span></a></sup>, where the index for way 0 is direct, as above, but the index for way 1 is formed with a <a href="/wiki/Hash_function" title="Hash function">hash function</a>. A good hash function has the property that addresses which conflict with the direct mapping tend not to conflict when mapped with the hash function, and so it is less likely that a program will suffer from an unexpectedly large number of conflict misses due to a pathological access pattern. The downside is extra latency from computing the hash function<sup id="cite_ref-CK_1-0" class="reference"><a href="#cite_note-CK-1" title=""><span>[</span>2<span>]</span></a></sup>. Additionally, when it comes time to load a new line and evict an old line, it may be difficult to determine which existing line was least recently used, because the new line conflicts with data at different indexes in each way; <a href="/wiki/Cache_algorithms" title="Cache algorithms">LRU</a> tracking for non-skewed caches is usually done on a per-set basis. Nevertheless, skewed-associative caches have major advantages over conventional set-associative ones.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2" title=""><span>[</span>3<span>]</span></a></sup></p>
<p><a name="Pseudo-associative_cache" id="Pseudo-associative_cache"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=4" title="Edit section: Pseudo-associative cache">edit</a>]</span> <span class="mw-headline">Pseudo-associative cache</span></h3>
<p>A true set-associative cache tests all the possible ways simultaneously, using something like a <a href="/wiki/Content_addressable_memory" title="Content addressable memory" class="mw-redirect">content addressable memory</a>. A pseudo-associative cache tests each possible way one at a time. A hash-rehash cache is one kind of pseudo-associative cache.</p>
<p>In the common case of finding a hit in the first way tested, a pseudo-associative cache is as fast as a direct-mapped cache. But it has a much lower conflict miss rate than a direct-mapped cache, closer to the miss rate of a fully associative cache. <sup id="cite_ref-CK_1-1" class="reference"><a href="#cite_note-CK-1" title=""><span>[</span>2<span>]</span></a></sup></p>
<p><a name="Cache_misses" id="Cache_misses"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=5" title="Edit section: Cache misses">edit</a>]</span> <span class="mw-headline">Cache misses</span></h2>
<p>A cache miss refers to a failed attempt to read or write a piece of data in the cache, which results in a main memory access with much longer latency. There are three kinds of cache misses: instruction read miss, data read miss, and data write miss.</p>
<p>A cache read miss from an instruction cache generally causes the most delay, because the processor, or at least <a href="/wiki/Simultaneous_multithreading" title="Simultaneous multithreading">the thread of execution</a>, has to wait (stall) until the instruction is fetched from main memory.</p>
<p>A cache read miss from a data cache usually causes less delay, because instructions not dependent on the cache read can be issued and continue execution until the data is returned from main memory, and the dependent instructions can resume execution.</p>
<p>A cache write miss to a data cache generally causes the least delay, because the write can be queued and there are few limitations on the execution of subsequent instructions. The processor can continue until the queue is full.</p>
<p>In order to lower cache miss rate, a great deal of analysis has been done on cache behavior in an attempt to find the best combination of size, associativity, block size, and so on. Sequences of memory references performed by benchmark programs are saved as <i>address traces</i>. Subsequent analyses simulate many different possible cache designs on these long address traces. Making sense of how the many variables affect the cache hit rate can be quite confusing. One significant contribution to this analysis was made by <a href="http://www.cs.wisc.edu/~markhill" class="external text" title="http://www.cs.wisc.edu/~markhill" rel="nofollow">Mark Hill</a>, who separated misses into three categories (known as the Three Cs):</p>
<ul>
<li><i>Compulsory misses</i> are those misses caused by the first reference to a datum. Cache size and associativity make no difference to the number of compulsory misses. Prefetching can help here, as can larger cache block sizes (which are a form of prefetching). Compulsory misses are sometimes referred to as Cold Misses.</li>
</ul>
<ul>
<li><i>Capacity misses</i> are those misses that occur regardless of associativity or block size, solely due to the finite size of the cache. The curve of capacity miss rate versus cache size gives some measure of the temporal locality of a particular reference stream. Note that there is no useful notion of a cache being "full" or "empty" or "near capacity": CPU caches almost always have nearly every line filled with a copy of some line in main memory, and nearly every allocation of a new line requires the eviction of an old line.</li>
</ul>
<ul>
<li><i>Conflict misses</i> are those misses that could have been avoided, had the cache not evicted an entry earlier. Conflict misses can be further broken down into <i>mapping misses</i>, that are unavoidable given a particular amount of associativity, and <i>replacement misses</i>, which are due to the particular victim choice of the replacement policy.</li>
</ul>
<div class="thumb tright">
<div class="thumbinner" style="width:402px;"><a href="/wiki/File:Cache,missrate.svg" class="image" title="Miss rate versus cache size on the Integer portion of SPEC CPU2000"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/thumb/5/56/Cache%2Cmissrate.svg/400px-Cache%2Cmissrate.svg.png" width="400" height="320" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Cache,missrate.svg" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
Miss rate versus cache size on the Integer portion of SPEC CPU2000</div>
</div>
</div>
<p>The graph to the right summarizes the cache performance seen on the Integer portion of the SPEC CPU2000 benchmarks, as collected by Hill and Cantin <a href="http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/" class="external autonumber" title="http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/" rel="nofollow">[1]</a>. These <a href="/wiki/Benchmark_(computing)" title="Benchmark (computing)">benchmarks</a> are intended to represent the kind of workload that an engineering workstation computer might see on any given day. The reader should keep in mind that finding benchmarks which are even usefully representative of many programs has been very difficult, and there will always be important programs with very different behavior than what is shown here.</p>
<p>We can see the different effects of the three Cs in this graph.</p>
<p>At the far right, with cache size labelled "Inf", we have the compulsory misses. If we wish to improve a machine's performance on SpecInt2000, increasing the cache size beyond 1 MiB is essentially futile. That's the insight given by the compulsory misses.</p>
<p>The fully-associative cache miss rate here is almost representative of the capacity miss rate. The difference is that the data presented is from simulations assuming an LRU replacement policy. Showing the capacity miss rate would require a perfect replacement policy, i.e. an oracle that looks into the future to find a cache entry which is actually not going to be hit.</p>
<p>Note that our approximation of the capacity miss rate falls steeply between 32 <a href="/wiki/Kibibyte" title="Kibibyte">KiB</a> and 64 KiB. This indicates that the benchmark has a <i><a href="/wiki/Working_set" title="Working set">working set</a></i> of roughly 64 KiB. A CPU cache designer examining this benchmark will have a strong incentive to set the cache size to 64 KiB rather than 32 KiB. Note that, on this benchmark, no amount of associativity can make a 32 KiB cache perform as well as a 64 KiB 4-way, or even a direct-mapped 128 KiB cache.</p>
<p>Finally, note that between 64 KiB and 1 MiB there is a large difference between direct-mapped and fully-associative caches. This difference is the conflict miss rate. The insight from looking at conflict miss rates is that secondary caches benefit a great deal from high associativity.</p>
<p>This benefit was well known in the late 80s and early 90s, when CPU designers could not fit large caches on-chip, and could not get sufficient bandwidth to either the cache data memory or cache tag memory to implement high associativity in off-chip caches. Desperate hacks were attempted: the <a href="/wiki/MIPS_architecture" title="MIPS architecture">MIPS</a> <a href="/wiki/R8000" title="R8000">R8000</a> used expensive off-chip dedicated tag <a href="/wiki/Static_random_access_memory" title="Static random access memory">SRAMs</a>, which had embedded tag comparators and large drivers on the match lines, in order to implement a 4 MiB 4-way associative cache. The MIPS <a href="/wiki/R10000" title="R10000">R10000</a> used ordinary SRAM chips for the tags. Tag access for both ways took two cycles. To reduce latency, the R10000 would guess which way of the cache would hit on each access.</p>
<p><a name="Address_translation" id="Address_translation"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=6" title="Edit section: Address translation">edit</a>]</span> <span class="mw-headline">Address translation</span></h2>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/Translation_lookaside_buffer" title="Translation lookaside buffer">translation lookaside buffer</a></div>
<p>Most general purpose CPUs implement some form of <a href="/wiki/Virtual_memory" title="Virtual memory">virtual memory</a>. To summarize, each program running on the machine sees its own simplified <a href="/wiki/Address_space" title="Address space">address space</a>, which contains code and data for that program only. Each program uses this virtual address space without regard for where it exists in physical memory.</p>
<p>Virtual memory requires the processor to translate virtual addresses generated by the program into physical addresses in main memory. The portion of the processor that does this translation is known as the <a href="/wiki/Memory_management_unit" title="Memory management unit">memory management unit</a> (MMU). The fast path through the MMU can perform those translations stored in the <a href="/wiki/Translation_Lookaside_Buffer" title="Translation Lookaside Buffer" class="mw-redirect">Translation Lookaside Buffer</a> (TLB), which is a cache of mappings from the operating system's <a href="/wiki/Page_table" title="Page table">page table</a>. ' For the purposes of the <b>present</b> discussion, there are three important features of address translation:</p>
<ul>
<li><b>Latency:</b> The physical address is available from the MMU some time, perhaps a few cycles, after the virtual address is available from the address generator.</li>
</ul>
<ul>
<li><b>Aliasing:</b> Multiple virtual addresses can map to a single physical address. Most processors guarantee that all updates to that single physical address will happen in program order. To deliver on that guarantee, the processor must ensure that only one copy of a physical address resides in the cache at any given time.</li>
</ul>
<ul>
<li><b>Granularity:</b> The virtual address space is broken up into pages. For instance, a 4 GiB virtual address space might be cut up into 1048576 4 KiB pages, each of which can be independently mapped. There may be multiple page sizes supported; see <a href="/wiki/Virtual_memory" title="Virtual memory">virtual memory</a> for elaboration.</li>
</ul>
<p>A historical note: the first virtual memory systems were very slow, because they required an access to the page table (held in main memory) before every programmed access to main memory. With no caches, this effectively cut the speed of the machine in half. The first hardware cache used in a computer system was not actually a data or instruction cache, but rather a TLB.</p>
<p>Virtually indexed caches use a portion of the virtual address for their index, which is available earlier than the physical address. If the cache is either direct mapped or virtually tagged, there is no need to consult the MMU to determine which data to feed back to the execution datapath, and so the cache can be very fast (especially if small, as were the primary data caches in the first two generations of the Intel Pentium 4). The speed of this recurrence (the <b>load latency</b>) is crucial to CPU performance, and so most modern level-1 caches are virtually indexed, which at least allows the MMU's TLB lookup to proceed in parallel with fetching the data from the cache RAM.</p>
<p>But virtual indexing is not the best choice for all cache levels. It introduces the problem of virtual aliases—the cache may have multiple locations which can store the value of a single physical address. The cost of dealing with virtual aliases grows with cache size, and as a result most level-2 and larger caches are physically indexed.</p>
<p>Caches have historically used both virtual and physical addresses for the cache tags, although virtual tagging is now uncommon. If the TLB lookup can finish before the cache RAM lookup, then the physical address is available in time for tag compare, and there is no need for virtual tagging. Large caches, then, tend to be physically tagged, and only small, very low latency caches are virtually tagged. In recent general-purpose CPUs, virtual tagging has been superseded by vhints, as described below.</p>
<p><a name="Virtual_indexing_and_virtual_aliases" id="Virtual_indexing_and_virtual_aliases"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=7" title="Edit section: Virtual indexing and virtual aliases">edit</a>]</span> <span class="mw-headline">Virtual indexing and virtual aliases</span></h3>
<p>The usual way the processor guarantees that virtually aliased addresses act as a single storage location is to arrange that only one virtual alias can be in the cache at any given time.</p>
<p>Whenever a new entry is added to a virtually-indexed cache, the processor searches for any virtual aliases already resident and evicts them first. This special handling happens only during a cache miss. No special work is necessary during a cache hit, which helps keep the fast path fast.</p>
<p>The most straightforward way to find aliases is to arrange for them all to map to the same location in the cache. This happens, for instance, if the TLB has e.g. 4 KiB pages, and the cache is direct mapped and 4 KiB or less.</p>
<p>Modern level-1 caches are much larger than 4 KiB, but virtual memory pages have stayed that size. If the cache is e.g. 16 KiB and virtually indexed, for any virtual address there are four cache locations that could hold the same physical location, but aliased to different virtual addresses. If the cache misses, all four locations must be probed to see if their corresponding physical addresses match the physical address of the access that generated the miss.</p>
<p>These probes are the same checks that a set associative cache uses to select a particular match. So if a 16 KiB virtually indexed cache is 4-way set associative and used with 4 KiB virtual memory pages, no special work is necessary to evict virtual aliases during cache misses because the checks have already happened while checking for a cache hit.</p>
<p>Using the AMD Athlon as an example again, it has a 64 KiB level-1 data cache, 4 KiB pages, and 2-way set associativity. When the level-1 data cache suffers a miss, 2 of the 16 (==64 KiB/4 KiB) possible virtual aliases have already been checked, and seven more cycles through the tag check hardware are necessary to complete the check for virtual aliases.</p>
<p><a name="Virtual_tags_and_vhints" id="Virtual_tags_and_vhints"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=8" title="Edit section: Virtual tags and vhints">edit</a>]</span> <span class="mw-headline">Virtual tags and vhints</span></h3>
<p>Virtual tagging is possible too. The great advantage of virtual tags is that, for associative caches, they allow the tag match to proceed before the virtual to physical translation is done. However,</p>
<ul>
<li>Coherence probes and evictions present a physical address for action. The hardware must have some means of converting the physical addresses into a cache index, generally by storing physical tags as well as virtual tags. For comparison, a physically tagged cache does not need to keep virtual tags, which is simpler.</li>
</ul>
<ul>
<li>When a virtual to physical mapping is deleted from the TLB, cache entries with those virtual addresses will have to be flushed somehow. Alternatively, if cache entries are allowed on pages not mapped by the TLB, then those entries will have to be flushed when the access rights on those pages are changed in the page table.</li>
</ul>
<p>It is also possible for the operating system to ensure that no virtual aliases are simultaneously resident in the cache. The operating system makes this guarantee by enforcing page coloring, which is described below. Some early RISC processors (SPARC, RS/6000) took this approach. It has not been used recently, as the hardware cost of detecting and evicting virtual aliases has fallen and the software complexity and performance penalty of perfect page coloring has risen.</p>
<p>It can be useful to distinguish the two functions of tags in an associative cache: they are used to determine which way of the entry set to select, and they are used to determine if the cache hit or missed. The second function must always be correct, but it is permissible for the first function to guess, and get the wrong answer occasionally.</p>
<p>Some processors (e.g. early SPARCs) have caches with both virtual and physical tags. The virtual tags are used for way selection, and the physical tags are used for determining hit or miss. This kind of cache enjoys the latency advantage of a virtually tagged cache, and the simple software interface of a physically tagged cache. It bears the added cost of duplicated tags, however. Also, during miss processing, the alternate ways of the cache line indexed have to be probed for virtual aliases and any matches evicted.</p>
<p>The extra area (and some latency) can be mitigated by keeping <i>virtual hints</i> with each cache entry instead of virtual tags. These hints are a subset or hash of the virtual tag, and are used for selecting the way of the cache from which to get data and a physical tag. Like a virtually tagged cache, there may be a virtual hint match but physical tag mismatch, in which case the cache entry with the matching hint must be evicted so that cache accesses after the cache fill at this address will have just one hint match. Since virtual hints have fewer bits than virtual tags distinguishing them from one another, a virtually hinted cache suffers more conflict misses than a virtually tagged cache.</p>
<p>Perhaps the ultimate reduction of virtual hints can be found in the Pentium 4 (Willamette and Northwood cores). In these processors the virtual hint is effectively 2 bits, and the cache is 4-way set associative. Effectively, the hardware maintains a simple permutation from virtual address to cache index, so that no content-addressable memory (CAM) is necessary to select the right one of the four ways fetched.</p>
<p><a name="Page_coloring" id="Page_coloring"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=9" title="Edit section: Page coloring">edit</a>]</span> <span class="mw-headline">Page coloring</span></h3>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/Cache_coloring" title="Cache coloring">Cache coloring</a></div>
<p>Large physically indexed caches (usually secondary caches) run into a problem: the operating system rather than the application controls which pages collide with one another in the cache. Differences in page allocation from one program run to the next lead to differences in the cache collision patterns, which can lead to very large differences in program performance. These differences can make it very difficult to get a consistent and repeatable timing for a benchmark run, which then leads to frustrated sales engineers demanding that the operating system authors fix the problem.</p>
<p>To understand the problem, consider a CPU with a 1 MiB physically indexed direct-mapped level-2 cache and 4 KiB virtual memory pages. Sequential physical pages map to sequential locations in the cache until after 256 pages the pattern wraps around. We can label each physical page with a color of 0–255 to denote where in the cache it can go. Locations within physical pages with different colors cannot conflict in the cache.</p>
<p>A programmer attempting to make maximum use of the cache may arrange his program's access patterns so that only 1 MiB of data need be cached at any given time, thus avoiding capacity misses. But he should also ensure that the access patterns do not have conflict misses. One way to think about this problem is to divide up the virtual pages the program uses and assign them virtual colors in the same way as physical colors were assigned to physical pages before. The programmer can then arrange the access patterns of his code so that no two pages with the same virtual color are in use at the same time. There is a wide literature on such optimizations (e.g. <a href="/wiki/Loop_nest_optimization" title="Loop nest optimization">loop nest optimization</a>), largely coming from the <a href="/wiki/High_Performance_Computing" title="High Performance Computing" class="mw-redirect">High Performance Computing (HPC)</a> community.</p>
<p>The snag is that while all the pages in use at any given moment may have different virtual colors, some may have the same physical colors. In fact, if the operating system assigns physical pages to virtual pages randomly and uniformly, it is extremely likely that some pages will have the same physical color, and then locations from those pages will collide in the cache (this is the <a href="/wiki/Birthday_paradox" title="Birthday paradox" class="mw-redirect">birthday paradox</a>).</p>
<p>The solution is to have the operating system attempt to assign different physical color pages to different virtual colors, a technique called <i>page coloring</i>. Although the actual mapping from virtual to physical color is irrelevant to system performance, odd mappings are difficult to keep track of and have little benefit, so most approaches to page coloring simply try to keep physical and virtual page colors the same.</p>
<p>If the operating system can guarantee that each physical page maps to only one virtual color, then there are no virtual aliases, and the processor can use virtually indexed caches with no need for extra virtual alias probes during miss handling. Alternatively, the O/S can flush a page from the cache whenever it changes from one virtual color to another. As mentioned above, this approach was used for some early SPARC and RS/6000 designs.</p>
<p><a name="Cache_hierarchy_in_a_modern_processor" id="Cache_hierarchy_in_a_modern_processor"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=10" title="Edit section: Cache hierarchy in a modern processor">edit</a>]</span> <span class="mw-headline">Cache hierarchy in a modern processor</span></h2>
<p>Modern processors have multiple interacting caches on chip.</p>
<p><a name="Specialized_caches" id="Specialized_caches"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=11" title="Edit section: Specialized caches">edit</a>]</span> <span class="mw-headline">Specialized caches</span></h3>
<p>Pipelined CPUs access memory from multiple points in the <a href="/wiki/Instruction_pipeline" title="Instruction pipeline">pipeline</a>: instruction fetch, <a href="/wiki/Virtual_memory" title="Virtual memory">virtual-to-physical</a> address translation, and data fetch (see <a href="/wiki/Classic_RISC_pipeline" title="Classic RISC pipeline">classic RISC pipeline</a>). The natural design is to use different physical caches for each of these points, so that no one physical resource has to be scheduled to service two points in the pipeline. Thus the pipeline naturally ends up with at least three separate caches (instruction, TLB, and data), each specialized to its particular role.</p>
<p>Pipelines with separate instruction and data caches, now predominant, are said to have a <a href="/wiki/Harvard_architecture" title="Harvard architecture">Harvard architecture</a>. Originally, this phrase referred to machines with separate instruction and data memories, which proved not at all popular. Most modern CPUs have a single-memory <a href="/wiki/Von_Neumann_architecture" title="Von Neumann architecture">von Neumann architecture</a>.</p>
<p><a name="Victim_cache" id="Victim_cache"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=12" title="Edit section: Victim cache">edit</a>]</span> <span class="mw-headline">Victim cache</span></h4>
<p>A <b>victim cache</b> is a cache used to hold blocks evicted from a CPU cache upon replacement. The victim cache lies between the main cache and its refill path, and only holds blocks that were evicted from the main cache. The victim cache is usually fully associative, and is intended to reduce the number of conflict misses. Many commonly used programs do not require an associative mapping for all the accesses. In fact, only a small fraction of the memory accesses of the program require high associativity. The victim cache exploits this property by providing high associativity to only these accesses. It was introduced by <a href="/w/index.php?title=Norman_Jouppi&amp;action=edit&amp;redlink=1" class="new" title="Norman Jouppi (page does not exist)">Norman Jouppi</a> in 1990.</p>
<p><a name="Trace_cache" id="Trace_cache"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=13" title="Edit section: Trace cache">edit</a>]</span> <span class="mw-headline">Trace cache</span></h4>
<p>One of the more extreme examples of cache specialization is the <b>trace cache</b> found in the Intel Pentium 4 microprocessors. A <b>trace cache</b> is a mechanism for increasing the instruction <a href="/w/index.php?title=Fetch_bandwidth&amp;action=edit&amp;redlink=1" class="new" title="Fetch bandwidth (page does not exist)">fetch bandwidth</a> and decreasing power consumption (in the case of the Pentium 4) by storing traces of <a href="/wiki/Instruction_(computer_science)" title="Instruction (computer science)">instructions</a> that have already been fetched and decoded.</p>
<p>The earliest widely acknowledged academic publication of trace cache was by <a href="/w/index.php?title=Eric_Rotenberg&amp;action=edit&amp;redlink=1" class="new" title="Eric Rotenberg (page does not exist)">Eric Rotenberg</a>, <a href="/wiki/Steve_Bennett" title="Steve Bennett">Steve Bennett</a>, and <a href="/w/index.php?title=James_E._Smith&amp;action=edit&amp;redlink=1" class="new" title="James E. Smith (page does not exist)">James E. Smith</a> in their 1996 paper <i>"Trace Cache: a Low Latency Approach to High Bandwidth Instruction Fetching."</i></p>
<p>An earlier publication is US Patent 5,381,533, "Dynamic flow instruction cache memory organized around trace segments independent of virtual address line", by <a href="/w/index.php?title=Alex_Peleg&amp;action=edit&amp;redlink=1" class="new" title="Alex Peleg (page does not exist)">Alex Peleg</a> and <a href="/w/index.php?title=Uri_Weiser&amp;action=edit&amp;redlink=1" class="new" title="Uri Weiser (page does not exist)">Uri Weiser</a> of Intel Corp., patent filed March 30, 1994, a continuation of an application filed in 1992, later abandoned.</p>
<p>A trace cache stores instructions either after they have been decoded, or as they are retired. Generally, instructions are added to trace caches in groups representing either individual basic blocks or dynamic instruction traces. A basic block consists of a group of non-branch instructions ending with a branch. A dynamic trace ("trace path") contains only instructions whose results are actually used, and eliminates instructions following taken branches (since they are not executed); a dynamic trace can be a concatenation of multiple basic blocks. This allows the instruction fetch unit of a processor to fetch several basic blocks, without having to worry about branches in the execution flow.</p>
<p>Trace lines are stored in the trace cache based on the <a href="/wiki/Program_counter" title="Program counter">program counter</a> of the first instruction in the trace and a set of branch predictions. This allows for storing different trace paths that start on the same address, each representing different branch outcomes. In the instruction fetch stage of a <a href="/wiki/Instruction_pipeline" title="Instruction pipeline">pipeline</a>, the current program counter along with a set of branch predictions is checked in the trace cache for a hit. If there is a hit, a trace line is supplied to fetch which does not have to go to a regular cache or to memory for these instructions. The trace cache continues to feed the fetch unit until the trace line ends or until there is a <a href="/wiki/Misprediction" title="Misprediction" class="mw-redirect">misprediction</a> in the pipeline. If there is a miss, a new trace starts to be built.</p>
<p>Trace caches are also used in processors like the <a href="/wiki/Intel" title="Intel" class="mw-redirect">Intel</a> <a href="/wiki/Pentium_4" title="Pentium 4">Pentium 4</a> to store already decoded micro-operations, or translations of complex x86 instructions, so that the next time an instruction is needed, it does not have to be decoded again.</p>
<p>See the full text of <a href="http://citeseer.ist.psu.edu/rotenberg96trace.html" class="external text" title="http://citeseer.ist.psu.edu/rotenberg96trace.html" rel="nofollow">Smith, Rotenberg and Bennett's paper</a> at <i><a href="/wiki/Citeseer" title="Citeseer" class="mw-redirect">Citeseer</a></i>.</p>
<p><a name="Multi-level_caches" id="Multi-level_caches"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=14" title="Edit section: Multi-level caches">edit</a>]</span> <span class="mw-headline">Multi-level caches</span></h3>
<p>Another issue is the fundamental tradeoff between cache latency and hit rate. Larger caches have better hit rates but longer latency. To address this tradeoff, many computers use multiple levels of cache, with small fast caches backed up by larger slower caches.</p>
<p>Multi-level caches generally operate by checking the smallest <b>Level 1</b> (L1) cache first; if it hits, the processor proceeds at high speed. If the smaller cache misses, the next larger cache (L2) is checked, and so on, before external memory is checked.</p>
<p>As the latency difference between main memory and the fastest cache has become larger, some processors have begun to utilize as many as three levels of on-chip cache. For example, the <a href="/wiki/Alpha_21164" title="Alpha 21164">Alpha 21164</a> (1995) had a 96 KB on-die L3 cache, the IBM <a href="/wiki/POWER4" title="POWER4">POWER4</a> (2001) had a 256 MB L3 cache off-chip, shared among several processors, the <a href="/wiki/Itanium_2" title="Itanium 2" class="mw-redirect">Itanium 2</a> (2003) had a 6 MB unified level 3 (L3) cache on-die, Intel's <a href="/wiki/Xeon" title="Xeon">Xeon</a> MP product code-named "Tulsa" (2006) features 16 MiB of on-die L3 cache shared between two processor cores, the AMD <a href="/wiki/Phenom" title="Phenom">Phenom</a> (2007) has a 2 MB on-die L3 cache and the <a href="/wiki/Intel_Core_i7" title="Intel Core i7">Intel Core i7</a> (2008) has an 8 MB on-die unified L3 cache that is inclusive, shared by all cores. The benefits of an L3 cache depend on the application's access patterns.</p>
<p>Finally, at the other end of the memory hierarchy, the CPU <a href="/wiki/Register_file" title="Register file">register file</a> itself can be considered the smallest, fastest cache in the system, with the special characteristic that it is scheduled in software—typically by a compiler, as it allocates registers to hold values retrieved from main memory. (See especially <a href="/wiki/Loop_nest_optimization" title="Loop nest optimization">loop nest optimization</a>.) Register files sometimes also have hierarchy: The <a href="/wiki/Cray-1" title="Cray-1">Cray-1</a> (circa 1976) had 8 address "A" and 8 scalar data "S" registers that were generally usable. There was also a set of 64 address "B" and 64 scalar data "T" registers that took longer to access, but were faster than main memory. The "B" and "T" registers were provided because the Cray-1 did not have a data cache. (The Cray-1 did, however, have an instruction cache.)</p>
<p><a name="Exclusive_versus_inclusive" id="Exclusive_versus_inclusive"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=15" title="Edit section: Exclusive versus inclusive">edit</a>]</span> <span class="mw-headline">Exclusive versus inclusive</span></h4>
<p>Multi-level caches introduce new design decisions. For instance, in some processors, all data in the L1 cache must also be somewhere in the L2 cache. These caches are called <b>strictly inclusive</b>. Other processors (like the AMD Athlon) have <b>exclusive</b> caches — data is guaranteed to be in at most one of the L1 and L2 caches, never in both. Still other processors (like the Intel <a href="/wiki/Pentium_II" title="Pentium II">Pentium II</a>, <a href="/wiki/Pentium_III" title="Pentium III">III</a>, and <a href="/wiki/Pentium_4" title="Pentium 4">4</a>), do not require that data in the L1 cache also reside in the L2 cache, although it may often do so. There is no universally accepted name for this intermediate policy, although the term <b>mainly inclusive</b> has been used.</p>
<p>The advantage of exclusive caches is that they store more data. This advantage is larger when the exclusive L1 cache is comparable to the L2 cache, and diminishes if the L2 cache is many times larger than the L1 cache. When the L1 misses and the L2 hits on an access, the hitting cache line in the L2 is exchanged with a line in the L1. This exchange is quite a bit more work than just copying a line from L2 to L1, which is what an inclusive cache does.</p>
<p>One advantage of strictly inclusive caches is that when external devices or other processors in a multiprocessor system wish to remove a cache line from the processor, they need only have the processor check the L2 cache. In cache hierarchies which do not enforce inclusion, the L1 cache must be checked as well. As a drawback, there is a correlation between the associativities of L1 and L2 caches: if the L2 cache does not have at least as many ways as all L1 caches together, the effective associativity of the L1 caches is restricted.</p>
<p>Another advantage of inclusive caches is that the larger cache can use larger cache lines, which reduces the size of the secondary cache tags. (Exclusive caches require both caches to have the same size cache lines, so that cache lines can be swapped on a L1 miss, L2 hit). If the secondary cache is an order of magnitude larger than the primary, and the cache data is an order of magnitude larger than the cache tags, this tag area saved can be comparable to the incremental area needed to store the L1 cache data in the L2.</p>
<p><a name="Example:_the_K8" id="Example:_the_K8"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=16" title="Edit section: Example: the K8">edit</a>]</span> <span class="mw-headline">Example: the K8</span></h3>
<p>To illustrate both specialization and multi-level caching, here is the cache hierarchy of the K8 core in the AMD <a href="/wiki/Athlon_64" title="Athlon 64">Athlon 64</a> CPU.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup></p>
<div class="thumb tnone">
<div class="thumbinner" style="width:602px;"><a href="/wiki/File:Cache,hierarchy-example.svg" class="image" title="Example of hierarchy, the K8"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Cache%2Chierarchy-example.svg/600px-Cache%2Chierarchy-example.svg.png" width="600" height="400" border="0" class="thumbimage" /></a>
<div class="thumbcaption">Example of hierarchy, the K8</div>
</div>
</div>
<p>The K8 has 4 specialized caches: an instruction cache, an instruction <a href="/wiki/Translation_lookaside_buffer" title="Translation lookaside buffer">TLB</a>, a data TLB, and a data cache. Each of these caches is specialized:</p>
<ul>
<li>The instruction cache keeps copies of 64 byte lines of memory, and fetches 16 bytes each cycle. Each byte in this cache is stored in ten bits rather than 8, with the extra bits marking the boundaries of instructions (this is an example of predecoding). The cache has only <a href="/wiki/Parity" title="Parity">parity</a> protection rather than <a href="/wiki/Error-correcting_code" title="Error-correcting code" class="mw-redirect">ECC</a>, because parity is smaller and any damaged data can be replaced by fresh data fetched from memory (which always has an up-to-date copy of instructions).</li>
</ul>
<ul>
<li>The instruction TLB keeps copies of page table entries (PTEs). Each cycle's instruction fetch has its virtual address translated through this TLB into a physical address. Each entry is either 4 or 8 bytes in memory. Each of the TLBs is split into two sections, one to keep PTEs that map 4 KiB, and one to keep PTEs that map 4 MiB or 2 MiB. The split allows the fully associative match circuitry in each section to be simpler. The operating system maps different sections of the virtual address space with different size PTEs.</li>
</ul>
<ul>
<li>The data TLB has two copies which keep identical entries. The two copies allow two data accesses per cycle to translate virtual addresses to physical addresses. Like the instruction TLB, this TLB is split into two kinds of entries.</li>
</ul>
<ul>
<li>The data cache keeps copies of 64 byte lines of memory. It is split into 8 banks (each storing 8 KiB of data), and can fetch two 8-byte data each cycle so long as those data are in different banks. There are two copies of the tags, because each 64 byte line is spread among all 8 banks. Each tag copy handles one of the two accesses per cycle.</li>
</ul>
<p>The K8 also has multiple-level caches. There are second-level instruction and data TLBs, which store only PTEs mapping 4 KiB. Both instruction and data caches, and the various TLBs, can fill from the large <b>unified</b> L2 cache. This cache is exclusive to both the L1 instruction and data caches, which means that any 8-byte line can only be in one of the L1 instruction cache, the L1 data cache, or the L2 cache. It is, however, possible for a line in the data cache to have a PTE which is also in one of the TLBs—the operating system is responsible for keeping the TLBs coherent by flushing portions of them when the page tables in memory are updated.</p>
<p>The K8 also caches information that is never stored in memory—prediction information. These caches are not shown in the above diagram. As is usual for this class of CPU, the K8 has fairly complex <a href="/wiki/Branch_prediction" title="Branch prediction" class="mw-redirect">branch prediction</a>, with tables that help predict whether branches are taken and other tables which predict the targets of branches and jumps. Some of this information is associated with instructions, in both the level 1 instruction cache and the unified secondary cache.</p>
<p>The K8 uses an interesting trick to store prediction information with instructions in the secondary cache. Lines in the secondary cache are protected from accidental data corruption (e.g. by an <a href="/wiki/Alpha_particle" title="Alpha particle">alpha particle</a> strike) by either <a href="/wiki/Error-correcting_code" title="Error-correcting code" class="mw-redirect">ECC</a> or <a href="/wiki/Parity_(telecommunication)" title="Parity (telecommunication)" class="mw-redirect">parity</a>, depending on whether those lines were evicted from the data or instruction primary caches. Since the parity code takes fewer bits than the ECC code, lines from the instruction cache have a few spare bits. These bits are used to cache branch prediction information associated with those instructions. The net result is that the branch predictor has a larger effective history table, and so has better accuracy.</p>
<p><a name="More_hierarchies" id="More_hierarchies"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=17" title="Edit section: More hierarchies">edit</a>]</span> <span class="mw-headline">More hierarchies</span></h3>
<p>Other processors have other kinds of predictors (e.g. the store-to-load bypass predictor in the <a href="/wiki/Digital_Equipment_Corporation" title="Digital Equipment Corporation">DEC</a> <a href="/wiki/Alpha_21264" title="Alpha 21264">Alpha 21264</a>), and various specialized predictors are likely to flourish in future processors.</p>
<p>These predictors are caches in that they store information that is costly to compute. Some of the terminology used when discussing predictors is the same as that for caches (one speaks of a <b>hit</b> in a branch predictor), but predictors are not generally thought of as part of the cache hierarchy.</p>
<p>The K8 keeps the instruction and data caches <a href="/wiki/Cache_coherency" title="Cache coherency" class="mw-redirect"><b>coherent</b></a> in hardware, which means that a store into an instruction closely following the store instruction will change that following instruction. Other processors, like those in the Alpha and MIPS family, have relied on software to keep the instruction cache coherent. Stores are not guaranteed to show up in the instruction stream until a program calls an operating system facility to ensure coherency. The idea is to save hardware complexity on the assumption that self-modifying code is rare.</p>
<p><a name="Implementation" id="Implementation"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=18" title="Edit section: Implementation">edit</a>]</span> <span class="mw-headline">Implementation</span></h2>
<p>Cache <b>reads</b> are the most common CPU operation that takes more than a single cycle. Program execution time tends to be very sensitive to the latency of a level-1 data cache hit. A great deal of design effort, and often power and silicon area are expended making the caches as fast as possible.</p>
<p>The simplest cache is a virtually indexed direct-mapped cache. The virtual address is calculated with an adder, the relevant portion of the address extracted and used to index an SRAM, which returns the loaded data. The data is byte aligned in a byte shifter, and from there is bypassed to the next operation. There is no need for any tag checking in the inner loop — in fact, the tags need not even be read. Later in the pipeline, but before the load instruction is retired, the tag for the loaded data must be read, and checked against the virtual address to make sure there was a cache hit. On a miss, the cache is updated with the requested cache line and the pipeline is restarted.</p>
<p>An associative cache is more complicated, because some form of tag must be read to determine which entry of the cache to select. An N-way set-associative level-1 cache usually reads all N possible tags and N data in parallel, and then chooses the data associated with the matching tag. Level-2 caches sometimes save power by reading the tags first, so that only one data element is read from the data SRAM.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:419px;"><a href="/wiki/File:Cache,associative-read.png" class="image" title="Read path for a 2-way associative cache"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/1/15/Cache%2Cassociative-read.png" width="417" height="275" border="0" class="thumbimage" /></a>
<div class="thumbcaption">Read path for a 2-way associative cache</div>
</div>
</div>
<p>The diagram to the right is intended to clarify the manner in which the various fields of the address are used. Address bit 31 is most significant, bit 0 is least significant. The diagram shows the SRAMs, indexing, and multiplexing for a 4 KiB, 2-way set-associative, virtually indexed and virtually tagged cache with 64 B lines, a 32b read width and 32b virtual address.</p>
<p>Because the cache is 4 KiB and has 64 B lines, there are just 64 lines in the cache, and we read two at a time from a Tag SRAM which has 32 rows, each with a pair of 21 bit tags. Although any function of virtual address bits 31 through 6 could be used to index the tag and data SRAMs, it is simplest to use the least significant bits.</p>
<p>Similarly, because the cache is 4 KiB and has a 4 B read path, and reads two ways for each access, the Data SRAM is 512 rows by 8 bytes wide.</p>
<p>A more modern cache might be 16 KiB, 4-way set-associative, virtually indexed, virtually hinted, and physically tagged, with 32 B lines, 32b read width and 36b physical addresses. The read path recurrence for such a cache looks very similar to the path above. Instead of tags, vhints are read, and matched against a subset of the virtual address. Later on in the pipeline, the virtual address is translated into a physical address by the TLB, and the physical tag is read (just one, as the vhint supplies which way of the cache to read). Finally the physical address is compared to the physical tag to determine if a hit has occurred.</p>
<p>Some SPARC designs have improved the speed of their L1 caches by a few gate delays by collapsing the virtual address adder into the SRAM decoders. See <a href="/wiki/Sum_addressed_decoder" title="Sum addressed decoder">Sum addressed decoder</a>.</p>
<p><a name="History" id="History"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=19" title="Edit section: History">edit</a>]</span> <span class="mw-headline">History</span></h3>
<p>The early history of cache technology is closely tied to the invention and use of virtual memory.<sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since March 2008" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup> Because of scarcity and cost of semi-conductors memories, early mainframe computers in 1960s used a complex hierarchy of physical memory, mapped onto a flat virtual memory used by programs. The memory technologies would span semi-conductor, magnetic core, drum and disc. Virtual memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the fastest memory ahead of processor access. Extensive studies were done to optimise the cache sizes. Optimal values were found to depend greatly on the programming language used with Algol needing the smallest and Fortran and Cobol needing the largest cache sizes.</p>
<p>The arrival of PCs coincided with a temporary decline in interest in caching. In the early days of PC technology, memory access was only slightly slower than <a href="/wiki/Processor_register" title="Processor register">register</a> access. But since the 1980s <sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup> the performance gap between processor and memory has been growing. Processors have advanced much faster than memory, especially in terms of their operating <a href="/wiki/Frequency" title="Frequency">frequency</a>, so memory became a performance <a href="/wiki/Bottleneck" title="Bottleneck">bottleneck</a>. While it was technically possible to have all the main memory as fast as the processor, a more economically viable path has been taken: use plenty of low-speed memory, but also introduce a small high-speed cache memory to alleviate the performance gap. This provided an order of magnitude more capacity—for the same price—with only a slightly-reduced combined performance.</p>
<p><a name="History_of_cache_in_x86_architecture" id="History_of_cache_in_x86_architecture"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=20" title="Edit section: History of cache in x86 architecture">edit</a>]</span> <span class="mw-headline">History of cache in x86 architecture</span></h3>
<p>As the <a href="/wiki/X86" title="X86">x86</a> CPU architecture reached clock speeds of 20 MHz and above in the <a href="/wiki/Intel_80386" title="Intel 80386">386</a>, small amounts of fast cache memory began to be included in the architecture to boost performance. This was because the <a href="/wiki/DRAM" title="DRAM" class="mw-redirect">DRAM</a> used for main memory had significant latency, up to 120ns, as well as refresh cycles. The cache was constructed from more expensive, but significantly faster, <a href="/wiki/Static_random_access_memory" title="Static random access memory">SRAM</a>, which at the time had latencies around 10ns. The early caches were external to the processor and typically located on the motherboard in the form of 8 or 9 <a href="/wiki/Dual_in-line_package" title="Dual in-line package">DIP</a> memory chips placed in sockets to enable the cache as an optional extra or upgrade feature.</p>
<p>Some versions of the Intel 386 type processor could support 16 to 64 <a href="/wiki/KB" title="KB">KB</a> of external cache.</p>
<p>With the <a href="/wiki/Intel_80486" title="Intel 80486">486</a> processor an 8 KB cache was integrated directly into the CPU die. This cache was termed Level 1 or L1 cache to differentiate it from the slower on-motherboard, or Level 2 (L2) cache. These on-motherboard caches were much larger, with the most common size being 256 KB and frequently utilizing a <a href="/wiki/SIMM" title="SIMM">SIMM</a> form factor. The popularity of on-motherboard cache continued on through the <a href="/wiki/Pentium" title="Pentium">Pentium MMX</a> era but was made obsolete by the introduction of <a href="/wiki/SDRAM" title="SDRAM">SDRAM</a> and the growing disparity between bus speed and CPU clock speed, which caused on-motherboard cache to be only slightly faster than main memory.</p>
<p>The next evolution in cache implementation in the x86 architecture began with the <a href="/wiki/Pentium_Pro" title="Pentium Pro">Pentium Pro</a>, which brought the secondary cache onto the same package as the microprocessor, clocked at same frequency as the microprocessor.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=21" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Cache_coherency" title="Cache coherency" class="mw-redirect">Cache coherency</a></li>
<li><a href="/wiki/No-write_allocation" title="No-write allocation" class="mw-redirect">No-write allocation</a></li>
<li><a href="/wiki/Memoization" title="Memoization">Memoization</a>, briefly defined in <a href="/wiki/List_of_computer_term_etymologies" title="List of computer term etymologies">List of computer term etymologies</a></li>
<li><a href="/wiki/Scratchpad_RAM" title="Scratchpad RAM">Scratchpad RAM</a></li>
</ul>
<p><a name="Notes_and_references" id="Notes_and_references"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=22" title="Edit section: Notes and references">edit</a>]</span> <span class="mw-headline">Notes and references</span></h2>
<div class="references-small references-column-count references-column-count-2" style="-moz-column-count:2; column-count:2;">
<ol class="references">
<li id="cite_note-Seznec-0">^ <a href="#cite_ref-Seznec_0-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Seznec_0-1" title=""><sup><i><b>b</b></i></sup></a> <cite style="font-style:normal" class="web" id="CITEREFAndr.C3.A9_Seznec">André Seznec. <a href="http://citeseer.ist.psu.edu/seznec93case.html" class="external text" title="http://citeseer.ist.psu.edu/seznec93case.html" rel="nofollow">"A Case for Two-Way Skewed-Associative Caches"</a><span class="printonly">. <a href="http://citeseer.ist.psu.edu/seznec93case.html" class="external free" title="http://citeseer.ist.psu.edu/seznec93case.html" rel="nofollow">http://citeseer.ist.psu.edu/seznec93case.html</a></span><span class="reference-accessdate">. Retrieved on 2007-12-13</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=A+Case+for+Two-Way+Skewed-Associative+Caches&amp;rft.atitle=&amp;rft.aulast=Andr%C3%A9+Seznec&amp;rft.au=Andr%C3%A9+Seznec&amp;rft_id=http%3A%2F%2Fciteseer.ist.psu.edu%2Fseznec93case.html&amp;rfr_id=info:sid/en.wikipedia.org:CPU_cache"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-CK-1">^ <a href="#cite_ref-CK_1-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-CK_1-1" title=""><sup><i><b>b</b></i></sup></a> <a href="http://www.stanford.edu/class/ee282/handouts/L03-Cache.pdf" class="external text" title="http://www.stanford.edu/class/ee282/handouts/L03-Cache.pdf" rel="nofollow">"Advanced Caching Techniques"</a> by C. Kozyrakis</li>
<li id="cite_note-2"><b><a href="#cite_ref-2" title="">^</a></b> <a href="http://www.irisa.fr/caps/PROJECTS/Architecture/" class="external text" title="http://www.irisa.fr/caps/PROJECTS/Architecture/" rel="nofollow">Micro-Architecture</a> "Skewed-associative caches have ... major advantages over conventional set-associative caches."</li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> <cite style="font-style:normal" class="web"><a href="http://www.sandpile.org/impl/k8.htm" class="external text" title="http://www.sandpile.org/impl/k8.htm" rel="nofollow">"AMD K8"</a>. Sandpile.org<span class="printonly">. <a href="http://www.sandpile.org/impl/k8.htm" class="external free" title="http://www.sandpile.org/impl/k8.htm" rel="nofollow">http://www.sandpile.org/impl/k8.htm</a></span><span class="reference-accessdate">. Retrieved on 2007-06-02</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=AMD+K8&amp;rft.atitle=&amp;rft.pub=Sandpile.org&amp;rft_id=http%3A%2F%2Fwww.sandpile.org%2Fimpl%2Fk8.htm&amp;rfr_id=info:sid/en.wikipedia.org:CPU_cache"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> <cite style="font-style:normal" class="web"><a href="http://www.acm.org/crossroads/xrds5-3/pmgap.html" class="external text" title="http://www.acm.org/crossroads/xrds5-3/pmgap.html" rel="nofollow">"The Processor-Memory performance gap"</a>. acm.org<span class="printonly">. <a href="http://www.acm.org/crossroads/xrds5-3/pmgap.html" class="external free" title="http://www.acm.org/crossroads/xrds5-3/pmgap.html" rel="nofollow">http://www.acm.org/crossroads/xrds5-3/pmgap.html</a></span><span class="reference-accessdate">. Retrieved on 2007-11-08</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=The+Processor-Memory+performance+gap&amp;rft.atitle=&amp;rft.pub=acm.org&amp;rft_id=http%3A%2F%2Fwww.acm.org%2Fcrossroads%2Fxrds5-3%2Fpmgap.html&amp;rfr_id=info:sid/en.wikipedia.org:CPU_cache"><span style="display: none;">&#160;</span></span></li>
</ol>
</div>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=CPU_cache&amp;action=edit&amp;section=23" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://lwn.net/Articles/252125/" class="external text" title="http://lwn.net/Articles/252125/" rel="nofollow">Memory part 2: CPU caches</a> An article on lwn.net by Ulrich Drepper describing CPU caches in detail.</li>
<li><a href="ftp://ftp.cs.wisc.edu/markhill/Papers/toc89_cpu_cache_associativity.pdf" class="external text" title="ftp://ftp.cs.wisc.edu/markhill/Papers/toc89_cpu_cache_associativity.pdf" rel="nofollow">Evaluating Associativity in CPU Caches</a> — Hill and Smith — 1989 — Introduces capacity, conflict, and compulsory classification.</li>
<li><a href="http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/" class="external text" title="http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/" rel="nofollow">Cache Performance for SPEC CPU2000 Benchmarks</a> — Hill and Cantin — 2003 — This reference paper has been updated several times. It has thorough and lucidly presented simulation results for a reasonably wide set of benchmarks and cache organizations.</li>
<li><a href="http://www.sun.com/blueprints/1102/817-0742.pdf" class="external text" title="http://www.sun.com/blueprints/1102/817-0742.pdf" rel="nofollow">Memory Hierarchy in Cache-Based Systems</a>, by Ruud van der Pas, 2002, Sun Microsystems, is a nice introductory article to CPU memory caching.</li>
<li><a href="http://www.freescale.com/files/32bit/doc/app_note/AN2663.pdf" class="external text" title="http://www.freescale.com/files/32bit/doc/app_note/AN2663.pdf" rel="nofollow">A Cache Primer</a> by Paul Genua, P.E., 2004, Freescale Semiconductor, another introductory article.</li>
<li><a href="http://www.zipcores.com/skin1/zipdocs/datasheets/cache_8way_set.pdf" class="external text" title="http://www.zipcores.com/skin1/zipdocs/datasheets/cache_8way_set.pdf" rel="nofollow">An 8-way set-associative cache</a> written in <a href="/wiki/VHDL" title="VHDL">VHDL</a></li>
</ul>
<table class="navbox" cellspacing="0" style=";">
<tr>
<td style="padding:2px;">
<table cellspacing="0" class="nowraplinks collapsible autocollapse" style="width:100%;background:transparent;color:inherit;;">
<tr>
<th style=";" colspan="2" class="navbox-title">
<div style="float:left; width:6em;text-align:left;">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;;;border:none;; font-size:xx-small;"><a href="/wiki/Template:CPU_technologies" title="Template:CPU technologies"><span title="View this template" style=";;border:none;">v</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/wiki/Template_talk:CPU_technologies" title="Template talk:CPU technologies"><span title="Discussion about this template" style=";;border:none;">d</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="http://en.wikipedia.org/w/index.php?title=Template:CPU_technologies&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Template:CPU_technologies&amp;action=edit" rel="nofollow"><span title="Edit this template" style=";;border:none;;">e</span></a></div>
</div>
<span style="font-size:110%;"><a href="/wiki/Central_processing_unit" title="Central processing unit">CPU technologies</a></span></th>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Instruction_set_architecture" title="Instruction set architecture" class="mw-redirect">Architecture</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Instruction_set" title="Instruction set">ISA</a>&#160;: <a href="/wiki/Complex_instruction_set_computer" title="Complex instruction set computer">CISC</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Explicit_Data_Graph_Execution" title="Explicit Data Graph Execution">EDGE</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Explicitly_parallel_instruction_computing" title="Explicitly parallel instruction computing">EPIC</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Minimal_instruction_set_computer" title="Minimal instruction set computer">MISC</a> <span style="font-weight:bold;">&#160;·</span> <a href="/wiki/One_instruction_set_computer" title="One instruction set computer">OISC</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Reduced_instruction_set_computer" title="Reduced instruction set computer">RISC</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Very_long_instruction_word" title="Very long instruction word">VLIW</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Zero_Instruction_Set_Computer" title="Zero Instruction Set Computer">ZISC</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Harvard_architecture" title="Harvard architecture">Harvard architecture</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Von_Neumann_architecture" title="Von Neumann architecture">Von Neumann architecture</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/8-bit" title="8-bit">8-bit</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/32-bit" title="32-bit">32-bit</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/64-bit" title="64-bit">64-bit</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/128-bit" title="128-bit">128-bit</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Parallel_computing" title="Parallel computing">Parallelism</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;">Pipeline</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Instruction_pipeline" title="Instruction pipeline">Instruction pipelining</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Out-of-order_execution" title="Out-of-order execution">In-Order &amp; Out-of-Order execution</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Register_renaming" title="Register renaming">Register renaming</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Speculative_execution" title="Speculative execution">Speculative execution</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;">Level</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Bit-level_parallelism" title="Bit-level parallelism">Bit</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Instruction_level_parallelism" title="Instruction level parallelism">Instruction</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Superscalar" title="Superscalar">Superscalar</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Data_parallelism" title="Data parallelism">Data</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Task_parallelism" title="Task parallelism">Task</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;">Threads</div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Multithreading_(computer_hardware)" title="Multithreading (computer hardware)">Multithreading</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Simultaneous_multithreading" title="Simultaneous multithreading">Simultaneous multithreading</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Hyper-threading" title="Hyper-threading">Hyperthreading</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Super-threading" title="Super-threading">Superthreading</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Flynn%27s_taxonomy" title="Flynn's taxonomy">Flynn's taxonomy</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/SISD" title="SISD">SISD</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/SIMD" title="SIMD">SIMD</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/MISD" title="MISD">MISD</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/MIMD" title="MIMD">MIMD</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Types</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Digital_signal_processor" title="Digital signal processor">Digital signal processor</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Microcontroller" title="Microcontroller">Microcontroller</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/System-on-a-chip" title="System-on-a-chip">System-on-a-chip</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Vector_processor" title="Vector processor">Vector processor</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Components</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Arithmetic_logic_unit" title="Arithmetic logic unit">Arithmetic logic unit (ALU)</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Barrel_shifter" title="Barrel shifter">Barrel shifter</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Floating-point_unit" title="Floating-point unit">Floating-point unit (FPU)</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Backside_bus" title="Backside bus" class="mw-redirect">Backside bus</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Multiplexer" title="Multiplexer">Multiplexer</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Demultiplexer" title="Demultiplexer" class="mw-redirect">Demultiplexer</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Processor_register" title="Processor register">Registers</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Memory_management_unit" title="Memory management unit">Memory management unit (MMU)</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Translation_lookaside_buffer" title="Translation lookaside buffer">Translation lookaside buffer (TLB)</a><span style="font-weight:bold;">&#160;·</span> <strong class="selflink">Cache</strong><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Register_file" title="Register file">register file</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Microcode" title="Microcode">microcode</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Control_unit" title="Control unit">control unit</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/CPU_clock" title="CPU clock" class="mw-redirect">CPU clock</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Power_management" title="Power management">Power management</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Advanced_Power_Management" title="Advanced Power Management">APM</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Advanced_Configuration_and_Power_Interface" title="Advanced Configuration and Power Interface">ACPI</a> <a href="/wiki/Advanced_Configuration_and_Power_Interface#States" title="Advanced Configuration and Power Interface">(states)</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Dynamic_frequency_scaling" title="Dynamic frequency scaling">Dynamic frequency scaling</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Dynamic_voltage_scaling" title="Dynamic voltage scaling">Dynamic voltage scaling</a><span style="font-weight:bold;">&#160;·</span> <a href="/wiki/Clock_gating" title="Clock gating">Clock gating</a></div>
</td>
</tr>
</table>
</td>
</tr>
</table>


<!-- 
NewPP limit report
Preprocessor node count: 2344/1000000
Post-expand include size: 46220/2048000 bytes
Template argument size: 24211/2048000 bytes
Expensive parser function count: 1/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:849181-0!1!0!default!!en!2 and timestamp 20090403220026 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/CPU_cache">http://en.wikipedia.org/wiki/CPU_cache</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Central_processing_unit" title="Category:Central processing unit">Central processing unit</a></span> | <span dir='ltr'><a href="/wiki/Category:Computer_memory" title="Category:Computer memory">Computer memory</a></span> | <span dir='ltr'><a href="/wiki/Category:Cache" title="Category:Cache">Cache</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_March_2008" title="Category:Articles with unsourced statements since March 2008">Articles with unsourced statements since March 2008</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/CPU_cache" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:CPU_cache" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=CPU_cache&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=CPU_cache&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=CPU_cache" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/CPU_cache" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/CPU_cache" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=CPU_cache&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=CPU_cache&amp;oldid=281582324" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=CPU_cache&amp;id=281582324">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Befehlscache">Deutsch</a></li>
				<li class="interwiki-et"><a href="http://et.wikipedia.org/wiki/Protsessori_vahem%C3%A4lu">Eesti</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/CPU_%EC%BA%90%EC%8B%9C">한국어</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/CPU_cache">Italiano</a></li>
				<li class="interwiki-lv"><a href="http://lv.wikipedia.org/wiki/CPU_cache">Latviešu</a></li>
				<li class="interwiki-hu"><a href="http://hu.wikipedia.org/wiki/Gyors%C3%ADt%C3%B3t%C3%A1r">Magyar</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E3%82%AD%E3%83%A3%E3%83%83%E3%82%B7%E3%83%A5%E3%83%A1%E3%83%A2%E3%83%AA">日本語</a></li>
				<li class="interwiki-mk"><a href="http://mk.wikipedia.org/wiki/%D0%9A%D0%B5%D1%88_%D0%BC%D0%B5%D0%BC%D0%BE%D1%80%D0%B8%D1%98%D0%B0">Македонски</a></li>
				<li class="interwiki-no"><a href="http://no.wikipedia.org/wiki/Hurtigminne">‪Norsk (bokmål)‬</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Pami%C4%99%C4%87_podr%C4%99czna_procesora">Polski</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/Trace_Cache">Русский</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 3 April 2009, at 22:00.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv208 in 0.057 secs. --></body></html>
