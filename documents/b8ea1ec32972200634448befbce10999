<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Markov chain,Articles with unsourced statements since March 2009,A mathematical theory of communication,Algorithmic composition,Andrey Markov,Andrey Nikolaevich Kolmogorov,Arithmetic coding,Bayes&#039; Rule,Bayesian inference,Belief propagation,Bernoulli process" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Markov_chain&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Markov_chain&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Markov chain - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Markov_chain";
		var wgTitle = "Markov chain";
		var wgAction = "view";
		var wgArticleId = "60876";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 280659640;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Markov_chain skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Markov chain</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p>In <a href="/wiki/Mathematics" title="Mathematics">mathematics</a>, a <b>Markov chain</b>, named after <a href="/wiki/Andrey_Markov" title="Andrey Markov">Andrey Markov</a>, is a <a href="/wiki/Stochastic_process" title="Stochastic process">stochastic process</a> with the <a href="/wiki/Markov_property" title="Markov property">Markov property</a>. Having the Markov property means that, <b><i>given the present state</i>, future states are independent of the past states.</b> In other words, the description of the present state fully captures all the information that could influence the future evolution of the process. Future states will be reached through a probabilistic process instead of a deterministic one.</p>
<p>At each step the system may change its state from the current state to another state, or remain in the same state, according to a certain probability distribution. The changes of state are called transitions, and the probabilities associated with various state-changes are called transition probabilities. An example of a Markov chain is a <a href="/wiki/Random_walk" title="Random walk">simple random walk</a> where the state space is a set of vertices of a <a href="/wiki/Graph_theory" title="Graph theory">graph</a> and the transition steps involve moving to any of the neighbours of the current vertex with equal probability (regardless of the history of the walk).</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Formal_definition"><span class="tocnumber">1</span> <span class="toctext">Formal definition</span></a>
<ul>
<li class="toclevel-2"><a href="#Variations"><span class="tocnumber">1.1</span> <span class="toctext">Variations</span></a></li>
<li class="toclevel-2"><a href="#Example"><span class="tocnumber">1.2</span> <span class="toctext">Example</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Properties_of_Markov_chains"><span class="tocnumber">2</span> <span class="toctext">Properties of Markov chains</span></a>
<ul>
<li class="toclevel-2"><a href="#Reducibility"><span class="tocnumber">2.1</span> <span class="toctext">Reducibility</span></a></li>
<li class="toclevel-2"><a href="#Periodicity"><span class="tocnumber">2.2</span> <span class="toctext">Periodicity</span></a></li>
<li class="toclevel-2"><a href="#Recurrence"><span class="tocnumber">2.3</span> <span class="toctext">Recurrence</span></a></li>
<li class="toclevel-2"><a href="#Ergodicity"><span class="tocnumber">2.4</span> <span class="toctext">Ergodicity</span></a></li>
<li class="toclevel-2"><a href="#Regularity"><span class="tocnumber">2.5</span> <span class="toctext">Regularity</span></a></li>
<li class="toclevel-2"><a href="#Steady-state_analysis_and_limiting_distributions"><span class="tocnumber">2.6</span> <span class="toctext">Steady-state analysis and limiting distributions</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Markov_chains_with_a_finite_state_space"><span class="tocnumber">3</span> <span class="toctext">Markov chains with a finite state space</span></a></li>
<li class="toclevel-1"><a href="#Reversible_Markov_chain"><span class="tocnumber">4</span> <span class="toctext">Reversible Markov chain</span></a></li>
<li class="toclevel-1"><a href="#Bernoulli_scheme"><span class="tocnumber">5</span> <span class="toctext">Bernoulli scheme</span></a></li>
<li class="toclevel-1"><a href="#Markov_chains_with_general_state_space"><span class="tocnumber">6</span> <span class="toctext">Markov chains with general state space</span></a></li>
<li class="toclevel-1"><a href="#Applications"><span class="tocnumber">7</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2"><a href="#Physics"><span class="tocnumber">7.1</span> <span class="toctext">Physics</span></a></li>
<li class="toclevel-2"><a href="#Testing"><span class="tocnumber">7.2</span> <span class="toctext">Testing</span></a></li>
<li class="toclevel-2"><a href="#Queueing_theory"><span class="tocnumber">7.3</span> <span class="toctext">Queueing theory</span></a></li>
<li class="toclevel-2"><a href="#Internet_applications"><span class="tocnumber">7.4</span> <span class="toctext">Internet applications</span></a></li>
<li class="toclevel-2"><a href="#Statistical"><span class="tocnumber">7.5</span> <span class="toctext">Statistical</span></a></li>
<li class="toclevel-2"><a href="#Economics"><span class="tocnumber">7.6</span> <span class="toctext">Economics</span></a></li>
<li class="toclevel-2"><a href="#Social_sciences"><span class="tocnumber">7.7</span> <span class="toctext">Social sciences</span></a></li>
<li class="toclevel-2"><a href="#Mathematical_biology"><span class="tocnumber">7.8</span> <span class="toctext">Mathematical biology</span></a></li>
<li class="toclevel-2"><a href="#Gambling"><span class="tocnumber">7.9</span> <span class="toctext">Gambling</span></a></li>
<li class="toclevel-2"><a href="#Music"><span class="tocnumber">7.10</span> <span class="toctext">Music</span></a></li>
<li class="toclevel-2"><a href="#Baseball"><span class="tocnumber">7.11</span> <span class="toctext">Baseball</span></a></li>
<li class="toclevel-2"><a href="#Markov_text_generators"><span class="tocnumber">7.12</span> <span class="toctext">Markov text generators</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#History"><span class="tocnumber">8</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">9</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#Further_reading"><span class="tocnumber">11</span> <span class="toctext">Further reading</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">12</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Formal_definition" id="Formal_definition"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=1" title="Edit section: Formal definition">edit</a>]</span> <span class="mw-headline">Formal definition</span></h2>
<p>A Markov chain is a sequence of <a href="/wiki/Random_variable" title="Random variable">random variables</a> <i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, <i>X</i><sub>3</sub>, ... with the <a href="/wiki/Markov_property" title="Markov property">Markov property</a>, namely that, given the present state, the future and past states are independent. Formally,</p>
<dl>
<dd><img class="tex" alt="\Pr(X_{n+1}=x|X_n=x_n, \ldots, X_1=x_1) = \Pr(X_{n+1}=x|X_n=x_n).\," src="http://upload.wikimedia.org/math/e/6/d/e6dd93aa981987037fd89287f662c81d.png" /></dd>
</dl>
<p>The possible values of <i>X</i><sub><i>i</i></sub> form a <a href="/wiki/Countable_set" title="Countable set">countable set</a> <i>S</i> called the <b>state space</b> of the chain.</p>
<p>Markov chains are often described by a <a href="/wiki/Directed_graph" title="Directed graph">directed graph</a>, where the edges are labeled by the probabilities of going from one state to the other states.</p>
<p><a name="Variations" id="Variations"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=2" title="Edit section: Variations">edit</a>]</span> <span class="mw-headline">Variations</span></h3>
<p><a href="/wiki/Continuous-time_Markov_process" title="Continuous-time Markov process">Continuous-time Markov processes</a> have a continuous index.</p>
<p><b>Time-homogeneous Markov chains</b> (or, Markov chains with time-homogeneous transition probabilities) are processes where</p>
<dl>
<dd><img class="tex" alt="\Pr(X_{n+1}=x|X_n=y) = \Pr(X_{n}=x|X_{n-1}=y)\," src="http://upload.wikimedia.org/math/e/8/9/e89a6f0462cae0b7434bc547cdc08161.png" /></dd>
</dl>
<p>for all <i>n</i>.</p>
<p>A <b>Markov chain of order <i>m</i></b> (or a Markov chain with memory <i>m</i>) where <i>m</i> is finite, is where</p>
<dl>
<dd><img class="tex" alt="\Pr(X_n=x_n|X_{n-1}=x_{n-1}, X_{n-2}=x_{n-2}, \dots , X_{1}=x_{1})" src="http://upload.wikimedia.org/math/b/1/f/b1f4ee90524def0e77777055635f2144.png" /></dd>
<dd><img class="tex" alt="  = \Pr(X_n=x_n|X_{n-1}=x_{n-1}, X_{n-2}=x_{n-2}, \dots, X_{n-m}=x_{n-m})" src="http://upload.wikimedia.org/math/3/b/c/3bcf342bb0258d1d36d7e5cfbaa46fe9.png" /></dd>
</dl>
<p>for all <i>n</i>. It is possible to construct a chain (<i>Y<sub>n</sub></i>) from (<i>X<sub>n</sub></i>) which has the 'classical' <a href="/wiki/Markov_property" title="Markov property">Markov property</a> as follows: Let <i>Y<sub>n</sub></i> = (<i>X<sub>n</sub></i>, <i>X</i><sub><i>n</i>−1</sub>, ..., <i>X</i><sub><i>n</i>−<i>m</i>+1</sub>), the ordered <i>m</i>-tuple of <i>X</i> values. Then <i>Y<sub>n</sub></i> is a Markov chain with state space <i>S<sup>m</sup></i> and has the classical <a href="/wiki/Markov_property" title="Markov property">Markov property</a>.</p>
<p><a name="Example" id="Example"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=3" title="Edit section: Example">edit</a>]</span> <span class="mw-headline">Example</span></h3>
<p>A <a href="/wiki/Finite_state_machine" title="Finite state machine">finite state machine</a> can be used as a representation of a Markov chain. Assuming a sequence of <a href="/wiki/Independent_and_identically_distributed" title="Independent and identically distributed" class="mw-redirect">independent and identically distributed</a> input signals (for example, symbols from a binary alphabet chosen by coin tosses), if the machine is in state <i>y</i> at time <i>n</i>, then the probability that it moves to state <i>x</i> at time <i>n</i>&#160;+&#160;1 depends only on the current state.</p>
<p>A thorough development and many examples can be found in the on-line monograph Meyn &amp; Tweedie 2005<sup id="cite_ref-MCSS_0-0" class="reference"><a href="#cite_note-MCSS-0" title=""><span>[</span>1<span>]</span></a></sup> The appendix of Meyn 2007,<sup id="cite_ref-CTCN_1-0" class="reference"><a href="#cite_note-CTCN-1" title=""><span>[</span>2<span>]</span></a></sup> also available on-line, contains an abridged Meyn &amp; Tweedie.</p>
<p><a name="Properties_of_Markov_chains" id="Properties_of_Markov_chains"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=4" title="Edit section: Properties of Markov chains">edit</a>]</span> <span class="mw-headline">Properties of Markov chains</span></h2>
<p>Define the probability of going from state <i>i</i> to state <i>j</i> in <i>n</i> time steps as</p>
<dl>
<dd><img class="tex" alt="p_{ij}^{(n)} = \Pr(X_n=j\mid X_0=i) \," src="http://upload.wikimedia.org/math/e/4/c/e4ca60660ae3848d42eea39b7970271a.png" /></dd>
</dl>
<p>and the single-step transition as</p>
<dl>
<dd><img class="tex" alt="p_{ij} = \Pr(X_1=j\mid X_0=i). \," src="http://upload.wikimedia.org/math/9/f/6/9f6eac996497bfe828a7b859bbd9123c.png" /></dd>
</dl>
<p>For a time-homogeneous Markov chain:</p>
<dl>
<dd><img class="tex" alt="p_{ij}^{(n)} = \Pr(X_{n+k}=j \mid X_{k}=i) \," src="http://upload.wikimedia.org/math/0/8/8/088a713e76ac05077dffb72a34f7f6d1.png" /> and</dd>
<dd><img class="tex" alt="p_{ij} = \Pr(X_{k+1}=j \mid X_k=i). \," src="http://upload.wikimedia.org/math/6/9/f/69fe7b3e16f31615ec25d42391f31278.png" /></dd>
</dl>
<p>so, the <i>n</i>-step transition satisfies the <a href="/wiki/Chapman%E2%80%93Kolmogorov_equation" title="Chapman–Kolmogorov equation">Chapman–Kolmogorov equation</a>, that for any <i>k</i> such that 0 &lt; <i>k</i> &lt; <i>n</i>,</p>
<dl>
<dd><img class="tex" alt="p_{ij}^{(n)} = \sum_{r \in S} p_{ir}^{(k)} p_{rj}^{(n-k)}." src="http://upload.wikimedia.org/math/7/5/0/750ba0baf4cd1c6818824667f6594cb8.png" /></dd>
</dl>
<p>The <a href="/wiki/Marginal_distribution" title="Marginal distribution">marginal distribution</a> Pr (<i>X</i><sub><i>n</i></sub> = <i>x</i>) is the distribution over states at time <i>n</i>. The initial distribution is Pr (<i>X</i><sub>0</sub> = <i>x</i>). The evolution of the process through one time step is described by</p>
<dl>
<dd><img class="tex" alt=" \Pr(X_{n}=j) = \sum_{r \in S} p_{rj} \Pr(X_{n-1}=r) = \sum_{r \in S} p_{rj}^{(n)} \Pr(X_0=r)." src="http://upload.wikimedia.org/math/f/3/f/f3f11fe8e5b105d189231e72a4d35ddb.png" /></dd>
</dl>
<p>The superscript <span class="texhtml">(<i>n</i>)</span> is intended to be an integer-valued label only; however, if the Markov chain is time-homogeneous, then this superscript can also be interpreted as a "raising to the power of", discussed further below.</p>
<p><a name="Reducibility" id="Reducibility"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=5" title="Edit section: Reducibility">edit</a>]</span> <span class="mw-headline">Reducibility</span></h3>
<p>A state <i>j</i> is said to be <b>accessible</b> from a different state <i>i</i> (written <i>i</i> → <i>j</i>) if, given that we are in state <i>i</i>, there is a non-zero probability that at some time in the future, the system will be in state <i>j</i>. Formally, state <i>j</i> is accessible from state <i>i</i> if there exists an integer <i>n</i>≥0 such that</p>
<dl>
<dd><img class="tex" alt=" \Pr(X_{n}=j | X_0=i) &gt; 0.\, " src="http://upload.wikimedia.org/math/5/3/8/538d563a45812cdf6cd97dbcbc51d752.png" /></dd>
</dl>
<p>Allowing <i>n</i> to be zero means that every state is defined to be accessible from itself.</p>
<p>A state <i>i</i> is said to <b>communicate</b> with state <i>j</i> (written <i>i</i> ↔ <i>j</i>) if it is true that both <i>i</i> is accessible from <i>j</i> and that <i>j</i> is accessible from <i>i</i>. A set of states <i>C</i> is a <b>communicating class</b> if every pair of states in <i>C</i> communicates with each other, and no state in <i>C</i> communicates with any state not in <i>C</i>. (It can be shown that communication in this sense is an <a href="/wiki/Equivalence_relation" title="Equivalence relation">equivalence relation</a>). A communicating class is <b>closed</b> if the probability of leaving the class is zero, namely that if <i>i</i> is in <i>C</i> but <i>j</i> is not, then <i>j</i> is not accessible from <i>i</i>.</p>
<p>Finally, a Markov chain is said to be <b>irreducible</b> if its state space is a communicating class; this means that, in an irreducible Markov chain, it is possible to get to any state from any state.</p>
<p><a name="Periodicity" id="Periodicity"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=6" title="Edit section: Periodicity">edit</a>]</span> <span class="mw-headline">Periodicity</span></h3>
<p>A state <i>i</i> has <b>period</b> <i>k</i> if any return to state <i>i</i> must occur in multiples of <i>k</i> time steps. Formally, the period of a state is defined as</p>
<dl>
<dd><img class="tex" alt=" k = \operatorname{gcd}\{ n: \Pr(X_n = i | X_0 = i) &gt; 0\}" src="http://upload.wikimedia.org/math/a/b/b/abb252e22416fb5d852be18c021ae6c9.png" /></dd>
</dl>
<p>(where "gcd" is the <a href="/wiki/Greatest_common_divisor" title="Greatest common divisor">greatest common divisor</a>). Note that even though a state has period <i>k</i>, it may not be possible to reach the state in <i>k</i> steps. For example, suppose it is possible to return to the state in {6,8,10,12,...} time steps; then <i>k</i> would be 2, even though 2 does not appear in this list.</p>
<p>If <i>k</i> = 1, then the state is said to be <b>aperiodic</b>; otherwise (k&gt;1), the state is said to be <b>periodic with period <i>k</i></b>.</p>
<p>It can be shown that every state in a communicating class must have the same period.</p>
<p><a name="Recurrence" id="Recurrence"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=7" title="Edit section: Recurrence">edit</a>]</span> <span class="mw-headline">Recurrence</span></h3>
<p>A state <i>i</i> is said to be <b>transient</b> if, given that we start in state <i>i</i>, there is a non-zero probability that we will never return to <i>i</i>. Formally, let the <a href="/wiki/Random_variable" title="Random variable">random variable</a> <i>T<sub>i</sub></i> be the first return time to state <i>i</i> (the "hitting time"):</p>
<dl>
<dd><img class="tex" alt=" T_i = \inf \{ n\ge1: X_n = i | X_0 = i\}." src="http://upload.wikimedia.org/math/7/b/5/7b526bfd8f7dfa5c7f4fa550289b8195.png" /></dd>
</dl>
<p>Then, state <i>i</i> is transient <a href="/wiki/If_and_only_if" title="If and only if">if and only if</a>:</p>
<dl>
<dd><img class="tex" alt=" \Pr(T_i = {\infty}) &gt; 0. " src="http://upload.wikimedia.org/math/5/3/9/539ad79e224359ec5bfd21c67f6d8d7c.png" /></dd>
</dl>
<p>If a state <i>i</i> is not transient (it has finite hitting time with probability 1), then it is said to be <b>recurrent</b> or <b>persistent</b>. Although the hitting time is finite, it need not have a finite <a href="/wiki/Expected_value" title="Expected value">expectation</a>. Let <i>M<sub>i</sub></i> be the expected return time,</p>
<dl>
<dd><img class="tex" alt=" M_i = E[T_i].\, " src="http://upload.wikimedia.org/math/a/3/0/a30ab722bde2f66e7d0b31ef1dcddc38.png" /></dd>
</dl>
<p>Then, state <i>i</i> is <b>positive recurrent</b> if <i>M<sub>i</sub></i> is finite; otherwise, state <i>i</i> is <b>null recurrent</b> (the terms <b>non-null persistent</b> and <b>null persistent</b> are also used, respectively).</p>
<p>It can be shown that a state is recurrent <a href="/wiki/If_and_only_if" title="If and only if">if and only if</a></p>
<dl>
<dd><img class="tex" alt="\sum_{n=0}^{\infty} p_{ii}^{(n)} = \infty." src="http://upload.wikimedia.org/math/1/b/7/1b712ff5bdd05c5717951becbdef7ee9.png" /></dd>
</dl>
<p>A state <i>i</i> is called <b>absorbing</b> if it is impossible to leave this state. Therefore, the state <i>i</i> is absorbing if and only if</p>
<dl>
<dd><span class="texhtml"><i>p</i><sub><i>i</i><i>i</i></sub> = 1</span> and <span class="texhtml"><i>p</i><sub><i>i</i><i>j</i></sub> = 0</span> for <img class="tex" alt="i \not= j." src="http://upload.wikimedia.org/math/3/7/a/37a6dd04f9b738239b2f323c137fb295.png" /></dd>
</dl>
<p><a name="Ergodicity" id="Ergodicity"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=8" title="Edit section: Ergodicity">edit</a>]</span> <span class="mw-headline">Ergodicity</span></h3>
<p>A state <i>i</i> is said to be <b><a href="/wiki/Ergodic_theory" title="Ergodic theory">ergodic</a></b> if it is aperiodic and positive recurrent. If all states in a Markov chain are ergodic, then the chain is said to be ergodic.</p>
<p>A finite state irreducible Markov chain is said to be ergodic if its states are aperiodic.</p>
<p><a name="Regularity" id="Regularity"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=9" title="Edit section: Regularity">edit</a>]</span> <span class="mw-headline">Regularity</span></h3>
<p>A markov chain is said to be regular if there is an <span class="texhtml"><i>n</i></span> such that</p>
<dl>
<dd><img class="tex" alt=" p_{ij}^{(n)} &gt; 0 " src="http://upload.wikimedia.org/math/4/3/9/439c64f38bd26d937cee585cb87f587d.png" /></dd>
</dl>
<p>for all i, j. Otherwise the chain is said to be irregular.</p>
<p><a name="Steady-state_analysis_and_limiting_distributions" id="Steady-state_analysis_and_limiting_distributions"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=10" title="Edit section: Steady-state analysis and limiting distributions">edit</a>]</span> <span class="mw-headline">Steady-state analysis and limiting distributions</span></h3>
<p>If the Markov chain is a time-homogeneous Markov chain, so that the process is described by a single, time-independent matrix <span class="texhtml"><i>p</i><sub><i>i</i><i>j</i></sub></span>, then the vector <img class="tex" alt="\boldsymbol{\pi}" src="http://upload.wikimedia.org/math/8/e/5/8e5ea39de75113adec3618b7c81dce6b.png" /> is called a <b>stationary distribution</b> (or <b><a href="/wiki/Invariant_measure" title="Invariant measure">invariant measure</a></b>) if its entries <span class="texhtml">π<sub><i>j</i></sub></span> sum to 1 and it satisfies</p>
<dl>
<dd><img class="tex" alt="\pi_j = \sum_{i \in S} \pi_i p_{ij}." src="http://upload.wikimedia.org/math/0/d/7/0d7cfdc008c25ce0f1a528110b38062d.png" /></dd>
</dl>
<p>An irreducible chain has a stationary distribution if and only if all of its states are positive recurrent. In that case, <i>π</i> is unique and is related to the expected return time:</p>
<dl>
<dd><img class="tex" alt="\pi_j = \frac{1}{M_j}.\," src="http://upload.wikimedia.org/math/5/e/4/5e4caf88b759af8ec084c03b0489d541.png" /></dd>
</dl>
<p>Further, if the chain is both irreducible and aperiodic, then for any <i>i</i> and <i>j</i>,</p>
<dl>
<dd><img class="tex" alt="\lim_{n \rarr \infty} p_{ij}^{(n)} = \frac{1}{M_j}." src="http://upload.wikimedia.org/math/1/5/9/159149f78429c8d94bf0614002fce313.png" /></dd>
</dl>
<p>Note that there is no assumption on the starting distribution; the chain converges to the stationary distribution regardless of where it begins. Such <i>π</i> is called the <b>equilibrium distribution</b> of the chain.</p>
<p>If a chain is not irreducible, its stationary distributions will not be unique (consider any <a href="#Reducibility" title="">closed communicating class</a> in the chain; each one will have its own unique stationary distribution. Any of these will extend to a stationary distribution for the overall chain, where the probability outside the class is set to zero). However, if a state <i>j</i> is aperiodic, then</p>
<dl>
<dd><img class="tex" alt="\lim_{n \rarr \infty} p_{jj}^{(n)} = \frac{1}{M_j}" src="http://upload.wikimedia.org/math/c/0/c/c0c4b5301d0143ed68f607beb51d495f.png" /></dd>
</dl>
<p>and for any other state <i>i</i>, let <i>f<sub>ij</sub></i> be the probability that the chain ever visits state <i>j</i> if it starts at <i>i</i>,</p>
<dl>
<dd><img class="tex" alt="\lim_{n \rarr \infty} p_{ij}^{(n)} = \frac{f_{ij}}{M_j}." src="http://upload.wikimedia.org/math/c/a/5/ca566c34e21a683c64b9b59736ef98c3.png" /></dd>
</dl>
<p>If a state is <i>i</i> periodic with period <i>k</i> &gt; 1 then the limit</p>
<dl>
<dd><img class="tex" alt="\lim_{n \rarr \infty} p_{ii}^{(n)}" src="http://upload.wikimedia.org/math/9/2/a/92afe237ba28f8e880e6e468aff682fe.png" /></dd>
</dl>
<p>does not exist, although the limit</p>
<dl>
<dd><img class="tex" alt="\lim_{n \rarr \infty} p_{ii}^{(kn+r)}" src="http://upload.wikimedia.org/math/a/8/c/a8ceb91b698938f27dcab65a21ca101d.png" /></dd>
</dl>
<p>does exist for every integer <i>r</i>.</p>
<p><a name="Markov_chains_with_a_finite_state_space" id="Markov_chains_with_a_finite_state_space"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=11" title="Edit section: Markov chains with a finite state space">edit</a>]</span> <span class="mw-headline">Markov chains with a finite state space</span></h2>
<p>If the state space is <a href="/wiki/Finite_set" title="Finite set">finite</a>, the transition probability distribution can be represented by a <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a>, called the <b>transition matrix</b>, with the (<i>i</i>, <i>j</i>)th element of <b>P</b> equal to</p>
<dl>
<dd><img class="tex" alt="p_{ij} = \Pr(X_{n+1}=j\mid X_n=i). \," src="http://upload.wikimedia.org/math/b/a/1/ba1a69dd20cf707169d9493c15a94b70.png" /></dd>
</dl>
<p><b>P</b> is a <a href="/wiki/Stochastic_matrix" title="Stochastic matrix">stochastic matrix</a>, which is an important fact to keep in mind for the rest of this discussion. If the Markov chain is time-homogeneous, then the transition matrix <b>P</b> is the same after each step, so the <i>k</i>-step transition probability can be computed as the <i>k</i>-th power of the transition matrix, <b>P</b><sup><i>k</i></sup>.</p>
<p>The stationary distribution <b>π</b> is a (row) vector whose entries sum to 1 that satisfies the equation</p>
<dl>
<dd><img class="tex" alt=" \pi = \pi\mathbf{P}.\," src="http://upload.wikimedia.org/math/1/1/3/11363f9469eb2f176c619c6d7d605456.png" /></dd>
</dl>
<p>In other words, the stationary distribution <b>π</b> is a normalized (meaning that the sum of its entries is 1) left <a href="/wiki/Eigenvector" title="Eigenvector" class="mw-redirect">eigenvector</a> of the transition matrix associated with the <a href="/wiki/Eigenvalue" title="Eigenvalue" class="mw-redirect">eigenvalue</a> 1.</p>
<p>Alternatively, <b>π</b> can be viewed as a fixed point of the linear (hence continuous) transformation on the unit <a href="/wiki/Simplex" title="Simplex">simplex</a> associated to the matrix <b>P</b>. As any continuous transformation in the unit simplex has a fixed point, a stationary distribution always exists, but is not guaranteed to be unique, in general. However, if the Markov chain is irreducible and aperiodic, then there is a unique stationary distribution <b>π</b>. Additionally, in this case <b>P</b><sup><i>k</i></sup> converges to a rank-one matrix in which each row is the stationary distribution <b>π</b>, that is,</p>
<dl>
<dd><img class="tex" alt="\lim_{k\rightarrow\infty}\mathbf{P}^k=\mathbf{1}\pi" src="http://upload.wikimedia.org/math/4/e/b/4eb1f69af574c093662297c9af11bf04.png" /></dd>
</dl>
<p>where <b>1</b> is the column vector with all entries equal to 1. This is stated by the <a href="/wiki/Perron-Frobenius_theorem" title="Perron-Frobenius theorem" class="mw-redirect">Perron-Frobenius theorem</a>. If, by whatever means, <img class="tex" alt="\scriptstyle \lim_{k\to\infty}\mathbf{P}^k" src="http://upload.wikimedia.org/math/f/6/4/f64fe8a0941efda9e4fa64aab6dd8c1d.png" /> is found, then the stationary distribution of the Markov chain in question can be easily determined for any starting distribution, as will be explained below.</p>
<p>Since <b>P</b> is a stochastic matrix, <img class="tex" alt="\scriptstyle \lim_{k\to\infty}\mathbf{P}^k" src="http://upload.wikimedia.org/math/f/6/4/f64fe8a0941efda9e4fa64aab6dd8c1d.png" /> always exists. Because there are a number of different special cases to consider, the process of finding this limit can be a lengthy task. All the same, there are several general rules and guidelines to keep in mind. Let <b>P</b> be an <i>n</i>×<i>n</i> matrix, and define <img class="tex" alt="\scriptstyle \mathbf{Q} = \lim_{k\to\infty}\mathbf{P}^k." src="http://upload.wikimedia.org/math/f/5/9/f59b1a174ec187a9a6bffa4347f55342.png" /></p>
<p>It is always true that</p>
<dl>
<dd><img class="tex" alt="\mathbf{QP} = \mathbf{Q}." src="http://upload.wikimedia.org/math/5/2/4/524df0cc578211b0573201061944fc5a.png" /></dd>
</dl>
<p>Subtracting <b>Q</b> from both sides and factoring then yields</p>
<dl>
<dd><img class="tex" alt="\mathbf{Q}(\mathbf{P} - \mathbf{I}_{n}) = \mathbf{0}_{n,n}" src="http://upload.wikimedia.org/math/4/3/9/4399871c41e1c7d348e806f2c989a770.png" /></dd>
</dl>
<p>where <b>I</b><sub><i>n</i></sub> is the <a href="/wiki/Identity_matrix" title="Identity matrix">identity matrix</a> of size <i>n</i>, and <b>0</b><sub><i>n</i>,<i>n</i></sub> is the <a href="/wiki/Zero_matrix" title="Zero matrix">zero matrix</a> of size <i>n</i>×<i>n</i>. Multiplying together stochastic matrices always yields another stochastic matrix, so <b>Q</b> must be a stochastic matrix. It is sometimes sufficient to use the matrix equation above and the fact that <b>Q</b> is a stochastic matrix to solve for <b>Q</b>.</p>
<p>Here is one method for doing so: first, define the function <i>f</i>(<b>A</b>) to return the matrix <b>A</b> with its right-most column replaced with all 1's. Then evaluate the following equation:<sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since March 2009" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup></p>
<dl>
<dd><img class="tex" alt="\mathbf{Q}=f(\mathbf{0}_{n,n})[f(\mathbf{P}-\mathbf{I}_n)]^{-1}." src="http://upload.wikimedia.org/math/d/2/4/d2409044bc560e731c8c1203f6c66062.png" /></dd>
</dl>
<p>This equation does not work when [<i>f</i>(<b>P</b> – <b>I</b><sub>n</sub>)]<sup>–1</sup> does not exist. If this is the case, then it is necessary to take into account more information in order to find <b>Q</b>. One thing to notice is that if <b>P</b> has an element <b>P</b><sub><i>i</i>,<i>i</i></sub> on its main diagonal that is equal to 1 and the <i>i</i>th row or column is otherwise filled with 0's, then that row or column will remain unchanged in all of the subsequent powers <b>P</b><sup><i>k</i></sup>. Hence, the <i>i</i>th row or column of <b>Q</b> will have the 1 and the 0's in the same positions as in <b>P</b>.</p>
<p>In most cases, <b>P</b><sup><i>k</i></sup> approaches but never actually equals its limit. There are numerous exceptions to this, however, such as the case in which</p>
<dl>
<dd><img class="tex" alt="\mathbf{P} = \begin{bmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}." src="http://upload.wikimedia.org/math/b/c/7/bc7a927858240879b6eaff056d506f7f.png" /></dd>
</dl>
<p>Then <b>P</b><sup><i>k</i></sup> = <b>P</b> for all <i>k</i>, so <b>Q</b> = <b>P</b>. Note that for any initial distribution <b>A</b><sub>0</sub>, all subsequent distributions will be</p>
<dl>
<dd><img class="tex" alt="\mathbf{A_1} = \mathbf{A_0 P} = \begin{bmatrix} 0 &amp; 0 &amp; 1 \end{bmatrix}." src="http://upload.wikimedia.org/math/4/a/7/4a7ea0cc0de5a77396b517814343632e.png" /></dd>
</dl>
<p><a name="Reversible_Markov_chain" id="Reversible_Markov_chain"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=12" title="Edit section: Reversible Markov chain">edit</a>]</span> <span class="mw-headline">Reversible Markov chain</span></h2>
<p>The idea of a reversible Markov chain comes from the ability to "invert" a conditional probability using <a href="/wiki/Bayes%27_Rule" title="Bayes' Rule" class="mw-redirect">Bayes' Rule</a>:</p>
<dl>
<dd><img class="tex" alt="
\begin{align}
\Pr(X_{n}=i\mid X_{n+1}=j) &amp; = \frac{\Pr(X_n = i, X_{n+1} = j)}{\Pr(X_{n+1} = j)} \\
&amp; = \frac{\Pr(X_{n} = i)\Pr(X_{n+1} = j\mid X_n=i)}{\Pr(X_{n+1} = j)}.
\end{align}
" src="http://upload.wikimedia.org/math/d/f/7/df791a56e106e8199fcc7afa912f5251.png" /></dd>
</dl>
<p>It now appears that time has been reversed. Thus, a Markov chain is said to be <b>reversible</b> if there is a <b>π</b> such that</p>
<dl>
<dd><img class="tex" alt="\pi_i p_{ij} = \pi_j p_{ji}.\," src="http://upload.wikimedia.org/math/a/b/1/ab1a862f250ca94f405c1273c27fae34.png" /></dd>
</dl>
<p>This condition is also known as the <a href="/wiki/Detailed_balance" title="Detailed balance">detailed balance</a> condition.</p>
<p>Summing over <i>i</i> gives</p>
<dl>
<dd><img class="tex" alt="\sum_i \pi_i p_{ij} = \pi_j\," src="http://upload.wikimedia.org/math/1/8/a/18a56a2a1b3d323fdbfa2c3621db4aa4.png" /></dd>
</dl>
<p>so for reversible Markov chains, <b>π</b> is always a stationary distribution.</p>
<p><a name="Bernoulli_scheme" id="Bernoulli_scheme"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=13" title="Edit section: Bernoulli scheme">edit</a>]</span> <span class="mw-headline">Bernoulli scheme</span></h2>
<p>A <a href="/wiki/Bernoulli_scheme" title="Bernoulli scheme">Bernoulli scheme</a> is a special case of a Markov chain where the transition probability matrix has identical rows, which means that the next state is even independent of the current state (in addition to being independent of the past states). A Bernoulli scheme with only two possible states is known as a <a href="/wiki/Bernoulli_process" title="Bernoulli process">Bernoulli process</a>.</p>
<p><a name="Markov_chains_with_general_state_space" id="Markov_chains_with_general_state_space"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=14" title="Edit section: Markov chains with general state space">edit</a>]</span> <span class="mw-headline">Markov chains with general state space</span></h2>
<p>Many results for Markov chains with finite state space can be generalized to chains with uncountable state space through <a href="/wiki/Harris_chain" title="Harris chain">Harris chains</a>. The main idea is to see if there is a point in the state space that the chain hits with probability one. Generally, it is not true for continuous state space, however, we can define sets <i>A</i> and <i>B</i> along with a positive number <i>ε</i> and a probability measure <i>ρ</i>, such that</p>
<ol>
<li><img class="tex" alt="\text{If }\tau_A = \inf\{n\geq 0: X_n \in A\},\text{ then } P_z(\tau_A&lt;\infty)&gt;0\text{ for all }z." src="http://upload.wikimedia.org/math/8/c/e/8ce695ee55678cc789776fcf3f4c8931.png" /></li>
<li><img class="tex" alt="\text{If }x \in A\text{ and }C\subset B,\text{ then } p(x, C)\geq \varepsilon \rho(C)." src="http://upload.wikimedia.org/math/5/b/a/5bab38e166131ee8c06ff8e32f12472b.png" /></li>
</ol>
<p>Then we could collapse the sets into an auxiliary point <i>α</i>, and a recurrent <a href="/wiki/Harris_chain" title="Harris chain">Harris chain</a> can be modified to contain <i>α</i>. Lastly, the collection of <a href="/wiki/Harris_chain" title="Harris chain">Harris chains</a> is a comfortable level of generality, which is broad enough to contain a large number of interesting examples, yet restrictive enough to allow for a rich theory.</p>
<p><a name="Applications" id="Applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=15" title="Edit section: Applications">edit</a>]</span> <span class="mw-headline">Applications</span></h2>
<p><a name="Physics" id="Physics"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=16" title="Edit section: Physics">edit</a>]</span> <span class="mw-headline">Physics</span></h3>
<p>Markovian systems appear extensively in <a href="/wiki/Physics" title="Physics">physics</a>, particularly <a href="/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a>, whenever probabilities are used to represent unknown or unmodelled details of the system, if it can be assumed that the dynamics are time-invariant, and that no relevant history need be considered which is not already included in the state description.</p>
<p><a name="Testing" id="Testing"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=17" title="Edit section: Testing">edit</a>]</span> <span class="mw-headline">Testing</span></h3>
<p>Several theorists have proposed the idea of the Markov chain statistical test (MCST), a method of conjoining Markov chains to form a 'Markov blanket', arranging these chains in several recursive layers ('wafering') and producing more efficient test sets — samples — as a replacement for exhaustive testing. MCSTs also have uses in temporal state-based networks; Chilukuri et al.'s paper entitled "Temporal Uncertainty Reasoning Networks for Evidence Fusion with Applications to Object Detection and Tracking" (ScienceDirect) gives an excellent background and case study for applying MCSTs to a wider range of applications.</p>
<p><a name="Queueing_theory" id="Queueing_theory"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=18" title="Edit section: Queueing theory">edit</a>]</span> <span class="mw-headline">Queueing theory</span></h3>
<p>Markov chains can also be used to model various processes in <a href="/wiki/Queueing_theory" title="Queueing theory">queueing theory</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>.<sup id="cite_ref-CTCN_1-1" class="reference"><a href="#cite_note-CTCN-1" title=""><span>[</span>2<span>]</span></a></sup> <a href="/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon's</a> famous 1948 paper <i><a href="/wiki/A_mathematical_theory_of_communication" title="A mathematical theory of communication" class="mw-redirect">A mathematical theory of communication</a></i>, which at a single step created the field of <a href="/wiki/Information_theory" title="Information theory">information theory</a>, opens by introducing the concept of <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">entropy</a> through Markov modeling of the English language. Such idealized models can capture many of the statistical regularities of systems. Even without describing the full structure of the system perfectly, such signal models can make possible very effective <a href="/wiki/Data_compression" title="Data compression">data compression</a> through <a href="/wiki/Entropy_coding" title="Entropy coding" class="mw-redirect">entropy coding</a> techniques such as <a href="/wiki/Arithmetic_coding" title="Arithmetic coding">arithmetic coding</a>. They also allow effective <a href="/wiki/State_estimation" title="State estimation" class="mw-redirect">state estimation</a> and <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>. The world's mobile telephone systems depend on the <a href="/wiki/Viterbi_algorithm" title="Viterbi algorithm">Viterbi algorithm</a> for error-correction, while <a href="/wiki/Hidden_Markov_models" title="Hidden Markov models" class="mw-redirect">hidden Markov models</a> are extensively used in <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a> and also in <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>, for instance for coding region/gene prediction. Markov chains also play an important role in <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a>.</p>
<p><a name="Internet_applications" id="Internet_applications"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=19" title="Edit section: Internet applications">edit</a>]</span> <span class="mw-headline">Internet applications</span></h3>
<p>The <a href="/wiki/PageRank" title="PageRank">PageRank</a> of a webpage as used by <a href="/wiki/Google" title="Google">Google</a> is defined by a Markov chain.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2" title=""><span>[</span>3<span>]</span></a></sup> It is the probability to be at page <span class="texhtml"><i>i</i></span> in the stationary distribution on the following Markov chain on all (known) webpages. If <span class="texhtml"><i>N</i></span> is the number of known webpages, and a page <span class="texhtml"><i>i</i></span> has <span class="texhtml"><i>k</i><sub><i>i</i></sub></span> links then it has transition probability <img class="tex" alt="\frac{\alpha}{k_i} + \frac{1-\alpha}{N}" src="http://upload.wikimedia.org/math/b/e/0/be09039510e71734e7fc4e8c5b24db1e.png" /> for all pages that are linked to and <img class="tex" alt="\frac{1-\alpha}{N}" src="http://upload.wikimedia.org/math/7/d/d/7dd2fe82b13345c8aa3aaaf477027d55.png" /> for all pages that are not linked to. The parameter <span class="texhtml">α</span> is taken to be about 0.85.<sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since March 2009" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup></p>
<p>Markov models have also been used to analyze web navigation behavior of users. A user's web link transition on a particular website can be modeled using first- or second-order Markov models and can be used to make predictions regarding future navigation and to personalize the web page for an individual user.</p>
<p><a name="Statistical" id="Statistical"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=20" title="Edit section: Statistical">edit</a>]</span> <span class="mw-headline">Statistical</span></h3>
<p>Markov chain methods have also become very important for generating sequences of random numbers to accurately reflect very complicated desired probability distributions, via a process called <a href="/wiki/Markov_chain_Monte_Carlo" title="Markov chain Monte Carlo">Markov chain Monte Carlo</a> (MCMC). In recent years this has revolutionised the practicability of <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a> methods, allowing a wide range of <a href="/wiki/Posterior_distribution" title="Posterior distribution" class="mw-redirect">posterior distributions</a> to be simulated and their parameters found numerically.</p>
<p><a name="Economics" id="Economics"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=21" title="Edit section: Economics">edit</a>]</span> <span class="mw-headline">Economics</span></h3>
<p>Dynamic macroeconomics heavily uses Markov chains. An example is using Markov chains to exogenously model prices of equity (stock) in a <a href="/wiki/General_equilibrium" title="General equilibrium">general equilibrium</a> setting.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup></p>
<p><a name="Social_sciences" id="Social_sciences"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=22" title="Edit section: Social sciences">edit</a>]</span> <span class="mw-headline">Social sciences</span></h3>
<p>Markov chains are generally used in describing <a href="/wiki/Path-dependent" title="Path-dependent" class="mw-redirect">path-dependent</a> arguments, where current structural configurations condition future outcomes. An example is the commonly argued link between <a href="/wiki/Economic_development" title="Economic development">economic development</a> and the rise of <a href="/wiki/Democracy" title="Democracy">democracy</a>. Once a country reaches a specific level of economic development, the configuration of structural factors, such as size of the <a href="/w/index.php?title=Commercial_bourgeoisie&amp;action=edit&amp;redlink=1" class="new" title="Commercial bourgeoisie (page does not exist)">commercial bourgeoisie</a>, the ratio of urban to rural residence, the rate of <a href="/wiki/Political" title="Political" class="mw-redirect">political</a> mobilization, etc, will generate a higher probability of <a href="/wiki/Transitioning" title="Transitioning" class="mw-redirect">transitioning</a> from authoritarian to democratic rule.</p>
<p><a name="Mathematical_biology" id="Mathematical_biology"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=23" title="Edit section: Mathematical biology">edit</a>]</span> <span class="mw-headline">Mathematical biology</span></h3>
<p>Markov chains also have many applications in biological modelling, particularly <a href="/wiki/Population_process" title="Population process">population processes</a>, which are useful in modelling processes that are (at least) analogous to biological populations. The <a href="/wiki/Leslie_matrix" title="Leslie matrix">Leslie matrix</a> is one such example, though some of its entries are not probabilities (they may be greater than 1). Another important example is the modeling of cell shape in dividing sheets of <a href="/wiki/Epithelial_cells" title="Epithelial cells" class="mw-redirect">epithelial cells</a>. The distribution of shapes – predominantly hexagonal – was a long standing mystery until it was explained by a simple Markov Model, where a cell's state is its number of sides. Empirical evidence from frogs, fruit flies, and hydra further suggests that the stationary distribution of cell shape is exhibited by almost all multicellular animals.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup> Yet another example is the state of <a href="/wiki/Ion_channel" title="Ion channel">Ion channels</a> in cell membranes.</p>
<p><a name="Gambling" id="Gambling"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=24" title="Edit section: Gambling">edit</a>]</span> <span class="mw-headline">Gambling</span></h3>
<p>Markov chains can be used to model many games of chance. The children's games <a href="/wiki/Snakes_and_Ladders" title="Snakes and Ladders" class="mw-redirect">Snakes and Ladders</a> and <a href="/w/index.php?title=Hi_ho_cherry-o&amp;action=edit&amp;redlink=1" class="new" title="Hi ho cherry-o (page does not exist)">"Hi Ho! Cherry-O"</a>, for example, are represented exactly by Markov chains. At each turn, the player starts in a given state (on a given square) and from there has fixed odds of moving to certain other states (squares).</p>
<p><a name="Music" id="Music"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=25" title="Edit section: Music">edit</a>]</span> <span class="mw-headline">Music</span></h3>
<p>Markov chains are employed in <a href="/wiki/Algorithmic_composition" title="Algorithmic composition">algorithmic music composition</a>, particularly in <a href="/wiki/Software" title="Software" class="mw-redirect">software</a> programs such as <a href="/wiki/CSound" title="CSound" class="mw-redirect">CSound</a> or <a href="/wiki/Max_(software)" title="Max (software)">Max</a>. In a first-order chain, the states of the system become note or pitch values, and a <a href="/wiki/Probability_vector" title="Probability vector">probability vector</a> for each note is constructed, completing a transition probability matrix (see below). An algorithm is constructed to produce and output note values based on the transition matrix weightings, which could be <a href="/wiki/MIDI" title="MIDI" class="mw-redirect">MIDI</a> note values, frequency (<a href="/wiki/Hz" title="Hz">Hz</a>), or any other desirable metric.</p>
<table class="wikitable" style="float: left">
<caption>1st-order matrix</caption>
<tr>
<th>Note</th>
<th>A</th>
<th>C#</th>
<th>Eb</th>
</tr>
<tr>
<th>A</th>
<td>0.1</td>
<td>0.6</td>
<td>0.3</td>
</tr>
<tr>
<th>C#</th>
<td>0.25</td>
<td>0.05</td>
<td>0.7</td>
</tr>
<tr>
<th>Eb</th>
<td>0.7</td>
<td>0.3</td>
<td>0</td>
</tr>
</table>
<table class="wikitable" style="float: left">
<caption>2nd-order matrix</caption>
<tr>
<th>Note</th>
<th>A</th>
<th>D</th>
<th>G</th>
</tr>
<tr>
<th>AA</th>
<td>0.18</td>
<td>0.6</td>
<td>0.22</td>
</tr>
<tr>
<th>AD</th>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
</tr>
<tr>
<th>AG</th>
<td>0.15</td>
<td>0.75</td>
<td>0.1</td>
</tr>
<tr>
<th>DD</th>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>DA</th>
<td>0.25</td>
<td>0</td>
<td>0.75</td>
</tr>
<tr>
<th>DG</th>
<td>0.9</td>
<td>0.1</td>
<td>0</td>
</tr>
<tr>
<th>GG</th>
<td>0.4</td>
<td>0.4</td>
<td>0.2</td>
</tr>
<tr>
<th>GA</th>
<td>0.5</td>
<td>0.25</td>
<td>0.25</td>
</tr>
<tr>
<th>GD</th>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</table>
<div style="clear:both"></div>
<p>A second-order Markov chain can be introduced by considering the current state <i>and</i> also the previous state, as indicated in the second table. Higher, <i>n</i>th-order chains tend to "group" particular notes together, while 'breaking off' into other patterns and sequences occasionally. These higher-order chains tend to generate results with a sense of <a href="/wiki/Phrase_(music)" title="Phrase (music)">phrasal</a> structure, rather than the 'aimless wandering' produced by a first-order system.<sup id="cite_ref-Roads_5-0" class="reference"><a href="#cite_note-Roads-5" title=""><span>[</span>6<span>]</span></a></sup></p>
<p><a name="Baseball" id="Baseball"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=26" title="Edit section: Baseball">edit</a>]</span> <span class="mw-headline">Baseball</span></h3>
<p>Markov chain models have been used in advanced baseball analysis since 1960, although their use is still rare. Each half-inning of a baseball game fits the Markov chain state when the number of runners and outs are considered. For each half-inning there are 24 possible run-out combinations. Markov chain models can be used to evaluate runs created for both individual players as well as a team.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup></p>
<p><a name="Markov_text_generators" id="Markov_text_generators"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=27" title="Edit section: Markov text generators">edit</a>]</span> <span class="mw-headline">Markov text generators</span></h3>
<p>Markov processes can also be used to generate superficially "real-looking" text given a sample document: they are used in a variety of recreational "parody generator" software (see <a href="/wiki/Dissociated_press" title="Dissociated press">dissociated press</a>, <a href="/wiki/Jeff_Harrison" title="Jeff Harrison">Jeff Harrison</a>, <a href="/wiki/Mark_V_Shaney" title="Mark V Shaney">Mark V Shaney</a><sup id="cite_ref-Travesty_7-0" class="reference"><a href="#cite_note-Travesty-7" title=""><span>[</span>8<span>]</span></a></sup> <sup id="cite_ref-Hartman_8-0" class="reference"><a href="#cite_note-Hartman-8" title=""><span>[</span>9<span>]</span></a></sup> ).</p>
<p>These processes are also used by <a href="/wiki/Spammers" title="Spammers" class="mw-redirect">spammers</a> to inject real-looking hidden paragraphs into <a href="/wiki/Unsolicited_email" title="Unsolicited email" class="mw-redirect">unsolicited email</a> in an attempt to get these messages past <a href="/wiki/Spam_filter" title="Spam filter" class="mw-redirect">spam filters</a>.</p>
<p><a name="History" id="History"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=28" title="Edit section: History">edit</a>]</span> <span class="mw-headline">History</span></h2>
<p><a href="/wiki/Andrey_Markov" title="Andrey Markov">Andrey Markov</a> produced the first results (1906) for these processes, purely theoretically. A generalization to countably infinite state spaces was given by <a href="/wiki/Andrey_Nikolaevich_Kolmogorov" title="Andrey Nikolaevich Kolmogorov" class="mw-redirect">Kolmogorov</a> (1936). Markov chains are related to <a href="/wiki/Brownian_motion" title="Brownian motion">Brownian motion</a> and the <a href="/wiki/Ergodic_hypothesis" title="Ergodic hypothesis">ergodic hypothesis</a>, two topics in physics which were important in the early years of the twentieth century, but Markov appears to have pursued this out of a mathematical motivation, namely the extension of the <a href="/wiki/Law_of_large_numbers" title="Law of large numbers">law of large numbers</a> to dependent events. In 1913, he applied his findings for the first time to the first 20,000 letters of Pushkin's "Eugene Onegin".</p>
<p>Seneta<sup id="cite_ref-9" class="reference"><a href="#cite_note-9" title=""><span>[</span>10<span>]</span></a></sup> provides an account of Markov's motivations and the theory's early development. The term "Markov chain" was used by Markov (1906).<sup id="cite_ref-10" class="reference"><a href="#cite_note-10" title=""><span>[</span>11<span>]</span></a></sup></p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=29" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<div style="-moz-column-count:4; column-count:4;">
<ul>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov model</a></li>
<li><a href="/wiki/Examples_of_Markov_chains" title="Examples of Markov chains">Examples of Markov chains</a></li>
<li><a href="/wiki/Markov_process" title="Markov process">Markov process</a></li>
<li><a href="/wiki/Markov_information_source" title="Markov information source">Markov information source</a></li>
<li><a href="/wiki/Markov_chain_Monte_Carlo" title="Markov chain Monte Carlo">Markov chain Monte Carlo</a></li>
<li><a href="/wiki/Semi-Markov_process" title="Semi-Markov process">Semi-Markov process</a></li>
<li><a href="/wiki/Variable-order_Markov_model" title="Variable-order Markov model">Variable-order Markov model</a></li>
<li><a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a></li>
<li><a href="/wiki/Shift_of_finite_type" title="Shift of finite type" class="mw-redirect">Shift of finite type</a></li>
<li><a href="/wiki/Mark_V_Shaney" title="Mark V Shaney">Mark V Shaney</a></li>
<li><a href="/wiki/Phase-type_distribution" title="Phase-type distribution">Phase-type distribution</a></li>
<li><a href="/wiki/Markov_chain_mixing_time" title="Markov chain mixing time">Markov chain mixing time</a></li>
<li><a href="/wiki/Quantum_Markov_chain" title="Quantum Markov chain">Quantum Markov chain</a></li>
<li><a href="/wiki/Markov_network" title="Markov network">Markov network</a></li>
<li><a href="/wiki/Belief_propagation" title="Belief propagation">Belief propagation</a></li>
<li><a href="/wiki/Factor_graph" title="Factor graph">Factor graph</a></li>
<li><a href="/wiki/Recurrence_period_density_entropy" title="Recurrence period density entropy">Recurrence period density entropy</a></li>
<li><a href="/wiki/Sequential_analysis" title="Sequential analysis">Sequential analysis</a></li>
<li><a href="/wiki/Markov_chain_geostatistics" title="Markov chain geostatistics">Markov chain geostatistics</a></li>
<li><a href="/wiki/Markovian_parallax_denigrate" title="Markovian parallax denigrate">Markovian parallax denigrate</a></li>
</ul>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=30" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<div class="references-small" style="-moz-column-count: 2; column-count: 2;">
<ol class="references">
<li id="cite_note-MCSS-0"><b><a href="#cite_ref-MCSS_0-0" title="">^</a></b> S. P. Meyn and R.L. Tweedie, 2005. <a href="http://decision.csl.uiuc.edu/~meyn/pages/book.html" class="external text" title="http://decision.csl.uiuc.edu/~meyn/pages/book.html" rel="nofollow">Markov Chains and Stochastic Stability</a>. Second edition to appear, Cambridge University Press, 2008.</li>
<li id="cite_note-CTCN-1">^ <a href="#cite_ref-CTCN_1-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-CTCN_1-1" title=""><sup><i><b>b</b></i></sup></a> S. P. Meyn, 2007. <a href="http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html" class="external text" title="http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html" rel="nofollow">Control Techniques for Complex Networks</a>, Cambridge University Press, 2007.</li>
<li id="cite_note-2"><b><a href="#cite_ref-2" title="">^</a></b> <span class="plainlinks"><a href="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999" class="external text" title="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999" rel="nofollow">U.S. Patent 6,285,999</a></span><span class="PDFlink noprint"><a href="http://www.pat2pdf.org/pat2pdf/foo.pl?number=6285999" class="external text" title="http://www.pat2pdf.org/pat2pdf/foo.pl?number=6285999" rel="nofollow">&#160;</a></span></li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> Brennan, Michael, and Yihong Xiab. Stock Price Volatility and the Equity Premium. Department of Finance, the Anderson School of Management, UCLA. <a href="http://bbs.cenet.org.cn/uploadImages/200352118122167693.pdf" class="external free" title="http://bbs.cenet.org.cn/uploadImages/200352118122167693.pdf" rel="nofollow">http://bbs.cenet.org.cn/uploadImages/200352118122167693.pdf</a></li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> <a href="http://www.eecs.harvard.edu/~abpatel/drosophila_wing_model.htm" class="external free" title="http://www.eecs.harvard.edu/~abpatel/drosophila_wing_model.htm" rel="nofollow">http://www.eecs.harvard.edu/~abpatel/drosophila_wing_model.htm</a></li>
<li id="cite_note-Roads-5"><b><a href="#cite_ref-Roads_5-0" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFCurtis_Roads_.28ed..291996">Curtis Roads (ed.) (1996). <i>The Computer Music Tutorial</i>. MIT Press. <a href="/wiki/Special:BookSources/0262181584" class="internal">ISBN 0262181584</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Computer+Music+Tutorial&amp;rft.aulast=Curtis+Roads+%28ed.%29&amp;rft.au=Curtis+Roads+%28ed.%29&amp;rft.date=1996&amp;rft.pub=MIT+Press&amp;rft.isbn=0262181584&amp;rfr_id=info:sid/en.wikipedia.org:Markov_chain"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> <cite style="font-style:normal" class="web" id="CITEREFPankin">Pankin, Mark D.. <a href="http://www.pankin.com/markov/theory.htm" class="external text" title="http://www.pankin.com/markov/theory.htm" rel="nofollow">"MARKOV CHAIN MODELS: THEORETICAL BACKGROUND"</a><span class="printonly">. <a href="http://www.pankin.com/markov/theory.htm" class="external free" title="http://www.pankin.com/markov/theory.htm" rel="nofollow">http://www.pankin.com/markov/theory.htm</a></span><span class="reference-accessdate">. Retrieved on 2007-11-26</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.btitle=MARKOV+CHAIN+MODELS%3A+THEORETICAL+BACKGROUND&amp;rft.atitle=&amp;rft.aulast=Pankin&amp;rft.aufirst=Mark+D.&amp;rft.au=Pankin%2C+Mark+D.&amp;rft_id=http%3A%2F%2Fwww.pankin.com%2Fmarkov%2Ftheory.htm&amp;rfr_id=info:sid/en.wikipedia.org:Markov_chain"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-Travesty-7"><b><a href="#cite_ref-Travesty_7-0" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFKennerO.27Rourke1984">Kenner, Hugh; <a href="/wiki/Joseph_O%27Rourke_(professor)" title="Joseph O'Rourke (professor)">O'Rourke, Joseph</a> (November 1984), "A Travesty Generator for Micros", <i>BYTE</i> <b>9</b> (12): 129–131, 449–469</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A+Travesty+Generator+for+Micros&amp;rft.jtitle=BYTE&amp;rft.aulast=Kenner&amp;rft.aufirst=Hugh&amp;rft.au=Kenner%2C+Hugh&amp;rft.au=O%27Rourke%2C+Joseph&amp;rft.date=November+1984&amp;rft.volume=9&amp;rft.issue=12&amp;rft.pages=129%E2%80%93131%2C+449%E2%80%93469&amp;rfr_id=info:sid/en.wikipedia.org:Markov_chain"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-Hartman-8"><b><a href="#cite_ref-Hartman_8-0" title="">^</a></b> <cite style="font-style:normal" class="" id="CITEREFHartman1996">Hartman, Charles (1996), <i>The Virtual Muse: Experiments in Computer Poetry</i>, Hanover, NH: Wesleyan University Press, <a href="/wiki/Special:BookSources/0819522392" class="internal">ISBN 0819522392</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Virtual+Muse%3A+Experiments+in+Computer+Poetry&amp;rft.aulast=Hartman&amp;rft.aufirst=Charles&amp;rft.au=Hartman%2C+Charles&amp;rft.date=1996&amp;rft.place=Hanover%2C+NH&amp;rft.pub=Wesleyan+University+Press&amp;rft.isbn=0819522392&amp;rfr_id=info:sid/en.wikipedia.org:Markov_chain"><span style="display: none;">&#160;</span></span></li>
<li id="cite_note-9"><b><a href="#cite_ref-9" title="">^</a></b> Seneta, E. (1996) Markov and the Birth of Chain Dependence Theory. International Statistical Review, 64(3), 255%–263</li>
<li id="cite_note-10"><b><a href="#cite_ref-10" title="">^</a></b> Upton, G., Cook, I. (2008) <i>Oxford Dictionary of Statistics</i>, OUP, <a href="/wiki/Special:BookSources/0199541454" class="internal">ISBN 0-19-954145-4</a></li>
</ol>
</div>
<ul>
<li>A.A. Markov. "Rasprostranenie zakona bol'shih chisel na velichiny, zavisyaschie drug ot druga". <i>Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete</i>, 2-ya seriya, tom 15, pp 135–156, 1906.</li>
</ul>
<ul>
<li>A.A. Markov. "Extension of the limit theorems of probability theory to a sum of variables connected in a chain". reprinted in Appendix B of: R. Howard. <i>Dynamic Probabilistic Systems, volume 1: Markov Chains</i>. John Wiley and Sons, 1971.</li>
</ul>
<ul>
<li>Classical Text in Translation: A. A. Markov, An Example of Statistical Investigation of the Text Eugene Onegin Concerning the Connection of Samples in Chains, trans. David Link. Science in Context 19.4 (2006): 591-600. Online: <a href="http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=637500" class="external free" title="http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=637500" rel="nofollow">http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=637500</a></li>
</ul>
<ul>
<li>Leo Breiman. <i>Probability</i>. Original edition published by Addison-Wesley, 1968; reprinted by Society for Industrial and Applied Mathematics, 1992. <a href="/wiki/Special:BookSources/0898712963" class="internal">ISBN 0-89871-296-3</a>. <i>(See Chapter 7.)</i></li>
</ul>
<ul>
<li>J.L. Doob. <i>Stochastic Processes</i>. New York: John Wiley and Sons, 1953. <a href="/wiki/Special:BookSources/0471523690" class="internal">ISBN 0-471-52369-0</a>.</li>
</ul>
<ul>
<li>S. P. Meyn and R. L. Tweedie. <i>Markov Chains and Stochastic Stability</i>. London: Springer-Verlag, 1993. <a href="/wiki/Special:BookSources/0387198326" class="internal">ISBN 0-387-19832-6</a>. online: <a href="http://decision.csl.uiuc.edu/~meyn/pages/book.html" class="external free" title="http://decision.csl.uiuc.edu/~meyn/pages/book.html" rel="nofollow">http://decision.csl.uiuc.edu/~meyn/pages/book.html</a> . Second edition to appear, Cambridge University Press, 2009.</li>
</ul>
<ul>
<li>S. P. Meyn. <i>Control Techniques for Complex Networks</i>. Cambridge University Press, 2007. ISBN-13: 9780521884419. Appendix contains abridged Meyn &amp; Tweedie. online: <a href="http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html" class="external free" title="http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html" rel="nofollow">http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html</a></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFBooth1967">Booth, Taylor L. (1967). <i>Sequential Machines and Automata Theory</i> (1st ed.). New York: John Wiley and Sons, Inc.. Library of Congress Card Catalog Number 67-25924.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Sequential+Machines+and+Automata+Theory&amp;rft.aulast=Booth&amp;rft.aufirst=Taylor+L.&amp;rft.au=Booth%2C+Taylor+L.&amp;rft.date=1967&amp;rft.edition=1st&amp;rft.place=New+York&amp;rft.pub=John+Wiley+and+Sons%2C+Inc.&amp;rfr_id=info:sid/en.wikipedia.org:Markov_chain"><span style="display: none;">&#160;</span></span> Extensive, wide-ranging book meant for specialists, written for both theoretical computer scientists as well as electrical engineers. With detailed explanations of state minimization techniques, FSMs, Turing machines, Markov processes, and undecidability. Excellent treatment of Markov processes pp.449ff. Discusses Z-transforms, D transforms in their context.</li>
</ul>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFKemenyHazleton_Mirkil.2C_J._Laurie_Snell.2C_Gerald_L._Thompson1959">Kemeny, John G.; Hazleton Mirkil, J. Laurie Snell, Gerald L. Thompson (1959). <i>Finite Mathematical Structures</i> (1st ed.). Englewood Cliffs, N.J.: Prentice-Hall, Inc.. Library of Congress Card Catalog Number 59-12841.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Finite+Mathematical+Structures&amp;rft.aulast=Kemeny&amp;rft.aufirst=John+G.&amp;rft.au=Kemeny%2C+John+G.&amp;rft.au=Hazleton+Mirkil%2C+J.+Laurie+Snell%2C+Gerald+L.+Thompson&amp;rft.date=1959&amp;rft.edition=1st&amp;rft.place=Englewood+Cliffs%2C+N.J.&amp;rft.pub=Prentice-Hall%2C+Inc.&amp;rfr_id=info:sid/en.wikipedia.org:Markov_chain"><span style="display: none;">&#160;</span></span> Classical text. cf Chapter 6 <i>Finite Markov Chains</i> pp.384ff.</li>
</ul>
<ul>
<li>E. Nummelin. "General irreducible Markov chains and non-negative operators". Cambridge University Press, 1984, 2004. <a href="/wiki/Special:BookSources/052160494X" class="internal">ISBN 0-521-60494-X</a></li>
</ul>
<p><a name="Further_reading" id="Further_reading"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=31" title="Edit section: Further reading">edit</a>]</span> <span class="mw-headline">Further reading</span></h2>
<ul>
<li><a href="/wiki/Persi_Diaconis" title="Persi Diaconis">Diaconis, Persi</a>, <a href="http://www.ams.org/bull/0000-000-00/S0273-0979-08-01238-X/home.html" class="external text" title="http://www.ams.org/bull/0000-000-00/S0273-0979-08-01238-X/home.html" rel="nofollow">"The Markov chain Monte Carlo revolution"</a>, Bull. Amer. Math. Soc. (2008)</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Markov_chain&amp;action=edit&amp;section=32" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://jasss.soc.surrey.ac.uk/12/1/6.html" class="external text" title="http://jasss.soc.surrey.ac.uk/12/1/6.html" rel="nofollow">Techniques to Understand Computer Simulations: Markov Chain Analysis</a></li>
<li><a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf" class="external text" title="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf" rel="nofollow">(pdf) Markov Chains chapter in American Mathematical Society's introductory probability book</a></li>
<li><a href="http://www.utilitymill.com/utility/Markov_Chain_Parody_Text_Generator" class="external text" title="http://www.utilitymill.com/utility/Markov_Chain_Parody_Text_Generator" rel="nofollow">Generates random parodies in the style of another body of text using a Markov chain algorithm</a></li>
<li><a href="http://www.fourteenminutes.com/fun/words/" class="external text" title="http://www.fourteenminutes.com/fun/words/" rel="nofollow">A generator that uses Markov Chains to create random words</a></li>
<li><a href="http://crypto.mat.sbg.ac.at/~ste/diss/node6.html" class="external text" title="http://crypto.mat.sbg.ac.at/~ste/diss/node6.html" rel="nofollow">Markov Chains</a></li>
<li><a href="http://planetmath.org/?op=getobj&amp;from=objects&amp;id=5765" class="external text" title="http://planetmath.org/?op=getobj&amp;from=objects&amp;id=5765" rel="nofollow">Class structure</a> on <a href="/wiki/PlanetMath" title="PlanetMath">PlanetMath</a></li>
<li><a href="http://www.math.rutgers.edu/courses/338/coursenotes/chapter5.pdf" class="external text" title="http://www.math.rutgers.edu/courses/338/coursenotes/chapter5.pdf" rel="nofollow">Chapter 5: Markov Chain Models</a></li>
<li><a href="http://www.cs.bell-labs.com/cm/cs/pearls/sec153.html" class="external text" title="http://www.cs.bell-labs.com/cm/cs/pearls/sec153.html" rel="nofollow">Generating Text</a> <i>(About generating random text using a Markov chain.)</i></li>
<li><a href="http://www.sohar.com/software/meadep/" class="external text" title="http://www.sohar.com/software/meadep/" rel="nofollow">Markov modeling tool</a><i>(MEADEP)</i></li>
<li><a href="http://www.mathworks.com/company/newsletters/news_notes/clevescorner/oct02_cleve.html" class="external text" title="http://www.mathworks.com/company/newsletters/news_notes/clevescorner/oct02_cleve.html" rel="nofollow">The World's Largest Matrix Computation</a> <i>(Google's PageRank as the stationary distribution of a random walk through the Web.)</i></li>
<li><a href="http://www.gnu.org/software/emacs/manual/html_node/emacs/Dissociated-Press.html" class="external text" title="http://www.gnu.org/software/emacs/manual/html_node/emacs/Dissociated-Press.html" rel="nofollow">Dissociated Press</a> in <a href="/wiki/Emacs" title="Emacs">Emacs</a> approximates a Markov process</li>
<li><a href="http://www.vanguardsw.com/DpHelp4/dph00147.htm" class="external text" title="http://www.vanguardsw.com/DpHelp4/dph00147.htm" rel="nofollow">Markov Chain Example</a></li>
<li><a href="http://en.kerouac3001.com/markov-chains-spam-that-search-engines-like-pt-1-5.htm" class="external text" title="http://en.kerouac3001.com/markov-chains-spam-that-search-engines-like-pt-1-5.htm" rel="nofollow">Markov Chains for Search Engines</a></li>
<li><a href="http://www.zentastic.com/blog/2005/03/03/now-im-definitely-getting-arrested/" class="external text" title="http://www.zentastic.com/blog/2005/03/03/now-im-definitely-getting-arrested/" rel="nofollow">Steganography proof-of-concept using Markov Chains.</a></li>
<li><a href="http://www.codeodor.com/index.cfm/2007/11/7/Fun-With-Markov-Models/1701" class="external text" title="http://www.codeodor.com/index.cfm/2007/11/7/Fun-With-Markov-Models/1701" rel="nofollow">n<sup>th</sup> order Markov Chain implementation in Ruby</a></li>
<li><a href="http://www.hardballtimes.com/main/article/introducing-markov-chains/" class="external text" title="http://www.hardballtimes.com/main/article/introducing-markov-chains/" rel="nofollow">Baseball Run Modeler using Markov Chains</a></li>
<li><a href="http://www.pankin.com/markov/theory.htm" class="external text" title="http://www.pankin.com/markov/theory.htm" rel="nofollow">Theory of Markov chains in baseball</a></li>
<li><a href="http://garnet.fsu.edu/~ajeong/DAT/index.htm" class="external text" title="http://garnet.fsu.edu/~ajeong/DAT/index.htm" rel="nofollow">Sequential analysis software for generating visual representations of probability models</a></li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 3415/1000000
Post-expand include size: 20267/2048000 bytes
Template argument size: 5439/2048000 bytes
Expensive parser function count: 2/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:60876-0!1!0!default!!en!2 and timestamp 20090403211400 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Markov_chain">http://en.wikipedia.org/wiki/Markov_chain</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Probability_theory" title="Category:Probability theory">Probability theory</a></span> | <span dir='ltr'><a href="/wiki/Category:Stochastic_processes" title="Category:Stochastic processes">Stochastic processes</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_models" title="Category:Statistical models">Statistical models</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_March_2009" title="Category:Articles with unsourced statements since March 2009">Articles with unsourced statements since March 2009</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Markov_chain" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Markov_chain" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Markov_chain&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Markov_chain&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Markov_chain" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Markov_chain" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Markov_chain" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Markov_chain&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Markov_chain&amp;oldid=280659640" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Markov_chain&amp;id=280659640">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D8%B3%D9%84%D8%B3%D9%84%D8%A9_%D9%85%D8%A7%D8%B1%D9%83%D9%88%D9%81">العربية</a></li>
				<li class="interwiki-bg"><a href="http://bg.wikipedia.org/wiki/%D0%9C%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D1%81%D0%BA%D0%B0_%D0%B2%D0%B5%D1%80%D0%B8%D0%B3%D0%B0">Български</a></li>
				<li class="interwiki-cs"><a href="http://cs.wikipedia.org/wiki/Markov%C5%AFv_%C5%99et%C4%9Bzec">Česky</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Markow-Kette">Deutsch</a></li>
				<li class="interwiki-et"><a href="http://et.wikipedia.org/wiki/Markovi_ahel">Eesti</a></li>
				<li class="interwiki-el"><a href="http://el.wikipedia.org/wiki/%CE%91%CE%BB%CF%85%CF%83%CE%AF%CE%B4%CE%B1_%CE%9C%CE%B1%CF%81%CE%BA%CF%8C%CF%86">Ελληνικά</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Cadena_de_M%C3%A1rkov">Español</a></li>
				<li class="interwiki-fa"><a href="http://fa.wikipedia.org/wiki/%D9%81%D8%B1%D8%A7%DB%8C%D9%86%D8%AF_%D9%85%D8%A7%D8%B1%DA%A9%D9%81">فارسی</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Cha%C3%AEne_de_Markov">Français</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84">한국어</a></li>
				<li class="interwiki-hr"><a href="http://hr.wikipedia.org/wiki/Markovljev_lanac">Hrvatski</a></li>
				<li class="interwiki-is"><a href="http://is.wikipedia.org/wiki/Markov-ke%C3%B0ja">Íslenska</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Processo_markoviano">Italiano</a></li>
				<li class="interwiki-he"><a href="http://he.wikipedia.org/wiki/%D7%A9%D7%A8%D7%A9%D7%A8%D7%AA_%D7%9E%D7%A8%D7%A7%D7%95%D7%91">עברית</a></li>
				<li class="interwiki-lt"><a href="http://lt.wikipedia.org/wiki/Markovo_grandin%C4%97">Lietuvių</a></li>
				<li class="interwiki-hu"><a href="http://hu.wikipedia.org/wiki/Markov-l%C3%A1nc">Magyar</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Markov-keten">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E9%80%A3%E9%8E%96">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Proces_Markowa">Polski</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/Cadeias_de_Markov">Português</a></li>
				<li class="interwiki-ro"><a href="http://ro.wikipedia.org/wiki/Lan%C5%A3_Markov">Română</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%A6%D0%B5%D0%BF%D1%8C_%D0%9C%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D0%B0">Русский</a></li>
				<li class="interwiki-su"><a href="http://su.wikipedia.org/wiki/Rant%C3%A9_Markov">Basa Sunda</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Markovkedja">Svenska</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/X%C3%ADch_Markov">Tiếng Việt</a></li>
				<li class="interwiki-tr"><a href="http://tr.wikipedia.org/wiki/Markov_Zinciri">Türkçe</a></li>
				<li class="interwiki-ur"><a href="http://ur.wikipedia.org/wiki/%D9%85%D8%A7%D8%B1%DA%A9%D9%88%D9%88_%D8%B2%D9%86%D8%AC%DB%8C%D8%B1">اردو</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 30 March 2009, at 15:18.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv36 in 0.222 secs. --></body></html>
