<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Decision tree learning,Artificial neural network,Bayesian network,Binary decision diagram,C4.5 algorithm,Categorical variable,Chance,Conditional probability,Conjunction,Data mining,Decision-tree pruning" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Decision_tree_learning&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Decision_tree_learning&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Decision tree learning - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Decision_tree_learning";
		var wgTitle = "Decision tree learning";
		var wgAction = "view";
		var wgArticleId = "577003";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281310650;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Decision_tree_learning skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Decision tree learning</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p><b>Decision tree learning</b>, used in <a href="/wiki/Data_mining" title="Data mining">data mining</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, uses a <a href="/wiki/Decision_tree" title="Decision tree">decision tree</a> as a <a href="/wiki/Predictive_modelling" title="Predictive modelling">predictive model</a> which maps observations about an item to conclusions about the item's target value. More descriptive names for such tree models are <b>classification trees</b> or <b>regression trees</b>. In these tree structures, <a href="/wiki/Leaf_node" title="Leaf node">leaves</a> represent classifications and branches represent <a href="/wiki/Conjunction" title="Conjunction">conjunctions</a> of features that lead to those classifications.</p>
<p>In <a href="/wiki/Decision_theory" title="Decision theory">decision theory</a> and <a href="/wiki/Decision_analysis" title="Decision analysis">decision analysis</a>, a decision tree is a <a href="/wiki/Diagram" title="Diagram">graph</a> or model of <a href="/wiki/Decisions" title="Decisions" class="mw-redirect">decisions</a> and their possible consequences, including <a href="/wiki/Chance" title="Chance">chance</a> event outcomes, resource costs, and <a href="/wiki/Utility" title="Utility">utility</a>. It can be used to create a <a href="/wiki/Plan" title="Plan">plan</a> to reach a <a href="/wiki/Objective_(goal)" title="Objective (goal)" class="mw-redirect">goal</a>. Decision trees are constructed in order to help with making decisions. A decision tree is a special form of <a href="/wiki/Tree_structure" title="Tree structure">tree structure</a>. Another use of trees is as a descriptive means for calculating <a href="/wiki/Conditional_probability" title="Conditional probability">conditional probabilities</a>.</p>
<p>In decision analysis, a decision tree can be used to visually and explicitly represent decisions and <a href="/wiki/Decision_making" title="Decision making">decision making</a>. In <a href="/wiki/Data_mining" title="Data mining">data mining</a>, a decision tree describes data but not decisions; rather the resulting classification tree can be an input for <a href="/wiki/Decision_making" title="Decision making">decision making</a>. This page deals with trees in <a href="/wiki/Data_mining" title="Data mining">data mining</a>.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#General"><span class="tocnumber">1</span> <span class="toctext">General</span></a></li>
<li class="toclevel-1"><a href="#Types"><span class="tocnumber">2</span> <span class="toctext">Types</span></a></li>
<li class="toclevel-1"><a href="#Practical_example"><span class="tocnumber">3</span> <span class="toctext">Practical example</span></a></li>
<li class="toclevel-1"><a href="#Formulae"><span class="tocnumber">4</span> <span class="toctext">Formulae</span></a>
<ul>
<li class="toclevel-2"><a href="#Gini_impurity"><span class="tocnumber">4.1</span> <span class="toctext">Gini impurity</span></a></li>
<li class="toclevel-2"><a href="#Information_gain"><span class="tocnumber">4.2</span> <span class="toctext">Information gain</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Decision_tree_advantages"><span class="tocnumber">5</span> <span class="toctext">Decision tree advantages</span></a></li>
<li class="toclevel-1"><a href="#Limitations"><span class="tocnumber">6</span> <span class="toctext">Limitations</span></a></li>
<li class="toclevel-1"><a href="#Extending_decision_trees_with_decision_graphs"><span class="tocnumber">7</span> <span class="toctext">Extending decision trees with decision graphs</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#External_sources"><span class="tocnumber">9</span> <span class="toctext">External sources</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="General" id="General"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=1" title="Edit section: General">edit</a>]</span> <span class="mw-headline">General</span></h2>
<p>Decision tree learning is a common method used in data mining. Each <a href="/wiki/Interior_node" title="Interior node" class="mw-redirect">interior node</a> corresponds to a variable; an arc to a child represents a possible value of that variable. A leaf represents a possible value of target variable given the values of the variables represented by the path from the root.</p>
<p>A tree can be "learned" by splitting the source <a href="/wiki/Set_(mathematics)" title="Set (mathematics)">set</a> into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called <a href="/wiki/Recursive_partitioning" title="Recursive partitioning">recursive partitioning</a>. The <a href="/wiki/Recursion" title="Recursion">recursion</a> is completed when splitting is either non-feasible, or a singular classification can be applied to each element of the derived subset. A <a href="/wiki/Random_forest" title="Random forest">random forest</a> classifier uses a number of decision trees, in order to improve the classification rate.</p>
<p>In <a href="/wiki/Data_mining" title="Data mining">data mining</a>, trees can be described also as the combination of mathematical and computing techniques to aid the description, categorisation and generalisation of a given set of data.</p>
<p>Data comes in records of the form:</p>
<p><b>(x, y) = (x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>..., x<sub>k</sub>, y)</b></p>
<p>The dependent variable, Y, is the variable that we are trying to understand, classify or generalise. The other variables, x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub> etc., are the variables that will help with that task.</p>
<p><a name="Types" id="Types"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=2" title="Edit section: Types">edit</a>]</span> <span class="mw-headline">Types</span></h2>
<p>In <a href="/wiki/Data_mining" title="Data mining">data mining</a>, trees have three more descriptive categories/names:</p>
<ul>
<li><i>Classification tree</i> analysis is when the predicted outcome is the class to which the data belongs.</li>
<li><i>Regression tree</i> analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital).</li>
<li><i>Classification And Regression Tree</i> (CART) analysis is used to refer to both of the above procedures, first introduced by <a href="/wiki/Leo_Breiman" title="Leo Breiman">Breiman</a> et al. (BFOS84).</li>
</ul>
<p><a name="Practical_example" id="Practical_example"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=3" title="Edit section: Practical example">edit</a>]</span> <span class="mw-headline">Practical example</span></h2>
<p>Our friend David is the manager of a famous golf club. Sadly, he is having some trouble with his customer attendance. There are days when everyone wants to play golf and the staff are overworked. On other days, for no apparent reason, no one plays golf and staff have too much slack time. David’s objective is to optimise staff availability by trying to predict when people will play golf. To accomplish that he needs to understand the reason people decide to play and if there is any explanation for that. He assumes that weather must be an important underlying factor, so he decides to use the weather forecast for the upcoming week. So during two weeks he has been recording:</p>
<ul>
<li>The outlook, whether it was sunny, overcast or raining.</li>
<li>The temperature (in degrees Fahrenheit).</li>
<li>The relative humidity in percent.</li>
<li>Whether it was windy or not.</li>
<li>Whether people attended the golf club on that day.</li>
</ul>
<p>David compiled this dataset into a table containing 14 rows and 5 columns as shown below.</p>
<p><a href="/wiki/File:Golf_dataset.png" class="image" title="Image:golf dataset.png"><img alt="Image:golf dataset.png" src="http://upload.wikimedia.org/wikipedia/commons/e/e0/Golf_dataset.png" width="600" height="450" border="0" /></a></p>
<p>He then applied a decision tree model to solve his problem.</p>
<p><a href="/wiki/File:Decision_tree_model.png" class="image" title="Image:decision tree model.png"><img alt="Image:decision tree model.png" src="http://upload.wikimedia.org/wikipedia/commons/f/ff/Decision_tree_model.png" width="573" height="404" border="0" /></a></p>
<p>A decision tree is a model of the data that encodes the distribution of the class label (again the Y) in terms of the predictor attributes. It is a <a href="/wiki/Directed_acyclic_graph" title="Directed acyclic graph">directed acyclic graph</a> in form of a tree. The top node represents all the data. The classification tree algorithm concludes that the best way to explain the dependent variable, play, is by using the variable "outlook". Using the categories of the variable outlook, three different groups were found:</p>
<ul>
<li>One that plays golf when the weather is sunny,</li>
<li>One that plays when the weather is cloudy, and</li>
<li>One that plays when it's raining.</li>
</ul>
<p>David's first conclusion: if the outlook is overcast people always play golf, and there are some fanatics who play golf even in the rain. Then he divided the sunny group in two. He realised that people don't like to play golf if the humidity is higher than seventy percent.</p>
<p>Finally, he divided the rain category in two and found that people will also not play golf if it is windy.</p>
<p>And lastly, here is the short solution of the problem given by the classification tree: David dismisses most of the staff on days that are sunny and humid or on rainy days that are windy, because almost no one is going to play golf on those days. On days when a lot of people will play golf, he hires extra staff. The conclusion is that the decision tree helped David turn a complex data representation into a much easier structure (parsimonious).</p>
<p><a name="Formulae" id="Formulae"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=4" title="Edit section: Formulae">edit</a>]</span> <span class="mw-headline">Formulae</span></h2>
<p><a name="Gini_impurity" id="Gini_impurity"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=5" title="Edit section: Gini impurity">edit</a>]</span> <span class="mw-headline">Gini impurity</span></h3>
<p>Used by the <a href="/wiki/Predictive_analytics#Classification_and_regression_trees" title="Predictive analytics">CART algorithm</a>, Gini impurity is based on squared probabilities of membership for each target category in the node. It reaches its minimum (zero) when all cases in the node fall into a single target category.</p>
<p>Suppose y takes on values in {1, 2, ..., m}, and let f(i, j) = probability of getting value j in node i. That is, f(i, j) is the proportion of records assigned to node i for which y = j.</p>
<p><img class="tex" alt="I_{G}(i) = 1 - \sum^{m}_{j=1} f (i,j)^{2} = \sum^{}_{j \ne k} f (i,j) f (i,k)" src="http://upload.wikimedia.org/math/3/c/2/3c2b552a206288bf9534154c94eb14cf.png" /></p>
<p><a name="Information_gain" id="Information_gain"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=6" title="Edit section: Information gain">edit</a>]</span> <span class="mw-headline">Information gain</span></h3>
<p>Used by the <a href="/wiki/ID3_algorithm" title="ID3 algorithm">ID3</a>, <a href="/wiki/C4.5_algorithm" title="C4.5 algorithm">C4.5</a> and C5.0 tree generation algorithms. <a href="/wiki/Information_gain_in_decision_trees" title="Information gain in decision trees">Information gain</a> is based on the concept of <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">entropy</a> used in <a href="/wiki/Information_theory" title="Information theory">information theory</a> .</p>
<p><img class="tex" alt="I_{E}(i) = - \sum^{m}_{j=1} f (i,j) \log^{}_2 f (i, j)" src="http://upload.wikimedia.org/math/7/e/0/7e07ecb22da81febc26057dabcc447a8.png" /></p>
<p><a name="Decision_tree_advantages" id="Decision_tree_advantages"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=7" title="Edit section: Decision tree advantages">edit</a>]</span> <span class="mw-headline">Decision tree advantages</span></h2>
<p>Amongst other data mining methods, decision trees have several advantages:</p>
<ul>
<li>Simple to understand and interpret. People are able to understand decision tree models after a brief explanation.</li>
<li>Requires little data preparation. Other techniques often require data normalisation, <a href="/wiki/Dummy_variable" title="Dummy variable">dummy variables</a> need to be created and blank values to be removed.</li>
<li>Able to handle both numerical and <a href="/wiki/Categorical_variable" title="Categorical variable" class="mw-redirect">categorical</a> data. Other techniques are usually specialised in analysing datasets that have only one type of variable. Ex: relation rules can be only used with nominal variables while neural networks can be used only with numerical variables.</li>
<li>Use a <a href="/wiki/White_box_(software_engineering)" title="White box (software engineering)">white box</a> model. If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic. An example of a black box model is an <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> since the explanation for the results is excessively complex to be comprehended.</li>
<li>Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.</li>
<li>Robust, perform well with large data in a short time. Large amounts of data can be analysed using personal computers in a time short enough to enable stakeholders to take decisions based on its analysis.</li>
</ul>
<p><a name="Limitations" id="Limitations"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=8" title="Edit section: Limitations">edit</a>]</span> <span class="mw-headline">Limitations</span></h2>
<ul>
<li>The problem of learning an optimal decision tree is known to be <a href="/wiki/NP-complete" title="NP-complete">NP-complete</a>.<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup> Consequently, practical decision-tree learning algorithms are based on weak (heuristic) algorithms such as the <a href="/wiki/Greedy_algorithm" title="Greedy algorithm">greedy algorithm</a> where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree.</li>
<li>Decision-tree learners create over-complex trees that do not generalise the data well. This is called <a href="/wiki/Overfitting" title="Overfitting">overfitting</a><sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup>. Mechanisms such as pruning are necessary to avoid this problem.</li>
<li>There are concepts that are hard to learn because decision trees do not express them easily, such as <a href="/wiki/XOR" title="XOR" class="mw-redirect">XOR</a>, <a href="/wiki/Parity" title="Parity">parity</a> or <a href="/wiki/Multiplexer" title="Multiplexer">multiplexer</a> problems. In such cases, the decision tree becomes prohibitively large. Approaches to solve the problem involve either changing the representation of the problem domain, known as propositionalisation<sup id="cite_ref-2" class="reference"><a href="#cite_note-2" title=""><span>[</span>3<span>]</span></a></sup> or using learning algorithms based on more expressive representations instead, such as <a href="/wiki/Statistical_relational_learning" title="Statistical relational learning">statistical relational learning</a> or <a href="/wiki/Inductive_logic_programming" title="Inductive logic programming">inductive logic programming</a>.</li>
</ul>
<p><a name="Extending_decision_trees_with_decision_graphs" id="Extending_decision_trees_with_decision_graphs"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=9" title="Edit section: Extending decision trees with decision graphs">edit</a>]</span> <span class="mw-headline">Extending decision trees with decision graphs</span></h2>
<p>In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or <i>AND</i>. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using <a href="/wiki/Minimum_Message_Length" title="Minimum Message Length" class="mw-redirect">Minimum Message Length</a> (MML)<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup>. Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup> The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring. In general, decision graphs infer models with fewer leaves than decision trees.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=10" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Decision-tree_pruning" title="Decision-tree pruning">Decision-tree pruning</a></li>
<li><a href="/wiki/Pruning_(algorithm)" title="Pruning (algorithm)">Pruning (algorithm)</a></li>
<li><a href="/wiki/Binary_decision_diagram" title="Binary decision diagram">Binary decision diagram</a></li>
<li><a href="/wiki/Predictive_analytics#Classification_and_regression_trees" title="Predictive analytics">CART</a></li>
<li><a href="/wiki/ID3_algorithm" title="ID3 algorithm">ID3 algorithm</a></li>
<li><a href="/wiki/C4.5_algorithm" title="C4.5 algorithm">C4.5 algorithm</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li>
<li><a href="/wiki/Decision_stump" title="Decision stump">Decision stump</a></li>
</ul>
<p><a name="External_sources" id="External_sources"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=11" title="Edit section: External sources">edit</a>]</span> <span class="mw-headline">External sources</span></h2>
<ul>
<li>V.Berikov, A.Litvinenko, "Methods for statistical data analysis with decision trees". Novosibirsk, Sobolev Institute of Mathematics, 2003. <a href="http://www.math.nsc.ru/AP/datamine/eng/decisiontree.htm" class="external text" title="http://www.math.nsc.ru/AP/datamine/eng/decisiontree.htm" rel="nofollow">Methods for statistical data analysis with decision trees</a></li>
<li>[BFOS84] L. Breiman, J. Friedman, R. A. Olshen and C. J. Stone, "Classification and regression trees". Wadsworth, 1984.</li>
<li>[1] T. Menzies, Y. Hu, <i>Data Mining For Very Busy People</i>. IEEE Computer, October 2003, pgs. 18-25.</li>
<li><a href="http://www.mindtools.com/pages/article/newTED_04.htm" class="external text" title="http://www.mindtools.com/pages/article/newTED_04.htm" rel="nofollow">Decision Tree Analysis</a> mindtools.com</li>
<li>J.W. Comley and <a href="http://www.csse.monash.edu.au/~dld" class="external text" title="http://www.csse.monash.edu.au/~dld" rel="nofollow">D.L. Dowe</a>, "<a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2005" class="external text" title="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2005" rel="nofollow">Minimum Message Length, MDL and Generalised Bayesian Networks with Asymmetric Languages</a>", <a href="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10478&amp;mode=toc" class="external text" title="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10478&amp;mode=toc" rel="nofollow">chapter 11</a> (pp<a href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p265.jpg" class="external text" title="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p265.jpg" rel="nofollow">265</a>-<a href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p294.jpg" class="external text" title="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p294.jpg" rel="nofollow">294</a>) in P. Grunwald, M.A. Pitt and I.J. Myung (eds)., <a href="http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&amp;ttype=2&amp;tid=10478" class="external text" title="http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&amp;ttype=2&amp;tid=10478" rel="nofollow">Advances in Minimum Description Length: Theory and Applications</a>, M.I.T. Press, April 2005, <a href="/wiki/Special:BookSources/0262072629" class="internal">ISBN 0-262-07262-9</a>. (This paper puts decision trees in internal nodes of <a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian networks</a> using <a href="http://www.csse.monash.edu.au/~dld/MML.html" class="external text" title="http://www.csse.monash.edu.au/~dld/MML.html" rel="nofollow">Minimum Message Length</a> (<a href="/wiki/Minimum_message_length" title="Minimum message length">MML</a>). An earlier version is <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2003" class="external text" title="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2003" rel="nofollow">Comley and Dowe (2003)</a>, <a href="http://www.csse.monash.edu.au/~dld/Publications/2003/Comley+Dowe03_HICS2003_GeneralBayesianNetworksAsymmetricLanguages.pdf" class="external text" title="http://www.csse.monash.edu.au/~dld/Publications/2003/Comley+Dowe03_HICS2003_GeneralBayesianNetworksAsymmetricLanguages.pdf" rel="nofollow">.pdf</a>.)</li>
<li>P.J. Tan and <a href="http://www.csse.monash.edu.au/~dld" class="external text" title="http://www.csse.monash.edu.au/~dld" rel="nofollow">D.L. Dowe</a> (2003), <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#TanDowe2003" class="external text" title="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#TanDowe2003" rel="nofollow">MML Inference of Decision Graphs with Multi-Way Joins and Dynamic Attributes</a>, Proc. 16th Australian Joint Conference on Artificial Intelligence (AI'03), Perth, Australia, 3-5 Dec. 2003, Published in Lecture Notes in Artificial Intelligence (LNAI) 2903, Springer-Verlag, <a href="http://www.csse.monash.edu.au/~dld/Publications/2003/Tan+Dowe2003_MMLDecisionGraphs.pdf" class="external text" title="http://www.csse.monash.edu.au/~dld/Publications/2003/Tan+Dowe2003_MMLDecisionGraphs.pdf" rel="nofollow">pp269-281</a>.</li>
<li>P.J. Tan and <a href="http://www.csse.monash.edu.au/~dld" class="external text" title="http://www.csse.monash.edu.au/~dld" rel="nofollow">D.L. Dowe</a> (2004), <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#TanDowe2004" class="external text" title="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#TanDowe2004" rel="nofollow">MML Inference of Oblique Decision Trees</a>, Lecture Notes in Artificial Intelligence (LNAI) 3339, Springer-Verlag, <a href="http://www.csse.monash.edu.au/~dld/Publications/2004/Tan+DoweAI2004.pdf" class="external text" title="http://www.csse.monash.edu.au/~dld/Publications/2004/Tan+DoweAI2004.pdf" rel="nofollow">pp1082-1088</a>. (This paper uses <a href="/wiki/Minimum_Message_Length" title="Minimum Message Length" class="mw-redirect">Minimum Message Length</a> and actually incorporates probabilistic <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machines</a> in the leaves of the decision trees.)</li>
<li><a href="http://decisiontrees.net" class="external text" title="http://decisiontrees.net" rel="nofollow">decisiontrees.net Interactive Tutorial</a></li>
<li><a href="http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html" class="external text" title="http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html" rel="nofollow">Building Decision Trees in Python</a> From O'Reilly.</li>
<li><a href="http://www.aaai.org/aitopics/html/trees.html" class="external text" title="http://www.aaai.org/aitopics/html/trees.html" rel="nofollow">Decision Trees page at aaai.org</a>, a page with commented links.</li>
<li><a href="http://www.dezide.com" class="external text" title="http://www.dezide.com" rel="nofollow">Bayesian Networks Applied in Real World Troubleshooting Scenario</a> Dezide</li>
<li><a href="http://blogs.isixsigma.com/archive/what_makes_a_satisfied_customer.html" class="external text" title="http://blogs.isixsigma.com/archive/what_makes_a_satisfied_customer.html" rel="nofollow">Practical Application of Decision Trees</a> by Robin Barnwell</li>
<li><a href="http://ai4r.rubyforge.org/index.html" class="external text" title="http://ai4r.rubyforge.org/index.html" rel="nofollow">Decision tree implementation in Ruby (AI4R)</a></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Decision_tree_learning&amp;action=edit&amp;section=12" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> Constructing Optimal Binary Decision Trees is NP-complete. Laurent Hyafil, RL Rivest. Information Processing Letters, Vol. 5, No. 1. (1976), pp. 15-17.</li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a href="http://dx.doi.org/10.1007%2F978-1-84628-766-4" class="external text" title="http://dx.doi.org/10.1007%2F978-1-84628-766-4" rel="nofollow">10.1007/978-1-84628-766-4</a></li>
<li id="cite_note-2"><b><a href="#cite_ref-2" title="">^</a></b> <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a href="http://dx.doi.org/10.1007%2Fb13700" class="external text" title="http://dx.doi.org/10.1007%2Fb13700" rel="nofollow">10.1007/b13700</a></li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> <a href="http://citeseer.ist.psu.edu/oliver93decision.html" class="external free" title="http://citeseer.ist.psu.edu/oliver93decision.html" rel="nofollow">http://citeseer.ist.psu.edu/oliver93decision.html</a></li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> <a href="http://www.csse.monash.edu.au/~dld/Publications/2003/Tan+Dowe2003_MMLDecisionGraphs.pdf" class="external text" title="http://www.csse.monash.edu.au/~dld/Publications/2003/Tan+Dowe2003_MMLDecisionGraphs.pdf" rel="nofollow">Tan &amp; Dowe (2003)</a></li>
</ol>
</div>


<!-- 
NewPP limit report
Preprocessor node count: 116/1000000
Post-expand include size: 501/2048000 bytes
Template argument size: 78/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:577003-0!1!0!default!!en!2 and timestamp 20090402160531 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Data_mining" title="Category:Data mining">Data mining</a></span> | <span dir='ltr'><a href="/wiki/Category:Decision_trees" title="Category:Decision trees">Decision trees</a></span> | <span dir='ltr'><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Decision_tree_learning" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Decision_tree_learning" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Decision_tree_learning&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Decision_tree_learning&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Decision_tree_learning" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Decision_tree_learning" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Decision_tree_learning" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Decision_tree_learning&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Decision_tree_learning&amp;oldid=281310650" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Decision_tree_learning&amp;id=281310650">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-cs"><a href="http://cs.wikipedia.org/wiki/Rozhodovac%C3%AD_stromy">Česky</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Entscheidungsbaum">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/%C3%81rbol_de_decisi%C3%B3n">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Arbre_de_d%C3%A9cision">Français</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Albero_di_decisione">Italiano</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Beslissingsboom">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E6%B1%BA%E5%AE%9A%E6%9C%A8">日本語</a></li>
				<li class="interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Drzewo_decyzyjne">Polski</a></li>
				<li class="interwiki-th"><a href="http://th.wikipedia.org/wiki/%E0%B8%95%E0%B9%89%E0%B8%99%E0%B9%84%E0%B8%A1%E0%B9%89%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%95%E0%B8%B1%E0%B8%94%E0%B8%AA%E0%B8%B4%E0%B8%99%E0%B9%83%E0%B8%88">ไทย</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/C%C3%A2y_quy%E1%BA%BFt_%C4%91%E1%BB%8Bnh">Tiếng Việt</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 2 April 2009, at 16:04.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv206 in 0.285 secs. --></body></html>
