<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Object recognition,FeatureDetectionCompVisNavbox,3D single object recognition,Affine shape adaptation,Affine transformation,Bayesian probability,Blob detection,Canny edge detector,Computer vision,Corner detection,Covariance matrix" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Object_recognition&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Object_recognition&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Object recognition - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Object_recognition";
		var wgTitle = "Object recognition";
		var wgAction = "view";
		var wgArticleId = "14661466";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 257597155;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Object_recognition skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Object recognition</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_style.png" class="image" title="Ambox style.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/d/d6/Ambox_style.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style=""><b>This article only describes one highly specialized aspect of its associated subject.</b><br />
<small>Please help <a href="http://en.wikipedia.org/w/index.php?title=Object_recognition&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Object_recognition&amp;action=edit" rel="nofollow">improve this article</a> by adding more general information.</small></td>
</tr>
</table>
<table class="metadata plainlinks ambox ambox-content" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_content.png" class="image" title="Ambox content.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/f/f4/Ambox_content.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style=""><b>Some parts of this article may be misleading</b>.<br />
Please help <a href="http://en.wikipedia.org/w/index.php?title=Object_recognition&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Object_recognition&amp;action=edit" rel="nofollow">clarify this article</a>. Suggestions may be on the <a href="/wiki/Talk:Object_recognition" title="Talk:Object recognition">talk page</a>.</td>
</tr>
</table>
<table class="infobox" cellspacing="5" style="width: 22em; text-align: left; font-size: 88%; line-height: 1.5em; width:20em;">
<tr>
<td colspan="2" class="" style="text-align:center; font-size: 125%; font-weight: bold;"><a href="/wiki/Feature_detection_(computer_vision)" title="Feature detection (computer vision)">Feature detection</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/File:Corner.png" class="image" title="Corner.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/thumb/5/5f/Corner.png/200px-Corner.png" width="200" height="174" border="0" /></a><br />
<span style="">Output of a typical corner detection algorithm</span></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Edge_detection" title="Edge detection">Edge detection</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Canny_edge_detector" title="Canny edge detector">Canny</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Canny_edge_detector#Conclusion" title="Canny edge detector">Canny-Deriche</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Edge_detection#Differential_edge_detection" title="Edge detection">Differential</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Sobel_operator" title="Sobel operator">Sobel</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Interest_point_detection" title="Interest point detection">Interest point detection</a></th>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Corner_detection" title="Corner detection">Corner detection</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_Harris_.26_Stephens_.2F_Plessey_corner_detection_algorithm" title="Corner detection">Harris operator</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_Shi_and_Tomasi_corner_detection_algorithm" title="Corner detection">Shi and Tomasi</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_level_curve_curvature_approach" title="Corner detection">Level curve curvature</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_SUSAN_corner_detector" title="Corner detection">SUSAN</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Corner_detection#The_FAST_feature_detector" title="Corner detection">FAST</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Blob_detection" title="Blob detection">Blob detection</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Blob_detection#The_Laplacian_of_Gaussian" title="Blob detection">Laplacian of Gaussian (LoG)</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Difference_of_Gaussians" title="Difference of Gaussians">Difference of Gaussians (DoG)</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Blob_detection#The_determinant_of_the_Hessian" title="Blob detection">Determinant of Hessian (DoH)</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Maximally_stable_extremal_regions" title="Maximally stable extremal regions">Maximally stable extremal regions</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Ridge_detection" title="Ridge detection">Ridge detection</a></th>
</tr>
<tr>
<th colspan="2" style="text-align:center;">Affine invariant feature detection</th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Affine_shape_adaptation" title="Affine shape adaptation">Affine shape adaptation</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Harris_affine_region_detector" title="Harris affine region detector">Harris affine</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Hessian_Affine_region_detector" title="Hessian Affine region detector">Hessian affine</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;">Feature description</th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/SURF" title="SURF">SURF</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/GLOH" title="GLOH">GLOH</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/LESH" title="LESH">LESH</a></td>
</tr>
<tr>
<th colspan="2" style="text-align:center;"><a href="/wiki/Scale-space" title="Scale-space" class="mw-redirect">Scale-space</a></th>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Scale-space_axioms" title="Scale-space axioms">Scale-space axioms</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Scale-space_implementation" title="Scale-space implementation" class="mw-redirect">Implementation details</a></td>
</tr>
<tr>
<td colspan="2" class="" style="text-align:center;"><a href="/wiki/Pyramid_(image_processing)" title="Pyramid (image processing)">Pyramids</a></td>
</tr>
<tr>
<td style="text-align:right;" colspan="2">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;; font-size:xx-small;">This box: <a href="/wiki/Template:FeatureDetectionCompVisNavbox" title="Template:FeatureDetectionCompVisNavbox"><span title="View this template" style="">view</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/w/index.php?title=Template_talk:FeatureDetectionCompVisNavbox&amp;action=edit&amp;redlink=1" class="new" title="Template talk:FeatureDetectionCompVisNavbox (page does not exist)"><span title="Discussion about this template" style="">talk</span></a></div>
</td>
</tr>
</table>
<p><b>Object recognition</b> in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> is the task of finding a given object in an image or video sequence. Humans recognize a multitude of objects in images with little effort, despite the fact that the image of the objects may vary somewhat in different view points, in many different sizes / scale or even when they are translated or rotated. Objects can even be recognized when they are partially obstructed from view. This task is still a challenge for <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> systems in general. <a href="/wiki/David_Lowe_(computer_scientist)" title="David Lowe (computer scientist)" class="mw-redirect">David Lowe</a> pioneered the <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> approach to extracting and using scale-invariant <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> features from images to perform reliable object recognition.</p>
<p>For any object in an image, there are many 'features' which are interesting points on the object, that can be extracted to provide a "feature" description of the object. This description extracted from a training image can then be used to identify the object when attempting to locate the object in a test image containing many other objects. It is important that the set of features extracted from the training image is robust to changes in image scale, noise, illumination and local geometric distortion, for performing reliable recognition. Lowe's patented method <sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup>can robustly identify objects even among clutter and under partial occlusion because his <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> feature descriptor is invariant to scale, orientation, affine distortion and partially invariant to illumination changes<sup id="cite_ref-lowe_1-0" class="reference"><a href="#cite_note-lowe-1" title=""><span>[</span>2<span>]</span></a></sup>. This article presents Lowe's object recognition method in a nutshell and mentions a few competing techniques available for object recognition under clutter and partial occlusion.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#David_Lowe.27s_method"><span class="tocnumber">1</span> <span class="toctext">David Lowe's method</span></a></li>
<li class="toclevel-1"><a href="#Key_stages"><span class="tocnumber">2</span> <span class="toctext">Key stages</span></a>
<ul>
<li class="toclevel-2"><a href="#Scale-invariant_feature_detection"><span class="tocnumber">2.1</span> <span class="toctext">Scale-invariant feature detection</span></a></li>
<li class="toclevel-2"><a href="#Feature_matching_and_indexing"><span class="tocnumber">2.2</span> <span class="toctext">Feature matching and indexing</span></a></li>
<li class="toclevel-2"><a href="#Cluster_identification_by_Hough_transform_voting"><span class="tocnumber">2.3</span> <span class="toctext">Cluster identification by Hough transform voting</span></a></li>
<li class="toclevel-2"><a href="#Model_verification_by_linear_least_squares"><span class="tocnumber">2.4</span> <span class="toctext">Model verification by linear least squares</span></a></li>
<li class="toclevel-2"><a href="#Outlier_detection"><span class="tocnumber">2.5</span> <span class="toctext">Outlier detection</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Competing_methods_for_scale_invariant_object_recognition_under_clutter_.2F_partial_occlusion"><span class="tocnumber">3</span> <span class="toctext">Competing methods for scale invariant object recognition under clutter / partial occlusion</span></a></li>
<li class="toclevel-1"><a href="#Applications"><span class="tocnumber">4</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">6</span> <span class="toctext">External links</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="David_Lowe.27s_method" id="David_Lowe.27s_method"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=1" title="Edit section: David Lowe's method">edit</a>]</span> <span class="mw-headline">David Lowe's method</span></h2>
<p><a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> keypoints of objects are first extracted from a set of reference images<sup id="cite_ref-lowe_1-1" class="reference"><a href="#cite_note-lowe-1" title=""><span>[</span>2<span>]</span></a></sup> and stored in a database. An object is recognised in a new image by individually comparing each feature from the new image to this database and finding candidate matching features based on <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> of their feature vectors. From the full set of matches, subsets of keypoints that agree on the object and its location, scale, and orientation in the new image are identified to filter out good matches. The determination of consistent clusters is performed rapidly by using an efficient <a href="/wiki/Hash_table" title="Hash table">hash table</a> implementation of the generalized <a href="/wiki/Hough_transform" title="Hough transform">Hough transform</a>. Each cluster of 3 or more features that agree on an object and its pose is then subject to further detailed model verification and subsequently outliers are discarded. Finally the probability that a particular set of features indicates the presence of an object is computed, given the accuracy of fit and number of probable false matches. Object matches that pass all these tests can be identified as correct with high confidence<sup id="cite_ref-lowe04_2-0" class="reference"><a href="#cite_note-lowe04-2" title=""><span>[</span>3<span>]</span></a></sup>.</p>
<p><br /></p>
<table class="wikitable">
<tr>
<th>Problem</th>
<th>Technique</th>
<th>Advantage</th>
</tr>
<tr>
<td>key localization / scale / rotation</td>
<td>DoG / scale - space pyramid / orientation assignment</td>
<td>accuracy, stability, scale &amp; rotational invariance</td>
</tr>
<tr>
<td>geometric distortion</td>
<td>blurring / resampling of local image orientation planes</td>
<td>affine invariance</td>
</tr>
<tr>
<td>indexing and matching</td>
<td>nearest neighbor / Best Bin First search</td>
<td>Efficiency / speed</td>
</tr>
<tr>
<td>Cluster identification</td>
<td>Hough Transform voting</td>
<td>reliable pose models</td>
</tr>
<tr>
<td>Model verification / outlier detection</td>
<td>Linear least squares</td>
<td>better error tolerance with fewer matches</td>
</tr>
<tr>
<td>Hypothesis acceptance</td>
<td>Bayesian Probability analysis</td>
<td>reliability</td>
</tr>
</table>
<p><a name="Key_stages" id="Key_stages"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=2" title="Edit section: Key stages">edit</a>]</span> <span class="mw-headline">Key stages</span></h2>
<p><a name="Scale-invariant_feature_detection" id="Scale-invariant_feature_detection"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=3" title="Edit section: Scale-invariant feature detection">edit</a>]</span> <span class="mw-headline">Scale-invariant feature detection</span></h3>
<p>Lowe's method for image feature generation called the <b>Scale Invariant Feature Transform</b> (<a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a>) transforms an image into a large collection of feature vectors, each of which is invariant to image translation, scaling, and rotation, partially invariant to illumination changes and robust to local geometric distortion. These features share similar properties with neurons in <a href="/wiki/Inferior_temporal_cortex" title="Inferior temporal cortex" class="mw-redirect">inferior temporal cortex</a> that are used for object recognition in primate vision<sup id="cite_ref-3" class="reference"><a href="#cite_note-3" title=""><span>[</span>4<span>]</span></a></sup>. Key locations are defined as maxima and minima of the result of <a href="/wiki/Difference_of_Gaussians" title="Difference of Gaussians">difference of Gaussians</a> function applied in <a href="/wiki/Scale-space" title="Scale-space" class="mw-redirect">scale-space</a> to a series of smoothed and resampled images. Low contrast candidate points and edge response points along an edge are discarded. Dominant orientations are assigned to localized keypoints. These steps ensure that the keypoints are more stable for matching and recognition. <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> descriptors robust to local affine distortion are then obtained by considering pixels around a radius of the key location, blurring and resampling of local image orientation planes.</p>
<p><a name="Feature_matching_and_indexing" id="Feature_matching_and_indexing"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=4" title="Edit section: Feature matching and indexing">edit</a>]</span> <span class="mw-headline">Feature matching and indexing</span></h3>
<p>Indexing is the problem of storing <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> keys and identifying matching keys from the new image. Lowe used a modification of the <a href="/wiki/K-d_tree" title="K-d tree" class="mw-redirect">k-d tree</a> algorithm called the <b>Best-bin-first search</b> method <sup id="cite_ref-4" class="reference"><a href="#cite_note-4" title=""><span>[</span>5<span>]</span></a></sup>that can identify the <a href="/wiki/Nearest_neighbor" title="Nearest neighbor">nearest neighbors</a> with high probability using only a limited amount of computation. The BBF algorithm uses a modified search ordering for the <a href="/wiki/K-d_tree" title="K-d tree" class="mw-redirect">k-d tree</a> algorithm so that bins in feature space are searched in the order of their closest distance from the query location. This search order requires the use of a <a href="/wiki/Heap_(data_structure)" title="Heap (data structure)">heap (data structure)</a> based <a href="/wiki/Priority_queue" title="Priority queue">priority queue</a> for efficient determination of the search order. The best candidate match for each keypoint is found by identifying its <a href="/wiki/Nearest_neighbor" title="Nearest neighbor">nearest neighbor</a> in the database of keypoints from training images. The <a href="/wiki/Nearest_neighbor" title="Nearest neighbor">nearest neighbors</a> are defined as the keypoints with minimum <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> from the given descriptor vector. The probability that a match is correct can be determined by taking the ratio of distance from the closest neighbor to the distance of the second closest.</p>
<p>Lowe<sup id="cite_ref-lowe04_2-1" class="reference"><a href="#cite_note-lowe04-2" title=""><span>[</span>3<span>]</span></a></sup> rejected all matches in which the distance ratio is greater than 0.8, which eliminates 90% of the false matches while discarding less than 5% of the correct matches. To further improve the efficiency of the best-bin-first algorithm search was cut off after checking the first 200 <a href="/wiki/Nearest_neighbor" title="Nearest neighbor">nearest neighbor</a> candidates. For a database of 100,000 keypoints, this provides a speedup over exact <a href="/wiki/Nearest_neighbor" title="Nearest neighbor">nearest neighbor</a> search by about 2 orders of magnitude yet results in less than a 5% loss in the number of correct matches.</p>
<p><a name="Cluster_identification_by_Hough_transform_voting" id="Cluster_identification_by_Hough_transform_voting"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=5" title="Edit section: Cluster identification by Hough transform voting">edit</a>]</span> <span class="mw-headline">Cluster identification by Hough transform voting</span></h3>
<p><a href="/wiki/Hough_Transform" title="Hough Transform" class="mw-redirect">Hough Transform</a> is used to cluster reliable model hypotheses to search for keys that agree upon a particular model <a href="/wiki/Pose" title="Pose" class="mw-redirect">pose</a>. <a href="/wiki/Hough_transform" title="Hough transform">Hough transform</a> identifies clusters of features with a consistent interpretation by using each feature to vote for all object <a href="/wiki/Pose" title="Pose" class="mw-redirect">poses</a> that are consistent with the feature. When clusters of features are found to vote for the same pose of an object, the probability of the interpretation being correct is much higher than for any single feature. An entry in a <a href="/wiki/Hash_table" title="Hash table">hash table</a> is created predicting the model location, orientation, and scale from the match hypothesis.The <a href="/wiki/Hash_table" title="Hash table">hash table</a> is searched to identify all clusters of at least 3 entries in a bin, and the bins are sorted into decreasing order of size.</p>
<p>Each of the <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> keypoints specifies 2D location, scale, and orientation, and each matched keypoint in the database has a record of the keypoint’s parameters relative to the training image in which it was found. The similarity transform implied by these 4 parameters is only an approximation to the full 6 degree-of-freedom pose space for a 3D object and also does not account for any non-rigid deformations. Therefore, Lowe<sup id="cite_ref-lowe04_2-2" class="reference"><a href="#cite_note-lowe04-2" title=""><span>[</span>3<span>]</span></a></sup> used broad bin sizes of 30 degrees for orientation, a factor of 2 for scale, and 0.25 times the maximum projected training image dimension (using the predicted scale) for location. The <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> key samples generated at the larger scale are given twice the weight of those at the smaller scale. This means that the larger scale is in effect able to filter the most likely neighbours for checking at the smaller scale. This also improves recognition performance by giving more weight to the least-noisy scale. To avoid the problem of boundary effects in bin assignment, each keypoint match votes for the 2 closest bins in each dimension, giving a total of 16 entries for each hypothesis and further broadening the pose range.</p>
<p><a name="Model_verification_by_linear_least_squares" id="Model_verification_by_linear_least_squares"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=6" title="Edit section: Model verification by linear least squares">edit</a>]</span> <span class="mw-headline">Model verification by linear least squares</span></h3>
<p>Each identified cluster is then subject to a verification procedure in which a <a href="/wiki/Linear_least_squares" title="Linear least squares">linear least squares</a> solution is performed for the parameters of the <a href="/wiki/Affine_transformation" title="Affine transformation">affine transformation</a> relating the model to the image. The <a href="/wiki/Affine_transformation" title="Affine transformation">affine transformation</a> of a model point [x y]<sup>T</sup> to an image point [u v]<sup>T</sup> can be written as below</p>
<p><br /></p>
<dl>
<dd><img class="tex" alt="
\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} m1 &amp; m2 \\ m3 &amp; m4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} tx \\ ty \end{bmatrix}
" src="http://upload.wikimedia.org/math/6/7/7/67755f97b12a99a4013fd075cd29800e.png" /></dd>
</dl>
<p><br />
where the model translation is [tx ty]<sup>T</sup> and the affine rotation, scale, and stretch are represented by the parameters m1, m2, m3 and m4. To solve for the transformation parameters the equation above can be rewritten to gather the unknowns into a column vector.</p>
<dl>
<dd><img class="tex" alt="
\begin{bmatrix} x &amp; y &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; x &amp; y &amp; 0 &amp; 1 \\ ....\\ ....\end{bmatrix} \begin{bmatrix}m1 \\ m2 \\ m3 \\ m4 \\ tx \\ ty \end{bmatrix} = \begin{bmatrix} u \\ v  \\ . \\  . \end{bmatrix}
" src="http://upload.wikimedia.org/math/d/a/b/dabb71e3150e2a91276a2021bd213e69.png" /></dd>
</dl>
<p>This equation shows a single match, but any number of further matches can be added, with each match contributing two more rows to the first and last matrix. At least 3 matches are needed to provide a solution. We can write this linear system as</p>
<dl>
<dd><img class="tex" alt="A\hat{\mathbf{x}} \approx \mathbf{b}," src="http://upload.wikimedia.org/math/f/7/2/f725967bdfce4bc258ae361af019a435.png" /></dd>
</dl>
<p>where <i>A</i> is a known <i>m</i>-by-<i>n</i> <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> (usually with <i>m</i> &gt; <i>n</i>), <b>x</b> is an unknown <i>n</i>-dimensional parameter <a href="/wiki/Vector_space" title="Vector space">vector</a>, and <b>b</b> is a known <i>m</i>-dimensional measurement vector.</p>
<p>Therefore the minimizing vector <img class="tex" alt="\hat{\mathbf{x}}" src="http://upload.wikimedia.org/math/9/2/e/92e8268edeafd1dc1839eaa7163f6bc7.png" /> is a solution of the <b>normal equation</b></p>
<dl>
<dd><img class="tex" alt=" A^T \! A \hat{\mathbf{x}} = A^T \mathbf{b}. " src="http://upload.wikimedia.org/math/c/7/7/c771965eb338f7b52a3e3b3152614b1f.png" /></dd>
</dl>
<p>The solution of the system of linear equations is given in terms of the matrix <span class="texhtml">(<i>A</i><sup><i>T</i></sup><i>A</i>) <sup>− 1</sup><i>A</i><sup><i>T</i></sup></span> , called the <a href="/wiki/Moore-Penrose_pseudoinverse" title="Moore-Penrose pseudoinverse" class="mw-redirect">pseudoinverse</a> of <i>A</i>, by</p>
<dl>
<dd><img class="tex" alt=" \hat{\mathbf{x}} = (A^T\!A)^{-1} A^T \mathbf{b}. " src="http://upload.wikimedia.org/math/a/e/e/aee615d92cbf0219591e4cb33e79df63.png" /></dd>
</dl>
<p>which minimizes the sum of the squares of the distances from the projected model locations to the corresponding image locations.</p>
<p><br /></p>
<p><a name="Outlier_detection" id="Outlier_detection"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=7" title="Edit section: Outlier detection">edit</a>]</span> <span class="mw-headline">Outlier detection</span></h3>
<p><a href="/wiki/Outlier" title="Outlier">Outliers</a> can now be removed by checking for agreement between each image feature and the model, given the parameter solution. Given the <a href="/wiki/Linear_least_squares" title="Linear least squares">linear least squares</a> solution, each match is required to agree within half the error range that was used for the parameters in the <a href="/wiki/Hough_transform" title="Hough transform">Hough transform</a> bins. As outliers are discarded, the <a href="/wiki/Linear_least_squares" title="Linear least squares">linear least squares</a> solution is re-solved with the remaining points, and the process iterated. If fewer than 3 points remain after discarding <a href="/wiki/Outlier" title="Outlier">outliers</a>, then the match is rejected. In addition, a top-down matching phase is used to add any further matches that agree with the projected model position, which may have been missed from the <a href="/wiki/Hough_transform" title="Hough transform">Hough transform</a> bin due to the similarity transform approximation or other errors.</p>
<p>The final decision to accept or reject a model hypothesis is based on a detailed probabilistic model<sup id="cite_ref-5" class="reference"><a href="#cite_note-5" title=""><span>[</span>6<span>]</span></a></sup>. This method first computes the expected number of false matches to the model pose, given the projected size of the model, the number of features within the region, and the accuracy of the fit. A <a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian probability</a> analysis then gives the probability that the object is present based on the actual number of matching features found. A model is accepted if the final probability for a correct interpretation is greater than 0.98. Lowe's SIFT based object recognition gives excellent results except under wide illumination variations and under non-rigid transformations.</p>
<p><a name="Competing_methods_for_scale_invariant_object_recognition_under_clutter_.2F_partial_occlusion" id="Competing_methods_for_scale_invariant_object_recognition_under_clutter_.2F_partial_occlusion"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=8" title="Edit section: Competing methods for scale invariant object recognition under clutter / partial occlusion">edit</a>]</span> <span class="mw-headline">Competing methods for scale invariant object recognition under clutter / partial occlusion</span></h2>
<p>RIFT <sup id="cite_ref-6" class="reference"><a href="#cite_note-6" title=""><span>[</span>7<span>]</span></a></sup> is a rotation-invariant generalization of SIFT. The RIFT descriptor is constructed using circular normalized patches divided into concentric rings of equal width and within each ring a gradient orientation histogram is computed. To maintain rotation invariance, the orientation is measured at each point relative to the direction pointing outward from the center.</p>
<p>G-RIF<sup id="cite_ref-7" class="reference"><a href="#cite_note-7" title=""><span>[</span>8<span>]</span></a></sup>&#160;: Generalized Robust Invariant Feature is a general context descriptor which encodes edge orientation, edge density and hue information in a unified form combining perceptual information with spatial encoding. The object recognition scheme uses neighbouring context based voting to estimate object models.</p>
<p>"<a href="/wiki/SURF" title="SURF">SURF</a><sup id="cite_ref-8" class="reference"><a href="#cite_note-8" title=""><span>[</span>9<span>]</span></a></sup>&#160;: Speeded Up Robust Features" is a high-performance scale and rotation-invariant interest point detector / descriptor claimed to approximate or even outperform previously proposed schemes with respect to repeatability, distinctiveness, and robustness. <a href="/wiki/SURF" title="SURF">SURF</a> relies on integral images for image convolutions to reduce computation time, builds on the strengths of the leading existing detectors and descriptors (using a fast <a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessian matrix</a>-based measure for the detector and a distribution-based descriptor). It describes a distribution of <a href="/wiki/Haar_wavelet" title="Haar wavelet">Haar wavelet</a> responses within the interest point neighbourhood. Integral images are used for speed and only 64 dimensions are used reducing the time for feature computation and matching. The indexing step is based on the sign of the <a href="/wiki/Laplacian" title="Laplacian" class="mw-redirect">Laplacian</a>,which increases the matching speed and the robustness of the descriptor.</p>
<p>PCA-SIFT <sup id="cite_ref-9" class="reference"><a href="#cite_note-9" title=""><span>[</span>10<span>]</span></a></sup>and <a href="/wiki/GLOH" title="GLOH">GLOH</a> <sup id="cite_ref-10" class="reference"><a href="#cite_note-10" title=""><span>[</span>11<span>]</span></a></sup> are variants of <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a>. PCA-SIFT descriptor is a vector of image gradients in x and y direction computed within the support region. The gradient region is sampled at 39x39 locations, therefore the vector is of dimension 3042. The dimension is reduced to 36 with <a href="/wiki/PCA" title="PCA">PCA</a>. Gradient location-orientation histogram (<a href="/wiki/GLOH" title="GLOH">GLOH</a>) is an extension of the <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> descriptor designed to increase its robustness and distinctiveness. The <a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">SIFT</a> descriptor is computed for a log-polar location grid with three bins in radial direction (the radius set to 6, 11, and 15) and 8 in angular direction, which results in 17 location bins. The central bin is not divided in angular directions. The gradient orientations are quantized in 16 bins resulting in 272 bin histogram. The size of this descriptor is reduced with <a href="/wiki/PCA" title="PCA">PCA</a>. The <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> for <a href="/wiki/PCA" title="PCA">PCA</a> is estimated on image patches collected from various images. The 128 largest <a href="/wiki/Eigenvector" title="Eigenvector" class="mw-redirect">eigenvectors</a> are used for description.</p>
<p><a name="Applications" id="Applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=9" title="Edit section: Applications">edit</a>]</span> <span class="mw-headline">Applications</span></h2>
<p>Object recognition methods has the following applications:</p>
<ul>
<li>Image panoramas<sup id="cite_ref-11" class="reference"><a href="#cite_note-11" title=""><span>[</span>12<span>]</span></a></sup></li>
<li>Image watermarking<sup id="cite_ref-12" class="reference"><a href="#cite_note-12" title=""><span>[</span>13<span>]</span></a></sup></li>
<li>Global robot localization<sup id="cite_ref-13" class="reference"><a href="#cite_note-13" title=""><span>[</span>14<span>]</span></a></sup></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=10" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> <span class="plainlinks"><a href="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6,711,293" class="external text" title="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6,711,293" rel="nofollow">U.S. patent 6,711,293</a></span><span class="plainlinks"><a href="http://www.pat2pdf.org/pat2pdf/foo.pl?number=6,711,293" class="external text" title="http://www.pat2pdf.org/pat2pdf/foo.pl?number=6,711,293" rel="nofollow">&#160;</a><a href="/wiki/File:Icons-mini-file_acrobat.gif" class="image" title="Image:Icons-mini-file acrobat.gif"><img alt="Image:Icons-mini-file acrobat.gif" src="http://upload.wikimedia.org/wikipedia/commons/2/23/Icons-mini-file_acrobat.gif" width="16" height="16" border="0" /></a></span>, "Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image", David Lowe's patent for the SIFT algorithm</li>
<li id="cite_note-lowe-1">^ <a href="#cite_ref-lowe_1-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lowe_1-1" title=""><sup><i><b>b</b></i></sup></a> Lowe, D. G., “Object recognition from local scale-invariant features”, International Conference on Computer Vision, Corfu, Greece, September 1999.</li>
<li id="cite_note-lowe04-2">^ <a href="#cite_ref-lowe04_2-0" title=""><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lowe04_2-1" title=""><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-lowe04_2-2" title=""><sup><i><b>c</b></i></sup></a> Lowe, D. G., “Distinctive Image Features from Scale-Invariant Keypoints”, International Journal of Computer Vision, 60, 2, pp. 91-110, 2004.</li>
<li id="cite_note-3"><b><a href="#cite_ref-3" title="">^</a></b> Serre, T., Kouh, M., Cadieu, C., Knoblich, U., Kreiman, G., Poggio, T., “A Theory of Object Recognition: Computations and Circuits in the Feedforward Path of the Ventral Stream in Primate Visual Cortex”, Computer Science and Artificial Intelligence Laboratory Technical Report, December 19, 2005 MIT-CSAIL-TR-2005-082.</li>
<li id="cite_note-4"><b><a href="#cite_ref-4" title="">^</a></b> Beis, J., and Lowe, D.G “Shape indexing using approximate nearest-neighbour search in high-dimensional spaces”, Conference on Computer Vision and Pattern Recognition,Puerto Rico, 1997, pp. 1000–1006.</li>
<li id="cite_note-5"><b><a href="#cite_ref-5" title="">^</a></b> Lowe, D.G., Local feature view clustering for 3D object recognition. IEEE Conference on Computer Vision and Pattern Recognition,Kauai, Hawaii, 2001, pp. 682-688.</li>
<li id="cite_note-6"><b><a href="#cite_ref-6" title="">^</a></b> Lazebnik, S., Schmid, C., and Ponce, J., Semi-Local Affine Parts for Object Recognition, BMVC, 2004.</li>
<li id="cite_note-7"><b><a href="#cite_ref-7" title="">^</a></b> Sungho Kim, Kuk-Jin Yoon, In So Kweon, "Object Recognition Using a Generalized Robust Invariant Feature and Gestalt’s Law of Proximity and Similarity," Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06), 2006</li>
<li id="cite_note-8"><b><a href="#cite_ref-8" title="">^</a></b> Bay, H., Tuytelaars, T., Gool, L.V., "SURF: Speeded Up Robust Features", Proceedings of the ninth European Conference on Computer Vision, May 2006.</li>
<li id="cite_note-9"><b><a href="#cite_ref-9" title="">^</a></b> Ke, Y., and Sukthankar, R., PCA-SIFT: A More Distinctive Representation for Local Image DescriptorsComputer Vision and Pattern Recognition, 2004.</li>
<li id="cite_note-10"><b><a href="#cite_ref-10" title="">^</a></b> Mikolajczyk, K., and Schmid, C., "A performance evaluation of local descriptors", IEEE Transactions on Pattern Analysis and Machine Intelligence, 10, 27, pp 1615--1630, 2005.</li>
<li id="cite_note-11"><b><a href="#cite_ref-11" title="">^</a></b> Brown, M., and Lowe, D.G., "Recognising Panoramas," ICCV, p. 1218, Ninth IEEE International Conference on Computer Vision (ICCV'03) - Volume 2, Nice,France, 2003</li>
<li id="cite_note-12"><b><a href="#cite_ref-12" title="">^</a></b> Li, L., Guo, B., and Shao, K., " Geometrically robust image watermarking using scale-invariant feature transform and Zernike moments," Chinese Optics Letters, Volume 5, Issue 6, pp. 332-335, 2007.</li>
<li id="cite_note-13"><b><a href="#cite_ref-13" title="">^</a></b> Se,S., Lowe, D.G., and Little, J.J.,"Vision-based global localization and mapping for mobile robots", IEEE Transactions on Robotics, 21, 3 (2005), pp. 364-375.</li>
</ol>
</div>
<p><br /></p>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=11" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://citeseer.ist.psu.edu/lowe04distinctive.html" class="external text" title="http://citeseer.ist.psu.edu/lowe04distinctive.html" rel="nofollow">Lowe, D. G., “Distinctive Image Features from Scale-Invariant Keypoints”, International Journal of Computer Vision, 60, 2, pp. 91-110, 2004.</a></li>
<li><a href="http://www.cs.ubc.ca/spider/lowe/pubs.html" class="external text" title="http://www.cs.ubc.ca/spider/lowe/pubs.html" rel="nofollow">David Lowe's Publications</a></li>
<li><a href="http://www.cs.ubc.ca/~lowe/keypoints/" class="external text" title="http://www.cs.ubc.ca/~lowe/keypoints/" rel="nofollow">David Lowe's Demo Software&#160;: SIFT keypoint detector</a></li>
<li><a href="http://www.vision.ee.ethz.ch/~surf/index.html" class="external text" title="http://www.vision.ee.ethz.ch/~surf/index.html" rel="nofollow">SURF: Speeded up robust features</a></li>
<li><a href="http://lear.inrialpes.fr/pubs/2005/MS05/" class="external text" title="http://lear.inrialpes.fr/pubs/2005/MS05/" rel="nofollow">Mikolajczyk, K., and Schmid, C., "A performance evaluation of local descriptors", IEEE Transactions on Pattern Analysis and Machine Intelligence, 10, 27, pp 1615--1630, 2005.</a></li>
<li><a href="http://www.cs.cmu.edu/~yke/pcasift/" class="external text" title="http://www.cs.cmu.edu/~yke/pcasift/" rel="nofollow">PCA-SIFT: A More Distinctive Representation for Local Image Descriptors</a></li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/bmvc04.pdf" class="external text" title="http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/bmvc04.pdf" rel="nofollow">Lazebnik, S., Schmid, C., and Ponce, J., Semi-Local Affine Parts for Object Recognition, BMVC, 2004.</a></li>
<li><a href="http://user.cs.tu-berlin.de/~nowozin/libsift/" class="external text" title="http://user.cs.tu-berlin.de/~nowozin/libsift/" rel="nofollow">libsift: Scale Invariant Feature Transform implementation</a></li>
</ul>
<p><br /></p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Object_recognition&amp;action=edit&amp;section=12" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/3D_single_object_recognition" title="3D single object recognition">3D single object recognition</a></li>
<li><a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">Scale-invariant feature transform</a> (SIFT)</li>
<li><a href="/wiki/SURF" title="SURF">SURF</a></li>
<li><a href="/wiki/Feature_detection_(computer_vision)" title="Feature detection (computer vision)">Feature detection (computer vision)</a></li>
<li><a href="/wiki/Interest_point_detection" title="Interest point detection">Interest point detection</a></li>
<li><a href="/wiki/Template_matching" title="Template matching">Template matching</a></li>
<li><a href="/wiki/Pattern_recognition" title="Pattern recognition">Pattern recognition</a></li>
<li><a href="/wiki/Opencv" title="Opencv" class="mw-redirect">opencv</a></li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 1445/1000000
Post-expand include size: 23238/2048000 bytes
Template argument size: 4133/2048000 bytes
Expensive parser function count: 0/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:14661466-0!1!0!default!!en!2 and timestamp 20090430184417 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Object_recognition">http://en.wikipedia.org/wiki/Object_recognition</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Computer_vision" title="Category:Computer vision">Computer vision</a></span> | <span dir='ltr'><a href="/wiki/Category:Object_recognition_and_categorization" title="Category:Object recognition and categorization">Object recognition and categorization</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Wikipedia_articles_needing_context" title="Category:Wikipedia articles needing context">Wikipedia articles needing context</a></span> | <span dir='ltr'><a href="/wiki/Category:Misleading_articles" title="Category:Misleading articles">Misleading articles</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Object_recognition" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Object_recognition" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Object_recognition&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Object_recognition&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Object_recognition" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Object_recognition" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Object_recognition" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Object_recognition&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Object_recognition&amp;oldid=257597155" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Object_recognition&amp;id=257597155">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Objekterkennung">Deutsch</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 13 December 2008, at 00:20 (UTC).</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv66 in 1.482 secs. --></body></html>
