<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Reinforcement learning,Cleanup from September 2008,Articles with dead external links since June 2008,Backgammon,Bellman equation,Bounded rationality,Chess,Computer science,Convergence,Digital object identifier,Dynamic programming" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Reinforcement_learning&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Reinforcement_learning&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Reinforcement learning - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Reinforcement_learning";
		var wgTitle = "Reinforcement learning";
		var wgAction = "view";
		var wgArticleId = "66294";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 278960604;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Reinforcement_learning skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Reinforcement learning</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Ambox_style.png" class="image" title="Ambox style.png"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/d/d6/Ambox_style.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article <b>may require <a href="/wiki/Wikipedia:Cleanup" title="Wikipedia:Cleanup">cleanup</a> to meet Wikipedia's <a href="/wiki/Wikipedia:Manual_of_Style" title="Wikipedia:Manual of Style">quality standards</a>.</b> Please <a href="http://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit" rel="nofollow">improve this article</a> if you can. <small><i>(September 2008)</i></small></td>
</tr>
</table>
<div class="dablink">For reinforcement learning in psychology, see <a href="/wiki/Reinforcement" title="Reinforcement">Reinforcement</a>.</div>
<p>Inspired by related psychological theory, in <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b>reinforcement learning</b> is a sub-area of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> concerned with how an <i>agent</i> ought to take <i>actions</i> in an <i>environment</i> so as to maximize some notion of long-term <i>reward</i>. Reinforcement learning algorithms attempt to find a <i>policy</i> that maps <i>states</i> of the world to the actions the agent ought to take in those states. In <a href="/wiki/Economics" title="Economics">economics</a> and <a href="/wiki/Game_theory" title="Game theory">game theory</a>, reinforcement learning is considered as a <a href="/wiki/Bounded_rationality" title="Bounded rationality">boundedly rational</a> interpretation of how <a href="/wiki/Equilibrium" title="Equilibrium">equilibrium</a> may arise.</p>
<p>The environment is typically formulated as a finite-state <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a> (MDP), and reinforcement learning algorithms for this context are highly related to <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a> techniques. State transition probabilities and reward probabilities in the MDP are typically stochastic but stationary over the course of the problem.</p>
<p>Reinforcement learning differs from the <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been mostly studied through the <a href="/wiki/Multi-armed_bandit" title="Multi-armed bandit">multi-armed bandit</a> problem.</p>
<p>Formally, the basic reinforcement learning model, as applied to MDPs, consists of:</p>
<ol>
<li>a set of environment states <span class="texhtml"><i>S</i></span>;</li>
<li>a set of actions <span class="texhtml"><i>A</i></span>; and</li>
<li>a set of scalar "rewards" in <img class="tex" alt=" \Bbb{R}" src="http://upload.wikimedia.org/math/6/9/a/69a45f1e602cd2b2c2e67e41811fd226.png" />.</li>
</ol>
<p>At each time <span class="texhtml"><i>t</i></span>, the agent perceives its state <img class="tex" alt="s_t \in S" src="http://upload.wikimedia.org/math/2/2/a/22ad84e59e3053b720d3a2f60c7c5b88.png" /> and the set of possible actions <span class="texhtml"><i>A</i>(<i>s</i><sub><i>t</i></sub>)</span>. It chooses an action <img class="tex" alt="a \in A(s_t)" src="http://upload.wikimedia.org/math/d/f/e/dfe6f500d29e25144860f49d25296921.png" /> and receives from the environment the new state <span class="texhtml"><i>s</i><sub><i>t</i> + 1</sub></span> and a reward <span class="texhtml"><i>r</i><sub><i>t</i> + 1</sub></span>. Based on these interactions, the reinforcement learning agent must develop a policy <img class="tex" alt="\pi:S\rightarrow A" src="http://upload.wikimedia.org/math/5/1/4/5147fd583a6031512f24b28bc16b8b35.png" /> which maximizes the quantity <img class="tex" alt="R=r_0 + r_1 + \cdots + r_n" src="http://upload.wikimedia.org/math/2/8/a/28a806f2735bbd6639f2d53834f36e02.png" /> for MDPs which have a terminal state, or the quantity</p>
<table>
<tr style='text-align: center;'>
<td><i>R</i> =</td>
<td><span style='font-size: x-large; font-family: serif;'>∑</span></td>
<td>γ<sup><i>t</i></sup><i>r</i><sub><i>t</i></sub></td>
</tr>
<tr style='text-align: center; vertical-align: top;'>
<td></td>
<td><i>t</i></td>
<td></td>
</tr>
</table>
<p>for MDPs without terminal states (where <img class="tex" alt="0\leq\gamma\leq1" src="http://upload.wikimedia.org/math/7/6/7/76775d1871db7917f2b95377c105ab83.png" /> is some "future reward" discounting factor).</p>
<p>Thus, reinforcement learning is particularly well suited to problems which include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including <a href="/wiki/Robot_control" title="Robot control">robot control</a>, elevator scheduling, <a href="/wiki/Telecommunications" title="Telecommunications" class="mw-redirect">telecommunications</a>, <a href="/wiki/Backgammon" title="Backgammon">backgammon</a> and <a href="/wiki/Chess" title="Chess">chess</a> (<a href="#refSutton1998" title="">Sutton 1998</a>, Chapter 11).</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Algorithms"><span class="tocnumber">1</span> <span class="toctext">Algorithms</span></a></li>
<li class="toclevel-1"><a href="#Current_research"><span class="tocnumber">2</span> <span class="toctext">Current research</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">3</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">4</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">5</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Algorithms" id="Algorithms"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=1" title="Edit section: Algorithms">edit</a>]</span> <span class="mw-headline">Algorithms</span></h2>
<p>After we have defined an appropriate return function to be maximized, we need to specify the algorithm that will be used to find the policy with the maximum return.</p>
<p>The naive brute force approach entails the following two steps: a) For each possible policy, sample returns while following it. b) Choose the policy with the largest expected return. One problem with this is that the number of policies can be extremely large, or even infinite. Another is that returns might be stochastic, in which case a large number of samples will be required to accurately estimate the return of each policy. These problems can be ameliorated if we assume some structure and perhaps allow samples generated from one policy to influence the estimates made for another. The two main approaches for achieving this are value function estimation and direct policy optimization.</p>
<p>Value function approaches do this by only maintaining a set of estimates of expected returns for one policy <span class="texhtml">π</span> (usually either the current or the optimal one). In such approaches one attempts to estimate either the expected return starting from state <span class="texhtml"><i>s</i></span> and following <span class="texhtml">π</span> thereafter,</p>
<dl>
<dd><span class="texhtml"><i>V</i>(<i>s</i>) = <i>E</i>[<i>R</i> | <i>s</i>,π]</span>,</dd>
</dl>
<p>or the expected return when taking action <span class="texhtml"><i>a</i></span> in state <span class="texhtml"><i>s</i></span> and following <span class="texhtml">π</span>; thereafter,</p>
<dl>
<dd><span class="texhtml"><i>Q</i>(<i>s</i>,<i>a</i>) = <i>E</i>[<i>R</i> | <i>s</i>,π,<i>a</i>]</span></dd>
</dl>
<p>If someone gives us <span class="texhtml"><i>Q</i></span> for the optimal policy, we can always choose optimal actions by simply choosing the action with the highest value at each state. In order to do this using <span class="texhtml"><i>V</i></span>, we must either have a model of the environment, in the form of probabilities <span class="texhtml"><i>P</i>(<i>s</i>' | <i>s</i>,<i>a</i>)</span>, which allow us to calculate <span class="texhtml"><i>Q</i></span> simply through</p>
<dl>
<dd>
<table>
<tr align='center'>
<td><i>Q</i>(<i>s</i>,<i>a</i>) =</td>
<td><font size='+2'>∑</font></td>
<td><i>V</i>(<i>s</i>')<i>P</i>(<i>s</i>' | <i>s</i>,<i>a</i>),</td>
</tr>
<tr align='center' valign='top'>
<td></td>
<td><i>s</i>'</td>
<td></td>
</tr>
</table>
</dd>
</dl>
<p>or we can employ so-called Actor-Critic methods, in which the model is split into two parts: the critic, which maintains the state value estimate <span class="texhtml"><i>V</i></span>, and the actor, which is responsible for choosing the appropriate actions at each state.</p>
<p>Given a fixed policy <span class="texhtml">π</span>, Estimating <img class="tex" alt="E[R|\cdot]" src="http://upload.wikimedia.org/math/a/e/6/ae681e5b335e7307e9a02a8cf3cc1ab4.png" /> for <span class="texhtml">γ = 0</span> is trivial, as one only has to average the immediate rewards. The most obvious way to do this for <span class="texhtml">γ &gt; 0</span> is to average the total return after each state. However this type of Monte Carlo sampling requires the MDP to terminate.</p>
<p>Thus carrying out this estimation for <span class="texhtml">γ &gt; 0</span> in the general does not seem obvious. In fact, it is quite simple once one realises that the expectation of <span class="texhtml"><i>R</i></span> forms a recursive <a href="/wiki/Bellman_equation" title="Bellman equation">Bellman equation</a>:</p>
<p><span class="texhtml"><i>E</i>[<i>R</i> | <i>s</i><sub><i>t</i></sub>] = <i>r</i><sub><i>t</i></sub> + γ<i>E</i>[<i>R</i> | <i>s</i><sub><i>t</i> + 1</sub>]</span></p>
<p>By replacing those expectations with our estimates, <span class="texhtml"><i>V</i></span>, and performing <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a> with a squared error cost function, we obtain the <a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">temporal difference learning</a> algorithm TD(0). In the simplest case, the set of states and actions are both discrete and we maintain tabular estimates for each state. Similar state-action pair methods are Adaptive Heuristic Critic (AHC), <a href="/wiki/SARSA" title="SARSA">SARSA</a> and <a href="/wiki/Q-Learning" title="Q-Learning" class="mw-redirect">Q-Learning</a>. All methods feature extensions whereby some approximating architecture is used, though in some cases <a href="/wiki/Convergence" title="Convergence">convergence</a> is not guaranteed. The estimates are usually updated with some form of gradient descent, though there have been recent developments with <a href="/wiki/Least_squares" title="Least squares">least squares</a> methods for the linear approximation case.</p>
<p>The above methods not only all converge to the correct estimates for a fixed policy, but can also be used to find the optimal policy. This is usually done by following a policy π that is somehow derived from the current value estimates, i.e. by choosing the action with the highest evaluation most of the time, while still occasionally taking random actions in order to explore the space. Proofs for convergence to the optimal policy also exist for the algorithms mentioned above, under certain conditions. However, all those proofs only demonstrate asymptotic convergence and little is known theoretically about the behaviour of RL algorithms in the small-sample case, apart from within very restricted settings.</p>
<p>An alternative method to find the optimal policy is to search directly in policy space. Policy space methods define the policy as a parameterised function <span class="texhtml">π(<i>s</i>,θ)</span> with parameters <span class="texhtml">θ</span>. Commonly, a gradient method is employed to adjust the parameters. However, the application of gradient methods is not trivial, since no gradient information is assumed. Rather, the gradient itself must be estimated from noisy samples of the return. Since this greatly increases the computational cost, it can be advantageous to use a more powerful gradient method than steepest gradient descent. Policy space gradient methods have received a lot of attention in the last 5 years and have now reached a relatively mature stage, but they remain an active field. There are many other approaches, such as <a href="/wiki/Simulated_annealing" title="Simulated annealing">simulated annealing</a>, that can be taken to explore the policy space. Other direct optimization techniques, such as <a href="/wiki/Evolutionary_computation" title="Evolutionary computation">evolutionary computation</a> are used in <a href="/wiki/Evolutionary_robotics" title="Evolutionary robotics">evolutionary robotics</a>.</p>
<p><a name="Current_research" id="Current_research"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=2" title="Edit section: Current research">edit</a>]</span> <span class="mw-headline">Current research</span></h2>
<p>Current research topics include: Alternative representations (such as the <a href="/wiki/Predictive_State_Representation" title="Predictive State Representation" class="mw-redirect">Predictive State Representation</a> approach), <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a> in policy space, small-sample convergence results, algorithms and convergence results for <a href="/wiki/Partially_observable_Markov_decision_process" title="Partially observable Markov decision process">partially observable MDPs</a>, modular and hierarchical reinforcement learning. Multiagent or Distributed Reinforcement Learning is also a topic of interest in current research in this field. There is also a growing interest in Real life applications of reinforcement learning.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=3" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<ul>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference learning</a></li>
<li><a href="/wiki/Q_learning" title="Q learning" class="mw-redirect">Q learning</a></li>
<li><a href="/wiki/SARSA" title="SARSA">SARSA</a></li>
<li><a href="/wiki/Fictitious_play" title="Fictitious play">Fictitious play</a></li>
<li><a href="/wiki/Optimal_control" title="Optimal control">Optimal control</a></li>
<li><a href="/wiki/Dynamic_treatment_regimes" title="Dynamic treatment regimes">Dynamic treatment regimes</a></li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=4" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><cite style="font-style:normal" class="" id="CITEREFKaelbling.5B.5BMichael_L._Littman.5D.5D.3B_.5B.5BAndrew_W._Moore.5D.5D1996"><a href="/wiki/Leslie_P._Kaelbling" title="Leslie P. Kaelbling">Kaelbling, Leslie P.</a>; <a href="/wiki/Michael_L._Littman" title="Michael L. Littman">Michael L. Littman</a>; <a href="/w/index.php?title=Andrew_W._Moore&amp;action=edit&amp;redlink=1" class="new" title="Andrew W. Moore (page does not exist)">Andrew W. Moore</a> (1996). "<a href="http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html" class="external text" title="http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html" rel="nofollow">Reinforcement Learning: A Survey</a>". <i>Journal of Artificial Intelligence Research</i> <b>4</b>: 237–285<span class="printonly">. <a href="http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html" class="external free" title="http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html" rel="nofollow">http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Reinforcement+Learning%3A+A+Survey&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.aulast=Kaelbling&amp;rft.aufirst=Leslie+P.&amp;rft.au=Kaelbling%2C+Leslie+P.&amp;rft.au=%5B%5BMichael+L.+Littman%5D%5D%3B+%5B%5BAndrew+W.+Moore%5D%5D&amp;rft.date=1996&amp;rft.volume=4&amp;rft.pages=237%26ndash%3B285&amp;rft_id=http%3A%2F%2Fwww.cs.washington.edu%2Fresearch%2Fjair%2Fabstracts%2Fkaelbling96a.html&amp;rfr_id=info:sid/en.wikipedia.org:Reinforcement_learning"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFBertsekas.5B.5BJohn_Tsitsiklis.5D.5D1996"><a href="/w/index.php?title=Dimitri_P._Bertsekas&amp;action=edit&amp;redlink=1" class="new" title="Dimitri P. Bertsekas (page does not exist)">Bertsekas, Dimitri P.</a>; <a href="/w/index.php?title=John_Tsitsiklis&amp;action=edit&amp;redlink=1" class="new" title="John Tsitsiklis (page does not exist)">John Tsitsiklis</a> (1996). <i><a href="http://www.athenasc.com/ndpbook.html" class="external text" title="http://www.athenasc.com/ndpbook.html" rel="nofollow">Neuro-Dynamic Programming</a></i>. Nashua, NH: Athena Scientific. <a href="/wiki/Special:BookSources/1886529108" class="internal">ISBN 1-886529-10-8</a><span class="printonly">. <a href="http://www.athenasc.com/ndpbook.html" class="external free" title="http://www.athenasc.com/ndpbook.html" rel="nofollow">http://www.athenasc.com/ndpbook.html</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neuro-Dynamic+Programming&amp;rft.aulast=Bertsekas&amp;rft.aufirst=Dimitri+P.&amp;rft.au=Bertsekas%2C+Dimitri+P.&amp;rft.au=%5B%5BJohn+Tsitsiklis%5D%5D&amp;rft.date=1996&amp;rft.place=Nashua%2C+NH&amp;rft.pub=Athena+Scientific&amp;rft.isbn=1-886529-10-8&amp;rft_id=http%3A%2F%2Fwww.athenasc.com%2Fndpbook.html&amp;rfr_id=info:sid/en.wikipedia.org:Reinforcement_learning"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFSutton.5B.5BAndrew_G._Barto.5D.5D1998"><a href="/wiki/Richard_S._Sutton" title="Richard S. Sutton">Sutton, Richard S.</a>; <a href="/w/index.php?title=Andrew_G._Barto&amp;action=edit&amp;redlink=1" class="new" title="Andrew G. Barto (page does not exist)">Andrew G. Barto</a> (1998). <i><a href="http://www.cs.ualberta.ca/~sutton/book/ebook/the-book.html" class="external text" title="http://www.cs.ualberta.ca/~sutton/book/ebook/the-book.html" rel="nofollow">Reinforcement Learning: An Introduction</a></i>. MIT Press. <a href="/wiki/Special:BookSources/0262193981" class="internal">ISBN 0-262-19398-1</a><span class="printonly">. <a href="http://www.cs.ualberta.ca/~sutton/book/ebook/the-book.html" class="external free" title="http://www.cs.ualberta.ca/~sutton/book/ebook/the-book.html" rel="nofollow">http://www.cs.ualberta.ca/~sutton/book/ebook/the-book.html</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Reinforcement+Learning%3A+An+Introduction&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard+S.&amp;rft.au=Sutton%2C+Richard+S.&amp;rft.au=%5B%5BAndrew+G.+Barto%5D%5D&amp;rft.date=1998&amp;rft.pub=MIT+Press&amp;rft.isbn=0-262-19398-1&amp;rft_id=http%3A%2F%2Fwww.cs.ualberta.ca%2F%7Esutton%2Fbook%2Febook%2Fthe-book.html&amp;rfr_id=info:sid/en.wikipedia.org:Reinforcement_learning"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFGosavi2009"><a href="/w/index.php?title=Abhijit_Gosavi&amp;action=edit&amp;redlink=1" class="new" title="Abhijit Gosavi (page does not exist)">Gosavi, Abhijit</a> (2003). <i><a href="http://web.mst.edu/~gosavia/book.html" class="external text" title="http://web.mst.edu/~gosavia/book.html" rel="nofollow">Simulation-based Optimization: Parametric Optimization Techniques and Reinforcement Learning</a></i>. Boston, MA: Kluwer (Springer). <a href="/wiki/Special:BookSources/1402074549" class="internal">ISBN 1-4020-7454-9</a><span class="printonly">. <a href="http://web.mst.edu/~gosavia/book.html" class="external free" title="http://web.mst.edu/~gosavia/book.html" rel="nofollow">http://web.mst.edu/~gosavia/book.html</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Simulation-based+Optimization%3A+Parametric+Optimization+Techniques+and+Reinforcement+Learning&amp;rft.aulast=Gosavi&amp;rft.aufirst=Abhijit&amp;rft.au=Gosavi%2C+Abhijit&amp;rft.date=2003&amp;rft.place=Boston%2C+MA&amp;rft.pub=Kluwer+%28Springer%29&amp;rft.isbn=1-4020-7454-9&amp;rft_id=http%3A%2F%2Fweb.mst.edu%2F%7Egosavia%2Fbook.html&amp;rfr_id=info:sid/en.wikipedia.org:Reinforcement_learning"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><a href="/wiki/Ron_Sun" title="Ron Sun">Ron Sun</a>, E. Merrill, and T. Peterson, From implicit skills to explicit knowledge: A bottom-up model of skill learning. Cognitive Science, Vol.25, No.2, pp.203-244. 2001. <a href="http://www.cogsci.rpi.edu/~rsun/sun.cs01.pdf" class="external free" title="http://www.cogsci.rpi.edu/~rsun/sun.cs01.pdf" rel="nofollow">http://www.cogsci.rpi.edu/~rsun/sun.cs01.pdf</a></li>
</ul>
<ul>
<li><a href="/wiki/Ron_Sun" title="Ron Sun">Ron Sun</a>, P. Slusarz, and C. Terry, The interaction of the explicit and the implicit in skill learning: A dual-process approach . Psychological Review, Vol.112, No.1, pp.159-192. 2005. <a href="http://www.cogsci.rpi.edu/~rsun/sun-pr2005-f.pdf" class="external free" title="http://www.cogsci.rpi.edu/~rsun/sun-pr2005-f.pdf" rel="nofollow">http://www.cogsci.rpi.edu/~rsun/sun-pr2005-f.pdf</a></li>
</ul>
<ul>
<li><cite style="font-style:normal">Peters, Jan; <a href="/w/index.php?title=Sethu_Vijayakumar&amp;action=edit&amp;redlink=1" class="new" title="Sethu Vijayakumar (page does not exist)">Sethu Vijayakumar</a>; <a href="/w/index.php?title=Stefan_Schaal&amp;action=edit&amp;redlink=1" class="new" title="Stefan Schaal (page does not exist)">Stefan Schaal</a> (2003). "<a href="http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf" class="external text" title="http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf" rel="nofollow">Reinforcement Learning for Humanoid Robotics</a>". <i>IEEE-RAS International Conference on Humanoid Robots</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.btitle=IEEE-RAS+International+Conference+on+Humanoid+Robots&amp;rft.atitle=Reinforcement+Learning+for+Humanoid+Robotics&amp;rft.aulast=Peters&amp;rft.aufirst=Jan&amp;rft.date=2003&amp;rft_id=http%3A%2F%2Fwww-clmc.usc.edu%2Fpublications%2Fp%2Fpeters-ICHR2003.pdf"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="" id="CITEREFGray.5B.5BChris_R._Sims.5D.5D.3B_.5B.5BWai-Tat_Fu.5D.5D.3B_.5B.5BMichael_J._Schoelles.5D.5D2009"><a href="/wiki/Wayne_D._Gray" title="Wayne D. Gray">Gray, Wayne D.</a>; <a href="/w/index.php?title=Chris_R._Sims&amp;action=edit&amp;redlink=1" class="new" title="Chris R. Sims (page does not exist)">Chris R. Sims</a>; <a href="/w/index.php?title=Wai-Tat_Fu&amp;action=edit&amp;redlink=1" class="new" title="Wai-Tat Fu (page does not exist)">Wai-Tat Fu</a>; <a href="/w/index.php?title=Michael_J._Schoelles&amp;action=edit&amp;redlink=1" class="new" title="Michael J. Schoelles (page does not exist)">Michael J. Schoelles</a> (2006). "<a href="http://www.rpi.edu/~grayw/pubs/papers/GSFS06_PsycRvw/GSFS06_PsycRvw.htm" class="external text" title="http://www.rpi.edu/~grayw/pubs/papers/GSFS06_PsycRvw/GSFS06_PsycRvw.htm" rel="nofollow">The Soft Constraints Hypothesis: A Rational Analysis Approach to Resource Allocation for Interactive Behavior</a>" (<sup class="noprint Inline-Template"><span title="&#160;since June 2008" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Dead_external_links" title="Wikipedia:Dead external links">dead link</a></i>]</span></sup> – <sup><a href="http://scholar.google.co.uk/scholar?hl=en&amp;lr=&amp;q=author%3AGray+intitle%3AThe+Soft+Constraints+Hypothesis%3A+A+Rational+Analysis+Approach+to+Resource+Allocation+for+Interactive+Behavior&amp;as_publication=Psychological+Review&amp;as_ylo=&amp;as_yhi=&amp;btnG=Search" class="external text" title="http://scholar.google.co.uk/scholar?hl=en&amp;lr=&amp;q=author%3AGray+intitle%3AThe+Soft+Constraints+Hypothesis%3A+A+Rational+Analysis+Approach+to+Resource+Allocation+for+Interactive+Behavior&amp;as_publication=Psychological+Review&amp;as_ylo=&amp;as_yhi=&amp;btnG=Search" rel="nofollow">Scholar search</a></sup>). <i>Psychological Review</i> <b>113</b> (3): 461–482. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1037%2F0033-295X.113.3.461" class="external text" title="http://dx.doi.org/10.1037%2F0033-295X.113.3.461" rel="nofollow">10.1037/0033-295X.113.3.461</a></span><span class="printonly">. <a href="http://www.rpi.edu/~grayw/pubs/papers/GSFS06_PsycRvw/GSFS06_PsycRvw.htm" class="external free" title="http://www.rpi.edu/~grayw/pubs/papers/GSFS06_PsycRvw/GSFS06_PsycRvw.htm" rel="nofollow">http://www.rpi.edu/~grayw/pubs/papers/GSFS06_PsycRvw/GSFS06_PsycRvw.htm</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The+Soft+Constraints+Hypothesis%3A+A+Rational+Analysis+Approach+to+Resource+Allocation+for+Interactive+Behavior&amp;rft.jtitle=Psychological+Review&amp;rft.aulast=Gray&amp;rft.aufirst=Wayne+D.&amp;rft.au=Gray%2C+Wayne+D.&amp;rft.au=%5B%5BChris+R.+Sims%5D%5D%3B+%5B%5BWai-Tat+Fu%5D%5D%3B+%5B%5BMichael+J.+Schoelles%5D%5D&amp;rft.date=2006&amp;rft.volume=113&amp;rft.issue=3&amp;rft.pages=461%E2%80%93482&amp;rft_id=info:doi/10.1037%2F0033-295X.113.3.461&amp;rft_id=http%3A%2F%2Fwww.rpi.edu%2F%7Egrayw%2Fpubs%2Fpapers%2FGSFS06_PsycRvw%2FGSFS06_PsycRvw.htm&amp;rfr_id=info:sid/en.wikipedia.org:Reinforcement_learning"><span style="display: none;">&#160;</span></span></li>
</ul>
<ul>
<li><cite style="font-style:normal" class="" id="CITEREFFu.5B.5BJohn_R._Anderson.5D.5D2009"><a href="/w/index.php?title=Wai-Tat_Fu&amp;action=edit&amp;redlink=1" class="new" title="Wai-Tat Fu (page does not exist)">Fu, Wai-Tat</a>; <a href="/wiki/John_R._Anderson" title="John R. Anderson" class="mw-redirect">John R. Anderson</a> (2006). "<a href="http://www.humanfactors.uiuc.edu/Reports&amp;PapersPDFs/JournalPubs/Fu&amp;Anderson06-JEPG(published)(ReinforcementLearning).pdf" class="external text" title="http://www.humanfactors.uiuc.edu/Reports&amp;PapersPDFs/JournalPubs/Fu&amp;Anderson06-JEPG(published)(ReinforcementLearning).pdf" rel="nofollow">From Recurrent Choice to Skill Learning: A Reinforcement-Learning Model</a>". <i>Journal of Experimental Psychology: General</i> <b>135</b> (2): 184 –206. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1037%2F0096-3445.135.2.184" class="external text" title="http://dx.doi.org/10.1037%2F0096-3445.135.2.184" rel="nofollow">10.1037/0096-3445.135.2.184</a></span><span class="printonly">. <a href="http://www.humanfactors.uiuc.edu/Reports&amp;PapersPDFs/JournalPubs/Fu&amp;Anderson06-JEPG(published)(ReinforcementLearning).pdf" class="external free" title="http://www.humanfactors.uiuc.edu/Reports&amp;PapersPDFs/JournalPubs/Fu&amp;Anderson06-JEPG(published)(ReinforcementLearning).pdf" rel="nofollow">http://www.humanfactors.uiuc.edu/Reports&amp;PapersPDFs/JournalPubs/Fu&amp;Anderson06-JEPG(published)(ReinforcementLearning).pdf</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=From+Recurrent+Choice+to+Skill+Learning%3A+A+Reinforcement-Learning+Model&amp;rft.jtitle=Journal+of+Experimental+Psychology%3A+General&amp;rft.aulast=Fu&amp;rft.aufirst=Wai-Tat&amp;rft.au=Fu%2C+Wai-Tat&amp;rft.au=%5B%5BJohn+R.+Anderson%5D%5D&amp;rft.date=2006&amp;rft.volume=135&amp;rft.issue=2&amp;rft.pages=184+%E2%80%93206&amp;rft_id=info:doi/10.1037%2F0096-3445.135.2.184&amp;rft_id=http%3A%2F%2Fwww.humanfactors.uiuc.edu%2FReports%26PapersPDFs%2FJournalPubs%2FFu%26Anderson06-JEPG%28published%29%28ReinforcementLearning%29.pdf&amp;rfr_id=info:sid/en.wikipedia.org:Reinforcement_learning"><span style="display: none;">&#160;</span></span></li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=5" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://www-anw.cs.umass.edu/rlr/" class="external text" title="http://www-anw.cs.umass.edu/rlr/" rel="nofollow">Reinforcement Learning Repository</a></li>
<li><a href="http://rlai.cs.ualberta.ca/" class="external text" title="http://rlai.cs.ualberta.ca/" rel="nofollow">Reinforcement Learning and Artificial Intelligence</a> (Sutton's lab at the University of Alberta)</li>
<li><a href="http://www-all.cs.umass.edu/" class="external text" title="http://www-all.cs.umass.edu/" rel="nofollow">Autonomous Learning Laboratory</a> (Barto's lab at the University of Massachusetts Amherst)</li>
<li><a href="http://web.mst.edu/~gosavia/rl_website.html" class="external text" title="http://web.mst.edu/~gosavia/rl_website.html" rel="nofollow">Reinforcement Learning Online</a></li>
<li><a href="http://glue.rl-community.org" class="external text" title="http://glue.rl-community.org" rel="nofollow">RL-Glue</a></li>
<li><a href="http://www.dia.fi.upm.es/~jamartin/download.htm" class="external text" title="http://www.dia.fi.upm.es/~jamartin/download.htm" rel="nofollow">Software Tools for Reinforcement Learning (Matlab and Python)</a></li>
<li><a href="http://rlai.cs.ualberta.ca/RLR/index.html" class="external text" title="http://rlai.cs.ualberta.ca/RLR/index.html" rel="nofollow">The UofA Reinforcement Learning Library (texts)</a></li>
<li><a href="http://www.igi.tugraz.at/ril-toolbox" class="external text" title="http://www.igi.tugraz.at/ril-toolbox" rel="nofollow">The Reinforcement Learning Toolbox from the (Graz University of Technology)</a></li>
<li><a href="http://www.cogsci.rpi.edu/~rsun/hybrid-rl.html" class="external text" title="http://www.cogsci.rpi.edu/~rsun/hybrid-rl.html" rel="nofollow">Hybrid reinforcement learning</a></li>
<li><a href="http://sourceforge.net/projects/piqle/" class="external text" title="http://sourceforge.net/projects/piqle/" rel="nofollow">Piqle: a Generic Java Platform for Reinforcement Learning</a></li>
<li><a href="http://www.user.cifnet.com/~lwebzem/ttt/ttt.html" class="external text" title="http://www.user.cifnet.com/~lwebzem/ttt/ttt.html" rel="nofollow">Reinforcement Learning applied to Tic-Tac-Toe Game</a></li>
<li><a href="http://www.scholarpedia.org/article/Reinforcement_Learning" class="external text" title="http://www.scholarpedia.org/article/Reinforcement_Learning" rel="nofollow">Scholarpedia Reinforcement Learning</a></li>
<li><a href="http://www.scholarpedia.org/article/Temporal_difference_learning" class="external text" title="http://www.scholarpedia.org/article/Temporal_difference_learning" rel="nofollow">Scholarpedia Temporal Difference Learning</a></li>
<li><a href="http://www.rl-competition.org/" class="external text" title="http://www.rl-competition.org/" rel="nofollow">Annual Reinforcement Learning Competition</a></li>
</ul>


<!-- 
NewPP limit report
Preprocessor node count: 3783/1000000
Post-expand include size: 37712/2048000 bytes
Template argument size: 14062/2048000 bytes
Expensive parser function count: 2/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:66294-0!1!0!default!!en!2 and timestamp 20090403212145 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Reinforcement_learning">http://en.wikipedia.org/wiki/Reinforcement_learning</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Cleanup_from_September_2008" title="Category:Cleanup from September 2008">Cleanup from September 2008</a></span> | <span dir='ltr'><a href="/wiki/Category:All_pages_needing_cleanup" title="Category:All pages needing cleanup">All pages needing cleanup</a></span> | <span dir='ltr'><a href="/wiki/Category:All_articles_with_dead_external_links" title="Category:All articles with dead external links">All articles with dead external links</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_dead_external_links_since_June_2008" title="Category:Articles with dead external links since June 2008">Articles with dead external links since June 2008</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Reinforcement_learning" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Reinforcement_learning" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Reinforcement_learning&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Reinforcement_learning" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Reinforcement_learning" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Reinforcement_learning" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Reinforcement_learning&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Reinforcement_learning&amp;oldid=278960604" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Reinforcement_learning&amp;id=278960604">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Best%C3%A4rkendes_Lernen">Deutsch</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Apprentissage_par_renforcement">Français</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5">한국어</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Apprendimento_per_rinforzo">Italiano</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">日本語</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81_%D0%BF%D0%BE%D0%B4%D0%BA%D1%80%D0%B5%D0%BF%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC">Русский</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/Vahvistusoppiminen">Suomi</a></li>
				<li class="interwiki-vi"><a href="http://vi.wikipedia.org/wiki/H%E1%BB%8Dc_t%C4%83ng_c%C6%B0%E1%BB%9Dng">Tiếng Việt</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 22 March 2009, at 16:21.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv178 in 0.061 secs. --></body></html>
