<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Maximum likelihood,Articles with unsourced statements since March 2009,Statistics,Statistics,Abductive reasoning,Alternative hypothesis,Analysis of covariance,Analysis of variance,Arithmetic mean,Asymptotic,Bar chart" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Maximum_likelihood&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Maximum_likelihood&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Maximum likelihood - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Maximum_likelihood";
		var wgTitle = "Maximum likelihood";
		var wgAction = "view";
		var wgArticleId = "140806";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282072484;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Maximum_likelihood skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Maximum likelihood</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p><b>Maximum likelihood estimation</b> (<b>MLE</b>) is a popular <a href="/wiki/Statistics" title="Statistics">statistical</a> method used for fitting a mathematical model to data. The modeling of real world data using estimation by maximum <a href="/wiki/Likelihood_function" title="Likelihood function">likelihood</a> offers a way of tuning the free parameters of the model to provide a good fit.</p>
<p>The method was pioneered by <a href="/wiki/Geneticist" title="Geneticist">geneticist</a> and <a href="/wiki/Statistician" title="Statistician">statistician</a> <a href="/wiki/Ronald_Fisher" title="Ronald Fisher">Sir R. A. Fisher</a> between 1912 and 1922.</p>
<p>The method of maximum likelihood corresponds to many well-known estimation methods in statistics. For example, suppose you are interested in the heights of Americans. You have a sample of some number of Americans, but not the entire population, and record their heights. Further, you are willing to assume that heights are <a href="/wiki/Normal_distribution" title="Normal distribution">normally distributed</a> with some unknown <a href="/wiki/Mean" title="Mean">mean</a> and <a href="/wiki/Variance" title="Variance">variance</a>. The sample mean is then the maximum likelihood estimator of the population mean, and the sample variance is a close approximation to the maximum likelihood estimator of the population variance (see examples below).</p>
<p>For a fixed set of data and underlying probability model, maximum likelihood picks the values of the model parameters that make the data "more likely" than any other values of the parameters would make them. Maximum likelihood estimation gives a unique and easy way to determine solution in the case of the <a href="/wiki/Normal_distribution" title="Normal distribution">normal distribution</a> and many other problems, although in very complex problems this may not be the case. If a <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniform</a> <a href="/wiki/Prior_probability" title="Prior probability">prior distribution</a> is assumed over the parameters, the maximum likelihood estimate coincides with the <a href="/wiki/Maximum_a_posteriori" title="Maximum a posteriori" class="mw-redirect">most probable</a> values thereof.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Motivational_example"><span class="tocnumber">1</span> <span class="toctext">Motivational example</span></a></li>
<li class="toclevel-1"><a href="#Principles"><span class="tocnumber">2</span> <span class="toctext">Principles</span></a></li>
<li class="toclevel-1"><a href="#Properties"><span class="tocnumber">3</span> <span class="toctext">Properties</span></a>
<ul>
<li class="toclevel-2"><a href="#Functional_invariance"><span class="tocnumber">3.1</span> <span class="toctext">Functional invariance</span></a></li>
<li class="toclevel-2"><a href="#Bias"><span class="tocnumber">3.2</span> <span class="toctext">Bias</span></a>
<ul>
<li class="toclevel-3"><a href="#Example"><span class="tocnumber">3.2.1</span> <span class="toctext">Example</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a href="#Asymptotics"><span class="tocnumber">3.3</span> <span class="toctext">Asymptotics</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Examples"><span class="tocnumber">4</span> <span class="toctext">Examples</span></a>
<ul>
<li class="toclevel-2"><a href="#Discrete_distribution.2C_finite_parameter_space"><span class="tocnumber">4.1</span> <span class="toctext">Discrete distribution, finite parameter space</span></a></li>
<li class="toclevel-2"><a href="#Discrete_distribution.2C_continuous_parameter_space"><span class="tocnumber">4.2</span> <span class="toctext">Discrete distribution, continuous parameter space</span></a></li>
<li class="toclevel-2"><a href="#Continuous_distribution.2C_continuous_parameter_space"><span class="tocnumber">4.3</span> <span class="toctext">Continuous distribution, continuous parameter space</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Non-independent_variables"><span class="tocnumber">5</span> <span class="toctext">Non-independent variables</span></a></li>
<li class="toclevel-1"><a href="#Applications"><span class="tocnumber">6</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Motivational_example" id="Motivational_example"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=1" title="Edit section: Motivational example">edit</a>]</span> <span class="mw-headline">Motivational example</span></h2>
<p>As a motivational example, let <i>X</i> be a <a href="/wiki/Binomial_random_variable" title="Binomial random variable" class="mw-redirect">binomial random variable</a> with <i>n = 10</i> and an unknown parameter <i>p</i>. Further suppose that <i>x = 3</i>, a realization of the random variable <i>X</i>, is observed. As in all estimation problems, the goal is to estimate the unknown parameter <i>p</i>. The fundamental idea in maximum likelihood estimation is to calculate the probability of generating the observed value <i>x = 3</i>, that is, <i>P(X = 3)</i>, under the different possible values that <i>p</i> can take. By construction of the binomial distribution, <i>p</i> is a probability and therefore <img class="tex" alt=" p \in [0, 1] " src="http://upload.wikimedia.org/math/5/5/0/550abf67410399d394e58560a62f657a.png" />. In order to calculate these probabilities, we view the binomial distribution as a function of <i>p</i>, holding <i>x</i> fixed at <i>3</i> :</p>
<p><br /></p>
<dl>
<dd><img class="tex" alt=" L&#160;: [0, 1] \longmapsto \mathbf{R}, \; p \longmapsto \frac{10!}{3!(10 - 3)!} p^3 (1 - p)^{10 - 3} = 120 p^3 (1 - p)^7. " src="http://upload.wikimedia.org/math/2/9/4/294cfb4a48c07045d1b47e7117fdce0e.png" /></dd>
</dl>
<p>In this example, <i>L</i> is the likelihood function. Here is a plot a of <i>L</i>:</p>
<p><a href="/wiki/File:Likelihood.png" class="image" title="Image:Likelihood.png"><img alt="Image:Likelihood.png" src="http://upload.wikimedia.org/wikipedia/en/6/69/Likelihood.png" width="662" height="252" border="0" /></a></p>
<p>As can be seen on the graph, it turns out that the probability of generating <i>x = 3</i> is the largest when <i>p = 0.3</i>. Hence <img class="tex" alt=" \hat{p} = 0.3 " src="http://upload.wikimedia.org/math/f/d/1/fd13a128693a37082a649e7d4597bc16.png" /> will be the maximum likelihood estimate of <i>p</i>. Of course, in general, inference will not be based on a single observed value <i>x</i> but on a random sample <img class="tex" alt=" X_1, \ldots, X_n " src="http://upload.wikimedia.org/math/f/1/c/f1cd4a9e98ab57398032caf52ec46a5a.png" /> yielding observations <img class="tex" alt=" x_1, \ldots, x_n " src="http://upload.wikimedia.org/math/c/2/a/c2a79f82f9c9783d20426062c9feebbe.png" />. Supposing that the random variables <img class="tex" alt=" X_1, \ldots, X_n " src="http://upload.wikimedia.org/math/f/1/c/f1cd4a9e98ab57398032caf52ec46a5a.png" /> are independent and identically distributed, the likelihood function is a product of binomial densities, regarded as a function of <i>p</i>.</p>
<p><br /></p>
<p><a name="Principles" id="Principles"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=2" title="Edit section: Principles">edit</a>]</span> <span class="mw-headline">Principles</span></h2>
<p>Consider a family <span class="texhtml"><i>D</i><sub>θ</sub></span> of probability distributions parameterized by an unknown parameter <span class="texhtml">θ</span> (which could be vector-valued), associated with either a known <a href="/wiki/Probability_density_function" title="Probability density function">probability density function</a> (continuous distribution) or a known <a href="/wiki/Probability_mass_function" title="Probability mass function">probability mass function</a> (discrete distribution), denoted as <span class="texhtml"><i>f</i><sub>θ</sub></span>. We draw a sample <img class="tex" alt="x_1,x_2,\dots,x_n" src="http://upload.wikimedia.org/math/3/4/f/34feb8e4d8944bfb796e8813d614d3bd.png" /> of <i>n</i> values from this distribution, and then using <span class="texhtml"><i>f</i><sub>θ</sub></span> we compute the (multivariate) probability density associated with our observed data, <img class="tex" alt=" f_\theta(x_1,\dots,x_n).\,\!" src="http://upload.wikimedia.org/math/0/b/8/0b8c521e51fa702151ba7e598ea0bdeb.png" /></p>
<p>As a function of θ with <i>x</i><sub>1</sub>, ..., <i>x</i><sub><i>n</i></sub> fixed, this is the <a href="/wiki/Likelihood_function" title="Likelihood function">likelihood function</a></p>
<dl>
<dd><img class="tex" alt="\mathcal{L}(\theta) = f_{\theta}(x_1,\dots,x_n).\,\!" src="http://upload.wikimedia.org/math/2/e/1/2e105efe91c5353a7fcdc2a046ad2b44.png" /></dd>
</dl>
<p>The method of maximum likelihood estimates θ by finding the value of θ that maximizes <img class="tex" alt="\mathcal{L}(\theta)" src="http://upload.wikimedia.org/math/5/6/7/5674dcd62d411dd7492483c87302c194.png" />. This is the <b>maximum likelihood estimator</b> (<b>MLE</b>) of θ:</p>
<dl>
<dd><img class="tex" alt="\widehat{\theta} = \underset{\theta}{\operatorname{arg\,max}}\ \mathcal{L}(\theta)." src="http://upload.wikimedia.org/math/7/7/2/772241f261ef345c0bca71d4b404aa70.png" /></dd>
</dl>
<p>From a simple point of view, the outcome of a maximum likelihood analysis is the maximum likelihood estimate. This can be supplemented by an approximation for the <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> of the MLE, where this approximation is derived from the <a href="/wiki/Likelihood_function" title="Likelihood function">likelihood function</a>. A more complete outcome from a maximum likelihood analysis would be the likelihood function itself, which can be used to construct improved versions of <a href="/wiki/Confidence_interval" title="Confidence interval">confidence intervals</a> compared to those obtained from the approximate variance matrix. See also <a href="/wiki/Likelihood_Ratio_Test" title="Likelihood Ratio Test" class="mw-redirect">Likelihood Ratio Test</a>.</p>
<p>Commonly, one assumes that the data drawn from a particular distribution are <a href="/wiki/Independent,_identically_distributed" title="Independent, identically distributed" class="mw-redirect">independent, identically distributed</a> (iid) with unknown parameters. This considerably simplifies the problem because the likelihood can then be written as a product of <i>n</i> univariate probability densities:</p>
<dl>
<dd><img class="tex" alt="\mathcal{L}(\theta) = \prod_{i=1}^n f_{\theta}(x_i)" src="http://upload.wikimedia.org/math/5/2/b/52b515bcbd52a36f0b48b1c2414b8f5c.png" /></dd>
</dl>
<p>and since maxima are unaffected by monotone transformations, one can take the logarithm of this expression to turn it into a sum:</p>
<dl>
<dd><img class="tex" alt="\mathcal{L}^*(\theta) = \sum_{i=1}^n \log f_{\theta}(x_i)." src="http://upload.wikimedia.org/math/5/7/4/574834f62e26c411cea8d975f2fb133c.png" /></dd>
</dl>
<p>The maximum of this expression can then be found numerically using various <a href="/wiki/Optimization_(mathematics)" title="Optimization (mathematics)">optimization</a> algorithms.</p>
<p>This contrasts with seeking an <a href="/wiki/Unbiased_estimator" title="Unbiased estimator" class="mw-redirect">unbiased estimator</a> of θ, which may not necessarily yield the MLE but which will yield a value that (on average) will neither tend to over-estimate nor under-estimate the true value of θ.<sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since March 2009" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup></p>
<p>Note that the maximum likelihood estimator may not be unique, or indeed may not even exist.</p>
<p><a name="Properties" id="Properties"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=3" title="Edit section: Properties">edit</a>]</span> <span class="mw-headline">Properties</span></h2>
<p><a name="Functional_invariance" id="Functional_invariance"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=4" title="Edit section: Functional invariance">edit</a>]</span> <span class="mw-headline">Functional invariance</span></h3>
<p>The maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability (or probability density, in the continuous case). If the parameter consists of a number of components, then we define their separate maximum likelihood estimators, as the corresponding component of the MLE of the complete parameter. Consistent with this, if <img class="tex" alt="\widehat{\theta}" src="http://upload.wikimedia.org/math/b/2/c/b2ccb78a21af931bc1ca89bcf2f4ba99.png" /> is the MLE for <i>θ</i>, and if <i>g</i> is any function of <i>θ</i>, then the MLE for <i>α</i> = <i>g</i>(<i>θ</i>) is by definition</p>
<dl>
<dd><img class="tex" alt="\widehat{\alpha} = g(\widehat{\theta}).\,\!" src="http://upload.wikimedia.org/math/c/c/0/cc00f955147ee32737f438f75a490b2f.png" /></dd>
</dl>
<p>It maximizes the so-called profile likelihood:</p>
<dl>
<dd><img class="tex" alt="\bar{L}(\alpha) = \sup_{\theta: \alpha = g(\theta)} L(\theta)." src="http://upload.wikimedia.org/math/2/b/e/2be7c66f43fdee9e4fac5134715723b4.png" /></dd>
</dl>
<p><a name="Bias" id="Bias"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=5" title="Edit section: Bias">edit</a>]</span> <span class="mw-headline">Bias</span></h3>
<p>For small samples, the <a href="/wiki/Unbiased_estimator" title="Unbiased estimator" class="mw-redirect">bias</a> of maximum-likelihood estimators can be substantial.</p>
<p><a name="Example" id="Example"></a></p>
<h4><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=6" title="Edit section: Example">edit</a>]</span> <span class="mw-headline">Example</span></h4>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/Maximum_of_a_discrete_uniform_distribution" title="Maximum of a discrete uniform distribution" class="mw-redirect">Maximum of a discrete uniform distribution</a></div>
<p>Consider a case where <i>n</i> tickets numbered from 1 to <i>n</i> are placed in a box and one is selected at random (<i>see <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniform distribution</a></i>); thus, the sample size is 1). If <i>n</i> is unknown, then the maximum-likelihood estimator <img class="tex" alt="\hat{n}" src="http://upload.wikimedia.org/math/3/a/7/3a742201fc955603688b0715c556290d.png" /> of <i>n</i> is the number <i>m</i> on the drawn ticket. (The likelihood is 0 for <i>n</i>&#160;&lt;&#160;<i>m</i>, 1/<i>n</i> for <i>n</i> ≥ <i>m</i>, and this is greatest when <i>n</i>&#160;=&#160;<i>m</i>. Note that the maximum likelihood estimate of <i>n</i> occurs at the lower extreme of possible values {<i>m</i>,&#160;<i>m</i>&#160;+&#160;1,&#160;...}, rather than somewhere in the "middle" of the range of possible values, which would result in less bias.) The <a href="/wiki/Expected_value" title="Expected value">expected value</a> of the number <i>m</i> on the drawn ticket, and therefore the expected value of <img class="tex" alt="\hat{n}" src="http://upload.wikimedia.org/math/3/a/7/3a742201fc955603688b0715c556290d.png" /> , is (<i>n</i>&#160;+&#160;1)/2. As a result, the maximum likelihood estimator for <i>n</i> will systematically underestimate <i>n</i> by (<i>n</i>&#160;−&#160;1)/2 with a sample size of 1.</p>
<p><a name="Asymptotics" id="Asymptotics"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=7" title="Edit section: Asymptotics">edit</a>]</span> <span class="mw-headline">Asymptotics</span></h3>
<p>In many cases, estimation is performed using a set of <a href="/wiki/Independent_identically_distributed" title="Independent identically distributed" class="mw-redirect">independent identically distributed</a> measurements. These may correspond to distinct elements from a random <a href="/wiki/Sample_(statistics)" title="Sample (statistics)">sample</a>, repeated observations, etc. In such cases, it is of interest to determine the behavior of a given estimator as the number of measurements increases to infinity, referred to as <i><a href="/wiki/Asymptotic" title="Asymptotic" class="mw-redirect">asymptotic</a> behaviour</i>.</p>
<p>Under certain (fairly weak) regularity conditions, which are listed below, the MLE exhibits several characteristics which can be interpreted to mean that it is "asymptotically optimal". These characteristics include:</p>
<ul>
<li>The MLE is <b>asymptotically unbiased</b>, i.e., its <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator">bias</a> tends to zero as the sample size increases to infinity.</li>
<li>The MLE is <b>asymptotically <a href="/wiki/Efficiency_(statistics)" title="Efficiency (statistics)">efficient</a></b>, i.e., it achieves the <a href="/wiki/Cram%C3%A9r-Rao_lower_bound" title="Cramér-Rao lower bound" class="mw-redirect">Cramér-Rao lower bound</a> when the sample size tends to infinity. This means that no asymptotically unbiased estimator has lower asymptotic <a href="/wiki/Mean_squared_error" title="Mean squared error">mean squared error</a> than the MLE.</li>
<li>The MLE is <b><a href="/wiki/Estimator#Asymptotic_normality" title="Estimator">asymptotically normal</a></b>. As the sample size increases, the distribution of the MLE tends to the Gaussian distribution with mean <span class="texhtml">θ</span> and covariance matrix equal to the inverse of the <a href="/wiki/Fisher_information" title="Fisher information">Fisher information</a> matrix.</li>
</ul>
<p>Since the Cramér-Rao bound only speaks of unbiased estimators while the maximum likelihood estimator is usually biased, asymptotic efficiency as defined here does not mean anything: perhaps there are other nearly unbiased estimators with much smaller variance. However, it can be shown that among all regular estimators, which are estimators which have an asymptotic distribution which is not dramatically disturbed by small changes in the parameters, the asymptotic distribution of the maximum likelihood estimator is the best possible, i.e., most concentrated. <sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup></p>
<p>Some regularity conditions which ensure this behavior are:</p>
<ol>
<li>The first and second derivatives of the log-likelihood function must be defined.</li>
<li>The Fisher information matrix must not be zero, and must be continuous as a function of the parameter.</li>
<li>The maximum likelihood estimator is <a href="/wiki/Consistent_estimator" title="Consistent estimator">consistent</a>.</li>
</ol>
<p>By the mathematical meaning of the word asymptotic, asymptotic properties are properties which only approached in the limit of larger and larger samples: they are approximately true when the sample size is large enough. The theory does not tell us how large the sample needs to be in order to obtain a good enough degree of approximation. Fortunately, in practice they often appear to be approximately true, when the sample size is moderately large. So in practice, inference about the estimated parameters is often based on the asymptotic Gaussian distribution of the MLE. When we do this, the Fisher information matrix is usefully estimated by the <a href="/wiki/Observed_information_matrix" title="Observed information matrix" class="mw-redirect">observed information matrix</a>.</p>
<p>Some cases where the asymptotic behaviour described above does not hold are outlined next.</p>
<p><b>Estimate on boundary.</b> Sometimes the maximum likelihood estimate lies on the boundary of the set of possible parameters, or (if the boundary is not, strictly speaking, allowed) the likelihood gets larger and larger as the parameter approaches the boundary. Standard asymptotic theory needs the assumption that the true parameter value lies away from the boundary. If we have enough data, the maximum likelihood estimate will keep away from the boundary too. But with smaller samples, the estimate can lie on the boundary. In such cases, the asymptotic theory clearly does not give a practically useful approximation. Examples here would be variance-component models, where each component of variance, σ<sup>2</sup>, must satisfy the constraint σ<sup>2</sup> ≥0.</p>
<p><b>Data boundary parameter-dependent.</b> For the theory to apply in a simple way, the set of data values which has positive probability (or positive probability density) should not depend on the unknown parameter. A simple example where such parameter-dependence does hold is the case of estimating θ from a set of independent identically distributed when the common distribution is <a href="/wiki/Uniform_distribution" title="Uniform distribution">uniform</a> on the range (0,θ). For estimation purposes the relevant range of θ is such that θ cannot be less than the largest observation. In this instance the maximum likelihood estimate exists and has some good behaviour, but the asymptotics are not as outlined above.</p>
<p><b>Nuisance parameters.</b> For maximum likelihood estimations, a model may have a number of <a href="/wiki/Nuisance_parameter" title="Nuisance parameter">nuisance parameters</a>. For the asymptotic behaviour outlined to hold, the number of nuisance parameters should not increase with the number of observations (the sample size). A well-known example of this case is where observations occur as pairs, where the observations in each pair have a different (unknown) mean but otherwise the observations are independent and Normally distributed with a common variance. Here for 2<i>N</i> observations, there are <i>N</i>+1 parameters. It is well-known that the maximum likelihood estimate for the variance does not converge to the true value of the variance.</p>
<p><b>Increasing information.</b> For the asymptotics to hold in cases where the assumption of <a href="/wiki/Independent_identically_distributed" title="Independent identically distributed" class="mw-redirect">independent identically distributed</a> observations does not hold, a basic requirement is that the amount of information in the data increases indefinitely as the sample size increases. Such a requirement may not be met if either there is too much dependence in the data (for example, if new observations are essentially identical to existing observations), or if new independent observations are subject to an increasing observation error.</p>
<p><a name="Examples" id="Examples"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=8" title="Edit section: Examples">edit</a>]</span> <span class="mw-headline">Examples</span></h2>
<p><a name="Discrete_distribution.2C_finite_parameter_space" id="Discrete_distribution.2C_finite_parameter_space"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=9" title="Edit section: Discrete distribution, finite parameter space">edit</a>]</span> <span class="mw-headline">Discrete distribution, finite parameter space</span></h3>
<p>Consider tossing an <a href="/wiki/Unfair_coin" title="Unfair coin" class="mw-redirect">unfair coin</a> 80 times (i.e., we sample something like <i>x</i><sub>1</sub>=H, <i>x</i><sub>2</sub>=T, ..., <i>x</i><sub>80</sub>=T, and count the number of HEADS "H" observed). Call the probability of tossing a HEAD <i>p</i>, and the probability of tossing TAILS 1-<i>p</i> (so here <i>p</i> is <i>θ</i> above). Suppose we toss 49 HEADS and 31 TAILS, and suppose the coin was taken from a box containing three coins: one which gives HEADS with probability <i>p</i>=1/3, one which gives HEADS with probability <i>p</i>=1/2 and another which gives HEADS with probability <i>p</i>=2/3. The coins have lost their labels, so we don't know which one it was. Using <b>maximum likelihood estimation</b> we can calculate which coin has the largest likelihood, given the data that we observed. The likelihood function (defined below) takes one of three values:</p>
<dl>
<dd><img class="tex" alt="
\begin{matrix}
\Pr(\mathrm{H} = 49 \mid p=1/3) &amp; = &amp; \binom{80}{49}(1/3)^{49}(1-1/3)^{31} \approx 0.000 \\
&amp;&amp;\\
\Pr(\mathrm{H} = 49 \mid p=1/2) &amp; = &amp; \binom{80}{49}(1/2)^{49}(1-1/2)^{31} \approx 0.012 \\
&amp;&amp;\\
\Pr(\mathrm{H} = 49 \mid p=2/3) &amp; = &amp; \binom{80}{49}(2/3)^{49}(1-2/3)^{31} \approx 0.054 \\
\end{matrix}
" src="http://upload.wikimedia.org/math/0/f/1/0f1552f3348586ec7144cccd92c06be4.png" /></dd>
</dl>
<p>We see that the likelihood is maximized when <i>p</i>=2/3, and so this is our <i>maximum likelihood estimate</i> for <i>p</i>.</p>
<p><a name="Discrete_distribution.2C_continuous_parameter_space" id="Discrete_distribution.2C_continuous_parameter_space"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=10" title="Edit section: Discrete distribution, continuous parameter space">edit</a>]</span> <span class="mw-headline">Discrete distribution, continuous parameter space</span></h3>
<p>Now suppose we had only one coin but its <i>p</i> could have been any value 0 ≤ <i>p</i> ≤ 1. We must maximize the likelihood function:</p>
<dl>
<dd><img class="tex" alt="
L(\theta) = f_D(\mathrm{H} = 49 \mid p) = \binom{80}{49} p^{49}(1-p)^{31}
" src="http://upload.wikimedia.org/math/c/f/1/cf1421739067714f76dbdacb4838d670.png" /></dd>
</dl>
<p>over all possible values 0 ≤ <i>p</i> ≤ 1.</p>
<p>One way to maximize this function is by <a href="/wiki/Derivative" title="Derivative">differentiating</a> with respect to <i>p</i> and setting to zero:</p>
<dl>
<dd><img class="tex" alt="
\begin{align}
{0}&amp;{} = \frac{\partial}{\partial p} \left( \binom{80}{49} p^{49}(1-p)^{31} \right) \\
  &amp; {}\propto 49p^{48}(1-p)^{31} - 31p^{49}(1-p)^{30} \\
  &amp; {}= p^{48}(1-p)^{30}\left[ 49(1-p) - 31p \right]  \\
  &amp; {}= p^{48}(1-p)^{30}\left[ 49 - 80p \right]
\end{align}
" src="http://upload.wikimedia.org/math/3/2/7/327ff4afbca81a71505cbcedad0f0173.png" /></dd>
</dl>
<div class="thumb tright">
<div class="thumbinner" style="width:202px;"><a href="/wiki/File:BinominalLikelihoodGraph.png" class="image" title="Likelihood of different proportion parameter values for a binomial process with t = 3 and n = 10; the ML estimator occurs at the mode with the peak (maximum) of the curve."><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/6/66/BinominalLikelihoodGraph.png/200px-BinominalLikelihoodGraph.png" width="200" height="164" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:BinominalLikelihoodGraph.png" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
Likelihood of different proportion parameter values for a binomial process with <i>t</i> = 3 and <i>n</i> = 10; the ML estimator occurs at the <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">mode</a> with the peak (maximum) of the curve.</div>
</div>
</div>
<p>which has solutions <i>p</i>=0, <i>p</i>=1, and <i>p</i>=49/80. The solution which maximizes the likelihood is clearly <i>p</i>=49/80 (since <i>p</i>=0 and <i>p</i>=1 result in a likelihood of zero). Thus we say the <i>maximum likelihood estimator</i> for <i>p</i> is 49/80.</p>
<p>This result is easily generalized by substituting a letter such as <i>t</i> in the place of 49 to represent the observed number of 'successes' of our <a href="/wiki/Bernoulli_trial" title="Bernoulli trial">Bernoulli trials</a>, and a letter such as <i>n</i> in the place of 80 to represent the number of Bernoulli trials. Exactly the same calculation yields the <i>maximum likelihood estimator</i> <i>t</i>&#160;/&#160;<i>n</i> for any sequence of <i>n</i> Bernoulli trials resulting in <i>t</i> 'successes'.</p>
<p><a name="Continuous_distribution.2C_continuous_parameter_space" id="Continuous_distribution.2C_continuous_parameter_space"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=11" title="Edit section: Continuous distribution, continuous parameter space">edit</a>]</span> <span class="mw-headline">Continuous distribution, continuous parameter space</span></h3>
<p>For the <a href="/wiki/Normal_distribution" title="Normal distribution">normal distribution</a> <img class="tex" alt="\mathcal{N}(\mu, \sigma^2)" src="http://upload.wikimedia.org/math/3/4/1/3416773987123369f5ad7baf7cba8bdc.png" /> which has <a href="/wiki/Probability_density_function" title="Probability density function">probability density function</a></p>
<dl>
<dd><img class="tex" alt="f(x\mid \mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\ \sigma\ } 
                               \exp{\left(-\frac {(x-\mu)^2}{2\sigma^2} \right)}, " src="http://upload.wikimedia.org/math/0/4/9/049ca770d3067fca4c2d35a7737d88b2.png" /></dd>
</dl>
<p>the corresponding <a href="/wiki/Probability_density_function" title="Probability density function">probability density function</a> for a sample of <i>n</i> <a href="/wiki/Independent_identically_distributed" title="Independent identically distributed" class="mw-redirect">independent identically distributed</a> normal random variables (the likelihood) is</p>
<dl>
<dd><img class="tex" alt="f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \prod_{i=1}^{n} f( x_{i}\mid  \mu, \sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left( -\frac{ \sum_{i=1}^{n}(x_i-\mu)^2}{2\sigma^2}\right)," src="http://upload.wikimedia.org/math/d/4/8/d48eee48e9c5b07962fcd0844edd8db8.png" /></dd>
</dl>
<p>or more conveniently:</p>
<dl>
<dd><img class="tex" alt="f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right)" src="http://upload.wikimedia.org/math/8/4/5/84569000029784d31ffa7667a9450574.png" />,</dd>
</dl>
<p>where <img class="tex" alt=" \bar{x} " src="http://upload.wikimedia.org/math/8/4/7/84790e2b15a305120bc3fbeb4a4eeb4f.png" /> is the <a href="/wiki/Sample_mean" title="Sample mean" class="mw-redirect">sample mean</a>.</p>
<p>This family of distributions has two parameters: <i>θ</i>=(<i>μ</i>,<i>σ</i>), so we maximize the likelihood, <img class="tex" alt="\mathcal{L} (\mu,\sigma) = f(x_1,\ldots,x_n \mid \mu, \sigma)" src="http://upload.wikimedia.org/math/a/7/0/a70f5a8ff97f2f6c9a797adca7d72c4b.png" />, over both parameters simultaneously, or if possible, individually.</p>
<p>Since the <a href="/wiki/Natural_logarithm" title="Natural logarithm">logarithm</a> is a <a href="/wiki/Continuous_function" title="Continuous function">continuous</a> <a href="/wiki/Strictly_increasing" title="Strictly increasing" class="mw-redirect">strictly increasing</a> function over the <a href="/wiki/Range_(mathematics)" title="Range (mathematics)">range</a> of the likelihood, the values which maximize the likelihood will also maximize its logarithm. Since maximizing the logarithm often requires simpler algebra, it is the logarithm which is maximized below. (Note: the log-likelihood is closely related to <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">information entropy</a> and <a href="/wiki/Fisher_information" title="Fisher information">Fisher information</a>.)</p>
<dl>
<dd><img class="tex" alt="
0 = \frac{\partial}{\partial \mu} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right) \right) " src="http://upload.wikimedia.org/math/1/5/1/151fec9f8ccbd22bb4a947067d88ba6c.png" /></dd>
</dl>
<p><br /></p>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" = \frac{\partial}{\partial \mu} \left( \log\left( \frac{1}{2\pi\sigma^2} \right)^{n/2} - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right)" src="http://upload.wikimedia.org/math/7/0/d/70d1c50ea184c28320500f377d6c96f4.png" /></dd>
</dl>
</dd>
</dl>
<p><br /></p>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" = 0 - \frac{-2n(\bar{x}-\mu)}{2\sigma^2} " src="http://upload.wikimedia.org/math/8/8/0/880b1eb33f7bc1c959dde6d44d84ab96.png" /></dd>
</dl>
</dd>
</dl>
<p>which is solved by</p>
<dl>
<dd><img class="tex" alt="\hat\mu = \bar{x} = \sum^{n}_{i=1}x_i/n " src="http://upload.wikimedia.org/math/7/f/3/7f32df490e6dba5af3daaba7394fc0be.png" />.</dd>
</dl>
<p>This is indeed the maximum of the function since it is the only turning point in μ and the second derivative is strictly less than zero. Its <a href="/wiki/Expectation_value" title="Expectation value" class="mw-redirect">expectation value</a> is equal to the parameter μ of the given distribution,</p>
<dl>
<dd><img class="tex" alt=" E \left[ \widehat\mu \right] = \mu," src="http://upload.wikimedia.org/math/f/a/4/fa43fb90273a7e483bd044c13fab190c.png" /></dd>
</dl>
<p>which means that the maximum-likelihood estimator <img class="tex" alt="\widehat\mu" src="http://upload.wikimedia.org/math/6/2/9/62975045639097b587af9770d1645d1a.png" /> is unbiased.</p>
<p>Similarly we differentiate the log likelihood with respect to σ and equate to zero:</p>
<dl>
<dd><img class="tex" alt=" 0 = \frac{\partial}{\partial \sigma} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right) \right) " src="http://upload.wikimedia.org/math/8/f/3/8f334b11b9d76324a34ff2625e77e18a.png" /></dd>
</dl>
<p><br /></p>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" = \frac{\partial}{\partial \sigma} \left( \frac{n}{2}\log\left( \frac{1}{2\pi\sigma^2} \right) - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right) " src="http://upload.wikimedia.org/math/c/e/1/ce1482a7caaea9a7bff7dfc8af06c293.png" /></dd>
</dl>
</dd>
</dl>
<p><br /></p>
<dl>
<dd>
<dl>
<dd><img class="tex" alt=" = -\frac{n}{\sigma} + \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{\sigma^3} " src="http://upload.wikimedia.org/math/d/d/e/dde5733ccd74688a8458740cc81c3bbc.png" /></dd>
</dl>
</dd>
</dl>
<p>which is solved by</p>
<dl>
<dd><img class="tex" alt="\widehat\sigma^2 = \sum_{i=1}^n(x_i-\widehat{\mu})^2/n" src="http://upload.wikimedia.org/math/3/9/1/391e29df2cf0e9c70ad2a3442d7e5f20.png" />.</dd>
</dl>
<p>Inserting <img class="tex" alt="\widehat\mu" src="http://upload.wikimedia.org/math/6/2/9/62975045639097b587af9770d1645d1a.png" /> we obtain</p>
<dl>
<dd><img class="tex" alt="\widehat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - \bar{x})^2 = \frac{1}{n}\sum_{i=1}^n x_i^2
                          -\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n x_i x_j" src="http://upload.wikimedia.org/math/b/1/5/b15a1d96167f42c16f856067c36a3867.png" />.</dd>
</dl>
<p>To calculate its expected value, it is convenient to rewrite the expression in terms of zero-mean random variables (<a href="/wiki/Statistical_error" title="Statistical error" class="mw-redirect">statistical error</a>) <img class="tex" alt="\delta_i \equiv \mu - x_i" src="http://upload.wikimedia.org/math/d/3/0/d30ae7b47bfd4b4bfbd1ec60bc15cbbf.png" />. Expressing the estimate in these variables yields</p>
<dl>
<dd><img class="tex" alt="\widehat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (\mu - \delta_i)^2 -\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n (\mu - \delta_i)(\mu - \delta_j)" src="http://upload.wikimedia.org/math/e/f/7/ef78fe6423a30c2e0aed43303be60f36.png" />.</dd>
</dl>
<p>Simplifying the expression above, utilizing the facts that <img class="tex" alt="E\left[\delta_i\right] = 0 " src="http://upload.wikimedia.org/math/1/e/8/1e8fa4403ce5626dcd6288f3b46b9e79.png" /> and <img class="tex" alt=" E[\delta_i^2] = \sigma^2 " src="http://upload.wikimedia.org/math/7/0/a/70a488218f5c06e05db0900b82dce630.png" />, allows us to obtain</p>
<dl>
<dd><img class="tex" alt="E \left[ \widehat{\sigma^2}  \right]= \frac{n-1}{n}\sigma^2" src="http://upload.wikimedia.org/math/f/1/9/f195158a4533fd75a7a7a150813cfc16.png" />.</dd>
</dl>
<p>This means that the estimator <img class="tex" alt="\widehat\sigma" src="http://upload.wikimedia.org/math/e/7/4/e74bae2b7fb5b99fc14cdcbd4a081f07.png" /> is biased (However, <img class="tex" alt="\widehat\sigma" src="http://upload.wikimedia.org/math/e/7/4/e74bae2b7fb5b99fc14cdcbd4a081f07.png" /> is consistent).</p>
<p>Formally we say that the <i>maximum likelihood estimator</i> for <span class="texhtml">θ = (μ,σ<sup>2</sup>)</span> is:</p>
<dl>
<dd><img class="tex" alt="\widehat{\theta} = \left(\widehat{\mu},\widehat{\sigma}^2\right)." src="http://upload.wikimedia.org/math/b/d/5/bd5b2b3ed33629c6a18d53b92baf36fd.png" /></dd>
</dl>
<p>In this case the MLEs could be obtained individually. In general this may not be the case, and the MLEs would have to be obtained simultaneously.</p>
<p><a name="Non-independent_variables" id="Non-independent_variables"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=12" title="Edit section: Non-independent variables">edit</a>]</span> <span class="mw-headline">Non-independent variables</span></h2>
<p>It may be the case that variables are correlated, in which case they are not independent. Two random variables <i>X</i> and <i>Y</i> are independent only if their joint probability density function is the product of the individual probability density functions, i.e.</p>
<dl>
<dd><img class="tex" alt="f(x,y)=f(x)f(y)\," src="http://upload.wikimedia.org/math/a/8/b/a8b51e82ea80740efb48b6d481d6d79a.png" /></dd>
</dl>
<p>Suppose one constructs an order-<i>n</i> Gaussian vector out of random variables <img class="tex" alt="(x_1,\ldots,x_n)\," src="http://upload.wikimedia.org/math/5/7/6/5762f2766c04b4ace101c7b53dd4adaa.png" />, where each variable has means given by <img class="tex" alt="(\mu_1, \ldots, \mu_n)\," src="http://upload.wikimedia.org/math/1/c/f/1cfc9984f93cf3ca4aae8263f1394491.png" />. Furthermore, let the <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> be denoted by <span class="texhtml">Σ,</span></p>
<p>The joint probability density function of these <i>n</i> random variables is then given by:</p>
<dl>
<dd><img class="tex" alt="f(x_1,\ldots,x_n)=\frac{1}{(2\pi)^{n/2}\sqrt{\text{det}(\Sigma)}} \exp\left( -\frac{1}{2} \left[x_1-\mu_1,\ldots,x_n-\mu_n\right]\Sigma^{-1}     \left[x_1-\mu_1,\ldots,x_n-\mu_n\right]^T \right)" src="http://upload.wikimedia.org/math/e/2/8/e28f5f95dcc04dff99de4ac2c719c596.png" /></dd>
</dl>
<p>In the two variable case, the joint probability density function is given by:</p>
<dl>
<dd><img class="tex" alt=" f(x,y) = \frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp\left[ -\frac{1}{2(1-\rho^2)} \left(\frac{(x-\mu_x)^2}{\sigma_x^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y} + \frac{(y-\mu_y)^2}{\sigma_y^2}\right) \right] " src="http://upload.wikimedia.org/math/f/d/e/fde247acd0fcb4158054abc305e5e2aa.png" /></dd>
</dl>
<p>In this and other cases where a joint density function exists, the likelihood function is defined as above, under <i>Principles</i>, using this density.</p>
<p><a name="Applications" id="Applications"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=13" title="Edit section: Applications">edit</a>]</span> <span class="mw-headline">Applications</span></h2>
<p>Maximum likelihood estimation is used for a wide range of statistical models, including:</p>
<dl>
<dd>
<ul>
<li><a href="/wiki/Linear_model" title="Linear model">linear models</a> and <a href="/wiki/Generalized_linear_model" title="Generalized linear model">generalized linear models</a>;</li>
<li><a href="/wiki/Factor_analysis" title="Factor analysis">exploratory</a> and <a href="/wiki/Confirmatory_factor_analysis" title="Confirmatory factor analysis">confirmatory factor analysis</a>;</li>
<li><a href="/wiki/Structural_equation_modeling" title="Structural equation modeling">structural equation modeling</a>;</li>
<li>many situations in the context of <a href="/wiki/Hypothesis_testing" title="Hypothesis testing" class="mw-redirect">hypothesis testing</a> and <a href="/wiki/Confidence_interval" title="Confidence interval">confidence interval</a> formation.</li>
</ul>
</dd>
</dl>
<p>These uses arise across applications in widespead set of fields, including:</p>
<dl>
<dd>
<ul>
<li><a href="/wiki/Communication_systems" title="Communication systems" class="mw-redirect">communication systems</a>;</li>
<li><a href="/wiki/Psychometrics" title="Psychometrics">psychometrics</a> and <a href="/wiki/Econometrics" title="Econometrics">econometrics</a>;</li>
<li>time-delay of arrival (TDOA) in acoustic or electromagnetic detection;</li>
<li>data modeling in nuclear and particle physics;</li>
<li><a href="/wiki/Computational_phylogenetics" title="Computational phylogenetics">computational phylogenetics</a>;</li>
<li>origin/destination and path-choice modeling in transport networks.</li>
</ul>
</dd>
</dl>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=14" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<div class="noprint tright portal" style="border:solid #aaa 1px;margin:0.5em 0 0.5em 0.5em;">
<table style="background:#f9f9f9; font-size:85%; line-height:110%;">
<tr>
<td><a href="/wiki/File:Fisher_iris_versicolor_sepalwidth.svg" class="image" title="Fisher iris versicolor sepalwidth.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/42px-Fisher_iris_versicolor_sepalwidth.svg.png" width="42" height="28" border="0" /></a></td>
<td style="padding:0 0.2em;"><i><b><a href="/wiki/Portal:Statistics" title="Portal:Statistics">Statistics portal</a></b></i></td>
</tr>
</table>
</div>
<ul>
<li><a href="/wiki/Abductive_reasoning" title="Abductive reasoning">Abductive reasoning</a>, a logical technique corresponding to maximum likelihood.</li>
<li><a href="/wiki/Censoring_(statistics)" title="Censoring (statistics)">Censoring (statistics)</a></li>
<li><a href="/wiki/Delta_method" title="Delta method">Delta method</a>, a method for finding the distribution of functions of a maximum likelihood estimator.</li>
<li><a href="/wiki/Generalized_method_of_moments" title="Generalized method of moments">Generalized method of moments</a>, a method related to maximum likelihood estimation.</li>
<li><a href="/wiki/Inferential_statistics" title="Inferential statistics" class="mw-redirect">Inferential statistics</a>, for an alternative to the maximum likelihood estimate.</li>
<li><a href="/wiki/Likelihood_function" title="Likelihood function">Likelihood function</a>, a description on what likelihood functions are.</li>
<li><a href="/wiki/Maximum_a_posteriori" title="Maximum a posteriori" class="mw-redirect">Maximum a posteriori (MAP) estimator</a>, for a contrast in the way to calculate estimators when prior knowledge is postulated.</li>
<li><a href="/wiki/Maximum_spacing_estimation" title="Maximum spacing estimation">Maximum spacing estimation</a>, a related method that is more robust in many situations.</li>
<li><a href="/wiki/Mean_squared_error" title="Mean squared error">Mean squared error</a>, a measure of how 'good' an estimator of a distributional parameter is (be it the maximum likelihood estimator or some other estimator).</li>
<li><a href="/wiki/Method_of_moments_(statistics)" title="Method of moments (statistics)">Method of moments (statistics)</a>, for another popular method for finding parameters of distributions.</li>
<li><a href="/wiki/Method_of_support" title="Method of support">Method of support</a>, a variation of the maximum likelihood technique.</li>
<li><a href="/wiki/Minimum_distance_estimation" title="Minimum distance estimation">Minimum distance estimation</a></li>
<li><a href="/wiki/Quasi-maximum_likelihood" title="Quasi-maximum likelihood">Quasi-maximum likelihood</a> estimator, a MLE estimator that is misspecified, but still consistent.</li>
<li>The <a href="/wiki/Rao%E2%80%93Blackwell_theorem" title="Rao–Blackwell theorem">Rao–Blackwell theorem</a>, a result which yields a process for finding the best possible unbiased estimator (in the sense of having minimal <a href="/wiki/Mean_squared_error" title="Mean squared error">mean squared error</a>). The MLE is often a good starting place for the process.</li>
<li><a href="/wiki/Sufficient_statistic" title="Sufficient statistic" class="mw-redirect">Sufficient statistic</a>, a function of the data through which the MLE (if it exists and is unique) will depend on the data.</li>
</ul>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=15" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> <a href="http://www.cambridge.org/catalogue/catalogue.asp?isbn=0521784506" class="external text" title="http://www.cambridge.org/catalogue/catalogue.asp?isbn=0521784506" rel="nofollow">A.W. van der Vaart, Asymptotic Statistics (Cambridge Series in Statistical and Probabilistic Mathematics) (1998)</a></li>
</ol>
<ul>
<li><cite style="font-style:normal" class="book" id="CITEREFKay1993">Kay, Steven M. (1993). <i>Fundamentals of Statistical Signal Processing: Estimation Theory</i>. Prentice Hall. pp.&#160;Ch. 7. <a href="/wiki/Special:BookSources/0133457117" class="internal">ISBN 0-13-345711-7</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Fundamentals+of+Statistical+Signal+Processing%3A+Estimation+Theory&amp;rft.aulast=Kay&amp;rft.aufirst=Steven+M.&amp;rft.au=Kay%2C+Steven+M.&amp;rft.date=1993&amp;rft.pages=pp.%26nbsp%3BCh.+7&amp;rft.pub=Prentice+Hall&amp;rft.isbn=0-13-345711-7&amp;rfr_id=info:sid/en.wikipedia.org:Maximum_likelihood"><span style="display: none;">&#160;</span></span></li>
<li><cite style="font-style:normal" class="book" id="CITEREFLehmannCasella.2C_G.1998">Lehmann, E. L.; Casella, G. (1998). <i>Theory of Point Estimation</i>. Springer. pp.&#160;2nd ed. <a href="/wiki/Special:BookSources/0387985026" class="internal">ISBN 0-387-98502-6</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Theory+of+Point+Estimation&amp;rft.aulast=Lehmann&amp;rft.aufirst=E.+L.&amp;rft.au=Lehmann%2C+E.+L.&amp;rft.au=Casella%2C+G.&amp;rft.date=1998&amp;rft.pages=pp.%26nbsp%3B2nd+ed&amp;rft.pub=Springer&amp;rft.isbn=0-387-98502-6&amp;rfr_id=info:sid/en.wikipedia.org:Maximum_likelihood"><span style="display: none;">&#160;</span></span></li>
<li>A paper on the history of Maximum Likelihood: <cite style="font-style:normal" class="" id="CITEREFAldrich1997">Aldrich, John (1997). "<a href="http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906" class="external text" title="http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906" rel="nofollow">R.A. Fisher and the making of maximum likelihood 1912-1922</a>". <i>Statistical Science</i> <b>12</b> (3): 162–176. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<span class="neverexpand"><a href="http://dx.doi.org/10.1214%2Fss%2F1030037906" class="external text" title="http://dx.doi.org/10.1214%2Fss%2F1030037906" rel="nofollow">10.1214/ss/1030037906</a></span><span class="printonly">. <a href="http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906" class="external free" title="http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906" rel="nofollow">http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=R.A.+Fisher+and+the+making+of+maximum+likelihood+1912-1922&amp;rft.jtitle=Statistical+Science&amp;rft.aulast=Aldrich&amp;rft.aufirst=John&amp;rft.au=Aldrich%2C+John&amp;rft.date=1997&amp;rft.volume=12&amp;rft.issue=3&amp;rft.pages=162%E2%80%93176&amp;rft_id=info:doi/10.1214%2Fss%2F1030037906&amp;rft_id=http%3A%2F%2Fprojecteuclid.org%2FDienst%2FUI%2F1.0%2FSummarize%2Feuclid.ss%2F1030037906&amp;rfr_id=info:sid/en.wikipedia.org:Maximum_likelihood"><span style="display: none;">&#160;</span></span></li>
<li>M. I. Ribeiro, <a href="http://users.isr.ist.utl.pt/~mir/pub/probability.pdf" class="external text" title="http://users.isr.ist.utl.pt/~mir/pub/probability.pdf" rel="nofollow">Gaussian Probability Density Functions: Properties and Error Characterization</a> (Accessed 19 March 2008)</li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Maximum_likelihood&amp;action=edit&amp;section=16" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://statgen.iop.kcl.ac.uk/bgim/mle/sslike_1.html" class="external text" title="http://statgen.iop.kcl.ac.uk/bgim/mle/sslike_1.html" rel="nofollow">Maximum Likelihood Estimation Primer (an excellent tutorial)</a></li>
<li><a href="http://www.mayin.org/ajayshah/KB/R/documents/mle/mle.html" class="external text" title="http://www.mayin.org/ajayshah/KB/R/documents/mle/mle.html" rel="nofollow">Implementing MLE for your own likelihood function using R</a></li>
</ul>
<table class="navbox" cellspacing="0" style=";">
<tr>
<td style="padding:2px;">
<table cellspacing="0" class="nowraplinks collapsible autocollapse" style="width:100%;background:transparent;color:inherit;;">
<tr>
<th style=";" colspan="2" class="navbox-title">
<div style="float:left; width:6em;text-align:left;">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;;;border:none;; font-size:xx-small;"><a href="/wiki/Template:Statistics" title="Template:Statistics"><span title="View this template" style=";;border:none;">v</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="/wiki/Template_talk:Statistics" title="Template talk:Statistics"><span title="Discussion about this template" style=";;border:none;">d</span></a>&#160;<span style="font-size:80%;">•</span>&#160;<a href="http://en.wikipedia.org/w/index.php?title=Template:Statistics&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Template:Statistics&amp;action=edit" rel="nofollow"><span title="Edit this template" style=";;border:none;;">e</span></a></div>
</div>
<span style="font-size:110%;"><a href="/wiki/Statistics" title="Statistics">Statistics</a></span></th>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Design_of_experiments" title="Design of experiments">Design of experiments</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Statistical_population" title="Statistical population">Population</a>&#160;• <a href="/wiki/Sampling_(statistics)" title="Sampling (statistics)">Sampling</a>&#160;• <a href="/wiki/Stratified_sampling" title="Stratified sampling">Stratified sampling</a>&#160;• <a href="/wiki/Replication_(statistics)" title="Replication (statistics)">Replication</a>&#160;• <a href="/wiki/Blocking_(statistics)" title="Blocking (statistics)">Blocking</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Sample_size" title="Sample size">Sample size estimation</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Null_hypothesis" title="Null hypothesis">Null hypothesis</a>&#160;• <a href="/wiki/Alternative_hypothesis" title="Alternative hypothesis">Alternative hypothesis</a>&#160;• <a href="/wiki/Type_I_and_Type_II_errors" title="Type I and Type II errors" class="mw-redirect">Type I and Type II errors</a>&#160;• <a href="/wiki/Statistical_power" title="Statistical power">Statistical power</a>&#160;• <a href="/wiki/Effect_size" title="Effect size">Effect size</a>&#160;• <a href="/wiki/Standard_error_(statistics)" title="Standard error (statistics)">Standard error</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Descriptive_statistics" title="Descriptive statistics">Descriptive statistics</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Continuous_probability_distribution" title="Continuous probability distribution">Continuous data</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Location_parameter" title="Location parameter">Location</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Mean" title="Mean">Mean</a> (<a href="/wiki/Arithmetic_mean" title="Arithmetic mean">Arithmetic</a>, <a href="/wiki/Geometric_mean" title="Geometric mean">Geometric</a>, <a href="/wiki/Harmonic_mean" title="Harmonic mean">Harmonic</a>)&#160;• <a href="/wiki/Median" title="Median">Median</a>&#160;• <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">Mode</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Statistical_dispersion" title="Statistical dispersion">Dispersion</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Range_(statistics)" title="Range (statistics)">Range</a>&#160;• <a href="/wiki/Standard_deviation" title="Standard deviation">Standard deviation</a>&#160;• <a href="/wiki/Coefficient_of_variation" title="Coefficient of variation">Coefficient of variation</a>&#160;• <a href="/wiki/Percentile" title="Percentile">Percentile</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Moment_(mathematics)" title="Moment (mathematics)">Moments</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Variance" title="Variance">Variance</a>&#160;• <a href="/wiki/Semivariance" title="Semivariance">Semivariance</a>&#160;• <a href="/wiki/Skewness" title="Skewness">Skewness</a>&#160;• <a href="/wiki/Kurtosis" title="Kurtosis">Kurtosis</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Discrete_probability_distribution" title="Discrete probability distribution">Categorical data</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Frequency_(statistics)" title="Frequency (statistics)">Frequency</a>&#160;• <a href="/wiki/Contingency_table" title="Contingency table">Contingency table</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Statistical_inference" title="Statistical inference">Inferential statistics</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a>&#160;• <a href="/wiki/Frequentist_inference" title="Frequentist inference" class="mw-redirect">Frequentist inference</a>&#160;• <a href="/wiki/Statistical_hypothesis_testing" title="Statistical hypothesis testing">Hypothesis testing</a>&#160;• <a href="/wiki/Statistical_significance" title="Statistical significance">Significance</a>&#160;• <a href="/wiki/P-value" title="P-value">P-value</a>&#160;• <a href="/wiki/Interval_estimation" title="Interval estimation">Interval estimation</a>&#160;• <a href="/wiki/Confidence_interval" title="Confidence interval">Confidence interval</a>&#160;• <a href="/wiki/Meta-analysis" title="Meta-analysis">Meta-analysis</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">General estimation</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Bayesian_estimator" title="Bayesian estimator" class="mw-redirect">Bayesian estimator</a>&#160;• <strong class="selflink">Maximum likelihood</strong>&#160;• <a href="/wiki/Method_of_moments_(statistics)" title="Method of moments (statistics)">Method of moments</a>&#160;• <a href="/wiki/Minimum_distance_estimation" title="Minimum distance estimation">Minimum distance</a>&#160;• <a href="/wiki/Maximum_spacing_estimation" title="Maximum spacing estimation">Maximum spacing</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Specific tests</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Z-test" title="Z-test">Z-test (normal)</a>&#160;• <a href="/wiki/Student%27s_t-test" title="Student's t-test">Student's t-test</a>&#160;• <a href="/wiki/Chi-square_test" title="Chi-square test">Chi-square test</a>&#160;• <a href="/wiki/F-test" title="F-test">F-test</a>&#160;• <a href="/wiki/Sensitivity_and_specificity" title="Sensitivity and specificity">Sensitivity and specificity</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Survival_analysis" title="Survival analysis">Survival analysis</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Survival_function" title="Survival function">Survival function</a>&#160;• <a href="/wiki/Kaplan-Meier_estimator" title="Kaplan-Meier estimator">Kaplan-Meier</a>&#160;• <a href="/wiki/Logrank_test" title="Logrank test">Logrank test</a>&#160;• <a href="/wiki/Failure_rate" title="Failure rate">Failure rate</a>&#160;• <a href="/wiki/Proportional_hazards_models" title="Proportional hazards models">Proportional hazards models</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Correlation" title="Correlation">Correlation</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Pearson_product-moment_correlation_coefficient" title="Pearson product-moment correlation coefficient">Pearson product-moment correlation coefficient</a>&#160;• <a href="/wiki/Rank_correlation" title="Rank correlation">Rank correlation</a> (<a href="/wiki/Spearman%27s_rank_correlation_coefficient" title="Spearman's rank correlation coefficient">Spearman's rho</a>, <a href="/wiki/Kendall_tau_rank_correlation_coefficient" title="Kendall tau rank correlation coefficient">Kendall's tau</a>)&#160;• <a href="/wiki/Confounding_variable" title="Confounding variable" class="mw-redirect">Confounding variable</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Linear_model" title="Linear model">Linear models</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/General_linear_model" title="General linear model">General linear model</a>&#160;• <a href="/wiki/Generalized_linear_model" title="Generalized linear model">Generalized linear model</a>&#160;• <a href="/wiki/Analysis_of_variance" title="Analysis of variance">Analysis of variance</a>&#160;• <a href="/wiki/Analysis_of_covariance" title="Analysis of covariance">Analysis of covariance</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Regression_analysis" title="Regression analysis">Regression analysis</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a>&#160;• <a href="/wiki/Nonlinear_regression" title="Nonlinear regression">Nonlinear regression</a>&#160;• <a href="/wiki/Nonparametric_regression" title="Nonparametric regression">Nonparametric regression</a>&#160;• <a href="/wiki/Semiparametric_regression" title="Semiparametric regression">Semiparametric regression</a>&#160;• <a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Statistical_graphics" title="Statistical graphics">Statistical graphics</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Bar_chart" title="Bar chart">Bar chart</a>&#160;• <a href="/wiki/Biplot" title="Biplot">Biplot</a>&#160;• <a href="/wiki/Box_plot" title="Box plot">Box plot</a>&#160;• <a href="/wiki/Control_chart" title="Control chart">Control chart</a>&#160;• <a href="/wiki/Forest_plot" title="Forest plot">Forest plot</a>&#160;• <a href="/wiki/Histogram" title="Histogram">Histogram</a>&#160;• <a href="/wiki/Q-Q_plot" title="Q-Q plot">Q-Q plot</a>&#160;• <a href="/wiki/Run_chart" title="Run chart">Run chart</a>&#160;• <a href="/wiki/Scatter_plot" title="Scatter plot">Scatter plot</a>&#160;• <a href="/wiki/Stemplot" title="Stemplot">Stemplot</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/History_of_statistics" title="History of statistics">History</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/History_of_statistics" title="History of statistics">History of statistics</a>&#160;• <a href="/wiki/Founders_of_statistics" title="Founders of statistics">Founders of statistics</a>&#160;• <a href="/wiki/Timeline_of_probability_and_statistics" title="Timeline of probability and statistics">Timeline of probability and statistics</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Publications</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/List_of_scientific_journals_in_statistics" title="List of scientific journals in statistics">Journals in statistics</a>&#160;• <a href="/wiki/List_of_important_publications_in_statistics" title="List of important publications in statistics">Important publications</a></div>
</td>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-abovebelow" style=";" colspan="2"><b><a href="/wiki/Category:Statistics" title="Category:Statistics">Category</a></b>&#160;• <b><a href="/wiki/Portal:Statistics" title="Portal:Statistics">Portal</a></b>&#160;• <b><a href="/wiki/Topic_outline_of_statistics" title="Topic outline of statistics">Topic outline</a></b>&#160;• <b><a href="/wiki/List_of_statistics_topics" title="List of statistics topics">List of topics</a></b></td>
</tr>
</table>
</td>
</tr>
</table>


<!-- 
NewPP limit report
Preprocessor node count: 3299/1000000
Post-expand include size: 58513/2048000 bytes
Template argument size: 27137/2048000 bytes
Expensive parser function count: 1/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:140806-0!1!0!default!!en!2 and timestamp 20090406135712 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Maximum_likelihood">http://en.wikipedia.org/wiki/Maximum_likelihood</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Estimation_theory" title="Category:Estimation theory">Estimation theory</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_March_2009" title="Category:Articles with unsourced statements since March 2009">Articles with unsourced statements since March 2009</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistics_articles_linked_to_the_portal" title="Category:Statistics articles linked to the portal">Statistics articles linked to the portal</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistics_articles_with_navigational_template" title="Category:Statistics articles with navigational template">Statistics articles with navigational template</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Maximum_likelihood" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Maximum_likelihood" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Maximum_likelihood&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Maximum_likelihood&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Maximum_likelihood" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Maximum_likelihood" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Maximum_likelihood" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Maximum_likelihood&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Maximum_likelihood&amp;oldid=282072484" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Maximum_likelihood&amp;id=282072484">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Maximum-Likelihood-Methode">Deutsch</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Maximum_de_vraisemblance">Français</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Metodo_della_massima_verosimiglianza">Italiano</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Meest_aannemelijke_schatter">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%A4%E6%B3%95">日本語</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/M%C3%A1xima_verossimilhan%C3%A7a">Português</a></li>
				<li class="interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%BE%D0%BF%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D1%8F">Русский</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/Suurimman_uskottavuuden_estimointi">Suomi</a></li>
				<li class="interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Maximum_Likelihood-metoden">Svenska</a></li>
				<li class="interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1">中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 6 April 2009, at 09:07.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv178 in 0.060 secs. --></body></html>
