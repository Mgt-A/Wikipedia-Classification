<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Style-Type" content="text/css" />
		<meta name="generator" content="MediaWiki 1.15alpha" />
		<meta name="keywords" content="Bayesian inference,Articles with unsourced statements since March 2008,Statistics,Statistics,Alan Turing,Alternative hypothesis,Analysis of covariance,Analysis of variance,Arithmetic mean,Artificial intelligence,Azores" />
		<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Bayesian_inference&amp;action=edit" />
		<link rel="edit" title="Edit this page" href="/w/index.php?title=Bayesian_inference&amp;action=edit" />
		<link rel="apple-touch-icon" href="http://en.wikipedia.org/apple-touch-icon.png" />
		<link rel="shortcut icon" href="/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
		<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html" />
		<link rel="alternate" type="application/rss+xml" title="Wikipedia RSS Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom Feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Bayesian inference - Wikipedia, the free encyclopedia</title>
		<link rel="stylesheet" href="/skins-1.5/common/shared.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/common/commonPrint.css?207xx" type="text/css" media="print" />
		<link rel="stylesheet" href="/skins-1.5/monobook/main.css?207xx" type="text/css" media="screen" />
		<link rel="stylesheet" href="/skins-1.5/chick/main.css?207xx" type="text/css" media="handheld" />
		<!--[if lt IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE50Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 5.5000]><link rel="stylesheet" href="/skins-1.5/monobook/IE55Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 6]><link rel="stylesheet" href="/skins-1.5/monobook/IE60Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<!--[if IE 7]><link rel="stylesheet" href="/skins-1.5/monobook/IE70Fixes.css?207xx" type="text/css" media="screen" /><![endif]-->
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Print.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="print" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Handheld.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" media="handheld" />
		<link rel="stylesheet" href="/w/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;ctype=text%2Fcss&amp;smaxage=2678400&amp;action=raw&amp;maxage=2678400" type="text/css" />
		<link rel="stylesheet" href="/w/index.php?title=-&amp;action=raw&amp;maxage=2678400&amp;gen=css" type="text/css" />
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js?207xx"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->

		<script type= "text/javascript">/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Bayesian_inference";
		var wgTitle = "Bayesian inference";
		var wgAction = "view";
		var wgArticleId = "49571";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 280843606;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/</script>

		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?207xx"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js?207xx"></script>
		<script type="text/javascript" src="/skins-1.5/common/mwsuggest.js?207xx"></script>
<script type="text/javascript">/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/</script>		<script type="text/javascript" src="http://upload.wikimedia.org/centralnotice/wikipedia/en/centralnotice.js?207xx"></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
	</head>
<body class="mediawiki ltr ns-0 ns-subject page-Bayesian_inference skin-monobook">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><script type='text/javascript'>if (wgNotice != '') document.writeln(wgNotice);</script></div>		<h1 id="firstHeading" class="firstHeading">Bayesian inference</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<table class="metadata plainlinks ambox ambox-style" style="">
<tr>
<td class="mbox-image">
<div style="width: 52px;"><a href="/wiki/File:Text_document_with_red_question_mark.svg" class="image" title="Text document with red question mark.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png" width="40" height="40" border="0" /></a></div>
</td>
<td class="mbox-text" style="">This article includes a <a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">list of references</a> or <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">external links</a>, but <b>its sources remain unclear because it has insufficient <a href="/wiki/Wikipedia:Citing_sources#Adding_the_citation" title="Wikipedia:Citing sources">inline citations</a></b>. Please help to <a href="/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check" title="Wikipedia:WikiProject Fact and Reference Check">improve</a> this article by introducing more precise citations <a href="/wiki/Wikipedia:When_to_cite" title="Wikipedia:When to cite">where appropriate</a>.</td>
</tr>
</table>
<p><b>Bayesian inference</b> is <a href="/wiki/Statistical_inference" title="Statistical inference">statistical inference</a> in which evidence or observations are used to update or to newly infer the <a href="/wiki/Probability" title="Probability">probability</a> that a hypothesis may be true. The name "Bayesian" comes from the frequent use of <a href="/wiki/Bayes%27_theorem" title="Bayes' theorem">Bayes' theorem</a> in the inference process. Bayes' theorem was derived from the work of the Reverend <a href="/wiki/Thomas_Bayes" title="Thomas Bayes">Thomas Bayes</a>.<sup id="cite_ref-0" class="reference"><a href="#cite_note-0" title=""><span>[</span>1<span>]</span></a></sup></p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a href="#Evidence_and_changing_beliefs"><span class="tocnumber">1</span> <span class="toctext">Evidence and changing beliefs</span></a></li>
<li class="toclevel-1"><a href="#Simple_examples_of_Bayesian_inference"><span class="tocnumber">2</span> <span class="toctext">Simple examples of Bayesian inference</span></a>
<ul>
<li class="toclevel-2"><a href="#From_which_bowl_is_the_cookie.3F"><span class="tocnumber">2.1</span> <span class="toctext">From which bowl is the cookie?</span></a></li>
<li class="toclevel-2"><a href="#False_positives_in_a_medical_test"><span class="tocnumber">2.2</span> <span class="toctext">False positives in a medical test</span></a></li>
<li class="toclevel-2"><a href="#In_the_courtroom"><span class="tocnumber">2.3</span> <span class="toctext">In the courtroom</span></a></li>
<li class="toclevel-2"><a href="#Search_theory"><span class="tocnumber">2.4</span> <span class="toctext">Search theory</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#More_mathematical_examples"><span class="tocnumber">3</span> <span class="toctext">More mathematical examples</span></a>
<ul>
<li class="toclevel-2"><a href="#Naive_Bayes_classifier"><span class="tocnumber">3.1</span> <span class="toctext">Naive Bayes classifier</span></a></li>
<li class="toclevel-2"><a href="#Posterior_distribution_of_the_binomial_parameter"><span class="tocnumber">3.2</span> <span class="toctext">Posterior distribution of the binomial parameter</span></a></li>
<li class="toclevel-2"><a href="#Computer_applications"><span class="tocnumber">3.3</span> <span class="toctext">Computer applications</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#See_also"><span class="tocnumber">4</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a href="#Notes"><span class="tocnumber">5</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a href="#External_links"><span class="tocnumber">7</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script>
<p><a name="Evidence_and_changing_beliefs" id="Evidence_and_changing_beliefs"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=1" title="Edit section: Evidence and changing beliefs">edit</a>]</span> <span class="mw-headline">Evidence and changing beliefs</span></h2>
<p>Bayesian inference uses aspects of the <a href="/wiki/Scientific_method" title="Scientific method">scientific method</a>, which involves collecting <a href="/wiki/Evidence" title="Evidence">evidence</a> that is meant to be consistent or inconsistent with a given <a href="/wiki/Hypothesis" title="Hypothesis">hypothesis</a>. As evidence accumulates, the degree of belief in a hypothesis ought to change. With enough evidence, it should become very high or very low. Thus, proponents of Bayesian inference say that it can be used to discriminate between conflicting hypotheses: hypotheses with very high support should be accepted as true and those with very low support should be rejected as false. However, detractors say that this inference method may be biased due to initial beliefs that one holds before any evidence is ever collected. (This is a form of <a href="/wiki/Inductive_bias" title="Inductive bias">inductive bias</a>).</p>
<p>Bayesian inference uses a numerical estimate of the degree of belief in a hypothesis before evidence has been observed and calculates a numerical estimate of the degree of belief in the hypothesis after evidence has been observed. (This process is repeated when additional evidence is obtained.) Bayesian inference usually relies on degrees of belief, or subjective probabilities, in the induction process and does not necessarily claim to provide an objective method of induction. Nonetheless, some Bayesian statisticians believe probabilities can have an objective value and therefore Bayesian inference can provide an objective method of induction. See <a href="/wiki/Scientific_method" title="Scientific method">scientific method</a>.</p>
<p>Bayes' theorem adjusts probabilities given new evidence in the following way:</p>
<dl>
<dd><img class="tex" alt="P(H|E) = \frac{P(E|H)\;P(H)}{P(E)}" src="http://upload.wikimedia.org/math/8/c/4/8c4e322009fcae942adf5465af5f8f30.png" /></dd>
</dl>
<p>where</p>
<ul>
<li><span class="texhtml"><i>H</i></span> represents a specific hypothesis, which may or may not be some <a href="/wiki/Null_hypothesis" title="Null hypothesis">null hypothesis</a>.</li>
<li><span class="texhtml"><i>P</i>(<i>H</i>)</span> is called the <i><a href="/wiki/Prior_probability" title="Prior probability">prior probability</a></i> of <span class="texhtml"><i>H</i></span> that was inferred before new evidence, <span class="texhtml"><i>E</i></span>, became available.</li>
<li><span class="texhtml"><i>P</i>(<i>E</i> | <i>H</i>)</span> is called the <i><a href="/wiki/Conditional_probability" title="Conditional probability">conditional probability</a></i> of seeing the evidence <span class="texhtml"><i>E</i></span> if the hypothesis <span class="texhtml"><i>H</i></span> happens to be true. It is also called a <i><a href="/wiki/Likelihood_function" title="Likelihood function">likelihood function</a></i> when it is considered as a function of <span class="texhtml"><i>H</i></span> for fixed <span class="texhtml"><i>E</i></span>.</li>
<li><span class="texhtml"><i>P</i>(<i>E</i>)</span> is called the <i><a href="/wiki/Marginal_probability" title="Marginal probability" class="mw-redirect">marginal probability</a></i> of <span class="texhtml"><i>E</i></span>: the <i>a priori</i> probability of witnessing the new evidence <span class="texhtml"><i>E</i></span> under all possible hypotheses. It can be calculated as the sum of the product of all probabilities of any complete set of mutually exclusive hypotheses and corresponding conditional probabilities:</li>
</ul>
<dl>
<dd><img class="tex" alt="P(E) = \sum  P(E|H_i)P(H_i)" src="http://upload.wikimedia.org/math/6/9/7/6971866c9c1afd003888fb416e5c03a0.png" />.</dd>
</dl>
<ul>
<li><span class="texhtml"><i>P</i>(<i>H</i> | <i>E</i>)</span> is called the <i><a href="/wiki/Posterior_probability" title="Posterior probability">posterior probability</a></i> of <span class="texhtml"><i>H</i></span> given <span class="texhtml"><i>E</i></span>.</li>
</ul>
<p>The factor <span class="texhtml"><i>P</i>(<i>E</i> | <i>H</i>) / <i>P</i>(<i>E</i>)</span> represents the impact that the evidence has on the belief in the hypothesis. If it is likely that the evidence <span class="texhtml"><i>E</i></span> would be observed when the hypothesis under consideration is true, but unlikely that <span class="texhtml"><i>E</i></span> would have been the outcome of the observation, then this factor will be large. Multiplying the prior probability of the hypothesis by this factor would result in a larger posterior probability of the hypothesis given the evidence. Conversely, if it is unlikely that the evidence <span class="texhtml"><i>E</i></span> would be observed if the hypothesis under consideration is true, but <i>a priori</i> likely that <span class="texhtml"><i>E</i></span> would be observed, then the factor would reduce the posterior probability for <span class="texhtml"><i>H</i></span>. Under Bayesian inference, Bayes' theorem therefore measures how much new evidence should alter a belief in a hypothesis.</p>
<p>Bayesian statisticians argue that even when people have very different prior subjective probabilities, new evidence from repeated observations will tend to bring their posterior subjective probabilities closer together. However, others argue that when people hold widely different prior subjective probabilities their posterior subjective probabilities may never converge even with repeated collection of evidence. These critics argue that worldviews which are completely different initially can remain completely different over time despite a large accumulation of evidence.<sup class="noprint Template-Fact"><span title="This claim needs references to reliable sources&#160;since March 2008" style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed">citation needed</a></i>]</span></sup></p>
<p>Multiplying the prior probability <span class="texhtml"><i>P</i>(<i>H</i>)</span> by the factor <span class="texhtml"><i>P</i>(<i>E</i> | <i>H</i>) / <i>P</i>(<i>E</i>)</span> will never yield a probability that is greater than 1, since <span class="texhtml"><i>P</i>(<i>E</i>)</span> is at least as great as <img class="tex" alt="P(E \cap H)" src="http://upload.wikimedia.org/math/9/f/b/9fb43dd9a275b53f6282ff85a70e2a8a.png" /> (where <img class="tex" alt="\cap" src="http://upload.wikimedia.org/math/e/7/8/e78f632038354aae583f795a73d4e6b8.png" /> denotes "and"), which equals <img class="tex" alt="P(E|H)\,P(H)" src="http://upload.wikimedia.org/math/f/5/b/f5bc48ebae3a5602ae5f974c27f54cdf.png" /> (see <a href="/wiki/Joint_probability" title="Joint probability" class="mw-redirect">joint probability</a>).</p>
<p>The probability of <span class="texhtml"><i>E</i></span> given <span class="texhtml"><i>H</i></span>, <span class="texhtml"><i>P</i>(<i>E</i> | <i>H</i>)</span>, can be represented as a function of its second argument with its first argument held fixed. Such a function is called a <a href="/wiki/Likelihood_function" title="Likelihood function">likelihood function</a>; it is a function of <span class="texhtml"><i>H</i></span> alone, with <span class="texhtml"><i>E</i></span> treated as a <a href="/wiki/Parameter" title="Parameter">parameter</a>. A ratio of two likelihood functions is called a likelihood ratio, <span class="texhtml">Λ</span>. For example,</p>
<dl>
<dd><img class="tex" alt="\Lambda_E = \frac{L(H|E)}{L(\neg\,H|E)} = \frac{P(E|H)}{P(E|\neg\,H)} " src="http://upload.wikimedia.org/math/2/2/d/22d71b5a2237d9f37a24121263515b35.png" />,</dd>
</dl>
<p>where the dependence of <span class="texhtml">Λ<sub><i>E</i></sub></span> on <span class="texhtml"><i>H</i></span> is suppressed for simplicity (as <span class="texhtml"><i>E</i></span> might have been, except we will need to use that parameter below). Since <span class="texhtml"><i>H</i></span> and not-<span class="texhtml"><i>H</i></span> are mutually exclusive and span all possibilities, the sum previously given for the marginal probability reduces to</p>
<dl>
<dd><img class="tex" alt="P(E) = P(E|H)\,P(H)+P(E|\neg\,H)\,P(\neg\,H) " src="http://upload.wikimedia.org/math/6/f/7/6f7f64fd3071e4533812457b56c76ef8.png" /></dd>
</dl>
<p>As a result, we can rewrite Bayes' theorem as</p>
<dl>
<dd><img class="tex" alt="P(H|E) = \frac{P(E|H)\,P(H)}{P(E|H)\,P(H)+ P(E|\neg\,H)\,P(\neg\,H)} = \frac{\Lambda_E P(H)}{\Lambda_E P(H) +P(\neg\,H)}" src="http://upload.wikimedia.org/math/e/c/d/ecdd7ca0d6d2ebdddf9da2bab22f10ea.png" />.</dd>
</dl>
<p>We could then exploit the identity</p>
<dl>
<dd><img class="tex" alt="P(\neg\,H) = 1 - P(H)" src="http://upload.wikimedia.org/math/8/9/b/89bb0b1cc98d60ac4861ae16241261ed.png" /></dd>
</dl>
<p>to exhibit <span class="texhtml"><i>P</i>(<i>H</i> | <i>E</i>)</span> as a function of just <span class="texhtml"><i>P</i>(<i>H</i>)</span> (and <span class="texhtml">Λ<sub><i>E</i></sub></span>, which is computed directly from the evidence).</p>
<p>With two pieces of evidence <span class="texhtml"><i>E</i><sub>1</sub></span> and <span class="texhtml"><i>E</i><sub>2</sub></span>, that are <a href="/wiki/Marginal_distribution" title="Marginal distribution">marginally</a> and <a href="/wiki/Conditional_Probability" title="Conditional Probability" class="mw-redirect">conditionally</a> <a href="/wiki/Statistical_independence" title="Statistical independence" class="mw-redirect">independent</a> of each other given the hypotheses, Bayesian inference can be applied iteratively. We could use the first piece of evidence to calculate an initial posterior probability, and then use that posterior probability as a new prior probability to calculate a second posterior probability given the second piece of evidence. Bayes' theorem applied iteratively yields</p>
<dl>
<dd><img class="tex" alt="P(H|E_1 \cap E_2) = \frac{P(E_2|H)\;P(E_1|H)\,P(H)}{P(E_2)\;P(E_1)}" src="http://upload.wikimedia.org/math/0/8/e/08e2c44211f5f0296916f87fb55d0065.png" /></dd>
</dl>
<p>Using likelihood ratios, we find that</p>
<dl>
<dd><img class="tex" alt="P(H|E_1 \cap E_2) = \frac{\Lambda_1 \Lambda_2 P(H)}{[\Lambda_1 P(H) + P(\neg\,H)]\;[\Lambda_2 P(H) + P(\neg\,H)]} " src="http://upload.wikimedia.org/math/d/f/5/df5b9cf48394f055fdf25a8f7a916fd8.png" />,</dd>
</dl>
<p>This iteration of Bayesian inference could be extended with more independent pieces of evidence.</p>
<p>Bayesian inference is used to calculate probabilities for decision making under uncertainty. Besides the probabilities, a <a href="/wiki/Loss_function" title="Loss function">loss function</a> should be evaluated to take into account the relative impact of the alternatives.</p>
<p><a name="Simple_examples_of_Bayesian_inference" id="Simple_examples_of_Bayesian_inference"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=2" title="Edit section: Simple examples of Bayesian inference">edit</a>]</span> <span class="mw-headline">Simple examples of Bayesian inference</span></h2>
<p><a name="From_which_bowl_is_the_cookie.3F" id="From_which_bowl_is_the_cookie.3F"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=3" title="Edit section: From which bowl is the cookie?">edit</a>]</span> <span class="mw-headline">From which bowl is the cookie?</span></h3>
<p>To illustrate, suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?</p>
<p>Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let <span class="texhtml"><i>H</i><sub>1</sub></span> correspond to bowl #1, and <span class="texhtml"><i>H</i><sub>2</sub></span> to bowl #2. It is given that the bowls are identical from Fred's point of view, thus <span class="texhtml"><i>P</i>(<i>H</i><sub>1</sub>) = <i>P</i>(<i>H</i><sub>2</sub>)</span>, and the two must add up to 1, so both are equal to 0.5. The event <span class="texhtml"><i>E</i></span> is the observation of a plain cookie. From the contents of the bowls, we know that <span class="texhtml"><i>P</i>(<i>E</i> | <i>H</i><sub>1</sub>) = 30 / 40 = 0.75</span> and <span class="texhtml"><i>P</i>(<i>E</i> | <i>H</i><sub>2</sub>) = 20 / 40 = 0.5</span>. Bayes' formula then yields</p>
<dl>
<dd><img class="tex" alt="

\begin{matrix} P(H_1|E) &amp;=&amp; \frac{P(E|H_1)\,P(H_1)}{P(E|H_1)\,P(H_1)\;+\;P(E|H_2)\,P(H_2)} \\  \\  \ &amp; =&amp; \frac{0.75 \times 0.5}{0.75 \times 0.5 + 0.5 \times 0.5} \\  \\  \ &amp; =&amp; 0.6 \end{matrix}
" src="http://upload.wikimedia.org/math/7/a/0/7a0b651574e0b977ab65b60622dc1661.png" /></dd>
</dl>
<p>Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, <span class="texhtml"><i>P</i>(<i>H</i><sub>1</sub>)</span>, which was 0.5. After observing the cookie, we must revise the probability to <span class="texhtml"><i>P</i>(<i>H</i><sub>1</sub> | <i>E</i>)</span>, which is 0.6.</p>
<p><a name="False_positives_in_a_medical_test" id="False_positives_in_a_medical_test"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=4" title="Edit section: False positives in a medical test">edit</a>]</span> <span class="mw-headline">False positives in a medical test</span></h3>
<p><a href="/wiki/Type_I_and_type_II_errors" title="Type I and type II errors">False positives</a> result when a test falsely or incorrectly reports a positive result. For example, a medical test for a <a href="/wiki/Disease" title="Disease">disease</a> may return a positive result indicating that patient has a disease even if the patient does not have the disease. We can use Bayes' theorem to determine the probability that a positive result is in fact a false positive. We find that if a disease is rare, then the majority of positive results may be false positives, even if the test is accurate.</p>
<p>Suppose that a test for a disease generates the following results:</p>
<ul>
<li>If a tested patient has the disease, the test returns a positive result 99% of the time, or with probability 0.99</li>
<li>If a tested patient does not have the disease, the test returns a positive result 5% of the time, or with probability 0.05.</li>
</ul>
<p>Naively, one might think that only 5% of positive test results are false, but that is quite wrong, as we shall see.</p>
<p>Suppose that only 0.1% of the population has that disease, so that a randomly selected patient has a 0.001 prior probability of having the disease.</p>
<p>We can use Bayes' theorem to calculate the probability that a positive test result is a false positive.</p>
<p>Let <i>A</i> represent the condition in which the patient has the disease, and <i>B</i> represent the evidence of a positive test result. Then, probability that the patient actually has the disease given the positive test result is</p>
<dl>
<dd><img class="tex" alt="\begin{matrix} P(A | B) &amp;=&amp; \frac{P(B | A) P(A)}{P(B | A)P(A) + P(B |\mathrm{not}\,A)P(\mathrm{not}\,A)} \\ \\

 &amp;= &amp;\frac{0.99\times 0.001}{0.99 \times 0.001 + 0.05\times 0.999}  \\ ~\\ &amp;\approx &amp;0.019 .\end{matrix}" src="http://upload.wikimedia.org/math/2/1/8/218001a0af4a59a8ddf8119c29bfd739.png" /></dd>
</dl>
<p>and hence the probability that a positive result is a false positive is about <span class="texhtml">1 − 0.019 = 0.98</span>, or 98%.</p>
<p>Despite the apparent high accuracy of the test, the incidence of the disease is so low that the vast majority of patients who test positive do not have the disease. Nonetheless, the fraction of patients who test positive who do have the disease (.019) is 19 times the fraction of people who have not yet taken the test who have the disease (.001). Thus the test is not useless, and re-testing may improve the reliability of the result.</p>
<p>In order to reduce the problem of false positives, a test should be very accurate in reporting a <i>negative</i> result when the patient does not have the disease. If the test reported a negative result in patients without the disease with probability 0.999, then</p>
<dl>
<dd><img class="tex" alt="P(A|B) = \frac{0.99\times 0.001}{0.99 \times 0.001 + 0.001\times 0.999} \approx 0.5 " src="http://upload.wikimedia.org/math/8/d/0/8d014382edcb263450464878bfb3e04d.png" />,</dd>
</dl>
<p>so that <span class="texhtml">1 − 0.5 = 0.5</span> now is the probability of a false positive.</p>
<p>On the other hand, <a href="/wiki/Type_I_and_type_II_errors" title="Type I and type II errors">false negatives</a> result when a test falsely or incorrectly reports a negative result. For example, a medical test for a <a href="/wiki/Disease" title="Disease">disease</a> may return a negative result indicating that patient does not have a disease even though the patient actually has the disease. We can also use Bayes' theorem to calculate the probability of a false negative. In the first example above,</p>
<dl>
<dd><img class="tex" alt="\begin{matrix} P(A |\mathrm{not}\,B) &amp;=&amp; \frac{P(\mathrm{not}\,B | A) P(A)}{P(\mathrm{not}\,B | A)P(A) + P(\mathrm{not}\,B |\mathrm{not}\,A)P(\mathrm{not}\,A)} \\ \\

 &amp;= &amp;\frac{0.01\times 0.001}{0.01 \times 0.001 + 0.95\times 0.999}\, ,\\ ~\\ &amp;\approx &amp;0.0000105\, .\end{matrix}" src="http://upload.wikimedia.org/math/6/d/2/6d2db656e061582c8a9d4460ce46815c.png" /></dd>
</dl>
<p>The probability that a negative result is a false negative is about 0.0000105 or 0.00105%. When a disease is rare, false negatives will not be a major problem with the test.</p>
<p>But if 60% of the population had the disease, then the probability of a false negative would be greater. With the above test, the probability of a false negative would be</p>
<dl>
<dd><img class="tex" alt="\begin{matrix} P(A |\mathrm{not}\,B) &amp;=&amp; \frac{P(\mathrm{not}\,B | A) P(A)}{P(\mathrm{not}\,B | A)P(A) + P(\mathrm{not}\,B |\mathrm{not}\,A)P(\mathrm{not}\,A)} \\ \\

 &amp;= &amp;\frac{0.01\times 0.6}{0.01 \times 0.6 + 0.95\times 0.4}\, ,\\ ~\\ &amp;\approx &amp;0.0155\, .\end{matrix}" src="http://upload.wikimedia.org/math/d/8/5/d857da508f8268d4de1e51e4049fd9e8.png" /></dd>
</dl>
<p>The probability that a negative result is a false negative rises to 0.0155 or 1.55%.</p>
<p><a name="In_the_courtroom" id="In_the_courtroom"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=5" title="Edit section: In the courtroom">edit</a>]</span> <span class="mw-headline">In the courtroom</span></h3>
<p>Bayesian inference can be used in a court setting by an individual juror to coherently accumulate the evidence for and against the guilt of the defendant, and to see whether, in totality, it meets their personal threshold for 'beyond a reasonable doubt'.</p>
<ul>
<li>Let <span class="texhtml"><i>G</i></span> denote the event that the defendant is guilty.</li>
</ul>
<ul>
<li>Let <span class="texhtml"><i>E</i></span> denote the event that the defendant's DNA matches DNA found at the crime scene.</li>
</ul>
<ul>
<li>Let <span class="texhtml"><i>P</i>(<i>E</i> | <i>G</i>)</span> denote the probability of seeing event <span class="texhtml"><i>E</i></span> if the defendant is actually guilty. (Usually this would be taken to be near unity.)</li>
</ul>
<ul>
<li>Let <span class="texhtml"><i>P</i>(<i>G</i> | <i>E</i>)</span> denote the probability that the defendant is guilty assuming the DNA match (event <span class="texhtml"><i>E</i></span>).</li>
</ul>
<ul>
<li>Let <span class="texhtml"><i>P</i>(<i>G</i>)</span> denote the juror's personal estimate of the probability that the defendant is guilty, based on the evidence <i>other than</i> the DNA match. This could be based on his responses under questioning, or previously presented evidence.</li>
</ul>
<p>Bayesian inference tells us that if we can assign a probability p(G) to the defendant's guilt before we take the DNA evidence into account, then we can revise this probability to the conditional probability <span class="texhtml"><i>P</i>(<i>G</i> | <i>E</i>)</span>, since</p>
<dl>
<dd><img class="tex" alt="P(G | E) = \frac{P(G) P(E | G)}{P(E)}." src="http://upload.wikimedia.org/math/4/3/2/4324e3ded9a7821b985de23045c7dc4d.png" /></dd>
</dl>
<p>Suppose, on the basis of other evidence, a juror decides that there is a 30% chance that the defendant is guilty. Suppose also that the forensic testimony was that the probability that a person chosen at random would have DNA that matched that at the crime scene is 1 in a million, or 10<sup>−6</sup>.</p>
<p>The event E can occur in two ways. Either the defendant is guilty (with prior probability 0.3) and thus his DNA is present with probability 1, or he is innocent (with prior probability 0.7) and he is unlucky enough to be one of the 1 in a million matching people.</p>
<p>Thus the juror could coherently revise his opinion to take into account the <a href="/wiki/DNA_evidence" title="DNA evidence" class="mw-redirect">DNA evidence</a> as follows:</p>
<dl>
<dd><img class="tex" alt="P(G | E) = (0.3 \times 1.0) /(0.3 \times 1.0 + 0.7 \times 10^{-6}) = 0.99999766667." src="http://upload.wikimedia.org/math/0/a/9/0a94995236d5d7e3c0181ab413fe0edc.png" /></dd>
</dl>
<p>The benefit of adopting a Bayesian approach is that it gives the juror a formal mechanism for combining the evidence presented. The approach can be applied successively to all the pieces of evidence presented in court, with the posterior from one stage becoming the prior for the next.</p>
<p>The juror would still have to have a prior estimate for the guilt probability before the first piece of evidence is considered. It has been suggested that this could reasonably be the guilt probability of a random person taken from the qualifying population. Thus, for a crime known to have been committed by an adult male living in a town containing 50,000 adult males, the appropriate initial prior probability might be 1/50,000.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:258px;"><a href="/wiki/File:Ebits2c.png" class="image" title="Adding up evidence."><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Ebits2c.png/256px-Ebits2c.png" width="256" height="270" border="0" class="thumbimage" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Ebits2c.png" class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="" /></a></div>
<center>Adding up evidence.</center>
</div>
</div>
</div>
<p>For the purpose of explaining Bayes' theorem to jurors, it will usually be appropriate to give it in the form of <a href="/wiki/Betting_odds" title="Betting odds" class="mw-redirect">betting odds</a> rather than probabilities, as these are more widely understood. In this form Bayes' theorem states that</p>
<dl>
<dd>Posterior odds = prior odds x <a href="/wiki/Bayes_factor" title="Bayes factor">Bayes factor</a></dd>
</dl>
<p>In the example above, the juror who has a prior probability of 0.3 for the defendant being guilty would now express that in the form of odds of 3:7 in favour of the defendant being guilty, the Bayes factor is one million, and the resulting posterior odds are 3 million to 7 or about 429,000 to one in favour of guilt.</p>
<p>A <a href="/wiki/Gambling_and_information_theory" title="Gambling and information theory">logarithmic approach</a> which replaces multiplication with addition and reduces the range of the numbers involved might be easier for a jury to handle. This approach, developed by <a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a> during <a href="/wiki/World_War_II" title="World War II">World War II</a> and later promoted by <a href="/wiki/I._J._Good" title="I. J. Good">I. J. Good</a> and <a href="/wiki/Edwin_Thompson_Jaynes" title="Edwin Thompson Jaynes">E. T. Jaynes</a> among others, amounts to the use of <a href="/wiki/Information_entropy" title="Information entropy" class="mw-redirect">information entropy</a>.</p>
<p>In the United Kingdom, Bayes' theorem was explained to the jury in the odds form by a statistician <a href="/wiki/Expert_witness" title="Expert witness">expert witness</a> in the rape case of <a href="/wiki/Regina_versus_Denis_John_Adams" title="Regina versus Denis John Adams" class="mw-redirect">Regina versus Denis John Adams</a>. A conviction was secured but the case went to Appeal, as no means of accumulating evidence had been provided for those jurors who did not want to use Bayes' theorem. The Court of Appeal upheld the conviction, but also gave their opinion that "To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the Jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task." No further appeal was allowed and the issue of Bayesian assessment of forensic DNA data remains controversial.</p>
<p>Gardner-Medwin argues that the criterion on which a verdict in a criminal trial should be based is <i>not</i> the probability of guilt, but rather the <i>probability of the evidence, given that the defendant is innocent</i> (akin to a <a href="/wiki/Frequentist" title="Frequentist" class="mw-redirect">frequentist</a> <a href="/wiki/P-value" title="P-value">p-value</a>). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:</p>
<p>A: The known facts and testimony could have arisen if the defendant is guilty,</p>
<p>B: The known facts and testimony could have arisen if the defendant is innocent,</p>
<p>C: The defendant is guilty.</p>
<p>Gardner-Medwin argues that the jury should believe both A and not-B in order to convict. A and not-B implies the truth of C, but the reverse is not true. It is possible that B and C are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also <a href="/wiki/Lindley%27s_paradox" title="Lindley's paradox">Lindley's paradox</a>.</p>
<p>Other court cases in which probabilistic arguments played some role were the <a href="/wiki/Howland_will_forgery_trial" title="Howland will forgery trial">Howland will forgery trial</a>, the <a href="/wiki/Sally_Clark" title="Sally Clark">Sally Clark</a> case, and the <a href="/wiki/Lucia_de_Berk" title="Lucia de Berk">Lucia de Berk</a> case.</p>
<p><a name="Search_theory" id="Search_theory"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=6" title="Edit section: Search theory">edit</a>]</span> <span class="mw-headline">Search theory</span></h3>
<div class="rellink noprint relarticle mainarticle">Main article: <a href="/wiki/Bayesian_search_theory" title="Bayesian search theory">Bayesian search theory</a></div>
<p>In May 1968 the US nuclear submarine <a href="/wiki/USS_Scorpion_(SSN-589)" title="USS Scorpion (SSN-589)"><i>Scorpion</i> (SSN-589)</a> failed to arrive as expected at her home port of <a href="/wiki/Norfolk,_Virginia" title="Norfolk, Virginia">Norfolk, Virginia</a>. The US Navy was convinced that the vessel had been lost off the Eastern seaboard but an extensive search failed to discover the wreck. The US Navy's deep water expert, <a href="/wiki/John_Craven_USN" title="John Craven USN" class="mw-redirect">John Craven USN</a>, believed that it was elsewhere and he organised a search south west of the <a href="/wiki/Azores" title="Azores">Azores</a> based on a controversial approximate triangulation by hydrophones. He was allocated only a single ship, the <a href="/wiki/USNS_Mizar_(AGOR-11)" title="USNS Mizar (AGOR-11)" class="mw-redirect"><i>Mizar</i></a>, and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of the <i>Scorpion</i>.</p>
<p>The sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.</p>
<p>This sea grid was systematically searched in a manner which started with the high probability regions first and worked down to the low probability regions last. Each time a grid square was searched and found to be empty its probability was reassessed using <a href="/wiki/Bayes%27_theorem" title="Bayes' theorem">Bayes' theorem</a>. This then forced the probabilities of all the other grid squares to be reassessed (upwards), also by Bayes' theorem. The use of this approach was a major computational challenge for the time but it was eventually successful and the <i>Scorpion</i> was found about 740 kilometers southwest of the <a href="/wiki/Azores" title="Azores">Azores</a> in October of that year.</p>
<p>Suppose a grid square has a probability <i>p</i> of containing the wreck and that the probability of successfully detecting the wreck if it is there is <i>q</i>. If the square is searched and no wreck is found, then, by Bayes' theorem, the revised probability of the wreck being in the square is given by</p>
<dl>
<dd><img class="tex" alt="  p' = \frac{p(1-q)}{(1-p)+p(1-q)} = p \frac{1-q}{1-pq} &lt; p." src="http://upload.wikimedia.org/math/7/2/d/72d60cc004e9ad4039ebc48b304fcf71.png" /></dd>
</dl>
<p>For each other grid square, if its prior probability is <i>r</i>, its posterior probability is given by</p>
<dl>
<dd><img class="tex" alt=" r' = r \frac{1}{1- pq} &gt; r. " src="http://upload.wikimedia.org/math/d/0/8/d08d2250cc46c236db4e9db129b8c7d7.png" /></dd>
</dl>
<p><a name="More_mathematical_examples" id="More_mathematical_examples"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=7" title="Edit section: More mathematical examples">edit</a>]</span> <span class="mw-headline">More mathematical examples</span></h2>
<p><a name="Naive_Bayes_classifier" id="Naive_Bayes_classifier"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=8" title="Edit section: Naive Bayes classifier">edit</a>]</span> <span class="mw-headline">Naive Bayes classifier</span></h3>
<p>See <a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">naive Bayes classifier</a>.</p>
<p><a name="Posterior_distribution_of_the_binomial_parameter" id="Posterior_distribution_of_the_binomial_parameter"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=9" title="Edit section: Posterior distribution of the binomial parameter">edit</a>]</span> <span class="mw-headline">Posterior distribution of the binomial parameter</span></h3>
<p>In this example we consider the computation of the posterior distribution for the binomial parameter. This is the same problem considered by Bayes in Proposition 9 of his essay.</p>
<p>We are given <i>m</i> observed successes and <i>n</i> observed failures in a binomial experiment. The experiment may be tossing a coin, drawing a ball from an urn, or asking someone their opinion, among many other possibilities. What we know about the parameter (let's call it <i>a</i>) is stated as the prior distribution, <i>p</i>(<i>a</i>).</p>
<p>For a given value of <i>a</i>, the probability of <i>m</i> successes in <i>m</i>+<i>n</i> trials is</p>
<dl>
<dd><img class="tex" alt=" p(m,n|a) = \begin{pmatrix} n+m \\ m \end{pmatrix} a^m (1-a)^n. " src="http://upload.wikimedia.org/math/f/5/3/f535a945924471afd2544477cf9af122.png" /></dd>
</dl>
<p>Since <i>m</i> and <i>n</i> are fixed, and <i>a</i> is unknown, this is a likelihood function for <i>a</i>. From the continuous form of the <a href="/wiki/Law_of_total_probability" title="Law of total probability">law of total probability</a> we have</p>
<dl>
<dd><img class="tex" alt=" p(a|m,n) = \frac{p(m,n|a)\,p(a)}{\int_0^1 p(m,n|a)\,p(a)\,da}

    = \frac{a^m (1-a)^n\,p(a)}
         {\int_0^1 a^m (1-a)^n\,p(a)\,da}.
" src="http://upload.wikimedia.org/math/d/d/1/dd19b40a4cb71e2808ca26f9ec064775.png" /></dd>
</dl>
<p>For some special choices of the prior distribution <i>p</i>(<i>a</i>), the integral can be solved and the posterior takes a convenient form. In particular, if <i>p</i>(<i>a</i>) is a <a href="/wiki/Beta_distribution" title="Beta distribution">beta distribution</a> with parameters <i>m</i><sub>0</sub> and <i>n</i><sub>0</sub>, then the posterior is also a beta distribution with parameters <i>m</i>+<i>m</i><sub>0</sub> and <i>n</i>+<i>n</i><sub>0</sub>.</p>
<p>A <i><a href="/wiki/Conjugate_prior" title="Conjugate prior">conjugate prior</a></i> is a prior distribution, such as the beta distribution in the above example, which has the property that the posterior is the same type of distribution.</p>
<p>What is "Bayesian" about Proposition 9 is that Bayes presented it as a probability for the parameter <i>a</i>. That is, not only can one compute probabilities for experimental outcomes, but also for the parameter which governs them, and the same algebra is used to make inferences of either kind. Interestingly, Bayes actually states his question in a way that might make the idea of assigning a probability distribution to a parameter palatable to a <a href="/wiki/Frequentist_statistics" title="Frequentist statistics" class="mw-redirect">frequentist</a>. He supposes that a billiard ball is thrown at random onto a billiard table, and that the probabilities <i>p</i> and <i>q</i> are the probabilities that subsequent billiard balls will fall above or below the first ball. By making the binomial parameter <i>a</i> depend on a random event, he cleverly escapes a philosophical quagmire that was an issue he most likely was not even aware of.</p>
<p><a name="Computer_applications" id="Computer_applications"></a></p>
<h3><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=10" title="Edit section: Computer applications">edit</a>]</span> <span class="mw-headline">Computer applications</span></h3>
<p>Bayesian inference has applications in <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> and <a href="/wiki/Expert_system" title="Expert system">expert systems</a>. Bayesian inference techniques have been a fundamental part of computerized <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> techniques since the late 1950s. There is also an ever growing connection between Bayesian methods and simulation-based <a href="/wiki/Monte_Carlo_method" title="Monte Carlo method">Monte Carlo</a> techniques since complex models cannot be processed in closed form by a Bayesian analysis, while the <a href="/wiki/Graphical_model" title="Graphical model">graphical model</a> structure inherent to statistical models, may allow for efficient simulation algorithms like the <a href="/wiki/Gibbs_sampling" title="Gibbs sampling">Gibbs sampling</a> and other <a href="/wiki/Metropolis-Hastings_algorithm" title="Metropolis-Hastings algorithm" class="mw-redirect">Metropolis-Hastings algorithm</a> schemes. Recently Bayesian inference has gained popularity amongst the <a href="/wiki/Phylogenetics" title="Phylogenetics">phylogenetics</a> community for these reasons; applications such as <a href="http://beast.bio.ed.ac.uk/" class="external text" title="http://beast.bio.ed.ac.uk/" rel="nofollow">BEAST</a>, <a href="http://mrbayes.csit.fsu.edu/" class="external text" title="http://mrbayes.csit.fsu.edu/" rel="nofollow">MrBayes</a> and <a href="http://www.bmnh.org/~pf/p4.html" class="external text" title="http://www.bmnh.org/~pf/p4.html" rel="nofollow">P4</a> allow many demographic and evolutionary parameters to be estimated simultaneously.</p>
<p>As applied to <a href="/wiki/Statistical_classification" title="Statistical classification">statistical classification</a>, Bayesian inference has been used in recent years to develop algorithms for identifying unsolicited bulk <a href="/wiki/E-mail_spam" title="E-mail spam">e-mail spam</a>. Applications which make use of Bayesian inference for spam filtering include <a href="/wiki/DSPAM" title="DSPAM">DSPAM</a>, <a href="/wiki/Bogofilter" title="Bogofilter">Bogofilter</a>, <a href="/wiki/SpamAssassin" title="SpamAssassin">SpamAssassin</a>, <a href="/wiki/SpamBayes" title="SpamBayes">SpamBayes</a>, and <a href="/wiki/Mozilla" title="Mozilla">Mozilla</a>. Spam classification is treated in more detail in the article on the <b><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">naive Bayes classifier</a></b>.</p>
<p>In some applications <a href="/wiki/Fuzzy_logic" title="Fuzzy logic">fuzzy logic</a> is an alternative to Bayesian inference. Fuzzy logic and Bayesian inference, however, are mathematically and semantically not compatible. You cannot, in general, understand the <i>degree of truth</i> in fuzzy logic as probability and vice versa; fuzziness measures "the degree to which an event occurs, not whether it occurs"<sup id="cite_ref-1" class="reference"><a href="#cite_note-1" title=""><span>[</span>2<span>]</span></a></sup>.</p>
<p><a name="See_also" id="See_also"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=11" title="Edit section: See also">edit</a>]</span> <span class="mw-headline">See also</span></h2>
<div class="noprint tright portal" style="border:solid #aaa 1px;margin:0.5em 0 0.5em 0.5em;">
<table style="background:#f9f9f9; font-size:85%; line-height:110%;">
<tr>
<td><a href="/wiki/File:Fisher_iris_versicolor_sepalwidth.svg" class="image" title="Fisher iris versicolor sepalwidth.svg"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/42px-Fisher_iris_versicolor_sepalwidth.svg.png" width="42" height="28" border="0" /></a></td>
<td style="padding:0 0.2em;"><i><b><a href="/wiki/Portal:Statistics" title="Portal:Statistics">Statistics portal</a></b></i></td>
</tr>
</table>
</div>
<div style="-moz-column-count:3; column-count:3;">
<ul>
<li><a href="/wiki/Bayesian_model_comparison" title="Bayesian model comparison">Bayesian model comparison</a></li>
<li><a href="/wiki/Bayesian_probability" title="Bayesian probability">Bayesian probability</a></li>
<li><a href="/wiki/Bayes_theorem" title="Bayes theorem" class="mw-redirect">Bayes theorem</a></li>
<li><a href="/wiki/Bayesian_estimation" title="Bayesian estimation" class="mw-redirect">Bayesian estimation</a></li>
<li><a href="/wiki/Bayesian_filtering" title="Bayesian filtering">Bayesian filtering</a></li>
<li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian network</a></li>
<li><a href="/wiki/Bayes_factor" title="Bayes factor">Bayes factor</a></li>
<li><a href="/wiki/Hierarchical_Bayes_model" title="Hierarchical Bayes model">Hierarchical Bayes model</a></li>
<li><a href="/wiki/Inferential_statistics" title="Inferential statistics" class="mw-redirect">Inferential statistics</a></li>
<li><a href="/wiki/Influence_diagram" title="Influence diagram">Influence diagram</a></li>
<li><a href="/wiki/Information_theory" title="Information theory">Information theory</a></li>
<li><a href="/wiki/Occam%27s_Razor" title="Occam's Razor" class="mw-redirect">Occam's Razor</a></li>
<li><a href="/wiki/Cromwell%27s_rule" title="Cromwell's rule">Cromwell's rule</a></li>
<li><a href="/wiki/Prosecutor%27s_fallacy" title="Prosecutor's fallacy">Prosecutor's fallacy</a></li>
<li><a href="/wiki/Minimum_message_length" title="Minimum message length">Minimum message length</a></li>
<li><a href="/wiki/Minimum_description_length" title="Minimum description length">Minimum description length</a></li>
<li><a href="/wiki/Gaussian_process_regression" title="Gaussian process regression" class="mw-redirect">Gaussian process regression</a></li>
<li><a href="/wiki/Maximum_entropy_thermodynamics" title="Maximum entropy thermodynamics">Maximum entropy thermodynamics</a></li>
<li><a href="/wiki/List_of_publications_in_statistics#Bayesian_statistics" title="List of publications in statistics" class="mw-redirect">Important publications in Bayesian statistics</a></li>
<li><a href="/wiki/The_Wisdom_of_Crowds" title="The Wisdom of Crowds">The Wisdom of Crowds</a></li>
<li><a href="/wiki/Raven_paradox" title="Raven paradox">Raven paradox</a></li>
</ul>
</div>
<p><a name="Notes" id="Notes"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=12" title="Edit section: Notes">edit</a>]</span> <span class="mw-headline">Notes</span></h2>
<div class="references-small">
<ol class="references">
<li id="cite_note-0"><b><a href="#cite_ref-0" title="">^</a></b> Douglas Hubbard "How to Measure Anything: Finding the Value of Intangibles in Business" pg. 46, John Wiley &amp; Sons, 2007</li>
<li id="cite_note-1"><b><a href="#cite_ref-1" title="">^</a></b> <cite style="font-style:normal" class="book" id="CITEREFKosko1992"><a href="/wiki/Bart_Kosko" title="Bart Kosko">Kosko, Bart</a> (1992). <i>Neural Networks and Fuzzy Systems: a dynamical systems approach to machine intelligence</i> (1st ed.). Englewood Cliffs, NJ, USA.: Prentice Hall. p.&#160;265. <a href="/wiki/Special:BookSources/9780136114352" class="internal">ISBN 9780136114352</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neural+Networks+and+Fuzzy+Systems%3A+a+dynamical+systems+approach+to+machine+intelligence&amp;rft.aulast=Kosko&amp;rft.aufirst=Bart&amp;rft.au=Kosko%2C+Bart&amp;rft.date=1992&amp;rft.pages=p.%26nbsp%3B265&amp;rft.edition=1st&amp;rft.place=Englewood+Cliffs%2C+NJ%2C+USA.&amp;rft.pub=Prentice+Hall&amp;rft.isbn=9780136114352&amp;rfr_id=info:sid/en.wikipedia.org:Bayesian_inference"><span style="display: none;">&#160;</span></span></li>
</ol>
</div>
<p><a name="References" id="References"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=13" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2>
<ul>
<li><a href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html" class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html" rel="nofollow">On-line textbook: Information Theory, Inference, and Learning Algorithms</a>, by David MacKay, has chapters on Bayesian methods, including examples; arguments in favour of Bayesian methods (in the style of <a href="/wiki/Edwin_Thompson_Jaynes" title="Edwin Thompson Jaynes">Edwin Jaynes</a>); modern <a href="/wiki/Monte_Carlo_method" title="Monte Carlo method">Monte Carlo methods</a>, <a href="/wiki/Message-passing_method" title="Message-passing method">message-passing methods</a>, and <a href="/wiki/Calculus_of_variations" title="Calculus of variations">variational methods</a>; and examples illustrating the connections between Bayesian inference and <a href="/wiki/Data_compression" title="Data compression">data compression</a>.</li>
<li>Berger, J.O. (1999) Statistical Decision Theory and Bayesian Analysis. Second Edition. Springer Verlag, New York. <a href="/wiki/Special:BookSources/0387960988" class="internal">ISBN 0-387-96098-8</a> and also <a href="/wiki/Special:BookSources/3540960988" class="internal">ISBN 3-540-96098-8</a>.</li>
<li>Bolstad, William M. (2004) Introduction to Bayesian Statistics, John Wiley <a href="/wiki/Special:BookSources/0471270202" class="internal">ISBN 0-471-27020-2</a></li>
<li>Bretthorst, G. Larry, 1988, <a href="http://bayes.wustl.edu/glb/book.pdf" class="external text" title="http://bayes.wustl.edu/glb/book.pdf" rel="nofollow"><i>Bayesian Spectrum Analysis and Parameter Estimation</i></a> in Lecture Notes in Statistics, 48, Springer-Verlag, New York, New York</li>
<li>Carlin, B.P. and Louis, T.A. (2008) Bayesian Methods for Data Analysis, Third Edition. Chapman &amp; Hall/CRC, Boca Raton, Florida. <a href="http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C6978" class="external autonumber" title="http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C6978" rel="nofollow">[1]</a> <a href="/wiki/Special:BookSources/1584886978" class="internal">ISBN 1-58488-697-8</a>.</li>
<li>Dawid, A.P. and Mortera, J. (1996) Coherent analysis of forensic identification evidence. <a href="/wiki/Journal_of_the_Royal_Statistical_Society" title="Journal of the Royal Statistical Society">Journal of the Royal Statistical Society</a>, Series B, 58,425-443.</li>
<li>Foreman, L.A; Smith, A.F.M. and Evett, I.W. (1997). Bayesian analysis of deoxyribonucleic acid profiling data in forensic identification applications (with discussion). Journal of the Royal Statistical Society, Series A, 160, 429-469.</li>
<li>Gardner-Medwin, A. <i>What probability should the jury address?</i>. Significance. Volume 2, Issue 1, March 2005</li>
<li>Gelman, A., Carlin, J., Stern, H., and Rubin, D.B. (2003). Bayesian Data Analysis. Second Edition. Chapman &amp; Hall/CRC, Boca Raton, Florida. <a href="http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C388X" class="external autonumber" title="http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C388X" rel="nofollow">[2]</a> <a href="/wiki/Special:BookSources/158488388X" class="internal">ISBN 1-58488-388-X</a>.</li>
<li>Gelman, A. and Meng, X.L. (2004). Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives: an essential journey with Donald Rubin's statistical family. John Wiley &amp; Sons, Chichester, UK. <a href="/wiki/Special:BookSources/047009043X" class="internal">ISBN 0-470-09043-X</a></li>
<li>Giffin, A. and Caticha, A. (2007) <a href="http://arxiv.org/abs/0708.1593" class="external text" title="http://arxiv.org/abs/0708.1593" rel="nofollow"><i>Updating Probabilities with Data and Moments</i></a></li>
<li>Jaynes, E.T. (1998) <a href="http://www-biba.inrialpes.fr/Jaynes/prob.html" class="external text" title="http://www-biba.inrialpes.fr/Jaynes/prob.html" rel="nofollow"><i>Probability Theory: The Logic of Science</i></a>.</li>
<li>Lee, Peter M. Bayesian Statistics: An Introduction. Second Edition. (1997). <a href="/wiki/Special:BookSources/0340677856" class="internal">ISBN 0-340-67785-6</a>.</li>
<li>Loredo, Thomas J. (1992) "Promise of Bayesian Inference in Astrophysics" in <i>Statistical Challenges in Modern Astronomy</i>, ed. Feigelson &amp; Babu.</li>
<li>O'Hagan, A. and Forster, J. (2003) Kendall's Advanced Theory of Statistics, Volume 2B: Bayesian Inference. Arnold, New York. <a href="/wiki/Special:BookSources/0340529229" class="internal">ISBN 0-340-52922-9</a>.</li>
<li>Pearl, J. (1988) <i>Probabilistic Reasoning in Intelligent Systems,</i> San Mateo, CA: Morgan Kaufmann.</li>
<li>Pole, Andy, West, Mike and Harrison, P. Jeff. Applied Bayesian Forecasting and Time Series Analysis, Chapman-Hall/Taylor Francis, 1994</li>
<li>Robert, C.P. (2001) The Bayesian Choice. Springer Verlag, New York.</li>
<li>Robertson, B. and Vignaux, G.A. (1995) Interpreting Evidence: Evaluating Forensic Science in the Courtroom. John Wiley and Sons. Chichester.</li>
<li>West, Mike, and Harrison, P. Jeff, Bayesian Forecasting and Dynamic Models, Springer-Verlag, 1997 (2nd ed.)</li>
<li>Winkler, Robert L, <i>Introduction to Bayesian Inference and Decision, 2nd Edition</i> (2003) Probabilistic. <a href="/wiki/Special:BookSources/0964793849" class="internal">ISBN 0-9647938-4-9</a></li>
</ul>
<p><a name="External_links" id="External_links"></a></p>
<h2><span class="editsection">[<a href="/w/index.php?title=Bayesian_inference&amp;action=edit&amp;section=14" title="Edit section: External links">edit</a>]</span> <span class="mw-headline">External links</span></h2>
<ul>
<li><a href="http://www.dcs.qmw.ac.uk/%7Enorman/BBNs/BBNs.htm" class="external text" title="http://www.dcs.qmw.ac.uk/%7Enorman/BBNs/BBNs.htm" rel="nofollow">A nice on-line introductory tutorial to Bayesian probability</a>from Queen Mary University of London</li>
<li><a href="http://yudkowsky.net/bayes/bayes.html" class="external text" title="http://yudkowsky.net/bayes/bayes.html" rel="nofollow">An Intuitive Explanation of Bayesian Reasoning</a> Bayes' Theorem for the curious and bewildered; an excruciatingly gentle introduction by <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li>
<li>Paul Graham. <a href="http://www.paulgraham.com/spam.html" class="external text" title="http://www.paulgraham.com/spam.html" rel="nofollow">"A Plan for Spam"</a> <i>(exposition of a popular approach for spam classification)</i></li>
<li><a href="http://www.mcs.vuw.ac.nz/~vignaux/docs/Adams_NLJ.html" class="external text" title="http://www.mcs.vuw.ac.nz/~vignaux/docs/Adams_NLJ.html" rel="nofollow">Commentary on Regina versus Adams</a></li>
<li><a href="http://webuser.bus.umich.edu/plenk/downloads.htm" class="external text" title="http://webuser.bus.umich.edu/plenk/downloads.htm" rel="nofollow">Mathematical notes on Bayesian statistics and Markov chain Monte Carlo</a></li>
<li><a href="http://www.thebroth.com/blog/118/bayesian-rating" class="external text" title="http://www.thebroth.com/blog/118/bayesian-rating" rel="nofollow">Bayesian Rating/Ranking</a> How to implement Bayes' Theorem for online rating and ranking systems</li>
<li><a href="http://cocosci.berkeley.edu/tom/bayes.html" class="external text" title="http://cocosci.berkeley.edu/tom/bayes.html" rel="nofollow">Bayesian reading list</a>, categorized and annotated. Designed for cognitive science; maintained by <a href="http://psychology.berkeley.edu/faculty/profiles/tgriffiths.html" class="external text" title="http://psychology.berkeley.edu/faculty/profiles/tgriffiths.html" rel="nofollow">Tom Griffiths</a>.</li>
<li><a href="http://www.eucognition.org/wiki/index.php?title=Bayesian_Multisensory_Perception" class="external autonumber" title="http://www.eucognition.org/wiki/index.php?title=Bayesian_Multisensory_Perception" rel="nofollow">[3]</a> A short article on Baysian Multisensory Perception</li>
<li><a href="http://www.eucognition.org/wiki/index.php?title=Bayesian_Probabilistic_Learning_in_Robots" class="external autonumber" title="http://www.eucognition.org/wiki/index.php?title=Bayesian_Probabilistic_Learning_in_Robots" rel="nofollow">[4]</a> Bayesian probabilistic learning in robots</li>
<li><a href="http://plato.stanford.edu/entries/logic-inductive/" class="external text" title="http://plato.stanford.edu/entries/logic-inductive/" rel="nofollow">Stanford Encyclopedia of Philosophy: Inductive Logic</a> a comprehensive Bayesian treatment of Inductive Logic and Confirmation Theory</li>
<li><a href="http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne%20--%20Confirmation%20Theory.pdf" class="external text" title="http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne%20--%20Confirmation%20Theory.pdf" rel="nofollow">Confirmation Theory</a> An extensive presentation of Bayesian Confirmation Theory</li>
<li><a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-7.html" class="external text" title="http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-7.html" rel="nofollow">What is Bayesian Learning?</a></li>
</ul>
<table class="navbox" cellspacing="0" style=";">
<tr>
<td style="padding:2px;">
<table cellspacing="0" class="nowraplinks collapsible autocollapse" style="width:100%;background:transparent;color:inherit;;">
<tr>
<th style=";" colspan="2" class="navbox-title">
<div style="float:left; width:6em;text-align:left;">
<div class="noprint plainlinksneverexpand navbar" style="background:none; padding:0; font-weight:normal;;;border:none;; font-size:xx-small;"><a href="/wiki/Template:Statistics" title="Template:Statistics"><span title="View this template" style=";;border:none;">v</span></a>&#160;•&#160;<a href="/wiki/Template_talk:Statistics" title="Template talk:Statistics"><span title="Discussion about this template" style=";;border:none;">d</span></a>&#160;•&#160;<a href="http://en.wikipedia.org/w/index.php?title=Template:Statistics&amp;action=edit" class="external text" title="http://en.wikipedia.org/w/index.php?title=Template:Statistics&amp;action=edit" rel="nofollow"><span title="Edit this template" style=";;border:none;;">e</span></a></div>
</div>
<span style="font-size:110%;"><a href="/wiki/Statistics" title="Statistics">Statistics</a></span></th>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Design_of_experiments" title="Design of experiments">Design of experiments</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Statistical_population" title="Statistical population">Population</a>&#160;• <a href="/wiki/Sampling_(statistics)" title="Sampling (statistics)">Sampling</a>&#160;• <a href="/wiki/Stratified_sampling" title="Stratified sampling">Stratified sampling</a>&#160;• <a href="/wiki/Replication_(statistics)" title="Replication (statistics)">Replication</a>&#160;• <a href="/wiki/Blocking_(statistics)" title="Blocking (statistics)">Blocking</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Sample_size" title="Sample size">Sample size estimation</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Null_hypothesis" title="Null hypothesis">Null hypothesis</a>&#160;• <a href="/wiki/Alternative_hypothesis" title="Alternative hypothesis">Alternative hypothesis</a>&#160;• <a href="/wiki/Type_I_and_Type_II_errors" title="Type I and Type II errors" class="mw-redirect">Type I and Type II errors</a>&#160;• <a href="/wiki/Statistical_power" title="Statistical power">Statistical power</a>&#160;• <a href="/wiki/Effect_size" title="Effect size">Effect size</a>&#160;• <a href="/wiki/Standard_error_(statistics)" title="Standard error (statistics)">Standard error</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Descriptive_statistics" title="Descriptive statistics">Descriptive statistics</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Continuous_probability_distribution" title="Continuous probability distribution">Continuous data</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"></div>
<table cellspacing="0" class="nowraplinks navbox-subgroup" style="width:100%;;;;">
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Location_parameter" title="Location parameter">Location</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Mean" title="Mean">Mean</a> (<a href="/wiki/Arithmetic_mean" title="Arithmetic mean">Arithmetic</a>, <a href="/wiki/Geometric_mean" title="Geometric mean">Geometric</a>, <a href="/wiki/Harmonic_mean" title="Harmonic mean">Harmonic</a>)&#160;• <a href="/wiki/Median" title="Median">Median</a>&#160;• <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">Mode</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Statistical_dispersion" title="Statistical dispersion">Dispersion</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Range_(statistics)" title="Range (statistics)">Range</a>&#160;• <a href="/wiki/Standard_deviation" title="Standard deviation">Standard deviation</a>&#160;• <a href="/wiki/Coefficient_of_variation" title="Coefficient of variation">Coefficient of variation</a>&#160;• <a href="/wiki/Percentile" title="Percentile">Percentile</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Moment_(mathematics)" title="Moment (mathematics)">Moments</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Variance" title="Variance">Variance</a>&#160;• <a href="/wiki/Semivariance" title="Semivariance">Semivariance</a>&#160;• <a href="/wiki/Skewness" title="Skewness">Skewness</a>&#160;• <a href="/wiki/Kurtosis" title="Kurtosis">Kurtosis</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";padding-left:0em;padding-right:0em;;">
<div style="padding:0em 0.75em;"><a href="/wiki/Discrete_probability_distribution" title="Discrete probability distribution">Categorical data</a></div>
</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Frequency_(statistics)" title="Frequency (statistics)">Frequency</a>&#160;• <a href="/wiki/Contingency_table" title="Contingency table">Contingency table</a></div>
</td>
</tr>
</table>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Statistical_inference" title="Statistical inference">Inferential statistics</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><strong class="selflink">Bayesian inference</strong>&#160;• <a href="/wiki/Frequentist_inference" title="Frequentist inference" class="mw-redirect">Frequentist inference</a>&#160;• <a href="/wiki/Statistical_hypothesis_testing" title="Statistical hypothesis testing">Hypothesis testing</a>&#160;• <a href="/wiki/Statistical_significance" title="Statistical significance">Significance</a>&#160;• <a href="/wiki/P-value" title="P-value">P-value</a>&#160;• <a href="/wiki/Interval_estimation" title="Interval estimation">Interval estimation</a>&#160;• <a href="/wiki/Confidence_interval" title="Confidence interval">Confidence interval</a>&#160;• <a href="/wiki/Meta-analysis" title="Meta-analysis">Meta-analysis</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">General estimation</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Bayesian_estimator" title="Bayesian estimator" class="mw-redirect">Bayesian estimator</a>&#160;• <a href="/wiki/Maximum_likelihood" title="Maximum likelihood">Maximum likelihood</a>&#160;• <a href="/wiki/Method_of_moments_(statistics)" title="Method of moments (statistics)">Method of moments</a>&#160;• <a href="/wiki/Minimum_distance_estimation" title="Minimum distance estimation">Minimum distance</a>&#160;• <a href="/wiki/Maximum_spacing_estimation" title="Maximum spacing estimation">Maximum spacing</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Specific tests</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Z-test" title="Z-test">Z-test(normal)</a>&#160;• <a href="/wiki/Student%27s_t-test" title="Student's t-test">Student's t-test</a>&#160;• <a href="/wiki/Chi-square_test" title="Chi-square test">Chi-square test</a>&#160;• <a href="/wiki/F-test" title="F-test">F-test</a>&#160;• <a href="/wiki/Sensitivity_and_specificity" title="Sensitivity and specificity">Sensitivity and specificity</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Survival_analysis" title="Survival analysis">Survival analysis</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Survival_function" title="Survival function">Survival function</a>&#160;• <a href="/wiki/Kaplan-Meier_estimator" title="Kaplan-Meier estimator">Kaplan-Meier</a>&#160;• <a href="/wiki/Logrank_test" title="Logrank test">Logrank test</a>&#160;• <a href="/wiki/Failure_rate" title="Failure rate">Failure rate</a>&#160;• <a href="/wiki/Proportional_hazards_models" title="Proportional hazards models">Proportional hazards models</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Correlation" title="Correlation">Correlation</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Pearson_product-moment_correlation_coefficient" title="Pearson product-moment correlation coefficient">Pearson product-moment correlation coefficient</a>&#160;• <a href="/wiki/Rank_correlation" title="Rank correlation">Rank correlation</a> (<a href="/wiki/Spearman%27s_rank_correlation_coefficient" title="Spearman's rank correlation coefficient">Spearman's rho</a>, <a href="/wiki/Kendall_tau_rank_correlation_coefficient" title="Kendall tau rank correlation coefficient">Kendall's tau</a>)&#160;• <a href="/wiki/Confounding_variable" title="Confounding variable" class="mw-redirect">Confounding variable</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Linear_model" title="Linear model">Linear models</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/General_linear_model" title="General linear model">General linear model</a>&#160;• <a href="/wiki/Generalized_linear_model" title="Generalized linear model">Generalized linear model</a>&#160;• <a href="/wiki/Analysis_of_variance" title="Analysis of variance">Analysis of variance</a>&#160;• <a href="/wiki/Analysis_of_covariance" title="Analysis of covariance">Analysis of covariance</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Regression_analysis" title="Regression analysis">Regression analysis</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a>&#160;• <a href="/wiki/Nonlinear_regression" title="Nonlinear regression">Nonlinear regression</a>&#160;• <a href="/wiki/Nonparametric_regression" title="Nonparametric regression">Nonparametric regression</a>&#160;• <a href="/wiki/Semiparametric_regression" title="Semiparametric regression">Semiparametric regression</a>&#160;• <a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/Statistical_graphics" title="Statistical graphics">Statistical graphics</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/Bar_chart" title="Bar chart">Bar chart</a>&#160;• <a href="/wiki/Biplot" title="Biplot">Biplot</a>&#160;• <a href="/wiki/Box_plot" title="Box plot">Box plot</a>&#160;• <a href="/wiki/Control_chart" title="Control chart">Control chart</a>&#160;• <a href="/wiki/Forest_plot" title="Forest plot">Forest plot</a>&#160;• <a href="/wiki/Histogram" title="Histogram">Histogram</a>&#160;• <a href="/wiki/Q-Q_plot" title="Q-Q plot">Q-Q plot</a>&#160;• <a href="/wiki/Run_chart" title="Run chart">Run chart</a>&#160;• <a href="/wiki/Scatter_plot" title="Scatter plot">Scatter plot</a>&#160;• <a href="/wiki/Stemplot" title="Stemplot">Stemplot</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;"><a href="/wiki/History_of_statistics" title="History of statistics">History</a></td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<div style="padding:0em 0.25em"><a href="/wiki/History_of_statistics" title="History of statistics">History of statistics</a>&#160;• <a href="/wiki/Founders_of_statistics" title="Founders of statistics">Founders of statistics</a>&#160;• <a href="/wiki/Timeline_of_probability_and_statistics" title="Timeline of probability and statistics">Timeline of probability and statistics</a></div>
</td>
</tr>
<tr style="height:2px">
<td></td>
</tr>
<tr>
<td class="navbox-group" style=";;">Publications</td>
<td style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<div style="padding:0em 0.25em"><a href="/wiki/List_of_scientific_journals_in_statistics" title="List of scientific journals in statistics">Journals in statistics</a>&#160;• <a href="/wiki/List_of_important_publications_in_statistics" title="List of important publications in statistics">Important publications</a></div>
</td>
</tr>
<tr style="height:2px;">
<td></td>
</tr>
<tr>
<td class="navbox-abovebelow" style=";" colspan="2"><b><a href="/wiki/Category:Statistics" title="Category:Statistics">Category</a></b>&#160;• <b><a href="/wiki/Portal:Statistics" title="Portal:Statistics">Portal</a></b>&#160;• <b><a href="/wiki/Topic_outline_of_statistics" title="Topic outline of statistics">Topic outline</a></b>&#160;• <b><a href="/wiki/List_of_statistics_topics" title="List of statistics topics">List of topics</a></b></td>
</tr>
</table>
</td>
</tr>
</table>


<!-- 
NewPP limit report
Preprocessor node count: 2474/1000000
Post-expand include size: 54789/2048000 bytes
Template argument size: 26524/2048000 bytes
Expensive parser function count: 1/500
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:49571-0!1!0!default!!en!2 and timestamp 20090331114525 -->
<div class="printfooter">
Retrieved from "<a href="http://en.wikipedia.org/wiki/Bayesian_inference">http://en.wikipedia.org/wiki/Bayesian_inference</a>"</div>
			<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>:&#32;<span dir='ltr'><a href="/wiki/Category:Bayesian_statistics" title="Category:Bayesian statistics">Bayesian statistics</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_theory" title="Category:Statistical theory">Statistical theory</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistical_inference" title="Category:Statistical inference">Statistical inference</a></span></div><div id="mw-hidden-catlinks" class="mw-hidden-cats-hidden">Hidden categories:&#32;<span dir='ltr'><a href="/wiki/Category:Articles_lacking_in-text_citations" title="Category:Articles lacking in-text citations">Articles lacking in-text citations</a></span> | <span dir='ltr'><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></span> | <span dir='ltr'><a href="/wiki/Category:Articles_with_unsourced_statements_since_March_2008" title="Category:Articles with unsourced statements since March 2008">Articles with unsourced statements since March 2008</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistics_articles_linked_to_the_portal" title="Category:Statistics articles linked to the portal">Statistics articles linked to the portal</a></span> | <span dir='ltr'><a href="/wiki/Category:Statistics_articles_with_navigational_template" title="Category:Statistics articles with navigational template">Statistics articles with navigational template</a></span></div></div>			<!-- end content -->
						<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Bayesian_inference" title="View the content page [c]" accesskey="c">Article</a></li>
				 <li id="ca-talk"><a href="/wiki/Talk:Bayesian_inference" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="/w/index.php?title=Bayesian_inference&amp;action=edit" title="You can edit this page. &#10;Please use the preview button before saving. [e]" accesskey="e">Edit this page</a></li>
				 <li id="ca-history"><a href="/w/index.php?title=Bayesian_inference&amp;action=history" title="Past versions of this page [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Bayesian_inference" title="You are encouraged to log in; however, it is not mandatory. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(http://upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png);" href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
				<li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content — the best of Wikipedia">Featured content</a></li>
				<li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
				<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/w/index.php" id="searchform"><div>
				<input type='hidden' name="title" value="Special:Search"/>
				<input id="searchInput" name="search" type="text" title="Search Wikipedia [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if one exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search Wikipedia for this text" />
			</div></form>
		</div>
	</div>
	<div class='generated-sidebar portlet' id='p-interaction'>
		<h5>Interaction</h5>
		<div class='pBody'>
			<ul>
				<li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
				<li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-contact"><a href="/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a href="http://wikimediafoundation.org/wiki/Donate" title="Support us">Donate to Wikipedia</a></li>
				<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Bayesian_inference" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Bayesian_inference" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-upload"><a href="/wiki/Wikipedia:Upload" title="Upload files [u]" accesskey="u">Upload file</a></li>
<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="/w/index.php?title=Bayesian_inference&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="/w/index.php?title=Bayesian_inference&amp;oldid=280843606" title="Permanent link to this version of the page">Permanent link</a></li><li id="t-cite"><a href="/w/index.php?title=Special:Cite&amp;page=Bayesian_inference&amp;id=280843606">Cite this page</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>Languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-cy"><a href="http://cy.wikipedia.org/wiki/Anwythiad_Bayesaidd">Cymraeg</a></li>
				<li class="interwiki-de"><a href="http://de.wikipedia.org/wiki/Bayessche_Statistik">Deutsch</a></li>
				<li class="interwiki-es"><a href="http://es.wikipedia.org/wiki/Inferencia_bayesiana">Español</a></li>
				<li class="interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Inf%C3%A9rence_bay%C3%A9sienne">Français</a></li>
				<li class="interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%B6%94%EB%A1%A0">한국어</a></li>
				<li class="interwiki-it"><a href="http://it.wikipedia.org/wiki/Inferenza_bayesiana">Italiano</a></li>
				<li class="interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Bayesiaanse_kansrekening">Nederlands</a></li>
				<li class="interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E5%AE%9A">日本語</a></li>
				<li class="interwiki-pt"><a href="http://pt.wikipedia.org/wiki/Infer%C3%AAncia_bayesiana">Português</a></li>
				<li class="interwiki-fi"><a href="http://fi.wikipedia.org/wiki/Bayesilainen_tilastotiede">Suomi</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
				<div id="f-copyrightico"><a href="http://wikimediafoundation.org/"><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
					<li id="lastmod"> This page was last modified on 31 March 2009, at 11:39.</li>
					<li id="copyright">All text is available under the terms of the <a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal' href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the <a href="http://www.wikimediafoundation.org">Wikimedia Foundation, Inc.</a>, a U.S. registered <a class='internal' href="http://en.wikipedia.org/wiki/501%28c%29#501.28c.29.283.29" title="501(c)(3)">501(c)(3)</a> <a href="http://wikimediafoundation.org/wiki/Deductibility_of_donations">tax-deductible</a> <a class='internal' href="http://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">nonprofit</a> <a href="http://en.wikipedia.org/wiki/Charitable_organization" title="Charitable organization">charity</a>.<br /></li>
					<li id="privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
					<li id="about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
					<li id="disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served by srv158 in 0.227 secs. --></body></html>
